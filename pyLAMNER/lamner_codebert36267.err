Some weights of the model checkpoint at huggingface/CodeBERTa-small-v1 were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/2577 [00:00<?, ?it/s]  0%|          | 1/2577 [02:00<86:06:36, 120.34s/it]  0%|          | 2/2577 [04:01<86:14:34, 120.57s/it]  0%|          | 3/2577 [06:00<85:45:19, 119.94s/it]  0%|          | 4/2577 [07:52<83:26:02, 116.74s/it]  0%|          | 5/2577 [09:47<82:56:13, 116.09s/it]  0%|          | 6/2577 [11:43<82:53:31, 116.07s/it]  0%|          | 7/2577 [13:37<82:29:48, 115.56s/it]  0%|          | 8/2577 [15:36<83:20:36, 116.79s/it]  0%|          | 9/2577 [17:32<83:01:09, 116.38s/it]  0%|          | 10/2577 [19:29<83:05:10, 116.52s/it]  0%|          | 11/2577 [20:55<76:22:38, 107.15s/it]  0%|          | 12/2577 [22:08<69:06:05, 96.98s/it]   1%|          | 13/2577 [23:29<65:30:30, 91.98s/it]  1%|          | 14/2577 [24:53<63:49:03, 89.64s/it]  1%|          | 15/2577 [25:53<57:23:07, 80.64s/it]  1%|          | 16/2577 [27:23<59:21:13, 83.43s/it]  1%|          | 17/2577 [28:48<59:42:45, 83.97s/it]  1%|          | 18/2577 [30:13<59:49:03, 84.15s/it]  1%|          | 19/2577 