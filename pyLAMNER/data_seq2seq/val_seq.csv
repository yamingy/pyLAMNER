code,summary
"def learn ( env , network , seed = None , lr = 5e-4 , total_timesteps = 100000 , buffer_size = 50000 , exploration_fraction = 0.1 , exploration_final_eps = 0.02 , train_freq = 1 , batch_size = 32 , print_freq = 100 , checkpoint_freq = 10000 , checkpoint_path = None , learning_starts = 1000 , gamma = 1.0 , target_network_update_freq = 500 , prioritized_replay = False , prioritized_replay_alpha = 0.6 , prioritized_replay_beta0 = 0.4 , prioritized_replay_beta_iters = None , prioritized_replay_eps = 1e-6 , param_noise = False , callback = None , load_path = None , * * network_kwargs ) : sess = get_session ( ) set_global_seeds ( seed ) q_func = build_q_func ( network , * * network_kwargs ) observation_space = env . observation_space def make_obs_ph ( name ) : return ObservationInput ( observation_space , name = name ) act , train , update_target , debug = deepq . build_train ( make_obs_ph = make_obs_ph , q_func = q_func , num_actions = env . action_space . n , optimizer = tf . train . AdamOptimizer ( learning_rate = lr ) , gamma = gamma , grad_norm_clipping = 10 , param_noise = param_noise ) act_params = { 'make_obs_ph' : make_obs_ph , 'q_func' : q_func , 'num_actions' : env . action_space . n , } act = ActWrapper ( act , act_params ) if prioritized_replay : replay_buffer = PrioritizedReplayBuffer ( buffer_size , alpha = prioritized_replay_alpha ) if prioritized_replay_beta_iters is None : prioritized_replay_beta_iters = total_timesteps beta_schedule = LinearSchedule ( prioritized_replay_beta_iters , initial_p = prioritized_replay_beta0 , final_p = 1.0 ) else : replay_buffer = ReplayBuffer ( buffer_size ) beta_schedule = None exploration = LinearSchedule ( schedule_timesteps = int ( exploration_fraction * total_timesteps ) , initial_p = 1.0 , final_p = exploration_final_eps ) U . initialize ( ) update_target ( ) episode_rewards = [ 0.0 ] saved_mean_reward = None obs = env . reset ( ) reset = True with tempfile . TemporaryDirectory ( ) as td : td = checkpoint_path or td model_file = os . path . join ( td , ""model"" ) model_saved = False if tf . train . latest_checkpoint ( td ) is not None : load_variables ( model_file ) logger . log ( 'Loaded model from {}' . format ( model_file ) ) model_saved = True elif load_path is not None : load_variables ( load_path ) logger . log ( 'Loaded model from {}' . format ( load_path ) ) for t in range ( total_timesteps ) : if callback is not None : if callback ( locals ( ) , globals ( ) ) : break kwargs = { } if not param_noise : update_eps = exploration . value ( t ) update_param_noise_threshold = 0. else : update_eps = 0. update_param_noise_threshold = - np . log ( 1. - exploration . value ( t ) + exploration . value ( t ) / float ( env . action_space . n ) ) kwargs [ 'reset' ] = reset kwargs [ 'update_param_noise_threshold' ] = update_param_noise_threshold kwargs [ 'update_param_noise_scale' ] = True action = act ( np . array ( obs ) [ None ] , update_eps = update_eps , * * kwargs ) [ 0 ] env_action = action reset = False new_obs , rew , done , _ = env . step ( env_action ) replay_buffer . add ( obs , action , rew , new_obs , float ( done ) ) obs = new_obs episode_rewards [ - 1 ] += rew if done : obs = env . reset ( ) episode_rewards . append ( 0.0 ) reset = True if t > learning_starts and t % train_freq == 0 : if prioritized_replay : experience = replay_buffer . sample ( batch_size , beta = beta_schedule . value ( t ) ) ( obses_t , actions , rewards , obses_tp1 , dones , weights , batch_idxes ) = experience else : obses_t , actions , rewards , obses_tp1 , dones = replay_buffer . sample ( batch_size ) weights , batch_idxes = np . ones_like ( rewards ) , None td_errors = train ( obses_t , actions , rewards , obses_tp1 , dones , weights ) if prioritized_replay : new_priorities = np . abs ( td_errors ) + prioritized_replay_eps replay_buffer . update_priorities ( batch_idxes , new_priorities ) if t > learning_starts and t % target_network_update_freq == 0 : update_target ( ) mean_100ep_reward = round ( np . mean ( episode_rewards [ - 101 : - 1 ] ) , 1 ) num_episodes = len ( episode_rewards ) if done and print_freq is not None and len ( episode_rewards ) % print_freq == 0 : logger . record_tabular ( ""steps"" , t ) logger . record_tabular ( ""episodes"" , num_episodes ) logger . record_tabular ( ""mean 100 episode reward"" , mean_100ep_reward ) logger . record_tabular ( ""% time spent exploring"" , int ( 100 * exploration . value ( t ) ) ) logger . dump_tabular ( ) if ( checkpoint_freq is not None and t > learning_starts and num_episodes > 100 and t % checkpoint_freq == 0 ) : if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward : if print_freq is not None : logger . log ( ""Saving model due to mean reward increase: {} -> {}"" . format ( saved_mean_reward , mean_100ep_reward ) ) save_variables ( model_file ) model_saved = True saved_mean_reward = mean_100ep_reward if model_saved : if print_freq is not None : logger . log ( ""Restored model with mean reward: {}"" . format ( saved_mean_reward ) ) load_variables ( model_file ) return act",Train a deepq model .
"def save_act ( self , path = None ) : if path is None : path = os . path . join ( logger . get_dir ( ) , ""model.pkl"" ) with tempfile . TemporaryDirectory ( ) as td : save_variables ( os . path . join ( td , ""model"" ) ) arc_name = os . path . join ( td , ""packed.zip"" ) with zipfile . ZipFile ( arc_name , 'w' ) as zipf : for root , dirs , files in os . walk ( td ) : for fname in files : file_path = os . path . join ( root , fname ) if file_path != arc_name : zipf . write ( file_path , os . path . relpath ( file_path , td ) ) with open ( arc_name , ""rb"" ) as f : model_data = f . read ( ) with open ( path , ""wb"" ) as f : cloudpickle . dump ( ( model_data , self . _act_params ) , f )",Save model to a pickle located at path
"def nature_cnn ( unscaled_images , * * conv_kwargs ) : scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. activ = tf . nn . relu h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , * * conv_kwargs ) ) h3 = conv_to_fc ( h3 ) return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) )",CNN from Nature paper .
"def mlp ( num_layers = 2 , num_hidden = 64 , activation = tf . tanh , layer_norm = False ) : def network_fn ( X ) : h = tf . layers . flatten ( X ) for i in range ( num_layers ) : h = fc ( h , 'mlp_fc{}' . format ( i ) , nh = num_hidden , init_scale = np . sqrt ( 2 ) ) if layer_norm : h = tf . contrib . layers . layer_norm ( h , center = True , scale = True ) h = activation ( h ) return h return network_fn",Stack of fully - connected layers to be used in a policy / q - function approximator
"def lstm ( nlstm = 128 , layer_norm = False ) : def network_fn ( X , nenv = 1 ) : nbatch = X . shape [ 0 ] nsteps = nbatch // nenv h = tf . layers . flatten ( X ) M = tf . placeholder ( tf . float32 , [ nbatch ] ) S = tf . placeholder ( tf . float32 , [ nenv , 2 * nlstm ] ) xs = batch_to_seq ( h , nenv , nsteps ) ms = batch_to_seq ( M , nenv , nsteps ) if layer_norm : h5 , snew = utils . lnlstm ( xs , ms , S , scope = 'lnlstm' , nh = nlstm ) else : h5 , snew = utils . lstm ( xs , ms , S , scope = 'lstm' , nh = nlstm ) h = seq_to_batch ( h5 ) initial_state = np . zeros ( S . shape . as_list ( ) , dtype = float ) return h , { 'S' : S , 'M' : M , 'state' : snew , 'initial_state' : initial_state } return network_fn",Builds LSTM ( Long - Short Term Memory ) network to be used in a policy . Note that the resulting function returns not only the output of the LSTM ( i . e . hidden state of lstm for each step in the sequence ) but also a dictionary with auxiliary tensors to be set as policy attributes .
"def conv_only ( convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , * * conv_kwargs ) : def network_fn ( X ) : out = tf . cast ( X , tf . float32 ) / 255. with tf . variable_scope ( ""convnet"" ) : for num_outputs , kernel_size , stride in convs : out = layers . convolution2d ( out , num_outputs = num_outputs , kernel_size = kernel_size , stride = stride , activation_fn = tf . nn . relu , * * conv_kwargs ) return out return network_fn",convolutions - only net
def get_network_builder ( name ) : if callable ( name ) : return name elif name in mapping : return mapping [ name ] else : raise ValueError ( 'Unknown network type: {}' . format ( name ) ),If you want to register your own network outside models . py you just need :
"def mlp ( hiddens = [ ] , layer_norm = False ) : return lambda * args , * * kwargs : _mlp ( hiddens , layer_norm = layer_norm , * args , * * kwargs )",This model takes as input an observation and returns values of all actions .
"def cnn_to_mlp ( convs , hiddens , dueling = False , layer_norm = False ) : return lambda * args , * * kwargs : _cnn_to_mlp ( convs , hiddens , dueling , layer_norm = layer_norm , * args , * * kwargs )",This model takes as input an observation and returns values of all actions .
"def make_vec_env ( env_id , env_type , num_env , seed , wrapper_kwargs = None , start_index = 0 , reward_scale = 1.0 , flatten_dict_observations = True , gamestate = None ) : wrapper_kwargs = wrapper_kwargs or { } mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 seed = seed + 10000 * mpi_rank if seed is not None else None logger_dir = logger . get_dir ( ) def make_thunk ( rank ) : return lambda : make_env ( env_id = env_id , env_type = env_type , mpi_rank = mpi_rank , subrank = rank , seed = seed , reward_scale = reward_scale , gamestate = gamestate , flatten_dict_observations = flatten_dict_observations , wrapper_kwargs = wrapper_kwargs , logger_dir = logger_dir ) set_global_seeds ( seed ) if num_env > 1 : return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) else : return DummyVecEnv ( [ make_thunk ( start_index ) ] )",Create a wrapped monitored SubprocVecEnv for Atari and MuJoCo .
"def make_mujoco_env ( env_id , seed , reward_scale = 1.0 ) : rank = MPI . COMM_WORLD . Get_rank ( ) myseed = seed + 1000 * rank if seed is not None else None set_global_seeds ( myseed ) env = gym . make ( env_id ) logger_path = None if logger . get_dir ( ) is None else os . path . join ( logger . get_dir ( ) , str ( rank ) ) env = Monitor ( env , logger_path , allow_early_resets = True ) env . seed ( seed ) if reward_scale != 1.0 : from baselines . common . retro_wrappers import RewardScaler env = RewardScaler ( env , reward_scale ) return env",Create a wrapped monitored gym . Env for MuJoCo .
"def make_robotics_env ( env_id , seed , rank = 0 ) : set_global_seeds ( seed ) env = gym . make ( env_id ) env = FlattenDictWrapper ( env , [ 'observation' , 'desired_goal' ] ) env = Monitor ( env , logger . get_dir ( ) and os . path . join ( logger . get_dir ( ) , str ( rank ) ) , info_keywords = ( 'is_success' , ) ) env . seed ( seed ) return env",Create a wrapped monitored gym . Env for MuJoCo .
"def common_arg_parser ( ) : parser = arg_parser ( ) parser . add_argument ( '--env' , help = 'environment ID' , type = str , default = 'Reacher-v2' ) parser . add_argument ( '--env_type' , help = 'type of environment, used when the environment type cannot be automatically determined' , type = str ) parser . add_argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add_argument ( '--alg' , help = 'Algorithm' , type = str , default = 'ppo2' ) parser . add_argument ( '--num_timesteps' , type = float , default = 1e6 ) , parser . add_argument ( '--network' , help = 'network type (mlp, cnn, lstm, cnn_lstm, conv_only)' , default = None ) parser . add_argument ( '--gamestate' , help = 'game state to load (so far only used in retro games)' , default = None ) parser . add_argument ( '--num_env' , help = 'Number of environment copies being run in parallel. When not specified, set to number of cpus for Atari, and to 1 for Mujoco' , default = None , type = int ) parser . add_argument ( '--reward_scale' , help = 'Reward scale factor. Default: 1.0' , default = 1.0 , type = float ) parser . add_argument ( '--save_path' , help = 'Path to save trained model to' , default = None , type = str ) parser . add_argument ( '--save_video_interval' , help = 'Save video every x steps (0 = disabled)' , default = 0 , type = int ) parser . add_argument ( '--save_video_length' , help = 'Length of recorded video. Default: 200' , default = 200 , type = int ) parser . add_argument ( '--play' , default = False , action = 'store_true' ) return parser",Create an argparse . ArgumentParser for run_mujoco . py .
"def robotics_arg_parser ( ) : parser = arg_parser ( ) parser . add_argument ( '--env' , help = 'environment ID' , type = str , default = 'FetchReach-v0' ) parser . add_argument ( '--seed' , help = 'RNG seed' , type = int , default = None ) parser . add_argument ( '--num-timesteps' , type = int , default = int ( 1e6 ) ) return parser",Create an argparse . ArgumentParser for run_mujoco . py .
def parse_unknown_args ( args ) : retval = { } preceded_by_key = False for arg in args : if arg . startswith ( '--' ) : if '=' in arg : key = arg . split ( '=' ) [ 0 ] [ 2 : ] value = arg . split ( '=' ) [ 1 ] retval [ key ] = value else : key = arg [ 2 : ] preceded_by_key = True elif preceded_by_key : retval [ key ] = arg preceded_by_key = False return retval,Parse arguments not consumed by arg parser into a dicitonary
"def clear_mpi_env_vars ( ) : removed_environment = { } for k , v in list ( os . environ . items ( ) ) : for prefix in [ 'OMPI_' , 'PMI_' ] : if k . startswith ( prefix ) : removed_environment [ k ] = v del os . environ [ k ] try : yield finally : os . environ . update ( removed_environment )",from mpi4py import MPI will call MPI_Init by default . If the child process has MPI environment variables MPI will think that the child process is an MPI process just like the parent and do bad things such as hang . This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing Processes .
"def learn ( * , network , env , total_timesteps , eval_env = None , seed = None , nsteps = 2048 , ent_coef = 0.0 , lr = 3e-4 , vf_coef = 0.5 , max_grad_norm = 0.5 , gamma = 0.99 , lam = 0.95 , log_interval = 10 , nminibatches = 4 , noptepochs = 4 , cliprange = 0.2 , save_interval = 0 , load_path = None , model_fn = None , * * network_kwargs ) : set_global_seeds ( seed ) if isinstance ( lr , float ) : lr = constfn ( lr ) else : assert callable ( lr ) if isinstance ( cliprange , float ) : cliprange = constfn ( cliprange ) else : assert callable ( cliprange ) total_timesteps = int ( total_timesteps ) policy = build_policy ( env , network , * * network_kwargs ) nenvs = env . num_envs ob_space = env . observation_space ac_space = env . action_space nbatch = nenvs * nsteps nbatch_train = nbatch // nminibatches if model_fn is None : from baselines . ppo2 . model import Model model_fn = Model model = model_fn ( policy = policy , ob_space = ob_space , ac_space = ac_space , nbatch_act = nenvs , nbatch_train = nbatch_train , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm ) if load_path is not None : model . load ( load_path ) runner = Runner ( env = env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) if eval_env is not None : eval_runner = Runner ( env = eval_env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) epinfobuf = deque ( maxlen = 100 ) if eval_env is not None : eval_epinfobuf = deque ( maxlen = 100 ) tfirststart = time . perf_counter ( ) nupdates = total_timesteps // nbatch for update in range ( 1 , nupdates + 1 ) : assert nbatch % nminibatches == 0 tstart = time . perf_counter ( ) frac = 1.0 - ( update - 1.0 ) / nupdates lrnow = lr ( frac ) cliprangenow = cliprange ( frac ) obs , returns , masks , actions , values , neglogpacs , states , epinfos = runner . run ( ) if eval_env is not None : eval_obs , eval_returns , eval_masks , eval_actions , eval_values , eval_neglogpacs , eval_states , eval_epinfos = eval_runner . run ( ) epinfobuf . extend ( epinfos ) if eval_env is not None : eval_epinfobuf . extend ( eval_epinfos ) mblossvals = [ ] if states is None : inds = np . arange ( nbatch ) for _ in range ( noptepochs ) : np . random . shuffle ( inds ) for start in range ( 0 , nbatch , nbatch_train ) : end = start + nbatch_train mbinds = inds [ start : end ] slices = ( arr [ mbinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) mblossvals . append ( model . train ( lrnow , cliprangenow , * slices ) ) else : assert nenvs % nminibatches == 0 envsperbatch = nenvs // nminibatches envinds = np . arange ( nenvs ) flatinds = np . arange ( nenvs * nsteps ) . reshape ( nenvs , nsteps ) for _ in range ( noptepochs ) : np . random . shuffle ( envinds ) for start in range ( 0 , nenvs , envsperbatch ) : end = start + envsperbatch mbenvinds = envinds [ start : end ] mbflatinds = flatinds [ mbenvinds ] . ravel ( ) slices = ( arr [ mbflatinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) mbstates = states [ mbenvinds ] mblossvals . append ( model . train ( lrnow , cliprangenow , * slices , mbstates ) ) lossvals = np . mean ( mblossvals , axis = 0 ) tnow = time . perf_counter ( ) fps = int ( nbatch / ( tnow - tstart ) ) if update % log_interval == 0 or update == 1 : ev = explained_variance ( values , returns ) logger . logkv ( ""serial_timesteps"" , update * nsteps ) logger . logkv ( ""nupdates"" , update ) logger . logkv ( ""total_timesteps"" , update * nbatch ) logger . logkv ( ""fps"" , fps ) logger . logkv ( ""explained_variance"" , float ( ev ) ) logger . logkv ( 'eprewmean' , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) logger . logkv ( 'eplenmean' , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) if eval_env is not None : logger . logkv ( 'eval_eprewmean' , safemean ( [ epinfo [ 'r' ] for epinfo in eval_epinfobuf ] ) ) logger . logkv ( 'eval_eplenmean' , safemean ( [ epinfo [ 'l' ] for epinfo in eval_epinfobuf ] ) ) logger . logkv ( 'time_elapsed' , tnow - tfirststart ) for ( lossval , lossname ) in zip ( lossvals , model . loss_names ) : logger . logkv ( lossname , lossval ) if MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 : logger . dumpkvs ( ) if save_interval and ( update % save_interval == 0 or update == 1 ) and logger . get_dir ( ) and ( MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 ) : checkdir = osp . join ( logger . get_dir ( ) , 'checkpoints' ) os . makedirs ( checkdir , exist_ok = True ) savepath = osp . join ( checkdir , '%.5i' % update ) print ( 'Saving to' , savepath ) model . save ( savepath ) return model",Learn policy using PPO algorithm ( https : // arxiv . org / abs / 1707 . 06347 )
"def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : p = b . copy ( ) r = b . copy ( ) x = np . zeros_like ( b ) rdotr = r . dot ( r ) fmtstr = ""%10i %10.3g %10.3g"" titlestr = ""%10s %10s %10s"" if verbose : print ( titlestr % ( ""iter"" , ""residual norm"" , ""soln norm"" ) ) for i in range ( cg_iters ) : if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) z = f_Ax ( p ) v = rdotr / p . dot ( z ) x += v * p r -= v * z newrdotr = r . dot ( r ) mu = newrdotr / rdotr p = r + mu * p rdotr = newrdotr if rdotr < residual_tol : break if callback is not None : callback ( x ) if verbose : print ( fmtstr % ( i + 1 , rdotr , np . linalg . norm ( x ) ) ) return x",Demmel p 312
"def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : assert isinstance ( ob_space , Discrete ) or isinstance ( ob_space , Box ) or isinstance ( ob_space , MultiDiscrete ) , 'Can only deal with Discrete and Box observation spaces for now' dtype = ob_space . dtype if dtype == np . int8 : dtype = np . uint8 return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name )",Create placeholder to feed observations into of the size appropriate to the observation space
"def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : placeholder = observation_placeholder ( ob_space , batch_size , name ) return placeholder , encode_observation ( ob_space , placeholder )",Create placeholder to feed observations into of the size appropriate to the observation space and add input encoder of the appropriate type .
"def encode_observation ( ob_space , placeholder ) : if isinstance ( ob_space , Discrete ) : return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) elif isinstance ( ob_space , Box ) : return tf . to_float ( placeholder ) elif isinstance ( ob_space , MultiDiscrete ) : placeholder = tf . cast ( placeholder , tf . int32 ) one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] return tf . concat ( one_hots , axis = - 1 ) else : raise NotImplementedError",Encode input in the way that is appropriate to the observation space
"def generate_rollouts ( self ) : self . reset_all_rollouts ( ) o = np . empty ( ( self . rollout_batch_size , self . dims [ 'o' ] ) , np . float32 ) ag = np . empty ( ( self . rollout_batch_size , self . dims [ 'g' ] ) , np . float32 ) o [ : ] = self . initial_o ag [ : ] = self . initial_ag obs , achieved_goals , acts , goals , successes = [ ] , [ ] , [ ] , [ ] , [ ] dones = [ ] info_values = [ np . empty ( ( self . T - 1 , self . rollout_batch_size , self . dims [ 'info_' + key ] ) , np . float32 ) for key in self . info_keys ] Qs = [ ] for t in range ( self . T ) : policy_output = self . policy . get_actions ( o , ag , self . g , compute_Q = self . compute_Q , noise_eps = self . noise_eps if not self . exploit else 0. , random_eps = self . random_eps if not self . exploit else 0. , use_target_net = self . use_target_net ) if self . compute_Q : u , Q = policy_output Qs . append ( Q ) else : u = policy_output if u . ndim == 1 : u = u . reshape ( 1 , - 1 ) o_new = np . empty ( ( self . rollout_batch_size , self . dims [ 'o' ] ) ) ag_new = np . empty ( ( self . rollout_batch_size , self . dims [ 'g' ] ) ) success = np . zeros ( self . rollout_batch_size ) obs_dict_new , _ , done , info = self . venv . step ( u ) o_new = obs_dict_new [ 'observation' ] ag_new = obs_dict_new [ 'achieved_goal' ] success = np . array ( [ i . get ( 'is_success' , 0.0 ) for i in info ] ) if any ( done ) : break for i , info_dict in enumerate ( info ) : for idx , key in enumerate ( self . info_keys ) : info_values [ idx ] [ t , i ] = info [ i ] [ key ] if np . isnan ( o_new ) . any ( ) : self . logger . warn ( 'NaN caught during rollout generation. Trying again...' ) self . reset_all_rollouts ( ) return self . generate_rollouts ( ) dones . append ( done ) obs . append ( o . copy ( ) ) achieved_goals . append ( ag . copy ( ) ) successes . append ( success . copy ( ) ) acts . append ( u . copy ( ) ) goals . append ( self . g . copy ( ) ) o [ ... ] = o_new ag [ ... ] = ag_new obs . append ( o . copy ( ) ) achieved_goals . append ( ag . copy ( ) ) episode = dict ( o = obs , u = acts , g = goals , ag = achieved_goals ) for key , value in zip ( self . info_keys , info_values ) : episode [ 'info_{}' . format ( key ) ] = value successful = np . array ( successes ) [ - 1 , : ] assert successful . shape == ( self . rollout_batch_size , ) success_rate = np . mean ( successful ) self . success_history . append ( success_rate ) if self . compute_Q : self . Q_history . append ( np . mean ( Qs ) ) self . n_episodes += self . rollout_batch_size return convert_episode_to_batch_major ( episode )",Performs rollout_batch_size rollouts in parallel for time horizon T with the current policy acting on it accordingly .
"def save_policy ( self , path ) : with open ( path , 'wb' ) as f : pickle . dump ( self . policy , f )",Pickles the current policy for later inspection .
"def logs ( self , prefix = 'worker' ) : logs = [ ] logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] if self . compute_Q : logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] logs += [ ( 'episode' , self . n_episodes ) ] if prefix != '' and not prefix . endswith ( '/' ) : return [ ( prefix + '/' + key , val ) for key , val in logs ] else : return logs",Generates a dictionary that contains all collected statistics .
"def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : assert mode in ( 'two_sided' , 'causal' ) if len ( y ) < 2 * radius + 1 : return np . ones_like ( y ) * y . mean ( ) elif mode == 'two_sided' : convkernel = np . ones ( 2 * radius + 1 ) out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) if valid_only : out [ : radius ] = out [ - radius : ] = np . nan elif mode == 'causal' : convkernel = np . ones ( radius ) out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) out = out [ : - radius + 1 ] if valid_only : out [ : radius ] = np . nan return out",Smooth signal y where radius is determines the size of the window
"def one_sided_ema ( xolds , yolds , low = None , high = None , n = 512 , decay_steps = 1. , low_counts_threshold = 1e-8 ) : low = xolds [ 0 ] if low is None else low high = xolds [ - 1 ] if high is None else high assert xolds [ 0 ] <= low , 'low = {} < xolds[0] = {} - extrapolation not permitted!' . format ( low , xolds [ 0 ] ) assert xolds [ - 1 ] >= high , 'high = {} > xolds[-1] = {}  - extrapolation not permitted!' . format ( high , xolds [ - 1 ] ) assert len ( xolds ) == len ( yolds ) , 'length of xolds ({}) and yolds ({}) do not match!' . format ( len ( xolds ) , len ( yolds ) ) xolds = xolds . astype ( 'float64' ) yolds = yolds . astype ( 'float64' ) luoi = 0 sum_y = 0. count_y = 0. xnews = np . linspace ( low , high , n ) decay_period = ( high - low ) / ( n - 1 ) * decay_steps interstep_decay = np . exp ( - 1. / decay_steps ) sum_ys = np . zeros_like ( xnews ) count_ys = np . zeros_like ( xnews ) for i in range ( n ) : xnew = xnews [ i ] sum_y *= interstep_decay count_y *= interstep_decay while True : xold = xolds [ luoi ] if xold <= xnew : decay = np . exp ( - ( xnew - xold ) / decay_period ) sum_y += decay * yolds [ luoi ] count_y += decay luoi += 1 else : break if luoi >= len ( xolds ) : break sum_ys [ i ] = sum_y count_ys [ i ] = count_y ys = sum_ys / count_ys ys [ count_ys < low_counts_threshold ] = np . nan return xnews , ys , count_ys",perform one - sided ( causal ) EMA ( exponential moving average ) smoothing and resampling to an even grid with n points . Does not do extrapolation so we assume xolds [ 0 ] < = low && high < = xolds [ - 1 ]
"def symmetric_ema ( xolds , yolds , low = None , high = None , n = 512 , decay_steps = 1. , low_counts_threshold = 1e-8 ) : xs , ys1 , count_ys1 = one_sided_ema ( xolds , yolds , low , high , n , decay_steps , low_counts_threshold = 0 ) _ , ys2 , count_ys2 = one_sided_ema ( - xolds [ : : - 1 ] , yolds [ : : - 1 ] , - high , - low , n , decay_steps , low_counts_threshold = 0 ) ys2 = ys2 [ : : - 1 ] count_ys2 = count_ys2 [ : : - 1 ] count_ys = count_ys1 + count_ys2 ys = ( ys1 * count_ys1 + ys2 * count_ys2 ) / count_ys ys [ count_ys < low_counts_threshold ] = np . nan return xs , ys , count_ys",perform symmetric EMA ( exponential moving average ) smoothing and resampling to an even grid with n points . Does not do extrapolation so we assume xolds [ 0 ] < = low && high < = xolds [ - 1 ]
"def load_results ( root_dir_or_dirs , enable_progress = True , enable_monitor = True , verbose = False ) : import re if isinstance ( root_dir_or_dirs , str ) : rootdirs = [ osp . expanduser ( root_dir_or_dirs ) ] else : rootdirs = [ osp . expanduser ( d ) for d in root_dir_or_dirs ] allresults = [ ] for rootdir in rootdirs : assert osp . exists ( rootdir ) , ""%s doesn't exist"" % rootdir for dirname , dirs , files in os . walk ( rootdir ) : if '-proc' in dirname : files [ : ] = [ ] continue monitor_re = re . compile ( r'(\d+\.)?(\d+\.)?monitor\.csv' ) if set ( [ 'metadata.json' , 'monitor.json' , 'progress.json' , 'progress.csv' ] ) . intersection ( files ) or any ( [ f for f in files if monitor_re . match ( f ) ] ) : result = { 'dirname' : dirname } if ""metadata.json"" in files : with open ( osp . join ( dirname , ""metadata.json"" ) , ""r"" ) as fh : result [ 'metadata' ] = json . load ( fh ) progjson = osp . join ( dirname , ""progress.json"" ) progcsv = osp . join ( dirname , ""progress.csv"" ) if enable_progress : if osp . exists ( progjson ) : result [ 'progress' ] = pandas . DataFrame ( read_json ( progjson ) ) elif osp . exists ( progcsv ) : try : result [ 'progress' ] = read_csv ( progcsv ) except pandas . errors . EmptyDataError : print ( 'skipping progress file in ' , dirname , 'empty data' ) else : if verbose : print ( 'skipping %s: no progress file' % dirname ) if enable_monitor : try : result [ 'monitor' ] = pandas . DataFrame ( monitor . load_results ( dirname ) ) except monitor . LoadMonitorResultsError : print ( 'skipping %s: no monitor files' % dirname ) except Exception as e : print ( 'exception loading monitor file in %s: %s' % ( dirname , e ) ) if result . get ( 'monitor' ) is not None or result . get ( 'progress' ) is not None : allresults . append ( Result ( * * result ) ) if verbose : print ( 'successfully loaded %s' % dirname ) if verbose : print ( 'loaded %i results' % len ( allresults ) ) return allresults",load summaries of runs from a list of directories ( including subdirectories ) Arguments :
"def plot_results ( allresults , * , xy_fn = default_xy_fn , split_fn = default_split_fn , group_fn = default_split_fn , average_group = False , shaded_std = True , shaded_err = True , figsize = None , legend_outside = False , resample = 0 , smooth_step = 1.0 ) : if split_fn is None : split_fn = lambda _ : '' if group_fn is None : group_fn = lambda _ : '' sk2r = defaultdict ( list ) for result in allresults : splitkey = split_fn ( result ) sk2r [ splitkey ] . append ( result ) assert len ( sk2r ) > 0 assert isinstance ( resample , int ) , ""0: don't resample. <integer>: that many samples"" nrows = len ( sk2r ) ncols = 1 figsize = figsize or ( 6 , 6 * nrows ) f , axarr = plt . subplots ( nrows , ncols , sharex = False , squeeze = False , figsize = figsize ) groups = list ( set ( group_fn ( result ) for result in allresults ) ) default_samples = 512 if average_group : resample = resample or default_samples for ( isplit , sk ) in enumerate ( sorted ( sk2r . keys ( ) ) ) : g2l = { } g2c = defaultdict ( int ) sresults = sk2r [ sk ] gresults = defaultdict ( list ) ax = axarr [ isplit ] [ 0 ] for result in sresults : group = group_fn ( result ) g2c [ group ] += 1 x , y = xy_fn ( result ) if x is None : x = np . arange ( len ( y ) ) x , y = map ( np . asarray , ( x , y ) ) if average_group : gresults [ group ] . append ( ( x , y ) ) else : if resample : x , y , counts = symmetric_ema ( x , y , x [ 0 ] , x [ - 1 ] , resample , decay_steps = smooth_step ) l , = ax . plot ( x , y , color = COLORS [ groups . index ( group ) % len ( COLORS ) ] ) g2l [ group ] = l if average_group : for group in sorted ( groups ) : xys = gresults [ group ] if not any ( xys ) : continue color = COLORS [ groups . index ( group ) % len ( COLORS ) ] origxs = [ xy [ 0 ] for xy in xys ] minxlen = min ( map ( len , origxs ) ) def allequal ( qs ) : return all ( ( q == qs [ 0 ] ) . all ( ) for q in qs [ 1 : ] ) if resample : low = max ( x [ 0 ] for x in origxs ) high = min ( x [ - 1 ] for x in origxs ) usex = np . linspace ( low , high , resample ) ys = [ ] for ( x , y ) in xys : ys . append ( symmetric_ema ( x , y , low , high , resample , decay_steps = smooth_step ) [ 1 ] ) else : assert allequal ( [ x [ : minxlen ] for x in origxs ] ) , 'If you want to average unevenly sampled data, set resample=<number of samples you want>' usex = origxs [ 0 ] ys = [ xy [ 1 ] [ : minxlen ] for xy in xys ] ymean = np . mean ( ys , axis = 0 ) ystd = np . std ( ys , axis = 0 ) ystderr = ystd / np . sqrt ( len ( ys ) ) l , = axarr [ isplit ] [ 0 ] . plot ( usex , ymean , color = color ) g2l [ group ] = l if shaded_err : ax . fill_between ( usex , ymean - ystderr , ymean + ystderr , color = color , alpha = .4 ) if shaded_std : ax . fill_between ( usex , ymean - ystd , ymean + ystd , color = color , alpha = .2 ) plt . tight_layout ( ) if any ( g2l . keys ( ) ) : ax . legend ( g2l . values ( ) , [ '%s (%i)' % ( g , g2c [ g ] ) for g in g2l ] if average_group else g2l . keys ( ) , loc = 2 if legend_outside else None , bbox_to_anchor = ( 1 , 1 ) if legend_outside else None ) ax . set_title ( sk ) return f , axarr",Plot multiple Results objects
"def check_synced ( localval , comm = None ) : comm = comm or MPI . COMM_WORLD vals = comm . gather ( localval ) if comm . rank == 0 : assert all ( val == vals [ 0 ] for val in vals [ 1 : ] )",It s common to forget to initialize your variables to the same values or ( less commonly ) if you update them in some other way than adam to get them out of sync . This function checks that variables on all MPI workers are the same and raises an AssertionError otherwise
"def copy_obs_dict ( obs ) : return { k : np . copy ( v ) for k , v in obs . items ( ) }",Deep - copy an observation dict .
"def obs_space_info ( obs_space ) : if isinstance ( obs_space , gym . spaces . Dict ) : assert isinstance ( obs_space . spaces , OrderedDict ) subspaces = obs_space . spaces else : subspaces = { None : obs_space } keys = [ ] shapes = { } dtypes = { } for key , box in subspaces . items ( ) : keys . append ( key ) shapes [ key ] = box . shape dtypes [ key ] = box . dtype return keys , shapes , dtypes",Get dict - structured information about a gym . Space .
"def q_retrace ( R , D , q_i , v , rho_i , nenvs , nsteps , gamma ) : rho_bar = batch_to_seq ( tf . minimum ( 1.0 , rho_i ) , nenvs , nsteps , True ) rs = batch_to_seq ( R , nenvs , nsteps , True ) ds = batch_to_seq ( D , nenvs , nsteps , True ) q_is = batch_to_seq ( q_i , nenvs , nsteps , True ) vs = batch_to_seq ( v , nenvs , nsteps + 1 , True ) v_final = vs [ - 1 ] qret = v_final qrets = [ ] for i in range ( nsteps - 1 , - 1 , - 1 ) : check_shape ( [ qret , ds [ i ] , rs [ i ] , rho_bar [ i ] , q_is [ i ] , vs [ i ] ] , [ [ nenvs ] ] * 6 ) qret = rs [ i ] + gamma * qret * ( 1.0 - ds [ i ] ) qrets . append ( qret ) qret = ( rho_bar [ i ] * ( qret - q_is [ i ] ) ) + vs [ i ] qrets = qrets [ : : - 1 ] qret = seq_to_batch ( qrets , flat = True ) return qret",Calculates q_retrace targets
"def learn ( network , env , seed = None , nsteps = 20 , total_timesteps = int ( 80e6 ) , q_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 10 , lr = 7e-4 , lrschedule = 'linear' , rprop_epsilon = 1e-5 , rprop_alpha = 0.99 , gamma = 0.99 , log_interval = 100 , buffer_size = 50000 , replay_ratio = 4 , replay_start = 10000 , c = 10.0 , trust_region = True , alpha = 0.99 , delta = 1 , load_path = None , * * network_kwargs ) : print ( ""Running Acer Simple"" ) print ( locals ( ) ) set_global_seeds ( seed ) if not isinstance ( env , VecFrameStack ) : env = VecFrameStack ( env , 1 ) policy = build_policy ( env , network , estimate_q = True , * * network_kwargs ) nenvs = env . num_envs ob_space = env . observation_space ac_space = env . action_space nstack = env . nstack model = Model ( policy = policy , ob_space = ob_space , ac_space = ac_space , nenvs = nenvs , nsteps = nsteps , ent_coef = ent_coef , q_coef = q_coef , gamma = gamma , max_grad_norm = max_grad_norm , lr = lr , rprop_alpha = rprop_alpha , rprop_epsilon = rprop_epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule , c = c , trust_region = trust_region , alpha = alpha , delta = delta ) runner = Runner ( env = env , model = model , nsteps = nsteps ) if replay_ratio > 0 : buffer = Buffer ( env = env , nsteps = nsteps , size = buffer_size ) else : buffer = None nbatch = nenvs * nsteps acer = Acer ( runner , model , buffer , log_interval ) acer . tstart = time . time ( ) for acer . steps in range ( 0 , total_timesteps , nbatch ) : acer . call ( on_policy = True ) if replay_ratio > 0 and buffer . has_atleast ( replay_start ) : n = np . random . poisson ( replay_ratio ) for _ in range ( n ) : acer . call ( on_policy = False ) return model",Main entrypoint for ACER ( Actor - Critic with Experience Replay ) algorithm ( https : // arxiv . org / pdf / 1611 . 01224 . pdf ) Train an agent with given network architecture on a given environment using ACER .
"def apply_stats ( self , statsUpdates ) : def updateAccumStats ( ) : if self . _full_stats_init : return tf . cond ( tf . greater ( self . sgd_step , self . _cold_iter ) , lambda : tf . group ( * self . _apply_stats ( statsUpdates , accumulate = True , accumulateCoeff = 1. / self . _stats_accum_iter ) ) , tf . no_op ) else : return tf . group ( * self . _apply_stats ( statsUpdates , accumulate = True , accumulateCoeff = 1. / self . _stats_accum_iter ) ) def updateRunningAvgStats ( statsUpdates , fac_iter = 1 ) : return tf . group ( * self . _apply_stats ( statsUpdates ) ) if self . _async_stats : update_stats = self . _apply_stats ( statsUpdates ) queue = tf . FIFOQueue ( 1 , [ item . dtype for item in update_stats ] , shapes = [ item . get_shape ( ) for item in update_stats ] ) enqueue_op = queue . enqueue ( update_stats ) def dequeue_stats_op ( ) : return queue . dequeue ( ) self . qr_stats = tf . train . QueueRunner ( queue , [ enqueue_op ] ) update_stats_op = tf . cond ( tf . equal ( queue . size ( ) , tf . convert_to_tensor ( 0 ) ) , tf . no_op , lambda : tf . group ( * [ dequeue_stats_op ( ) , ] ) ) else : update_stats_op = tf . cond ( tf . greater_equal ( self . stats_step , self . _stats_accum_iter ) , lambda : updateRunningAvgStats ( statsUpdates ) , updateAccumStats ) self . _update_stats_op = update_stats_op return update_stats_op",compute stats and update / apply the new stats to the running average
"def tile_images ( img_nhwc ) : img_nhwc = np . asarray ( img_nhwc ) N , h , w , c = img_nhwc . shape H = int ( np . ceil ( np . sqrt ( N ) ) ) W = int ( np . ceil ( float ( N ) / H ) ) img_nhwc = np . array ( list ( img_nhwc ) + [ img_nhwc [ 0 ] * 0 for _ in range ( N , H * W ) ] ) img_HWhwc = img_nhwc . reshape ( H , W , h , w , c ) img_HhWwc = img_HWhwc . transpose ( 0 , 2 , 1 , 3 , 4 ) img_Hh_Ww_c = img_HhWwc . reshape ( H * h , W * w , c ) return img_Hh_Ww_c",Tile N images into one big PxQ image ( P Q ) are chosen to be as close as possible and if N is square then P = Q .
"def sum ( self , start = 0 , end = None ) : return super ( SumSegmentTree , self ) . reduce ( start , end )",Returns arr [ start ] + ... + arr [ end ]
"def find_prefixsum_idx ( self , prefixsum ) : assert 0 <= prefixsum <= self . sum ( ) + 1e-5 idx = 1 while idx < self . _capacity : if self . _value [ 2 * idx ] > prefixsum : idx = 2 * idx else : prefixsum -= self . _value [ 2 * idx ] idx = 2 * idx + 1 return idx - self . _capacity",Find the highest index i in the array such that sum ( arr [ 0 ] + arr [ 1 ] + ... + arr [ i - i ] ) < = prefixsum
"def min ( self , start = 0 , end = None ) : return super ( MinSegmentTree , self ) . reduce ( start , end )",Returns min ( arr [ start ] ... arr [ end ] )
"def value ( self , t ) : for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : if l_t <= t and t < r_t : alpha = float ( t - l_t ) / ( r_t - l_t ) return self . _interpolation ( l , r , alpha ) assert self . _outside_value is not None return self . _outside_value",See Schedule . value
"def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : def _write_obs ( maybe_dict_obs ) : flatdict = obs_to_dict ( maybe_dict_obs ) for k in keys : dst = obs_bufs [ k ] . get_obj ( ) dst_np = np . frombuffer ( dst , dtype = obs_dtypes [ k ] ) . reshape ( obs_shapes [ k ] ) np . copyto ( dst_np , flatdict [ k ] ) env = env_fn_wrapper . x ( ) parent_pipe . close ( ) try : while True : cmd , data = pipe . recv ( ) if cmd == 'reset' : pipe . send ( _write_obs ( env . reset ( ) ) ) elif cmd == 'step' : obs , reward , done , info = env . step ( data ) if done : obs = env . reset ( ) pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) elif cmd == 'render' : pipe . send ( env . render ( mode = 'rgb_array' ) ) elif cmd == 'close' : pipe . send ( None ) break else : raise RuntimeError ( 'Got unrecognized cmd %s' % cmd ) except KeyboardInterrupt : print ( 'ShmemVecEnv worker: got KeyboardInterrupt' ) finally : env . close ( )",Control a single environment instance using IPC and shared memory .
"def learn ( network , env , seed = None , nsteps = 5 , total_timesteps = int ( 80e6 ) , vf_coef = 0.5 , ent_coef = 0.01 , max_grad_norm = 0.5 , lr = 7e-4 , lrschedule = 'linear' , epsilon = 1e-5 , alpha = 0.99 , gamma = 0.99 , log_interval = 100 , load_path = None , * * network_kwargs ) : set_global_seeds ( seed ) nenvs = env . num_envs policy = build_policy ( env , network , * * network_kwargs ) model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) if load_path is not None : model . load ( load_path ) runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) epinfobuf = deque ( maxlen = 100 ) nbatch = nenvs * nsteps tstart = time . time ( ) for update in range ( 1 , total_timesteps // nbatch + 1 ) : obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) epinfobuf . extend ( epinfos ) policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) nseconds = time . time ( ) - tstart fps = int ( ( update * nbatch ) / nseconds ) if update % log_interval == 0 or update == 1 : ev = explained_variance ( values , rewards ) logger . record_tabular ( ""nupdates"" , update ) logger . record_tabular ( ""total_timesteps"" , update * nbatch ) logger . record_tabular ( ""fps"" , fps ) logger . record_tabular ( ""policy_entropy"" , float ( policy_entropy ) ) logger . record_tabular ( ""value_loss"" , float ( value_loss ) ) logger . record_tabular ( ""explained_variance"" , float ( ev ) ) logger . record_tabular ( ""eprewmean"" , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) logger . record_tabular ( ""eplenmean"" , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) logger . dump_tabular ( ) return model",Main entrypoint for A2C algorithm . Train a policy with given network architecture on a given environment using a2c algorithm .
"def sf01 ( arr ) : s = arr . shape return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] )",swap and then flatten axes 0 and 1
"def step ( self , observation , * * extra_feed ) : a , v , state , neglogp = self . _evaluate ( [ self . action , self . vf , self . state , self . neglogp ] , observation , * * extra_feed ) if state . size == 0 : state = None return a , v , state , neglogp",Compute next action ( s ) given the observation ( s )
"def value ( self , ob , * args , * * kwargs ) : return self . _evaluate ( self . vf , ob , * args , * * kwargs )",Compute value estimate ( s ) given the observation ( s )
"def pretty_eta ( seconds_left ) : minutes_left = seconds_left // 60 seconds_left %= 60 hours_left = minutes_left // 60 minutes_left %= 60 days_left = hours_left // 24 hours_left %= 24 def helper ( cnt , name ) : return ""{} {}{}"" . format ( str ( cnt ) , name , ( 's' if cnt > 1 else '' ) ) if days_left > 0 : msg = helper ( days_left , 'day' ) if hours_left > 0 : msg += ' and ' + helper ( hours_left , 'hour' ) return msg if hours_left > 0 : msg = helper ( hours_left , 'hour' ) if minutes_left > 0 : msg += ' and ' + helper ( minutes_left , 'minute' ) return msg if minutes_left > 0 : return helper ( minutes_left , 'minute' ) return 'less than a minute'",Print the number of seconds in human readable format .
"def boolean_flag ( parser , name , default = False , help = None ) : dest = name . replace ( '-' , '_' ) parser . add_argument ( ""--"" + name , action = ""store_true"" , default = default , dest = dest , help = help ) parser . add_argument ( ""--no-"" + name , action = ""store_false"" , dest = dest )",Add a boolean flag to argparse parser .
"def get_wrapper_by_name ( env , classname ) : currentenv = env while True : if classname == currentenv . class_name ( ) : return currentenv elif isinstance ( currentenv , gym . Wrapper ) : currentenv = currentenv . env else : raise ValueError ( ""Couldn't find wrapper named %s"" % classname )",Given an a gym environment possibly wrapped multiple times returns a wrapper of class named classname or raises ValueError if no such wrapper was applied
"def relatively_safe_pickle_dump ( obj , path , compression = False ) : temp_storage = path + "".relatively_safe"" if compression : with tempfile . NamedTemporaryFile ( ) as uncompressed_file : pickle . dump ( obj , uncompressed_file ) uncompressed_file . file . flush ( ) with zipfile . ZipFile ( temp_storage , ""w"" , compression = zipfile . ZIP_DEFLATED ) as myzip : myzip . write ( uncompressed_file . name , ""data"" ) else : with open ( temp_storage , ""wb"" ) as f : pickle . dump ( obj , f ) os . rename ( temp_storage , path )",This is just like regular pickle dump except from the fact that failure cases are different :
"def pickle_load ( path , compression = False ) : if compression : with zipfile . ZipFile ( path , ""r"" , compression = zipfile . ZIP_DEFLATED ) as myzip : with myzip . open ( ""data"" ) as f : return pickle . load ( f ) else : with open ( path , ""rb"" ) as f : return pickle . load ( f )",Unpickle a possible compressed pickle .
"def update ( self , new_val ) : if self . _value is None : self . _value = new_val else : self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val",Update the estimate .
"def store_args ( method ) : argspec = inspect . getfullargspec ( method ) defaults = { } if argspec . defaults is not None : defaults = dict ( zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) if argspec . kwonlydefaults is not None : defaults . update ( argspec . kwonlydefaults ) arg_names = argspec . args [ 1 : ] @ functools . wraps ( method ) def wrapper ( * positional_args , * * keyword_args ) : self = positional_args [ 0 ] args = defaults . copy ( ) for name , value in zip ( arg_names , positional_args [ 1 : ] ) : args [ name ] = value args . update ( keyword_args ) self . __dict__ . update ( args ) return method ( * positional_args , * * keyword_args ) return wrapper",Stores provided method args as instance attributes .
"def import_function ( spec ) : mod_name , fn_name = spec . split ( ':' ) module = importlib . import_module ( mod_name ) fn = getattr ( module , fn_name ) return fn",Import a function identified by a string like pkg . module : fn_name .
"def flatten_grads ( var_list , grads ) : return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) for ( v , grad ) in zip ( var_list , grads ) ] , 0 )",Flattens a variables and their gradients .
"def nn ( input , layers_sizes , reuse = None , flatten = False , name = """" ) : for i , size in enumerate ( layers_sizes ) : activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None input = tf . layers . dense ( inputs = input , units = size , kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , reuse = reuse , name = name + '_' + str ( i ) ) if activation : input = activation ( input ) if flatten : assert layers_sizes [ - 1 ] == 1 input = tf . reshape ( input , [ - 1 ] ) return input",Creates a simple neural network
"def mpi_fork ( n , extra_mpi_args = [ ] ) : if n <= 1 : return ""child"" if os . getenv ( ""IN_MPI"" ) is None : env = os . environ . copy ( ) env . update ( MKL_NUM_THREADS = ""1"" , OMP_NUM_THREADS = ""1"" , IN_MPI = ""1"" ) args = [ ""mpirun"" , ""-np"" , str ( n ) ] + extra_mpi_args + [ sys . executable ] args += sys . argv subprocess . check_call ( args , env = env ) return ""parent"" else : install_mpi_excepthook ( ) return ""child""",Re - launches the current script with workers Returns parent for original parent child for MPI children
"def convert_episode_to_batch_major ( episode ) : episode_batch = { } for key in episode . keys ( ) : val = np . array ( episode [ key ] ) . copy ( ) episode_batch [ key ] = val . swapaxes ( 0 , 1 ) return episode_batch",Converts an episode to have the batch dimension in the major ( first ) dimension .
"def reshape_for_broadcasting ( source , target ) : dim = len ( target . get_shape ( ) ) shape = ( [ 1 ] * ( dim - 1 ) ) + [ - 1 ] return tf . reshape ( tf . cast ( source , target . dtype ) , shape )",Reshapes a tensor ( source ) to have the correct shape and dtype of the target before broadcasting it with MPI .
"def add_vtarg_and_adv ( seg , gamma , lam ) : new = np . append ( seg [ ""new"" ] , 0 ) vpred = np . append ( seg [ ""vpred"" ] , seg [ ""nextvpred"" ] ) T = len ( seg [ ""rew"" ] ) seg [ ""adv"" ] = gaelam = np . empty ( T , 'float32' ) rew = seg [ ""rew"" ] lastgaelam = 0 for t in reversed ( range ( T ) ) : nonterminal = 1 - new [ t + 1 ] delta = rew [ t ] + gamma * vpred [ t + 1 ] * nonterminal - vpred [ t ] gaelam [ t ] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam seg [ ""tdlamret"" ] = seg [ ""adv"" ] + seg [ ""vpred"" ]",Compute target value using TD ( lambda ) estimator and advantage with GAE ( lambda )
"def switch ( condition , then_expression , else_expression ) : x_shape = copy . copy ( then_expression . get_shape ( ) ) x = tf . cond ( tf . cast ( condition , 'bool' ) , lambda : then_expression , lambda : else_expression ) x . set_shape ( x_shape ) return x",Switches between two operations depending on a scalar value ( int or bool ) . Note that both then_expression and else_expression should be symbolic tensors of the * same shape * .
"def huber_loss ( x , delta = 1.0 ) : return tf . where ( tf . abs ( x ) < delta , tf . square ( x ) * 0.5 , delta * ( tf . abs ( x ) - 0.5 * delta ) )",Reference : https : // en . wikipedia . org / wiki / Huber_loss
"def get_session ( config = None ) : sess = tf . get_default_session ( ) if sess is None : sess = make_session ( config = config , make_default = True ) return sess",Get default session or create one with a given config
"def make_session ( config = None , num_cpu = None , make_default = False , graph = None ) : if num_cpu is None : num_cpu = int ( os . getenv ( 'RCALL_NUM_CPU' , multiprocessing . cpu_count ( ) ) ) if config is None : config = tf . ConfigProto ( allow_soft_placement = True , inter_op_parallelism_threads = num_cpu , intra_op_parallelism_threads = num_cpu ) config . gpu_options . allow_growth = True if make_default : return tf . InteractiveSession ( config = config , graph = graph ) else : return tf . Session ( config = config , graph = graph )",Returns a session that will use <num_cpu > CPU s only
def initialize ( ) : new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) ALREADY_INITIALIZED . update ( new_variables ),Initialize all the uninitialized variables in the global scope .
"def function ( inputs , outputs , updates = None , givens = None ) : if isinstance ( outputs , list ) : return _Function ( inputs , outputs , updates , givens = givens ) elif isinstance ( outputs , ( dict , collections . OrderedDict ) ) : f = _Function ( inputs , outputs . values ( ) , updates , givens = givens ) return lambda * args , * * kwargs : type ( outputs ) ( zip ( outputs . keys ( ) , f ( * args , * * kwargs ) ) ) else : f = _Function ( inputs , [ outputs ] , updates , givens = givens ) return lambda * args , * * kwargs : f ( * args , * * kwargs ) [ 0 ]",Just like Theano function . Take a bunch of tensorflow placeholders and expressions computed based on those placeholders and produces f ( inputs ) - > outputs . Function f takes values to be fed to the input s placeholders and produces the values of the expressions in outputs .
"def adjust_shape ( placeholder , data ) : if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : return data if isinstance ( data , list ) : data = np . array ( data ) placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] assert _check_shape ( placeholder_shape , data . shape ) , 'Shape of data {} is not compatible with shape of the placeholder {}' . format ( data . shape , placeholder_shape ) return np . reshape ( data , placeholder_shape )",adjust shape of the data to the shape of the placeholder if possible . If shape is incompatible AssertionError is thrown
"def _check_shape ( placeholder_shape , data_shape ) : return True squeezed_placeholder_shape = _squeeze_shape ( placeholder_shape ) squeezed_data_shape = _squeeze_shape ( data_shape ) for i , s_data in enumerate ( squeezed_data_shape ) : s_placeholder = squeezed_placeholder_shape [ i ] if s_placeholder != - 1 and s_data != s_placeholder : return False return True",check if two shapes are compatible ( i . e . differ only by dimensions of size 1 or by the batch dimension )
"def profile ( n ) : def decorator_with_name ( func ) : def func_wrapper ( * args , * * kwargs ) : with profile_kv ( n ) : return func ( * args , * * kwargs ) return func_wrapper return decorator_with_name",Usage :
"def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : if episode_life : env = EpisodicLifeEnv ( env ) if 'FIRE' in env . unwrapped . get_action_meanings ( ) : env = FireResetEnv ( env ) env = WarpFrame ( env ) if scale : env = ScaledFloatFrame ( env ) if clip_rewards : env = ClipRewardEnv ( env ) if frame_stack : env = FrameStack ( env , 4 ) return env",Configure environment for DeepMind - style Atari .
"def reset ( self , * * kwargs ) : if self . was_real_done : obs = self . env . reset ( * * kwargs ) else : obs , _ , _ , _ = self . env . step ( 0 ) self . lives = self . env . unwrapped . ale . lives ( ) return obs",Reset only when lives are exhausted . This way all states are still reachable even though lives are episodic and the learner need not know about any of this behind - the - scenes .
"def sync_from_root ( sess , variables , comm = None ) : if comm is None : comm = MPI . COMM_WORLD import tensorflow as tf values = comm . bcast ( sess . run ( variables ) ) sess . run ( [ tf . assign ( var , val ) for ( var , val ) in zip ( variables , values ) ] )",Send the root node s parameters to every worker . Arguments : sess : the TensorFlow session . variables : all parameter variables including optimizer s
"def gpu_count ( ) : if shutil . which ( 'nvidia-smi' ) is None : return 0 output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) return max ( 0 , len ( output . split ( b'\n' ) ) - 2 )",Count the GPUs on this machine .
"def setup_mpi_gpus ( ) : if 'CUDA_VISIBLE_DEVICES' not in os . environ : if sys . platform == 'darwin' : ids = [ ] else : lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) ids = [ lrank ] os . environ [ ""CUDA_VISIBLE_DEVICES"" ] = "","" . join ( map ( str , ids ) )",Set CUDA_VISIBLE_DEVICES to MPI rank if not already set
"def get_local_rank_size ( comm ) : this_node = platform . node ( ) ranks_nodes = comm . allgather ( ( comm . Get_rank ( ) , this_node ) ) node2rankssofar = defaultdict ( int ) local_rank = None for ( rank , node ) in ranks_nodes : if rank == comm . Get_rank ( ) : local_rank = node2rankssofar [ node ] node2rankssofar [ node ] += 1 assert local_rank is not None return local_rank , node2rankssofar [ this_node ]",Returns the rank of each process on its machine The processes on a given machine will be assigned ranks 0 1 2 ... N - 1 where N is the number of processes on this machine .
"def share_file ( comm , path ) : localrank , _ = get_local_rank_size ( comm ) if comm . Get_rank ( ) == 0 : with open ( path , 'rb' ) as fh : data = fh . read ( ) comm . bcast ( data ) else : data = comm . bcast ( None ) if localrank == 0 : os . makedirs ( os . path . dirname ( path ) , exist_ok = True ) with open ( path , 'wb' ) as fh : fh . write ( data ) comm . Barrier ( )",Copies the file from rank 0 to all other ranks Puts it in the same place on all machines
"def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : if comm is None : return d alldicts = comm . allgather ( d ) size = comm . size k2li = defaultdict ( list ) for d in alldicts : for ( k , v ) in d . items ( ) : k2li [ k ] . append ( v ) result = { } for ( k , li ) in k2li . items ( ) : if assert_all_have_data : assert len ( li ) == size , ""only %i out of %i MPI workers have sent '%s'"" % ( len ( li ) , size , k ) if op == 'mean' : result [ k ] = np . mean ( li , axis = 0 ) elif op == 'sum' : result [ k ] = np . sum ( li , axis = 0 ) else : assert 0 , op return result",Perform a reduction operation over dicts
"def mpi_weighted_mean ( comm , local_name2valcount ) : all_name2valcount = comm . gather ( local_name2valcount ) if comm . rank == 0 : name2sum = defaultdict ( float ) name2count = defaultdict ( float ) for n2vc in all_name2valcount : for ( name , ( val , count ) ) in n2vc . items ( ) : try : val = float ( val ) except ValueError : if comm . rank == 0 : warnings . warn ( 'WARNING: tried to compute mean on non-float {}={}' . format ( name , val ) ) else : name2sum [ name ] += val * count name2count [ name ] += count return { name : name2sum [ name ] / name2count [ name ] for name in name2sum } else : return { }",Perform a weighted average over dicts that are each on a different node Input : local_name2valcount : dict mapping key - > ( value count ) Returns : key - > mean
"def learn ( * , network , env , total_timesteps , timesteps_per_batch = 1024 , max_kl = 0.001 , cg_iters = 10 , gamma = 0.99 , lam = 1.0 , seed = None , ent_coef = 0.0 , cg_damping = 1e-2 , vf_stepsize = 3e-4 , vf_iters = 3 , max_episodes = 0 , max_iters = 0 , callback = None , load_path = None , * * network_kwargs ) : if MPI is not None : nworkers = MPI . COMM_WORLD . Get_size ( ) rank = MPI . COMM_WORLD . Get_rank ( ) else : nworkers = 1 rank = 0 cpus_per_worker = 1 U . get_session ( config = tf . ConfigProto ( allow_soft_placement = True , inter_op_parallelism_threads = cpus_per_worker , intra_op_parallelism_threads = cpus_per_worker ) ) policy = build_policy ( env , network , value_network = 'copy' , * * network_kwargs ) set_global_seeds ( seed ) np . set_printoptions ( precision = 3 ) ob_space = env . observation_space ac_space = env . action_space ob = observation_placeholder ( ob_space ) with tf . variable_scope ( ""pi"" ) : pi = policy ( observ_placeholder = ob ) with tf . variable_scope ( ""oldpi"" ) : oldpi = policy ( observ_placeholder = ob ) atarg = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) ret = tf . placeholder ( dtype = tf . float32 , shape = [ None ] ) ac = pi . pdtype . sample_placeholder ( [ None ] ) kloldnew = oldpi . pd . kl ( pi . pd ) ent = pi . pd . entropy ( ) meankl = tf . reduce_mean ( kloldnew ) meanent = tf . reduce_mean ( ent ) entbonus = ent_coef * meanent vferr = tf . reduce_mean ( tf . square ( pi . vf - ret ) ) ratio = tf . exp ( pi . pd . logp ( ac ) - oldpi . pd . logp ( ac ) ) surrgain = tf . reduce_mean ( ratio * atarg ) optimgain = surrgain + entbonus losses = [ optimgain , meankl , entbonus , surrgain , meanent ] loss_names = [ ""optimgain"" , ""meankl"" , ""entloss"" , ""surrgain"" , ""entropy"" ] dist = meankl all_var_list = get_trainable_variables ( ""pi"" ) var_list = get_pi_trainable_variables ( ""pi"" ) vf_var_list = get_vf_trainable_variables ( ""pi"" ) vfadam = MpiAdam ( vf_var_list ) get_flat = U . GetFlat ( var_list ) set_from_flat = U . SetFromFlat ( var_list ) klgrads = tf . gradients ( dist , var_list ) flat_tangent = tf . placeholder ( dtype = tf . float32 , shape = [ None ] , name = ""flat_tan"" ) shapes = [ var . get_shape ( ) . as_list ( ) for var in var_list ] start = 0 tangents = [ ] for shape in shapes : sz = U . intprod ( shape ) tangents . append ( tf . reshape ( flat_tangent [ start : start + sz ] , shape ) ) start += sz gvp = tf . add_n ( [ tf . reduce_sum ( g * tangent ) for ( g , tangent ) in zipsame ( klgrads , tangents ) ] ) fvp = U . flatgrad ( gvp , var_list ) assign_old_eq_new = U . function ( [ ] , [ ] , updates = [ tf . assign ( oldv , newv ) for ( oldv , newv ) in zipsame ( get_variables ( ""oldpi"" ) , get_variables ( ""pi"" ) ) ] ) compute_losses = U . function ( [ ob , ac , atarg ] , losses ) compute_lossandgrad = U . function ( [ ob , ac , atarg ] , losses + [ U . flatgrad ( optimgain , var_list ) ] ) compute_fvp = U . function ( [ flat_tangent , ob , ac , atarg ] , fvp ) compute_vflossandgrad = U . function ( [ ob , ret ] , U . flatgrad ( vferr , vf_var_list ) ) @ contextmanager def timed ( msg ) : if rank == 0 : print ( colorize ( msg , color = 'magenta' ) ) tstart = time . time ( ) yield print ( colorize ( ""done in %.3f seconds"" % ( time . time ( ) - tstart ) , color = 'magenta' ) ) else : yield def allmean ( x ) : assert isinstance ( x , np . ndarray ) if MPI is not None : out = np . empty_like ( x ) MPI . COMM_WORLD . Allreduce ( x , out , op = MPI . SUM ) out /= nworkers else : out = np . copy ( x ) return out U . initialize ( ) if load_path is not None : pi . load ( load_path ) th_init = get_flat ( ) if MPI is not None : MPI . COMM_WORLD . Bcast ( th_init , root = 0 ) set_from_flat ( th_init ) vfadam . sync ( ) print ( ""Init param sum"" , th_init . sum ( ) , flush = True ) seg_gen = traj_segment_generator ( pi , env , timesteps_per_batch , stochastic = True ) episodes_so_far = 0 timesteps_so_far = 0 iters_so_far = 0 tstart = time . time ( ) lenbuffer = deque ( maxlen = 40 ) rewbuffer = deque ( maxlen = 40 ) if sum ( [ max_iters > 0 , total_timesteps > 0 , max_episodes > 0 ] ) == 0 : return pi assert sum ( [ max_iters > 0 , total_timesteps > 0 , max_episodes > 0 ] ) < 2 , 'out of max_iters, total_timesteps, and max_episodes only one should be specified' while True : if callback : callback ( locals ( ) , globals ( ) ) if total_timesteps and timesteps_so_far >= total_timesteps : break elif max_episodes and episodes_so_far >= max_episodes : break elif max_iters and iters_so_far >= max_iters : break logger . log ( ""********** Iteration %i ************"" % iters_so_far ) with timed ( ""sampling"" ) : seg = seg_gen . __next__ ( ) add_vtarg_and_adv ( seg , gamma , lam ) ob , ac , atarg , tdlamret = seg [ ""ob"" ] , seg [ ""ac"" ] , seg [ ""adv"" ] , seg [ ""tdlamret"" ] vpredbefore = seg [ ""vpred"" ] atarg = ( atarg - atarg . mean ( ) ) / atarg . std ( ) if hasattr ( pi , ""ret_rms"" ) : pi . ret_rms . update ( tdlamret ) if hasattr ( pi , ""ob_rms"" ) : pi . ob_rms . update ( ob ) args = seg [ ""ob"" ] , seg [ ""ac"" ] , atarg fvpargs = [ arr [ : : 5 ] for arr in args ] def fisher_vector_product ( p ) : return allmean ( compute_fvp ( p , * fvpargs ) ) + cg_damping * p assign_old_eq_new ( ) with timed ( ""computegrad"" ) : * lossbefore , g = compute_lossandgrad ( * args ) lossbefore = allmean ( np . array ( lossbefore ) ) g = allmean ( g ) if np . allclose ( g , 0 ) : logger . log ( ""Got zero gradient. not updating"" ) else : with timed ( ""cg"" ) : stepdir = cg ( fisher_vector_product , g , cg_iters = cg_iters , verbose = rank == 0 ) assert np . isfinite ( stepdir ) . all ( ) shs = .5 * stepdir . dot ( fisher_vector_product ( stepdir ) ) lm = np . sqrt ( shs / max_kl ) fullstep = stepdir / lm expectedimprove = g . dot ( fullstep ) surrbefore = lossbefore [ 0 ] stepsize = 1.0 thbefore = get_flat ( ) for _ in range ( 10 ) : thnew = thbefore + fullstep * stepsize set_from_flat ( thnew ) meanlosses = surr , kl , * _ = allmean ( np . array ( compute_losses ( * args ) ) ) improve = surr - surrbefore logger . log ( ""Expected: %.3f Actual: %.3f"" % ( expectedimprove , improve ) ) if not np . isfinite ( meanlosses ) . all ( ) : logger . log ( ""Got non-finite value of losses -- bad!"" ) elif kl > max_kl * 1.5 : logger . log ( ""violated KL constraint. shrinking step."" ) elif improve < 0 : logger . log ( ""surrogate didn't improve. shrinking step."" ) else : logger . log ( ""Stepsize OK!"" ) break stepsize *= .5 else : logger . log ( ""couldn't compute a good step"" ) set_from_flat ( thbefore ) if nworkers > 1 and iters_so_far % 20 == 0 : paramsums = MPI . COMM_WORLD . allgather ( ( thnew . sum ( ) , vfadam . getflat ( ) . sum ( ) ) ) assert all ( np . allclose ( ps , paramsums [ 0 ] ) for ps in paramsums [ 1 : ] ) for ( lossname , lossval ) in zip ( loss_names , meanlosses ) : logger . record_tabular ( lossname , lossval ) with timed ( ""vf"" ) : for _ in range ( vf_iters ) : for ( mbob , mbret ) in dataset . iterbatches ( ( seg [ ""ob"" ] , seg [ ""tdlamret"" ] ) , include_final_partial_batch = False , batch_size = 64 ) : g = allmean ( compute_vflossandgrad ( mbob , mbret ) ) vfadam . update ( g , vf_stepsize ) logger . record_tabular ( ""ev_tdlam_before"" , explained_variance ( vpredbefore , tdlamret ) ) lrlocal = ( seg [ ""ep_lens"" ] , seg [ ""ep_rets"" ] ) if MPI is not None : listoflrpairs = MPI . COMM_WORLD . allgather ( lrlocal ) else : listoflrpairs = [ lrlocal ] lens , rews = map ( flatten_lists , zip ( * listoflrpairs ) ) lenbuffer . extend ( lens ) rewbuffer . extend ( rews ) logger . record_tabular ( ""EpLenMean"" , np . mean ( lenbuffer ) ) logger . record_tabular ( ""EpRewMean"" , np . mean ( rewbuffer ) ) logger . record_tabular ( ""EpThisIter"" , len ( lens ) ) episodes_so_far += len ( lens ) timesteps_so_far += sum ( lens ) iters_so_far += 1 logger . record_tabular ( ""EpisodesSoFar"" , episodes_so_far ) logger . record_tabular ( ""TimestepsSoFar"" , timesteps_so_far ) logger . record_tabular ( ""TimeElapsed"" , time . time ( ) - tstart ) if rank == 0 : logger . dump_tabular ( ) return pi",learn a policy function with TRPO algorithm
"def discount ( x , gamma ) : assert x . ndim >= 1 return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ]",computes discounted sums along 0th dimension of x .
"def explained_variance ( ypred , y ) : assert y . ndim == 1 and ypred . ndim == 1 vary = np . var ( y ) return np . nan if vary == 0 else 1 - np . var ( y - ypred ) / vary",Computes fraction of variance that ypred explains about y . Returns 1 - Var [ y - ypred ] / Var [ y ]
"def discount_with_boundaries ( X , New , gamma ) : Y = np . zeros_like ( X ) T = X . shape [ 0 ] Y [ T - 1 ] = X [ T - 1 ] for t in range ( T - 2 , - 1 , - 1 ) : Y [ t ] = X [ t ] + gamma * Y [ t + 1 ] * ( 1 - New [ t + 1 ] ) return Y",X : 2d array of floats time x features New : 2d array of bools indicating when a new episode has started
"def sample ( self , batch_size ) : idxes = [ random . randint ( 0 , len ( self . _storage ) - 1 ) for _ in range ( batch_size ) ] return self . _encode_sample ( idxes )",Sample a batch of experiences .
"def add ( self , * args , * * kwargs ) : idx = self . _next_idx super ( ) . add ( * args , * * kwargs ) self . _it_sum [ idx ] = self . _max_priority ** self . _alpha self . _it_min [ idx ] = self . _max_priority ** self . _alpha",See ReplayBuffer . store_effect
"def sample ( self , batch_size , beta ) : assert beta > 0 idxes = self . _sample_proportional ( batch_size ) weights = [ ] p_min = self . _it_min . min ( ) / self . _it_sum . sum ( ) max_weight = ( p_min * len ( self . _storage ) ) ** ( - beta ) for idx in idxes : p_sample = self . _it_sum [ idx ] / self . _it_sum . sum ( ) weight = ( p_sample * len ( self . _storage ) ) ** ( - beta ) weights . append ( weight / max_weight ) weights = np . array ( weights ) encoded_sample = self . _encode_sample ( idxes ) return tuple ( list ( encoded_sample ) + [ weights , idxes ] )",Sample a batch of experiences .
"def update_priorities ( self , idxes , priorities ) : assert len ( idxes ) == len ( priorities ) for idx , priority in zip ( idxes , priorities ) : assert priority > 0 assert 0 <= idx < len ( self . _storage ) self . _it_sum [ idx ] = priority ** self . _alpha self . _it_min [ idx ] = priority ** self . _alpha self . _max_priority = max ( self . _max_priority , priority )",Update priorities of sampled transitions .
"def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : env = WarpFrame ( env ) env = ClipRewardEnv ( env ) if frame_stack > 1 : env = FrameStack ( env , frame_stack ) if scale : env = ScaledFloatFrame ( env ) return env",Configure environment for retro games using config similar to DeepMind - style Atari in wrap_deepmind
"def scope_vars ( scope , trainable_only = False ) : return tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES if trainable_only else tf . GraphKeys . GLOBAL_VARIABLES , scope = scope if isinstance ( scope , str ) else scope . name )",Get variables inside a scope The scope can be specified as a string Parameters ---------- scope : str or VariableScope scope in which the variables reside . trainable_only : bool whether or not to return only the variables that were marked as trainable . Returns ------- vars : [ tf . Variable ] list of variables in scope .
"def build_act ( make_obs_ph , q_func , num_actions , scope = ""deepq"" , reuse = None ) : with tf . variable_scope ( scope , reuse = reuse ) : observations_ph = make_obs_ph ( ""observation"" ) stochastic_ph = tf . placeholder ( tf . bool , ( ) , name = ""stochastic"" ) update_eps_ph = tf . placeholder ( tf . float32 , ( ) , name = ""update_eps"" ) eps = tf . get_variable ( ""eps"" , ( ) , initializer = tf . constant_initializer ( 0 ) ) q_values = q_func ( observations_ph . get ( ) , num_actions , scope = ""q_func"" ) deterministic_actions = tf . argmax ( q_values , axis = 1 ) batch_size = tf . shape ( observations_ph . get ( ) ) [ 0 ] random_actions = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = num_actions , dtype = tf . int64 ) chose_random = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = 1 , dtype = tf . float32 ) < eps stochastic_actions = tf . where ( chose_random , random_actions , deterministic_actions ) output_actions = tf . cond ( stochastic_ph , lambda : stochastic_actions , lambda : deterministic_actions ) update_eps_expr = eps . assign ( tf . cond ( update_eps_ph >= 0 , lambda : update_eps_ph , lambda : eps ) ) _act = U . function ( inputs = [ observations_ph , stochastic_ph , update_eps_ph ] , outputs = output_actions , givens = { update_eps_ph : - 1.0 , stochastic_ph : True } , updates = [ update_eps_expr ] ) def act ( ob , stochastic = True , update_eps = - 1 ) : return _act ( ob , stochastic , update_eps ) return act",Creates the act function :
"def build_act_with_param_noise ( make_obs_ph , q_func , num_actions , scope = ""deepq"" , reuse = None , param_noise_filter_func = None ) : if param_noise_filter_func is None : param_noise_filter_func = default_param_noise_filter with tf . variable_scope ( scope , reuse = reuse ) : observations_ph = make_obs_ph ( ""observation"" ) stochastic_ph = tf . placeholder ( tf . bool , ( ) , name = ""stochastic"" ) update_eps_ph = tf . placeholder ( tf . float32 , ( ) , name = ""update_eps"" ) update_param_noise_threshold_ph = tf . placeholder ( tf . float32 , ( ) , name = ""update_param_noise_threshold"" ) update_param_noise_scale_ph = tf . placeholder ( tf . bool , ( ) , name = ""update_param_noise_scale"" ) reset_ph = tf . placeholder ( tf . bool , ( ) , name = ""reset"" ) eps = tf . get_variable ( ""eps"" , ( ) , initializer = tf . constant_initializer ( 0 ) ) param_noise_scale = tf . get_variable ( ""param_noise_scale"" , ( ) , initializer = tf . constant_initializer ( 0.01 ) , trainable = False ) param_noise_threshold = tf . get_variable ( ""param_noise_threshold"" , ( ) , initializer = tf . constant_initializer ( 0.05 ) , trainable = False ) q_values = q_func ( observations_ph . get ( ) , num_actions , scope = ""q_func"" ) q_values_perturbed = q_func ( observations_ph . get ( ) , num_actions , scope = ""perturbed_q_func"" ) def perturb_vars ( original_scope , perturbed_scope ) : all_vars = scope_vars ( absolute_scope_name ( original_scope ) ) all_perturbed_vars = scope_vars ( absolute_scope_name ( perturbed_scope ) ) assert len ( all_vars ) == len ( all_perturbed_vars ) perturb_ops = [ ] for var , perturbed_var in zip ( all_vars , all_perturbed_vars ) : if param_noise_filter_func ( perturbed_var ) : op = tf . assign ( perturbed_var , var + tf . random_normal ( shape = tf . shape ( var ) , mean = 0. , stddev = param_noise_scale ) ) else : op = tf . assign ( perturbed_var , var ) perturb_ops . append ( op ) assert len ( perturb_ops ) == len ( all_vars ) return tf . group ( * perturb_ops ) q_values_adaptive = q_func ( observations_ph . get ( ) , num_actions , scope = ""adaptive_q_func"" ) perturb_for_adaption = perturb_vars ( original_scope = ""q_func"" , perturbed_scope = ""adaptive_q_func"" ) kl = tf . reduce_sum ( tf . nn . softmax ( q_values ) * ( tf . log ( tf . nn . softmax ( q_values ) ) - tf . log ( tf . nn . softmax ( q_values_adaptive ) ) ) , axis = - 1 ) mean_kl = tf . reduce_mean ( kl ) def update_scale ( ) : with tf . control_dependencies ( [ perturb_for_adaption ] ) : update_scale_expr = tf . cond ( mean_kl < param_noise_threshold , lambda : param_noise_scale . assign ( param_noise_scale * 1.01 ) , lambda : param_noise_scale . assign ( param_noise_scale / 1.01 ) , ) return update_scale_expr update_param_noise_threshold_expr = param_noise_threshold . assign ( tf . cond ( update_param_noise_threshold_ph >= 0 , lambda : update_param_noise_threshold_ph , lambda : param_noise_threshold ) ) deterministic_actions = tf . argmax ( q_values_perturbed , axis = 1 ) batch_size = tf . shape ( observations_ph . get ( ) ) [ 0 ] random_actions = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = num_actions , dtype = tf . int64 ) chose_random = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = 1 , dtype = tf . float32 ) < eps stochastic_actions = tf . where ( chose_random , random_actions , deterministic_actions ) output_actions = tf . cond ( stochastic_ph , lambda : stochastic_actions , lambda : deterministic_actions ) update_eps_expr = eps . assign ( tf . cond ( update_eps_ph >= 0 , lambda : update_eps_ph , lambda : eps ) ) updates = [ update_eps_expr , tf . cond ( reset_ph , lambda : perturb_vars ( original_scope = ""q_func"" , perturbed_scope = ""perturbed_q_func"" ) , lambda : tf . group ( * [ ] ) ) , tf . cond ( update_param_noise_scale_ph , lambda : update_scale ( ) , lambda : tf . Variable ( 0. , trainable = False ) ) , update_param_noise_threshold_expr , ] _act = U . function ( inputs = [ observations_ph , stochastic_ph , update_eps_ph , reset_ph , update_param_noise_threshold_ph , update_param_noise_scale_ph ] , outputs = output_actions , givens = { update_eps_ph : - 1.0 , stochastic_ph : True , reset_ph : False , update_param_noise_threshold_ph : False , update_param_noise_scale_ph : False } , updates = updates ) def act ( ob , reset = False , update_param_noise_threshold = False , update_param_noise_scale = False , stochastic = True , update_eps = - 1 ) : return _act ( ob , stochastic , update_eps , reset , update_param_noise_threshold , update_param_noise_scale ) return act",Creates the act function with support for parameter space noise exploration ( https : // arxiv . org / abs / 1706 . 01905 ) :
"def build_train ( make_obs_ph , q_func , num_actions , optimizer , grad_norm_clipping = None , gamma = 1.0 , double_q = True , scope = ""deepq"" , reuse = None , param_noise = False , param_noise_filter_func = None ) : if param_noise : act_f = build_act_with_param_noise ( make_obs_ph , q_func , num_actions , scope = scope , reuse = reuse , param_noise_filter_func = param_noise_filter_func ) else : act_f = build_act ( make_obs_ph , q_func , num_actions , scope = scope , reuse = reuse ) with tf . variable_scope ( scope , reuse = reuse ) : obs_t_input = make_obs_ph ( ""obs_t"" ) act_t_ph = tf . placeholder ( tf . int32 , [ None ] , name = ""action"" ) rew_t_ph = tf . placeholder ( tf . float32 , [ None ] , name = ""reward"" ) obs_tp1_input = make_obs_ph ( ""obs_tp1"" ) done_mask_ph = tf . placeholder ( tf . float32 , [ None ] , name = ""done"" ) importance_weights_ph = tf . placeholder ( tf . float32 , [ None ] , name = ""weight"" ) q_t = q_func ( obs_t_input . get ( ) , num_actions , scope = ""q_func"" , reuse = True ) q_func_vars = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = tf . get_variable_scope ( ) . name + ""/q_func"" ) q_tp1 = q_func ( obs_tp1_input . get ( ) , num_actions , scope = ""target_q_func"" ) target_q_func_vars = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = tf . get_variable_scope ( ) . name + ""/target_q_func"" ) q_t_selected = tf . reduce_sum ( q_t * tf . one_hot ( act_t_ph , num_actions ) , 1 ) if double_q : q_tp1_using_online_net = q_func ( obs_tp1_input . get ( ) , num_actions , scope = ""q_func"" , reuse = True ) q_tp1_best_using_online_net = tf . argmax ( q_tp1_using_online_net , 1 ) q_tp1_best = tf . reduce_sum ( q_tp1 * tf . one_hot ( q_tp1_best_using_online_net , num_actions ) , 1 ) else : q_tp1_best = tf . reduce_max ( q_tp1 , 1 ) q_tp1_best_masked = ( 1.0 - done_mask_ph ) * q_tp1_best q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked td_error = q_t_selected - tf . stop_gradient ( q_t_selected_target ) errors = U . huber_loss ( td_error ) weighted_error = tf . reduce_mean ( importance_weights_ph * errors ) if grad_norm_clipping is not None : gradients = optimizer . compute_gradients ( weighted_error , var_list = q_func_vars ) for i , ( grad , var ) in enumerate ( gradients ) : if grad is not None : gradients [ i ] = ( tf . clip_by_norm ( grad , grad_norm_clipping ) , var ) optimize_expr = optimizer . apply_gradients ( gradients ) else : optimize_expr = optimizer . minimize ( weighted_error , var_list = q_func_vars ) update_target_expr = [ ] for var , var_target in zip ( sorted ( q_func_vars , key = lambda v : v . name ) , sorted ( target_q_func_vars , key = lambda v : v . name ) ) : update_target_expr . append ( var_target . assign ( var ) ) update_target_expr = tf . group ( * update_target_expr ) train = U . function ( inputs = [ obs_t_input , act_t_ph , rew_t_ph , obs_tp1_input , done_mask_ph , importance_weights_ph ] , outputs = td_error , updates = [ optimize_expr ] ) update_target = U . function ( [ ] , [ ] , updates = [ update_target_expr ] ) q_values = U . function ( [ obs_t_input ] , q_t ) return act_f , train , update_target , { 'q_values' : q_values }",Creates the train function :
"def profile_tf_runningmeanstd ( ) : import time from baselines . common import tf_util tf_util . get_session ( config = tf . ConfigProto ( inter_op_parallelism_threads = 1 , intra_op_parallelism_threads = 1 , allow_soft_placement = True ) ) x = np . random . random ( ( 376 , ) ) n_trials = 10000 rms = RunningMeanStd ( ) tfrms = TfRunningMeanStd ( ) tic1 = time . time ( ) for _ in range ( n_trials ) : rms . update ( x ) tic2 = time . time ( ) for _ in range ( n_trials ) : tfrms . update ( x ) tic3 = time . time ( ) print ( 'rms update time ({} trials): {} s' . format ( n_trials , tic2 - tic1 ) ) print ( 'tfrms update time ({} trials): {} s' . format ( n_trials , tic3 - tic2 ) ) tic1 = time . time ( ) for _ in range ( n_trials ) : z1 = rms . mean tic2 = time . time ( ) for _ in range ( n_trials ) : z2 = tfrms . mean assert z1 == z2 tic3 = time . time ( ) print ( 'rms get mean time ({} trials): {} s' . format ( n_trials , tic2 - tic1 ) ) print ( 'tfrms get mean time ({} trials): {} s' . format ( n_trials , tic3 - tic2 ) )",options = tf . RunOptions ( trace_level = tf . RunOptions . FULL_TRACE ) #pylint : disable = E1101 run_metadata = tf . RunMetadata () profile_opts = dict ( options = options run_metadata = run_metadata )
"def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : if replay_strategy == 'future' : future_p = 1 - ( 1. / ( 1 + replay_k ) ) else : future_p = 0 def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : """"""episode_batch is {key: array(buffer_size x T x dim_key)}
        """""" T = episode_batch [ 'u' ] . shape [ 1 ] rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] batch_size = batch_size_in_transitions episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) t_samples = np . random . randint ( T , size = batch_size ) transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) for key in episode_batch . keys ( ) } her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) future_offset = future_offset . astype ( int ) future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] transitions [ 'g' ] [ her_indexes ] = future_ag info = { } for key , value in transitions . items ( ) : if key . startswith ( 'info_' ) : info [ key . replace ( 'info_' , '' ) ] = value reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } reward_params [ 'info' ] = info transitions [ 'r' ] = reward_fun ( * * reward_params ) transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) for k in transitions . keys ( ) } assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) return transitions return _sample_her_transitions",Creates a sample function that can be used for HER experience replay .
"def model ( inpt , num_actions , scope , reuse = False ) : with tf . variable_scope ( scope , reuse = reuse ) : out = inpt out = layers . fully_connected ( out , num_outputs = 64 , activation_fn = tf . nn . tanh ) out = layers . fully_connected ( out , num_outputs = num_actions , activation_fn = None ) return out",This model takes as input an observation and returns values of all actions .
"def sample ( self , batch_size ) : buffers = { } with self . lock : assert self . current_size > 0 for key in self . buffers . keys ( ) : buffers [ key ] = self . buffers [ key ] [ : self . current_size ] buffers [ 'o_2' ] = buffers [ 'o' ] [ : , 1 : , : ] buffers [ 'ag_2' ] = buffers [ 'ag' ] [ : , 1 : , : ] transitions = self . sample_transitions ( buffers , batch_size ) for key in ( [ 'r' , 'o_2' , 'ag_2' ] + list ( self . buffers . keys ( ) ) ) : assert key in transitions , ""key %s missing from transitions"" % key return transitions",Returns a dict { key : array ( batch_size x shapes [ key ] ) }
"def store_episode ( self , episode_batch ) : batch_sizes = [ len ( episode_batch [ key ] ) for key in episode_batch . keys ( ) ] assert np . all ( np . array ( batch_sizes ) == batch_sizes [ 0 ] ) batch_size = batch_sizes [ 0 ] with self . lock : idxs = self . _get_storage_idx ( batch_size ) for key in self . buffers . keys ( ) : self . buffers [ key ] [ idxs ] = episode_batch [ key ] self . n_transitions_stored += batch_size * self . T",episode_batch : array ( batch_size x ( T or T + 1 ) x dim_key )
"def store_episode ( self , episode_batch , update_stats = True ) : self . buffer . store_episode ( episode_batch ) if update_stats : episode_batch [ 'o_2' ] = episode_batch [ 'o' ] [ : , 1 : , : ] episode_batch [ 'ag_2' ] = episode_batch [ 'ag' ] [ : , 1 : , : ] num_normalizing_transitions = transitions_in_episode_batch ( episode_batch ) transitions = self . sample_transitions ( episode_batch , num_normalizing_transitions ) o , g , ag = transitions [ 'o' ] , transitions [ 'g' ] , transitions [ 'ag' ] transitions [ 'o' ] , transitions [ 'g' ] = self . _preprocess_og ( o , ag , g ) self . o_stats . update ( transitions [ 'o' ] ) self . g_stats . update ( transitions [ 'g' ] ) self . o_stats . recompute_stats ( ) self . g_stats . recompute_stats ( )",episode_batch : array of batch_size x ( T or T + 1 ) x dim_key o is of size T + 1 others are of size T
"def parse_cmdline_kwargs ( args ) : def parse ( v ) : assert isinstance ( v , str ) try : return eval ( v ) except ( NameError , SyntaxError ) : return v return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) }",convert a list of = - spaced command - line arguments to a dictionary evaluating python objects when possible
def cached_make_env ( make_env ) : if make_env not in CACHED_ENVS : env = make_env ( ) CACHED_ENVS [ make_env ] = env return CACHED_ENVS [ make_env ],Only creates a new environment from the provided function if one has not yet already been created . This is useful here because we need to infer certain properties of the env e . g . its observation and action spaces without any intend of actually using it .
"def compute_geometric_median ( X , eps = 1e-5 ) : y = np . mean ( X , 0 ) while True : D = scipy . spatial . distance . cdist ( X , [ y ] ) nonzeros = ( D != 0 ) [ : , 0 ] Dinv = 1 / D [ nonzeros ] Dinvs = np . sum ( Dinv ) W = Dinv / Dinvs T = np . sum ( W * X [ nonzeros ] , 0 ) num_zeros = len ( X ) - np . sum ( nonzeros ) if num_zeros == 0 : y1 = T elif num_zeros == len ( X ) : return y else : R = ( T - y ) * Dinvs r = np . linalg . norm ( R ) rinv = 0 if r == 0 else num_zeros / r y1 = max ( 0 , 1 - rinv ) * T + min ( 1 , rinv ) * y if scipy . spatial . distance . euclidean ( y , y1 ) < eps : return y1 y = y1",Estimate the geometric median of points in 2D .
"def project ( self , from_shape , to_shape ) : xy_proj = project_coords ( [ ( self . x , self . y ) ] , from_shape , to_shape ) return self . deepcopy ( x = xy_proj [ 0 ] [ 0 ] , y = xy_proj [ 0 ] [ 1 ] )",Project the keypoint onto a new position on a new image .
"def shift ( self , x = 0 , y = 0 ) : return self . deepcopy ( self . x + x , self . y + y )",Move the keypoint around on an image .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : if copy : image = np . copy ( image ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( ""Got a 2D image. Expected then 'color' to be a single number, "" ""but got %s."" % ( str ( color ) , ) ) elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] input_dtype = image . dtype alpha_color = color if alpha < 0.01 : return image elif alpha > 0.99 : alpha = 1 else : image = image . astype ( np . float32 , copy = False ) alpha_color = alpha * np . array ( color ) height , width = image . shape [ 0 : 2 ] y , x = self . y_int , self . x_int x1 = max ( x - size // 2 , 0 ) x2 = min ( x + 1 + size // 2 , width ) y1 = max ( y - size // 2 , 0 ) y2 = min ( y + 1 + size // 2 , height ) x1_clipped , x2_clipped = np . clip ( [ x1 , x2 ] , 0 , width ) y1_clipped , y2_clipped = np . clip ( [ y1 , y2 ] , 0 , height ) x1_clipped_ooi = ( x1_clipped < 0 or x1_clipped >= width ) x2_clipped_ooi = ( x2_clipped < 0 or x2_clipped >= width + 1 ) y1_clipped_ooi = ( y1_clipped < 0 or y1_clipped >= height ) y2_clipped_ooi = ( y2_clipped < 0 or y2_clipped >= height + 1 ) x_ooi = ( x1_clipped_ooi and x2_clipped_ooi ) y_ooi = ( y1_clipped_ooi and y2_clipped_ooi ) x_zero_size = ( x2_clipped - x1_clipped ) < 1 y_zero_size = ( y2_clipped - y1_clipped ) < 1 if not x_ooi and not y_ooi and not x_zero_size and not y_zero_size : if alpha == 1 : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = color else : image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] = ( ( 1 - alpha ) * image [ y1_clipped : y2_clipped , x1_clipped : x2_clipped ] + alpha_color ) else : if raise_if_out_of_image : raise Exception ( ""Cannot draw keypoint x=%.8f, y=%.8f on image with "" ""shape %s."" % ( y , x , image . shape ) ) if image . dtype . name != input_dtype . name : if input_dtype . name == ""uint8"" : image = np . clip ( image , 0 , 255 , out = image ) image = image . astype ( input_dtype , copy = False ) return image",Draw the keypoint onto a given image .
"def generate_similar_points_manhattan ( self , nb_steps , step_size , return_array = False ) : points = np . zeros ( ( nb_steps + 1 + nb_steps + 2 * ( nb_steps ** 2 ) , 2 ) , dtype = np . float32 ) yy = np . linspace ( self . y - nb_steps * step_size , self . y + nb_steps * step_size , nb_steps + 1 + nb_steps ) width = 1 nth_point = 0 for i_y , y in enumerate ( yy ) : if width == 1 : xx = [ self . x ] else : xx = np . linspace ( self . x - ( width - 1 ) // 2 * step_size , self . x + ( width - 1 ) // 2 * step_size , width ) for x in xx : points [ nth_point ] = [ x , y ] nth_point += 1 if i_y < nb_steps : width += 2 else : width -= 2 if return_array : return points return [ self . deepcopy ( x = points [ i , 0 ] , y = points [ i , 1 ] ) for i in sm . xrange ( points . shape [ 0 ] ) ]",Generate nearby points to this keypoint based on manhattan distance .
"def copy ( self , x = None , y = None ) : return self . deepcopy ( x = x , y = y )",Create a shallow copy of the Keypoint object .
"def deepcopy ( self , x = None , y = None ) : x = self . x if x is None else x y = self . y if y is None else y return Keypoint ( x = x , y = y )",Create a deep copy of the Keypoint object .
"def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) else : keypoints = [ kp . project ( self . shape , shape ) for kp in self . keypoints ] return self . deepcopy ( keypoints , shape )",Project keypoints from one image to a new one .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : image = np . copy ( image ) if copy else image for keypoint in self . keypoints : image = keypoint . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image",Draw all keypoints onto a given image .
"def shift ( self , x = 0 , y = 0 ) : keypoints = [ keypoint . shift ( x = x , y = y ) for keypoint in self . keypoints ] return self . deepcopy ( keypoints )",Move the keypoints around on an image .
"def to_xy_array ( self ) : result = np . zeros ( ( len ( self . keypoints ) , 2 ) , dtype = np . float32 ) for i , keypoint in enumerate ( self . keypoints ) : result [ i , 0 ] = keypoint . x result [ i , 1 ] = keypoint . y return result",Convert keypoint coordinates to ( N 2 ) array .
"def from_xy_array ( cls , xy , shape ) : keypoints = [ Keypoint ( x = coord [ 0 ] , y = coord [ 1 ] ) for coord in xy ] return KeypointsOnImage ( keypoints , shape )",Convert an array ( N 2 ) with a given image shape to a KeypointsOnImage object .
"def to_keypoint_image ( self , size = 1 ) : ia . do_assert ( len ( self . keypoints ) > 0 ) height , width = self . shape [ 0 : 2 ] image = np . zeros ( ( height , width , len ( self . keypoints ) ) , dtype = np . uint8 ) ia . do_assert ( size % 2 != 0 ) sizeh = max ( 0 , ( size - 1 ) // 2 ) for i , keypoint in enumerate ( self . keypoints ) : y = keypoint . y_int x = keypoint . x_int x1 = np . clip ( x - sizeh , 0 , width - 1 ) x2 = np . clip ( x + sizeh + 1 , 0 , width ) y1 = np . clip ( y - sizeh , 0 , height - 1 ) y2 = np . clip ( y + sizeh + 1 , 0 , height ) if x1 < x2 and y1 < y2 : image [ y1 : y2 , x1 : x2 , i ] = 128 if 0 <= y < height and 0 <= x < width : image [ y , x , i ] = 255 return image",Draws a new black image of shape ( H W N ) in which all keypoint coordinates are set to 255 . ( H = shape height W = shape width N = number of keypoints )
"def from_keypoint_image ( image , if_not_found_coords = { ""x"" : - 1 , ""y"" : - 1 } , threshold = 1 , nb_channels = None ) : ia . do_assert ( len ( image . shape ) == 3 ) height , width , nb_keypoints = image . shape drop_if_not_found = False if if_not_found_coords is None : drop_if_not_found = True if_not_found_x = - 1 if_not_found_y = - 1 elif isinstance ( if_not_found_coords , ( tuple , list ) ) : ia . do_assert ( len ( if_not_found_coords ) == 2 ) if_not_found_x = if_not_found_coords [ 0 ] if_not_found_y = if_not_found_coords [ 1 ] elif isinstance ( if_not_found_coords , dict ) : if_not_found_x = if_not_found_coords [ ""x"" ] if_not_found_y = if_not_found_coords [ ""y"" ] else : raise Exception ( ""Expected if_not_found_coords to be None or tuple or list or dict, got %s."" % ( type ( if_not_found_coords ) , ) ) keypoints = [ ] for i in sm . xrange ( nb_keypoints ) : maxidx_flat = np . argmax ( image [ ... , i ] ) maxidx_ndim = np . unravel_index ( maxidx_flat , ( height , width ) ) found = ( image [ maxidx_ndim [ 0 ] , maxidx_ndim [ 1 ] , i ] >= threshold ) if found : keypoints . append ( Keypoint ( x = maxidx_ndim [ 1 ] , y = maxidx_ndim [ 0 ] ) ) else : if drop_if_not_found : pass else : keypoints . append ( Keypoint ( x = if_not_found_x , y = if_not_found_y ) ) out_shape = ( height , width ) if nb_channels is not None : out_shape += ( nb_channels , ) return KeypointsOnImage ( keypoints , shape = out_shape )",Converts an image generated by to_keypoint_image () back to a KeypointsOnImage object .
"def to_distance_maps ( self , inverted = False ) : ia . do_assert ( len ( self . keypoints ) > 0 ) height , width = self . shape [ 0 : 2 ] distance_maps = np . zeros ( ( height , width , len ( self . keypoints ) ) , dtype = np . float32 ) yy = np . arange ( 0 , height ) xx = np . arange ( 0 , width ) grid_xx , grid_yy = np . meshgrid ( xx , yy ) for i , keypoint in enumerate ( self . keypoints ) : y , x = keypoint . y , keypoint . x distance_maps [ : , : , i ] = ( grid_xx - x ) ** 2 + ( grid_yy - y ) ** 2 distance_maps = np . sqrt ( distance_maps ) if inverted : return 1 / ( distance_maps + 1 ) return distance_maps",Generates a ( H W K ) output containing K distance maps for K keypoints .
"def from_distance_maps ( distance_maps , inverted = False , if_not_found_coords = { ""x"" : - 1 , ""y"" : - 1 } , threshold = None , nb_channels = None ) : ia . do_assert ( len ( distance_maps . shape ) == 3 ) height , width , nb_keypoints = distance_maps . shape drop_if_not_found = False if if_not_found_coords is None : drop_if_not_found = True if_not_found_x = - 1 if_not_found_y = - 1 elif isinstance ( if_not_found_coords , ( tuple , list ) ) : ia . do_assert ( len ( if_not_found_coords ) == 2 ) if_not_found_x = if_not_found_coords [ 0 ] if_not_found_y = if_not_found_coords [ 1 ] elif isinstance ( if_not_found_coords , dict ) : if_not_found_x = if_not_found_coords [ ""x"" ] if_not_found_y = if_not_found_coords [ ""y"" ] else : raise Exception ( ""Expected if_not_found_coords to be None or tuple or list or dict, got %s."" % ( type ( if_not_found_coords ) , ) ) keypoints = [ ] for i in sm . xrange ( nb_keypoints ) : if inverted : hitidx_flat = np . argmax ( distance_maps [ ... , i ] ) else : hitidx_flat = np . argmin ( distance_maps [ ... , i ] ) hitidx_ndim = np . unravel_index ( hitidx_flat , ( height , width ) ) if not inverted and threshold is not None : found = ( distance_maps [ hitidx_ndim [ 0 ] , hitidx_ndim [ 1 ] , i ] < threshold ) elif inverted and threshold is not None : found = ( distance_maps [ hitidx_ndim [ 0 ] , hitidx_ndim [ 1 ] , i ] >= threshold ) else : found = True if found : keypoints . append ( Keypoint ( x = hitidx_ndim [ 1 ] , y = hitidx_ndim [ 0 ] ) ) else : if drop_if_not_found : pass else : keypoints . append ( Keypoint ( x = if_not_found_x , y = if_not_found_y ) ) out_shape = ( height , width ) if nb_channels is not None : out_shape += ( nb_channels , ) return KeypointsOnImage ( keypoints , shape = out_shape )",Converts maps generated by to_distance_maps () back to a KeypointsOnImage object .
"def copy ( self , keypoints = None , shape = None ) : result = copy . copy ( self ) if keypoints is not None : result . keypoints = keypoints if shape is not None : result . shape = shape return result",Create a shallow copy of the KeypointsOnImage object .
"def deepcopy ( self , keypoints = None , shape = None ) : if keypoints is None : keypoints = [ kp . deepcopy ( ) for kp in self . keypoints ] if shape is None : shape = tuple ( self . shape ) return KeypointsOnImage ( keypoints , shape )",Create a deep copy of the KeypointsOnImage object .
"def contains ( self , other ) : if isinstance ( other , tuple ) : x , y = other else : x , y = other . x , other . y return self . x1 <= x <= self . x2 and self . y1 <= y <= self . y2",Estimate whether the bounding box contains a point .
"def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( [ ( self . x1 , self . y1 ) , ( self . x2 , self . y2 ) ] , from_shape , to_shape ) return self . copy ( x1 = coords_proj [ 0 ] [ 0 ] , y1 = coords_proj [ 0 ] [ 1 ] , x2 = coords_proj [ 1 ] [ 0 ] , y2 = coords_proj [ 1 ] [ 1 ] , label = self . label )",Project the bounding box onto a differently shaped image .
"def extend ( self , all_sides = 0 , top = 0 , right = 0 , bottom = 0 , left = 0 ) : return BoundingBox ( x1 = self . x1 - all_sides - left , x2 = self . x2 + all_sides + right , y1 = self . y1 - all_sides - top , y2 = self . y2 + all_sides + bottom )",Extend the size of the bounding box along its sides .
"def intersection ( self , other , default = None ) : x1_i = max ( self . x1 , other . x1 ) y1_i = max ( self . y1 , other . y1 ) x2_i = min ( self . x2 , other . x2 ) y2_i = min ( self . y2 , other . y2 ) if x1_i > x2_i or y1_i > y2_i : return default else : return BoundingBox ( x1 = x1_i , y1 = y1_i , x2 = x2_i , y2 = y2_i )",Compute the intersection bounding box of this bounding box and another one .
"def union ( self , other ) : return BoundingBox ( x1 = min ( self . x1 , other . x1 ) , y1 = min ( self . y1 , other . y1 ) , x2 = max ( self . x2 , other . x2 ) , y2 = max ( self . y2 , other . y2 ) , )",Compute the union bounding box of this bounding box and another one .
"def iou ( self , other ) : inters = self . intersection ( other ) if inters is None : return 0.0 else : area_union = self . area + other . area - inters . area return inters . area / area_union if area_union > 0 else 0.0",Compute the IoU of this bounding box with another one .
"def is_fully_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] return self . x1 >= 0 and self . x2 < width and self . y1 >= 0 and self . y2 < height",Estimate whether the bounding box is fully inside the image area .
"def is_partly_within_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] eps = np . finfo ( np . float32 ) . eps img_bb = BoundingBox ( x1 = 0 , x2 = width - eps , y1 = 0 , y2 = height - eps ) return self . intersection ( img_bb ) is not None",Estimate whether the bounding box is at least partially inside the image area .
"def is_out_of_image ( self , image , fully = True , partly = False ) : if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully",Estimate whether the bounding box is partially or fully outside of the image area .
"def clip_out_of_image ( self , image ) : shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] ia . do_assert ( height > 0 ) ia . do_assert ( width > 0 ) eps = np . finfo ( np . float32 ) . eps x1 = np . clip ( self . x1 , 0 , width - eps ) x2 = np . clip ( self . x2 , 0 , width - eps ) y1 = np . clip ( self . y1 , 0 , height - eps ) y2 = np . clip ( self . y2 , 0 , height - eps ) return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = self . label )",Clip off all parts of the bounding box that are outside of the image .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : top = top if top is not None else 0 right = right if right is not None else 0 bottom = bottom if bottom is not None else 0 left = left if left is not None else 0 return self . copy ( x1 = self . x1 + left - right , x2 = self . x2 + left - right , y1 = self . y1 + top - bottom , y2 = self . y2 + top - bottom )",Shift the bounding box from one or more image sides i . e . move it on the x / y - axis .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : if thickness is not None : ia . warn_deprecated ( ""Usage of argument 'thickness' in BoundingBox.draw_on_image() "" ""is deprecated. The argument was renamed to 'size'."" ) size = thickness if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( ""Cannot draw bounding box x1=%.8f, y1=%.8f, x2=%.8f, y2=%.8f on image with shape %s."" % ( self . x1 , self . y1 , self . x2 , self . y2 , image . shape ) ) result = np . copy ( image ) if copy else image if isinstance ( color , ( tuple , list ) ) : color = np . uint8 ( color ) for i in range ( size ) : y1 , y2 , x1 , x2 = self . y1_int , self . y2_int , self . x1_int , self . x2_int if self . is_fully_within_image ( image ) : y1 = np . clip ( y1 , 0 , image . shape [ 0 ] - 1 ) y2 = np . clip ( y2 , 0 , image . shape [ 0 ] - 1 ) x1 = np . clip ( x1 , 0 , image . shape [ 1 ] - 1 ) x2 = np . clip ( x2 , 0 , image . shape [ 1 ] - 1 ) y = [ y1 - i , y1 - i , y2 + i , y2 + i ] x = [ x1 - i , x2 + i , x2 + i , x1 - i ] rr , cc = skimage . draw . polygon_perimeter ( y , x , shape = result . shape ) if alpha >= 0.99 : result [ rr , cc , : ] = color else : if ia . is_float_array ( result ) : result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) else : input_dtype = result . dtype result = result . astype ( np . float32 ) result [ rr , cc , : ] = ( 1 - alpha ) * result [ rr , cc , : ] + alpha * color result = np . clip ( result , 0 , 255 ) . astype ( input_dtype ) return result",Draw the bounding box on an image .
"def extract_from_image ( self , image , pad = True , pad_max = None , prevent_zero_size = True ) : pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 height , width = image . shape [ 0 ] , image . shape [ 1 ] x1 , x2 , y1 , y2 = self . x1_int , self . x2_int , self . y1_int , self . y2_int fully_within = self . is_fully_within_image ( image ) if fully_within : y1 , y2 = np . clip ( [ y1 , y2 ] , 0 , height - 1 ) x1 , x2 = np . clip ( [ x1 , x2 ] , 0 , width - 1 ) if prevent_zero_size : if abs ( x2 - x1 ) < 1 : x2 = x1 + 1 if abs ( y2 - y1 ) < 1 : y2 = y1 + 1 if pad : if x1 < 0 : pad_left = abs ( x1 ) x2 = x2 + pad_left width = width + pad_left x1 = 0 if y1 < 0 : pad_top = abs ( y1 ) y2 = y2 + pad_top height = height + pad_top y1 = 0 if x2 >= width : pad_right = x2 - width if y2 >= height : pad_bottom = y2 - height paddings = [ pad_top , pad_right , pad_bottom , pad_left ] any_padded = any ( [ val > 0 for val in paddings ] ) if any_padded : if pad_max is None : pad_max = max ( paddings ) image = ia . pad ( image , top = min ( pad_top , pad_max ) , right = min ( pad_right , pad_max ) , bottom = min ( pad_bottom , pad_max ) , left = min ( pad_left , pad_max ) ) return image [ y1 : y2 , x1 : x2 ] else : within_image = ( ( 0 , 0 , 0 , 0 ) <= ( x1 , y1 , x2 , y2 ) < ( width , height , width , height ) ) out_height , out_width = ( y2 - y1 ) , ( x2 - x1 ) nonzero_height = ( out_height > 0 ) nonzero_width = ( out_width > 0 ) if within_image and nonzero_height and nonzero_width : return image [ y1 : y2 , x1 : x2 ] if prevent_zero_size : out_height = 1 out_width = 1 else : out_height = 0 out_width = 0 if image . ndim == 2 : return np . zeros ( ( out_height , out_width ) , dtype = image . dtype ) return np . zeros ( ( out_height , out_width , image . shape [ - 1 ] ) , dtype = image . dtype )",Extract the image pixels within the bounding box .
"def to_keypoints ( self ) : from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = self . x1 , y = self . y1 ) , Keypoint ( x = self . x2 , y = self . y1 ) , Keypoint ( x = self . x2 , y = self . y2 ) , Keypoint ( x = self . x1 , y = self . y2 ) ]",Convert the corners of the bounding box to keypoints ( clockwise starting at top left ) .
"def copy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return BoundingBox ( x1 = self . x1 if x1 is None else x1 , x2 = self . x2 if x2 is None else x2 , y1 = self . y1 if y1 is None else y1 , y2 = self . y2 if y2 is None else y2 , label = self . label if label is None else label )",Create a shallow copy of the BoundingBox object .
"def deepcopy ( self , x1 = None , y1 = None , x2 = None , y2 = None , label = None ) : return self . copy ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 , label = label )",Create a deep copy of the BoundingBox object .
"def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) bounding_boxes = [ bb . project ( self . shape , shape ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bounding_boxes , shape )",Project bounding boxes from one image to a new one .
"def from_xyxy_array ( cls , xyxy , shape ) : ia . do_assert ( xyxy . shape [ 1 ] == 4 , ""Expected input array of shape (N, 4), got shape %s."" % ( xyxy . shape , ) ) boxes = [ BoundingBox ( * row ) for row in xyxy ] return cls ( boxes , shape )",Convert an ( N 4 ) ndarray to a BoundingBoxesOnImage object .
"def to_xyxy_array ( self , dtype = np . float32 ) : xyxy_array = np . zeros ( ( len ( self . bounding_boxes ) , 4 ) , dtype = np . float32 ) for i , box in enumerate ( self . bounding_boxes ) : xyxy_array [ i ] = [ box . x1 , box . y1 , box . x2 , box . y2 ] return xyxy_array . astype ( dtype )",Convert the BoundingBoxesOnImage object to an ( N 4 ) ndarray .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 1 , copy = True , raise_if_out_of_image = False , thickness = None ) : image = np . copy ( image ) if copy else image for bb in self . bounding_boxes : image = bb . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = False , raise_if_out_of_image = raise_if_out_of_image , thickness = thickness ) return image",Draw all bounding boxes onto a given image .
"def remove_out_of_image ( self , fully = True , partly = False ) : bbs_clean = [ bb for bb in self . bounding_boxes if not bb . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return BoundingBoxesOnImage ( bbs_clean , shape = self . shape )",Remove all bounding boxes that are fully or partially outside of the image .
"def clip_out_of_image ( self ) : bbs_cut = [ bb . clip_out_of_image ( self . shape ) for bb in self . bounding_boxes if bb . is_partly_within_image ( self . shape ) ] return BoundingBoxesOnImage ( bbs_cut , shape = self . shape )",Clip off all parts from all bounding boxes that are outside of the image .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : bbs_new = [ bb . shift ( top = top , right = right , bottom = bottom , left = left ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs_new , shape = self . shape )",Shift all bounding boxes from one or more image sides i . e . move them on the x / y - axis .
"def deepcopy ( self ) : bbs = [ bb . deepcopy ( ) for bb in self . bounding_boxes ] return BoundingBoxesOnImage ( bbs , tuple ( self . shape ) )",Create a deep copy of the BoundingBoxesOnImage object .
"def Emboss ( alpha = 0 , strength = 1 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , ""alpha"" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) strength_param = iap . handle_continuous_param ( strength , ""strength"" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) strength_sample = strength_param . draw_sample ( random_state = random_state_func ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ - 1 - strength_sample , 0 - strength_sample , 0 ] , [ 0 - strength_sample , 1 , 0 + strength_sample ] , [ 0 , 0 + strength_sample , 1 + strength_sample ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that embosses images and overlays the result with the original image .
"def EdgeDetect ( alpha = 0 , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , ""alpha"" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix_effect = np . array ( [ [ 0 , 1 , 0 ] , [ 1 , - 4 , 1 ] , [ 0 , 1 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that detects all edges in images marks them in a black and white image and then overlays the result with the original image .
"def DirectedEdgeDetect ( alpha = 0 , direction = ( 0.0 , 1.0 ) , name = None , deterministic = False , random_state = None ) : alpha_param = iap . handle_continuous_param ( alpha , ""alpha"" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , ""direction"" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( _image , nb_channels , random_state_func ) : alpha_sample = alpha_param . draw_sample ( random_state = random_state_func ) ia . do_assert ( 0 <= alpha_sample <= 1.0 ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) deg = int ( direction_sample * 360 ) % 360 rad = np . deg2rad ( deg ) x = np . cos ( rad - 0.5 * np . pi ) y = np . sin ( rad - 0.5 * np . pi ) direction_vector = np . array ( [ x , y ] ) matrix_effect = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) for x in [ - 1 , 0 , 1 ] : for y in [ - 1 , 0 , 1 ] : if ( x , y ) != ( 0 , 0 ) : cell_vector = np . array ( [ x , y ] ) distance_deg = np . rad2deg ( ia . angle_between_vectors ( cell_vector , direction_vector ) ) distance = distance_deg / 180 similarity = ( 1 - distance ) ** 4 matrix_effect [ y + 1 , x + 1 ] = similarity matrix_effect = matrix_effect / np . sum ( matrix_effect ) matrix_effect = matrix_effect * ( - 1 ) matrix_effect [ 1 , 1 ] = 1 matrix_nochange = np . array ( [ [ 0 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 0 ] ] , dtype = np . float32 ) matrix = ( 1 - alpha_sample ) * matrix_nochange + alpha_sample * matrix_effect return [ matrix ] * nb_channels if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that detects edges that have certain directions and marks them in a black and white image and then overlays the result with the original image .
"def normalize_shape ( shape ) : if isinstance ( shape , tuple ) : return shape assert ia . is_np_array ( shape ) , ( ""Expected tuple of ints or array, got %s."" % ( type ( shape ) , ) ) return shape . shape",Normalize a shape tuple or array to a shape tuple .
"def project_coords ( coords , from_shape , to_shape ) : from_shape = normalize_shape ( from_shape ) to_shape = normalize_shape ( to_shape ) if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return coords from_height , from_width = from_shape [ 0 : 2 ] to_height , to_width = to_shape [ 0 : 2 ] assert all ( [ v > 0 for v in [ from_height , from_width , to_height , to_width ] ] ) coords_proj = np . array ( coords ) . astype ( np . float32 ) coords_proj [ : , 0 ] = ( coords_proj [ : , 0 ] / from_width ) * to_width coords_proj [ : , 1 ] = ( coords_proj [ : , 1 ] / from_height ) * to_height return coords_proj",Project coordinates from one image shape to another .
"def AdditiveGaussianNoise ( loc = 0 , scale = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : loc2 = iap . handle_continuous_param ( loc , ""loc"" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) scale2 = iap . handle_continuous_param ( scale , ""scale"" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return AddElementwise ( iap . Normal ( loc = loc2 , scale = scale2 ) , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Add gaussian noise ( aka white noise ) to images .
"def AdditivePoissonNoise ( lam = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : lam2 = iap . handle_continuous_param ( lam , ""lam"" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return AddElementwise ( iap . RandomSign ( iap . Poisson ( lam = lam2 ) ) , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Create an augmenter to add poisson noise to images .
"def Dropout ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( ""Expected p to be float or int or StochasticParameter, got %s."" % ( type ( p ) , ) ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p2 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that sets a certain fraction of pixels in images to zero .
"def CoarseDropout ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : if ia . is_single_number ( p ) : p2 = iap . Binomial ( 1 - p ) elif ia . is_iterable ( p ) : ia . do_assert ( len ( p ) == 2 ) ia . do_assert ( p [ 0 ] < p [ 1 ] ) ia . do_assert ( 0 <= p [ 0 ] <= 1.0 ) ia . do_assert ( 0 <= p [ 1 ] <= 1.0 ) p2 = iap . Binomial ( iap . Uniform ( 1 - p [ 1 ] , 1 - p [ 0 ] ) ) elif isinstance ( p , iap . StochasticParameter ) : p2 = p else : raise Exception ( ""Expected p to be float or int or StochasticParameter, got %s."" % ( type ( p ) , ) ) if size_px is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_px = size_px , min_size = min_size ) elif size_percent is not None : p3 = iap . FromLowerResolution ( other_param = p2 , size_percent = size_percent , min_size = min_size ) else : raise Exception ( ""Either size_px or size_percent must be set."" ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return MultiplyElementwise ( p3 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that sets rectangular areas within images to zero .
"def ImpulseNoise ( p = 0 , name = None , deterministic = False , random_state = None ) : return SaltAndPepper ( p = p , per_channel = True , name = name , deterministic = deterministic , random_state = random_state )",Creates an augmenter to apply impulse noise to an image .
"def SaltAndPepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = iap . Beta ( 0.5 , 0.5 ) * 255 , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Adds salt and pepper noise to an image i . e . some white - ish and black - ish pixels .
"def Pepper ( p = 0 , per_channel = False , name = None , deterministic = False , random_state = None ) : replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = ""invert"" ) + 0.5 replacement = replacement01 * 255 if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = p , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Adds pepper noise to an image i . e . black - ish pixels .
"def CoarsePepper ( p = 0 , size_px = None , size_percent = None , per_channel = False , min_size = 4 , name = None , deterministic = False , random_state = None ) : mask = iap . handle_probability_param ( p , ""p"" , tuple_to_uniform = True , list_to_choice = True ) if size_px is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_px = size_px , min_size = min_size ) elif size_percent is not None : mask_low = iap . FromLowerResolution ( other_param = mask , size_percent = size_percent , min_size = min_size ) else : raise Exception ( ""Either size_px or size_percent must be set."" ) replacement01 = iap . ForceSign ( iap . Beta ( 0.5 , 0.5 ) - 0.5 , positive = False , mode = ""invert"" ) + 0.5 replacement = replacement01 * 255 if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return ReplaceElementwise ( mask = mask_low , replacement = replacement , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Adds coarse pepper noise to an image i . e . rectangles that contain noisy black - ish pixels .
"def ContrastNormalization ( alpha = 1.0 , per_channel = False , name = None , deterministic = False , random_state = None ) : from . import contrast as contrast_lib return contrast_lib . LinearContrast ( alpha = alpha , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that changes the contrast of images .
"def is_single_float ( val ) : return isinstance ( val , numbers . Real ) and not is_single_integer ( val ) and not isinstance ( val , bool )",Checks whether a variable is a float .
"def is_integer_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . integer )",Checks whether a variable is a numpy integer array .
"def is_float_array ( val ) : return is_np_array ( val ) and issubclass ( val . dtype . type , np . floating )",Checks whether a variable is a numpy float array .
"def is_callable ( val ) : if sys . version_info [ 0 ] == 3 and sys . version_info [ 1 ] <= 2 : return hasattr ( val , '__call__' ) else : return callable ( val )",Checks whether a variable is a callable e . g . a function .
"def flatten ( nested_iterable ) : if not isinstance ( nested_iterable , ( list , tuple ) ) : yield nested_iterable else : for i in nested_iterable : if isinstance ( i , ( list , tuple ) ) : for j in flatten ( i ) : yield j else : yield i",Flattens arbitrarily nested lists / tuples .
"def new_random_state ( seed = None , fully_random = False ) : if seed is None : if not fully_random : seed = CURRENT_RANDOM_STATE . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return np . random . RandomState ( seed )",Returns a new random state .
"def copy_random_state ( random_state , force_copy = False ) : if random_state == np . random and not force_copy : return random_state else : rs_copy = dummy_random_state ( ) orig_state = random_state . get_state ( ) rs_copy . set_state ( orig_state ) return rs_copy",Creates a copy of a random state .
"def derive_random_states ( random_state , n = 1 ) : seed_ = random_state . randint ( SEED_MIN_VALUE , SEED_MAX_VALUE , 1 ) [ 0 ] return [ new_random_state ( seed_ + i ) for i in sm . xrange ( n ) ]",Create N new random states based on an existing random state or seed .
"def _quokka_normalize_extract ( extract ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage if extract == ""square"" : bb = BoundingBox ( x1 = 0 , y1 = 0 , x2 = 643 , y2 = 643 ) elif isinstance ( extract , tuple ) and len ( extract ) == 4 : bb = BoundingBox ( x1 = extract [ 0 ] , y1 = extract [ 1 ] , x2 = extract [ 2 ] , y2 = extract [ 3 ] ) elif isinstance ( extract , BoundingBox ) : bb = extract elif isinstance ( extract , BoundingBoxesOnImage ) : do_assert ( len ( extract . bounding_boxes ) == 1 ) do_assert ( extract . shape [ 0 : 2 ] == ( 643 , 960 ) ) bb = extract . bounding_boxes [ 0 ] else : raise Exception ( ""Expected 'square' or tuple of four entries or BoundingBox or BoundingBoxesOnImage "" + ""for parameter 'extract', got %s."" % ( type ( extract ) , ) ) return bb",Generate a normalized rectangle to be extract from the standard quokka image .
"def _compute_resized_shape ( from_shape , to_shape ) : if is_np_array ( from_shape ) : from_shape = from_shape . shape if is_np_array ( to_shape ) : to_shape = to_shape . shape to_shape_computed = list ( from_shape ) if to_shape is None : pass elif isinstance ( to_shape , tuple ) : do_assert ( len ( from_shape ) in [ 2 , 3 ] ) do_assert ( len ( to_shape ) in [ 2 , 3 ] ) if len ( from_shape ) == 3 and len ( to_shape ) == 3 : do_assert ( from_shape [ 2 ] == to_shape [ 2 ] ) elif len ( to_shape ) == 3 : to_shape_computed . append ( to_shape [ 2 ] ) do_assert ( all ( [ v is None or is_single_number ( v ) for v in to_shape [ 0 : 2 ] ] ) , ""Expected the first two entries in to_shape to be None or numbers, "" + ""got types %s."" % ( str ( [ type ( v ) for v in to_shape [ 0 : 2 ] ] ) , ) ) for i , from_shape_i in enumerate ( from_shape [ 0 : 2 ] ) : if to_shape [ i ] is None : to_shape_computed [ i ] = from_shape_i elif is_single_integer ( to_shape [ i ] ) : to_shape_computed [ i ] = to_shape [ i ] else : to_shape_computed [ i ] = int ( np . round ( from_shape_i * to_shape [ i ] ) ) elif is_single_integer ( to_shape ) or is_single_float ( to_shape ) : to_shape_computed = _compute_resized_shape ( from_shape , ( to_shape , to_shape ) ) else : raise Exception ( ""Expected to_shape to be None or ndarray or tuple of floats or tuple of ints or single int "" + ""or single float, got %s."" % ( type ( to_shape ) , ) ) return tuple ( to_shape_computed )",Computes the intended new shape of an image - like array after resizing .
"def quokka ( size = None , extract = None ) : img = imageio . imread ( QUOKKA_FP , pilmode = ""RGB"" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is not None : shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) return img",Returns an image of a quokka as a numpy array .
"def quokka_heatmap ( size = None , extract = None ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage img = imageio . imread ( QUOKKA_DEPTH_MAP_HALFRES_FP , pilmode = ""RGB"" ) img = imresize_single_image ( img , ( 643 , 960 ) , interpolation = ""cubic"" ) if extract is not None : bb = _quokka_normalize_extract ( extract ) img = bb . extract_from_image ( img ) if size is None : size = img . shape [ 0 : 2 ] shape_resized = _compute_resized_shape ( img . shape , size ) img = imresize_single_image ( img , shape_resized [ 0 : 2 ] ) img_0to1 = img [ ... , 0 ] img_0to1 = img_0to1 . astype ( np . float32 ) / 255.0 img_0to1 = 1 - img_0to1 return HeatmapsOnImage ( img_0to1 , shape = img_0to1 . shape [ 0 : 2 ] + ( 3 , ) )",Returns a heatmap ( here : depth map ) for the standard example quokka image .
"def quokka_segmentation_map ( size = None , extract = None ) : from imgaug . augmentables . segmaps import SegmentationMapOnImage with open ( QUOKKA_ANNOTATIONS_FP , ""r"" ) as f : json_dict = json . load ( f ) xx = [ ] yy = [ ] for kp_dict in json_dict [ ""polygons"" ] [ 0 ] [ ""keypoints"" ] : x = kp_dict [ ""x"" ] y = kp_dict [ ""y"" ] xx . append ( x ) yy . append ( y ) img_seg = np . zeros ( ( 643 , 960 , 1 ) , dtype = np . float32 ) rr , cc = skimage . draw . polygon ( np . array ( yy ) , np . array ( xx ) , shape = img_seg . shape ) img_seg [ rr , cc ] = 1.0 if extract is not None : bb = _quokka_normalize_extract ( extract ) img_seg = bb . extract_from_image ( img_seg ) segmap = SegmentationMapOnImage ( img_seg , shape = img_seg . shape [ 0 : 2 ] + ( 3 , ) ) if size is not None : shape_resized = _compute_resized_shape ( img_seg . shape , size ) segmap = segmap . resize ( shape_resized [ 0 : 2 ] ) segmap . shape = tuple ( shape_resized [ 0 : 2 ] ) + ( 3 , ) return segmap",Returns a segmentation map for the standard example quokka image .
"def quokka_keypoints ( size = None , extract = None ) : from imgaug . augmentables . kps import Keypoint , KeypointsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , ""r"" ) as f : json_dict = json . load ( f ) keypoints = [ ] for kp_dict in json_dict [ ""keypoints"" ] : keypoints . append ( Keypoint ( x = kp_dict [ ""x"" ] - left , y = kp_dict [ ""y"" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) kpsoi = KeypointsOnImage ( keypoints , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) kpsoi = kpsoi . on ( shape_resized ) return kpsoi",Returns example keypoints on the standard example quokke image .
"def quokka_bounding_boxes ( size = None , extract = None ) : from imgaug . augmentables . bbs import BoundingBox , BoundingBoxesOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , ""r"" ) as f : json_dict = json . load ( f ) bbs = [ ] for bb_dict in json_dict [ ""bounding_boxes"" ] : bbs . append ( BoundingBox ( x1 = bb_dict [ ""x1"" ] - left , y1 = bb_dict [ ""y1"" ] - top , x2 = bb_dict [ ""x2"" ] - left , y2 = bb_dict [ ""y2"" ] - top ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) bbsoi = BoundingBoxesOnImage ( bbs , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) bbsoi = bbsoi . on ( shape_resized ) return bbsoi",Returns example bounding boxes on the standard example quokke image .
"def quokka_polygons ( size = None , extract = None ) : from imgaug . augmentables . polys import Polygon , PolygonsOnImage left , top = 0 , 0 if extract is not None : bb_extract = _quokka_normalize_extract ( extract ) left = bb_extract . x1 top = bb_extract . y1 with open ( QUOKKA_ANNOTATIONS_FP , ""r"" ) as f : json_dict = json . load ( f ) polygons = [ ] for poly_json in json_dict [ ""polygons"" ] : polygons . append ( Polygon ( [ ( point [ ""x"" ] - left , point [ ""y"" ] - top ) for point in poly_json [ ""keypoints"" ] ] ) ) if extract is not None : shape = ( bb_extract . height , bb_extract . width , 3 ) else : shape = ( 643 , 960 , 3 ) psoi = PolygonsOnImage ( polygons , shape = shape ) if size is not None : shape_resized = _compute_resized_shape ( shape , size ) psoi = psoi . on ( shape_resized ) return psoi",Returns example polygons on the standard example quokke image .
"def angle_between_vectors ( v1 , v2 ) : l1 = np . linalg . norm ( v1 ) l2 = np . linalg . norm ( v2 ) v1_u = ( v1 / l1 ) if l1 > 0 else np . float32 ( v1 ) * 0 v2_u = ( v2 / l2 ) if l2 > 0 else np . float32 ( v2 ) * 0 return np . arccos ( np . clip ( np . dot ( v1_u , v2_u ) , - 1.0 , 1.0 ) )",Returns the angle in radians between vectors v1 and v2 .
"def compute_line_intersection_point ( x1 , y1 , x2 , y2 , x3 , y3 , x4 , y4 ) : def _make_line ( p1 , p2 ) : A = ( p1 [ 1 ] - p2 [ 1 ] ) B = ( p2 [ 0 ] - p1 [ 0 ] ) C = ( p1 [ 0 ] * p2 [ 1 ] - p2 [ 0 ] * p1 [ 1 ] ) return A , B , - C L1 = _make_line ( ( x1 , y1 ) , ( x2 , y2 ) ) L2 = _make_line ( ( x3 , y3 ) , ( x4 , y4 ) ) D = L1 [ 0 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 0 ] Dx = L1 [ 2 ] * L2 [ 1 ] - L1 [ 1 ] * L2 [ 2 ] Dy = L1 [ 0 ] * L2 [ 2 ] - L1 [ 2 ] * L2 [ 0 ] if D != 0 : x = Dx / D y = Dy / D return x , y else : return False",Compute the intersection point of two lines .
"def draw_text ( img , y , x , text , color = ( 0 , 255 , 0 ) , size = 25 ) : do_assert ( img . dtype in [ np . uint8 , np . float32 ] ) input_dtype = img . dtype if img . dtype == np . float32 : img = img . astype ( np . uint8 ) img = PIL_Image . fromarray ( img ) font = PIL_ImageFont . truetype ( DEFAULT_FONT_FP , size ) context = PIL_ImageDraw . Draw ( img ) context . text ( ( x , y ) , text , fill = tuple ( color ) , font = font ) img_np = np . asarray ( img ) if not img_np . flags [ ""WRITEABLE"" ] : try : img_np . setflags ( write = True ) except ValueError as ex : if ""cannot set WRITEABLE flag to True of this array"" in str ( ex ) : img_np = np . copy ( img_np ) if img_np . dtype != input_dtype : img_np = img_np . astype ( input_dtype ) return img_np",Draw text on an image .
"def imresize_many_images ( images , sizes = None , interpolation = None ) : if len ( images ) == 0 : return images do_assert ( all ( [ image . shape [ 0 ] > 0 and image . shape [ 1 ] > 0 for image in images ] ) , ( ""Cannot resize images, because at least one image has a height and/or width of zero. "" + ""Observed shapes were: %s."" ) % ( str ( [ image . shape for image in images ] ) , ) ) if is_single_number ( sizes ) and sizes <= 0 : raise Exception ( ""Cannot resize to the target size %.8f, because the value is zero or lower than zero."" % ( sizes , ) ) elif isinstance ( sizes , tuple ) and ( sizes [ 0 ] <= 0 or sizes [ 1 ] <= 0 ) : sizes_str = [ ""int %d"" % ( sizes [ 0 ] , ) if is_single_integer ( sizes [ 0 ] ) else ""float %.8f"" % ( sizes [ 0 ] , ) , ""int %d"" % ( sizes [ 1 ] , ) if is_single_integer ( sizes [ 1 ] ) else ""float %.8f"" % ( sizes [ 1 ] , ) , ] sizes_str = ""(%s, %s)"" % ( sizes_str [ 0 ] , sizes_str [ 1 ] ) raise Exception ( ""Cannot resize to the target sizes %s. At least one value is zero or lower than zero."" % ( sizes_str , ) ) if is_single_number ( sizes ) : sizes = ( sizes , sizes ) else : do_assert ( len ( sizes ) == 2 , ""Expected tuple with exactly two entries, got %d entries."" % ( len ( sizes ) , ) ) do_assert ( all ( [ is_single_number ( val ) for val in sizes ] ) , ""Expected tuple with two ints or floats, got types %s."" % ( str ( [ type ( val ) for val in sizes ] ) , ) ) if isinstance ( images , list ) : nb_shapes = len ( set ( [ image . shape for image in images ] ) ) if nb_shapes == 1 : return list ( imresize_many_images ( np . array ( images ) , sizes = sizes , interpolation = interpolation ) ) else : return [ imresize_many_images ( image [ np . newaxis , ... ] , sizes = sizes , interpolation = interpolation ) [ 0 , ... ] for image in images ] shape = images . shape do_assert ( images . ndim in [ 3 , 4 ] , ""Expected array of shape (N, H, W, [C]), got shape %s"" % ( str ( shape ) , ) ) nb_images = shape [ 0 ] im_height , im_width = shape [ 1 ] , shape [ 2 ] nb_channels = shape [ 3 ] if images . ndim > 3 else None height , width = sizes [ 0 ] , sizes [ 1 ] height = int ( np . round ( im_height * height ) ) if is_single_float ( height ) else height width = int ( np . round ( im_width * width ) ) if is_single_float ( width ) else width if height == im_height and width == im_width : return np . copy ( images ) ip = interpolation do_assert ( ip is None or ip in IMRESIZE_VALID_INTERPOLATIONS ) if ip is None : if height > im_height or width > im_width : ip = cv2 . INTER_AREA else : ip = cv2 . INTER_LINEAR elif ip in [ ""nearest"" , cv2 . INTER_NEAREST ] : ip = cv2 . INTER_NEAREST elif ip in [ ""linear"" , cv2 . INTER_LINEAR ] : ip = cv2 . INTER_LINEAR elif ip in [ ""area"" , cv2 . INTER_AREA ] : ip = cv2 . INTER_AREA else : ip = cv2 . INTER_CUBIC from . import dtypes as iadt if ip == cv2 . INTER_NEAREST : iadt . gate_dtypes ( images , allowed = [ ""bool"" , ""uint8"" , ""uint16"" , ""int8"" , ""int16"" , ""int32"" , ""float16"" , ""float32"" , ""float64"" ] , disallowed = [ ""uint32"" , ""uint64"" , ""uint128"" , ""uint256"" , ""int64"" , ""int128"" , ""int256"" , ""float96"" , ""float128"" , ""float256"" ] , augmenter = None ) else : iadt . gate_dtypes ( images , allowed = [ ""bool"" , ""uint8"" , ""uint16"" , ""int8"" , ""int16"" , ""float16"" , ""float32"" , ""float64"" ] , disallowed = [ ""uint32"" , ""uint64"" , ""uint128"" , ""uint256"" , ""int32"" , ""int64"" , ""int128"" , ""int256"" , ""float96"" , ""float128"" , ""float256"" ] , augmenter = None ) result_shape = ( nb_images , height , width ) if nb_channels is not None : result_shape = result_shape + ( nb_channels , ) result = np . zeros ( result_shape , dtype = images . dtype ) for i , image in enumerate ( images ) : input_dtype = image . dtype if image . dtype . type == np . bool_ : image = image . astype ( np . uint8 ) * 255 elif image . dtype . type == np . int8 and ip != cv2 . INTER_NEAREST : image = image . astype ( np . int16 ) elif image . dtype . type == np . float16 : image = image . astype ( np . float32 ) result_img = cv2 . resize ( image , ( width , height ) , interpolation = ip ) assert result_img . dtype == image . dtype if len ( result_img . shape ) == 2 and nb_channels is not None and nb_channels == 1 : result_img = result_img [ : , : , np . newaxis ] if input_dtype . type == np . bool_ : result_img = result_img > 127 elif input_dtype . type == np . int8 and ip != cv2 . INTER_NEAREST : from . import dtypes as iadt result_img = iadt . restore_dtypes_ ( result_img , np . int8 ) elif input_dtype . type == np . float16 : from . import dtypes as iadt result_img = iadt . restore_dtypes_ ( result_img , np . float16 ) result [ i ] = result_img return result",Resize many images to a specified size .
"def imresize_single_image ( image , sizes , interpolation = None ) : grayscale = False if image . ndim == 2 : grayscale = True image = image [ : , : , np . newaxis ] do_assert ( len ( image . shape ) == 3 , image . shape ) rs = imresize_many_images ( image [ np . newaxis , : , : , : ] , sizes , interpolation = interpolation ) if grayscale : return np . squeeze ( rs [ 0 , : , : , 0 ] ) else : return rs [ 0 , ... ]",Resizes a single image .
"def pad ( arr , top = 0 , right = 0 , bottom = 0 , left = 0 , mode = ""constant"" , cval = 0 ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( top >= 0 ) do_assert ( right >= 0 ) do_assert ( bottom >= 0 ) do_assert ( left >= 0 ) if top > 0 or right > 0 or bottom > 0 or left > 0 : mapping_mode_np_to_cv2 = { ""constant"" : cv2 . BORDER_CONSTANT , ""edge"" : cv2 . BORDER_REPLICATE , ""linear_ramp"" : None , ""maximum"" : None , ""mean"" : None , ""median"" : None , ""minimum"" : None , ""reflect"" : cv2 . BORDER_REFLECT_101 , ""symmetric"" : cv2 . BORDER_REFLECT , ""wrap"" : None , cv2 . BORDER_CONSTANT : cv2 . BORDER_CONSTANT , cv2 . BORDER_REPLICATE : cv2 . BORDER_REPLICATE , cv2 . BORDER_REFLECT_101 : cv2 . BORDER_REFLECT_101 , cv2 . BORDER_REFLECT : cv2 . BORDER_REFLECT } bad_mode_cv2 = mapping_mode_np_to_cv2 . get ( mode , None ) is None bad_datatype_cv2 = arr . dtype . name in [ ""uint32"" , ""uint64"" , ""int64"" , ""float16"" , ""float128"" , ""bool"" ] if not bad_datatype_cv2 and not bad_mode_cv2 : cval = float ( cval ) if arr . dtype . kind == ""f"" else int ( cval ) if arr . ndim == 2 or arr . shape [ 2 ] <= 4 : if arr . ndim == 3 : cval = tuple ( [ cval ] * arr . shape [ 2 ] ) arr_pad = cv2 . copyMakeBorder ( arr , top = top , bottom = bottom , left = left , right = right , borderType = mapping_mode_np_to_cv2 [ mode ] , value = cval ) if arr . ndim == 3 and arr_pad . ndim == 2 : arr_pad = arr_pad [ ... , np . newaxis ] else : result = [ ] channel_start_idx = 0 while channel_start_idx < arr . shape [ 2 ] : arr_c = arr [ ... , channel_start_idx : channel_start_idx + 4 ] cval_c = tuple ( [ cval ] * arr_c . shape [ 2 ] ) arr_pad_c = cv2 . copyMakeBorder ( arr_c , top = top , bottom = bottom , left = left , right = right , borderType = mapping_mode_np_to_cv2 [ mode ] , value = cval_c ) arr_pad_c = np . atleast_3d ( arr_pad_c ) result . append ( arr_pad_c ) channel_start_idx += 4 arr_pad = np . concatenate ( result , axis = 2 ) else : paddings_np = [ ( top , bottom ) , ( left , right ) ] if arr . ndim == 3 : paddings_np . append ( ( 0 , 0 ) ) if mode == ""constant"" : arr_pad = np . pad ( arr , paddings_np , mode = mode , constant_values = cval ) elif mode == ""linear_ramp"" : arr_pad = np . pad ( arr , paddings_np , mode = mode , end_values = cval ) else : arr_pad = np . pad ( arr , paddings_np , mode = mode ) return arr_pad return np . copy ( arr )",Pad an image - like array on its top / right / bottom / left side .
"def compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) : do_assert ( arr . ndim in [ 2 , 3 ] ) do_assert ( aspect_ratio > 0 ) height , width = arr . shape [ 0 : 2 ] do_assert ( height > 0 ) aspect_ratio_current = width / height pad_top = 0 pad_right = 0 pad_bottom = 0 pad_left = 0 if aspect_ratio_current < aspect_ratio : diff = ( aspect_ratio * height ) - width pad_right = int ( np . ceil ( diff / 2 ) ) pad_left = int ( np . floor ( diff / 2 ) ) elif aspect_ratio_current > aspect_ratio : diff = ( ( 1 / aspect_ratio ) * width ) - height pad_top = int ( np . floor ( diff / 2 ) ) pad_bottom = int ( np . ceil ( diff / 2 ) ) return pad_top , pad_right , pad_bottom , pad_left",Compute the amount of pixels by which an array has to be padded to fulfill an aspect ratio .
"def pad_to_aspect_ratio ( arr , aspect_ratio , mode = ""constant"" , cval = 0 , return_pad_amounts = False ) : pad_top , pad_right , pad_bottom , pad_left = compute_paddings_for_aspect_ratio ( arr , aspect_ratio ) arr_padded = pad ( arr , top = pad_top , right = pad_right , bottom = pad_bottom , left = pad_left , mode = mode , cval = cval ) if return_pad_amounts : return arr_padded , ( pad_top , pad_right , pad_bottom , pad_left ) else : return arr_padded",Pad an image - like array on its sides so that it matches a target aspect ratio .
"def pool ( arr , block_size , func , cval = 0 , preserve_dtype = True ) : from . import dtypes as iadt iadt . gate_dtypes ( arr , allowed = [ ""bool"" , ""uint8"" , ""uint16"" , ""uint32"" , ""int8"" , ""int16"" , ""int32"" , ""float16"" , ""float32"" , ""float64"" , ""float128"" ] , disallowed = [ ""uint64"" , ""uint128"" , ""uint256"" , ""int64"" , ""int128"" , ""int256"" , ""float256"" ] , augmenter = None ) do_assert ( arr . ndim in [ 2 , 3 ] ) is_valid_int = is_single_integer ( block_size ) and block_size >= 1 is_valid_tuple = is_iterable ( block_size ) and len ( block_size ) in [ 2 , 3 ] and [ is_single_integer ( val ) and val >= 1 for val in block_size ] do_assert ( is_valid_int or is_valid_tuple ) if is_single_integer ( block_size ) : block_size = [ block_size , block_size ] if len ( block_size ) < arr . ndim : block_size = list ( block_size ) + [ 1 ] input_dtype = arr . dtype arr_reduced = skimage . measure . block_reduce ( arr , tuple ( block_size ) , func , cval = cval ) if preserve_dtype and arr_reduced . dtype . type != input_dtype : arr_reduced = arr_reduced . astype ( input_dtype ) return arr_reduced",Resize an array by pooling values within blocks .
"def avg_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . average , cval = cval , preserve_dtype = preserve_dtype )",Resize an array using average pooling .
"def max_pool ( arr , block_size , cval = 0 , preserve_dtype = True ) : return pool ( arr , block_size , np . max , cval = cval , preserve_dtype = preserve_dtype )",Resize an array using max - pooling .
"def draw_grid ( images , rows = None , cols = None ) : nb_images = len ( images ) do_assert ( nb_images > 0 ) if is_np_array ( images ) : do_assert ( images . ndim == 4 ) else : do_assert ( is_iterable ( images ) and is_np_array ( images [ 0 ] ) and images [ 0 ] . ndim == 3 ) dts = [ image . dtype . name for image in images ] nb_dtypes = len ( set ( dts ) ) do_assert ( nb_dtypes == 1 , ( ""All images provided to draw_grid() must have the same dtype, "" + ""found %d dtypes (%s)"" ) % ( nb_dtypes , "", "" . join ( dts ) ) ) cell_height = max ( [ image . shape [ 0 ] for image in images ] ) cell_width = max ( [ image . shape [ 1 ] for image in images ] ) channels = set ( [ image . shape [ 2 ] for image in images ] ) do_assert ( len ( channels ) == 1 , ""All images are expected to have the same number of channels, "" + ""but got channel set %s with length %d instead."" % ( str ( channels ) , len ( channels ) ) ) nb_channels = list ( channels ) [ 0 ] if rows is None and cols is None : rows = cols = int ( math . ceil ( math . sqrt ( nb_images ) ) ) elif rows is not None : cols = int ( math . ceil ( nb_images / rows ) ) elif cols is not None : rows = int ( math . ceil ( nb_images / cols ) ) do_assert ( rows * cols >= nb_images ) width = cell_width * cols height = cell_height * rows dt = images . dtype if is_np_array ( images ) else images [ 0 ] . dtype grid = np . zeros ( ( height , width , nb_channels ) , dtype = dt ) cell_idx = 0 for row_idx in sm . xrange ( rows ) : for col_idx in sm . xrange ( cols ) : if cell_idx < nb_images : image = images [ cell_idx ] cell_y1 = cell_height * row_idx cell_y2 = cell_y1 + image . shape [ 0 ] cell_x1 = cell_width * col_idx cell_x2 = cell_x1 + image . shape [ 1 ] grid [ cell_y1 : cell_y2 , cell_x1 : cell_x2 , : ] = image cell_idx += 1 return grid",Converts multiple input images into a single image showing them in a grid .
"def show_grid ( images , rows = None , cols = None ) : grid = draw_grid ( images , rows = rows , cols = cols ) imshow ( grid )",Converts the input images to a grid image and shows it in a new window .
"def imshow ( image , backend = IMSHOW_BACKEND_DEFAULT ) : do_assert ( backend in [ ""matplotlib"" , ""cv2"" ] , ""Expected backend 'matplotlib' or 'cv2', got %s."" % ( backend , ) ) if backend == ""cv2"" : image_bgr = image if image . ndim == 3 and image . shape [ 2 ] in [ 3 , 4 ] : image_bgr = image [ ... , 0 : 3 ] [ ... , : : - 1 ] win_name = ""imgaug-default-window"" cv2 . namedWindow ( win_name , cv2 . WINDOW_NORMAL ) cv2 . imshow ( win_name , image_bgr ) cv2 . waitKey ( 0 ) cv2 . destroyWindow ( win_name ) else : import matplotlib . pyplot as plt dpi = 96 h , w = image . shape [ 0 ] / dpi , image . shape [ 1 ] / dpi w = max ( w , 6 ) fig , ax = plt . subplots ( figsize = ( w , h ) , dpi = dpi ) fig . canvas . set_window_title ( ""imgaug.imshow(%s)"" % ( image . shape , ) ) ax . imshow ( image , cmap = ""gray"" ) plt . show ( )",Shows an image in a window .
"def warn_deprecated ( msg , stacklevel = 2 ) : import warnings warnings . warn ( msg , category = DeprecationWarning , stacklevel = stacklevel )",Generate a non - silent deprecation warning with stacktrace .
"def is_activated ( self , images , augmenter , parents , default ) : if self . activator is None : return default else : return self . activator ( images , augmenter , parents , default )",Returns whether an augmenter may be executed .
"def is_propagating ( self , images , augmenter , parents , default ) : if self . propagator is None : return default else : return self . propagator ( images , augmenter , parents , default )",Returns whether an augmenter may call its children to augment an image . This is independent of the augmenter itself possible changing the image without calling its children . ( Most ( all? ) augmenters with children currently dont perform any changes themselves . )
"def preprocess ( self , images , augmenter , parents ) : if self . preprocessor is None : return images else : return self . preprocessor ( images , augmenter , parents )",A function to be called before the augmentation of images starts ( per augmenter ) .
"def postprocess ( self , images , augmenter , parents ) : if self . postprocessor is None : return images else : return self . postprocessor ( images , augmenter , parents )",A function to be called after the augmentation of images was performed .
"def pool ( self ) : if self . _pool is None : processes = self . processes if processes is not None and processes < 0 : try : processes = multiprocessing . cpu_count ( ) - abs ( processes ) processes = max ( processes , 1 ) except ( ImportError , NotImplementedError ) : processes = None self . _pool = multiprocessing . Pool ( processes , initializer = _Pool_initialize_worker , initargs = ( self . augseq , self . seed ) , maxtasksperchild = self . maxtasksperchild ) return self . _pool",Return the multiprocessing . Pool instance or create it if not done yet .
"def map_batches ( self , batches , chunksize = None ) : assert isinstance ( batches , list ) , ( ""Expected to get a list as 'batches', got type %s. "" + ""Call imap_batches() if you use generators."" ) % ( type ( batches ) , ) return self . pool . map ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize )",Augment batches .
"def map_batches_async ( self , batches , chunksize = None , callback = None , error_callback = None ) : assert isinstance ( batches , list ) , ( ""Expected to get a list as 'batches', got type %s. "" + ""Call imap_batches() if you use generators."" ) % ( type ( batches ) , ) return self . pool . map_async ( _Pool_starworker , self . _handle_batch_ids ( batches ) , chunksize = chunksize , callback = callback , error_callback = error_callback )",Augment batches asynchonously .
"def imap_batches ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( ""Expected to get a generator as 'batches', got type %s. "" + ""Call map_batches() if you use lists."" ) % ( type ( batches ) , ) gen = self . pool . imap ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch",Augment batches from a generator .
"def imap_batches_unordered ( self , batches , chunksize = 1 ) : assert ia . is_generator ( batches ) , ( ""Expected to get a generator as 'batches', got type %s. "" + ""Call map_batches() if you use lists."" ) % ( type ( batches ) , ) gen = self . pool . imap_unordered ( _Pool_starworker , self . _handle_batch_ids_gen ( batches ) , chunksize = chunksize ) for batch in gen : yield batch",Augment batches from a generator in a way that does not guarantee to preserve order .
def terminate ( self ) : if self . _pool is not None : self . _pool . terminate ( ) self . _pool . join ( ) self . _pool = None,Terminate the pool immediately .
"def terminate ( self ) : if not self . join_signal . is_set ( ) : self . join_signal . set ( ) time . sleep ( 0.01 ) if self . main_worker_thread . is_alive ( ) : self . main_worker_thread . join ( ) if self . threaded : for worker in self . workers : if worker . is_alive ( ) : worker . join ( ) else : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) worker . join ( ) while not self . all_finished ( ) : time . sleep ( 0.001 ) if self . queue . full ( ) : self . queue . get ( ) self . queue . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 ) while True : try : self . _queue_internal . get ( timeout = 0.005 ) except QueueEmpty : break if not self . _queue_internal . _closed : self . _queue_internal . close ( ) if not self . queue . _closed : self . queue . close ( ) self . _queue_internal . join_thread ( ) self . queue . join_thread ( ) time . sleep ( 0.025 )",Stop all workers .
def get_batch ( self ) : if self . all_finished ( ) : return None batch_str = self . queue_result . get ( ) batch = pickle . loads ( batch_str ) if batch is not None : return batch else : self . nb_workers_finished += 1 if self . nb_workers_finished >= self . nb_workers : try : self . queue_source . get ( timeout = 0.001 ) except QueueEmpty : pass return None else : return self . get_batch ( ),Returns a batch from the queue of augmented batches .
"def _augment_images_worker ( self , augseq , queue_source , queue_result , seedval ) : np . random . seed ( seedval ) random . seed ( seedval ) augseq . reseed ( seedval ) ia . seed ( seedval ) loader_finished = False while not loader_finished : try : batch_str = queue_source . get ( timeout = 0.1 ) batch = pickle . loads ( batch_str ) if batch is None : loader_finished = True queue_source . put ( pickle . dumps ( None , protocol = - 1 ) ) else : batch_aug = augseq . augment_batch ( batch ) batch_str = pickle . dumps ( batch_aug , protocol = - 1 ) queue_result . put ( batch_str ) except QueueEmpty : time . sleep ( 0.01 ) queue_result . put ( pickle . dumps ( None , protocol = - 1 ) ) time . sleep ( 0.01 )",Augment endlessly images in the source queue .
def terminate ( self ) : for worker in self . workers : if worker . is_alive ( ) : worker . terminate ( ) self . nb_workers_finished = len ( self . workers ) if not self . queue_result . _closed : self . queue_result . close ( ) time . sleep ( 0.01 ),Terminates all background processes immediately .
"def to_normalized_batch ( self ) : assert all ( [ attr is None for attr_name , attr in self . __dict__ . items ( ) if attr_name . endswith ( ""_aug"" ) ] ) , ""Expected UnnormalizedBatch to not contain any augmented data "" ""before normalization, but at least one '*_aug' attribute was "" ""already set."" images_unaug = nlib . normalize_images ( self . images_unaug ) shapes = None if images_unaug is not None : shapes = [ image . shape for image in images_unaug ] return Batch ( images = images_unaug , heatmaps = nlib . normalize_heatmaps ( self . heatmaps_unaug , shapes ) , segmentation_maps = nlib . normalize_segmentation_maps ( self . segmentation_maps_unaug , shapes ) , keypoints = nlib . normalize_keypoints ( self . keypoints_unaug , shapes ) , bounding_boxes = nlib . normalize_bounding_boxes ( self . bounding_boxes_unaug , shapes ) , polygons = nlib . normalize_polygons ( self . polygons_unaug , shapes ) , line_strings = nlib . normalize_line_strings ( self . line_strings_unaug , shapes ) , data = self . data )",Convert this unnormalized batch to an instance of Batch .
"def fill_from_augmented_normalized_batch ( self , batch_aug_norm ) : batch = UnnormalizedBatch ( images = self . images_unaug , heatmaps = self . heatmaps_unaug , segmentation_maps = self . segmentation_maps_unaug , keypoints = self . keypoints_unaug , bounding_boxes = self . bounding_boxes_unaug , polygons = self . polygons_unaug , line_strings = self . line_strings_unaug , data = batch_aug_norm . data ) batch . images_aug = nlib . invert_normalize_images ( batch_aug_norm . images_aug , self . images_unaug ) batch . heatmaps_aug = nlib . invert_normalize_heatmaps ( batch_aug_norm . heatmaps_aug , self . heatmaps_unaug ) batch . segmentation_maps_aug = nlib . invert_normalize_segmentation_maps ( batch_aug_norm . segmentation_maps_aug , self . segmentation_maps_unaug ) batch . keypoints_aug = nlib . invert_normalize_keypoints ( batch_aug_norm . keypoints_aug , self . keypoints_unaug ) batch . bounding_boxes_aug = nlib . invert_normalize_bounding_boxes ( batch_aug_norm . bounding_boxes_aug , self . bounding_boxes_unaug ) batch . polygons_aug = nlib . invert_normalize_polygons ( batch_aug_norm . polygons_aug , self . polygons_unaug ) batch . line_strings_aug = nlib . invert_normalize_line_strings ( batch_aug_norm . line_strings_aug , self . line_strings_unaug ) return batch",Fill this batch with ( normalized ) augmentation results .
"def Positive ( other_param , mode = ""invert"" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = True , mode = mode , reroll_count_max = reroll_count_max )",Converts another parameter s results to positive values .
"def Negative ( other_param , mode = ""invert"" , reroll_count_max = 2 ) : return ForceSign ( other_param = other_param , positive = False , mode = mode , reroll_count_max = reroll_count_max )",Converts another parameter s results to negative values .
"def create_for_noise ( other_param , threshold = ( - 10 , 10 ) , activated = True ) : return Sigmoid ( other_param , threshold , activated , mul = 20 , add = - 10 )",Creates a Sigmoid that is adjusted to be used with noise parameters i . e . with parameters which s output values are in the range [ 0 . 0 1 . 0 ] .
"def area ( self ) : if len ( self . exterior ) < 3 : raise Exception ( ""Cannot compute the polygon's area because it contains less than three points."" ) poly = self . to_shapely_polygon ( ) return poly . area",Estimate the area of the polygon .
"def project ( self , from_shape , to_shape ) : if from_shape [ 0 : 2 ] == to_shape [ 0 : 2 ] : return self . copy ( ) ls_proj = self . to_line_string ( closed = False ) . project ( from_shape , to_shape ) return self . copy ( exterior = ls_proj . coords )",Project the polygon onto an image with different shape .
"def find_closest_point_index ( self , x , y , return_distance = False ) : ia . do_assert ( len ( self . exterior ) > 0 ) distances = [ ] for x2 , y2 in self . exterior : d = ( x2 - x ) ** 2 + ( y2 - y ) ** 2 distances . append ( d ) distances = np . sqrt ( distances ) closest_idx = np . argmin ( distances ) if return_distance : return closest_idx , distances [ closest_idx ] return closest_idx",Find the index of the point within the exterior that is closest to the given coordinates .
"def is_fully_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = True )",Estimate whether the polygon is fully inside the image area .
"def is_partly_within_image ( self , image ) : return not self . is_out_of_image ( image , fully = True , partly = False )",Estimate whether the polygon is at least partially inside the image area .
"def is_out_of_image ( self , image , fully = True , partly = False ) : if len ( self . exterior ) == 0 : raise Exception ( ""Cannot determine whether the polygon is inside the image, because it contains no points."" ) ls = self . to_line_string ( ) return ls . is_out_of_image ( image , fully = fully , partly = partly )",Estimate whether the polygon is partially or fully outside of the image area .
"def clip_out_of_image ( self , image ) : import shapely . geometry if self . is_out_of_image ( image , fully = True , partly = False ) : return [ ] h , w = image . shape [ 0 : 2 ] if ia . is_np_array ( image ) else image [ 0 : 2 ] poly_shapely = self . to_shapely_polygon ( ) poly_image = shapely . geometry . Polygon ( [ ( 0 , 0 ) , ( w , 0 ) , ( w , h ) , ( 0 , h ) ] ) multipoly_inter_shapely = poly_shapely . intersection ( poly_image ) if not isinstance ( multipoly_inter_shapely , shapely . geometry . MultiPolygon ) : ia . do_assert ( isinstance ( multipoly_inter_shapely , shapely . geometry . Polygon ) ) multipoly_inter_shapely = shapely . geometry . MultiPolygon ( [ multipoly_inter_shapely ] ) polygons = [ ] for poly_inter_shapely in multipoly_inter_shapely . geoms : polygons . append ( Polygon . from_shapely ( poly_inter_shapely , label = self . label ) ) polygons_reordered = [ ] for polygon in polygons : found = False for x , y in self . exterior : closest_idx , dist = polygon . find_closest_point_index ( x = x , y = y , return_distance = True ) if dist < 1e-6 : polygon_reordered = polygon . change_first_point_by_index ( closest_idx ) polygons_reordered . append ( polygon_reordered ) found = True break ia . do_assert ( found ) return polygons_reordered",Cut off all parts of the polygon that are outside of the image .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : ls_shifted = self . to_line_string ( closed = False ) . shift ( top = top , right = right , bottom = bottom , left = left ) return self . copy ( exterior = ls_shifted . coords )",Shift the polygon from one or more image sides i . e . move it on the x / y - axis .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_face = None , color_lines = None , color_points = None , alpha = 1.0 , alpha_face = None , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_face = color_face if color_face is not None else np . array ( color ) color_lines = color_lines if color_lines is not None else np . array ( color ) * 0.5 color_points = color_points if color_points is not None else np . array ( color ) * 0.5 alpha_face = alpha_face if alpha_face is not None else alpha * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else alpha alpha_points = alpha_points if alpha_points is not None else alpha size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 if image . ndim == 2 : assert ia . is_single_number ( color_face ) , ( ""Got a 2D image. Expected then 'color_face' to be a single "" ""number, but got %s."" % ( str ( color_face ) , ) ) color_face = [ color_face ] elif image . ndim == 3 and ia . is_single_number ( color_face ) : color_face = [ color_face ] * image . shape [ - 1 ] if alpha_face < 0.01 : alpha_face = 0 elif alpha_face > 0.99 : alpha_face = 1 if raise_if_out_of_image and self . is_out_of_image ( image ) : raise Exception ( ""Cannot draw polygon %s on image with shape %s."" % ( str ( self ) , image . shape ) ) input_dtype = image . dtype result = image . astype ( np . float32 ) rr , cc = skimage . draw . polygon ( self . yy_int , self . xx_int , shape = image . shape ) if len ( rr ) > 0 : if alpha_face == 1 : result [ rr , cc ] = np . float32 ( color_face ) elif alpha_face == 0 : pass else : result [ rr , cc ] = ( ( 1 - alpha_face ) * result [ rr , cc , : ] + alpha_face * np . float32 ( color_face ) ) ls_open = self . to_line_string ( closed = False ) ls_closed = self . to_line_string ( closed = True ) result = ls_closed . draw_lines_on_image ( result , color = color_lines , alpha = alpha_lines , size = size_lines , raise_if_out_of_image = raise_if_out_of_image ) result = ls_open . draw_points_on_image ( result , color = color_points , alpha = alpha_points , size = size_points , raise_if_out_of_image = raise_if_out_of_image ) if input_dtype . type == np . uint8 : result = np . clip ( np . round ( result ) , 0 , 255 ) . astype ( input_dtype ) else : result = result . astype ( input_dtype ) return result",Draw the polygon on an image .
"def extract_from_image ( self , image ) : ia . do_assert ( image . ndim in [ 2 , 3 ] ) if len ( self . exterior ) <= 2 : raise Exception ( ""Polygon must be made up of at least 3 points to extract its area from an image."" ) bb = self . to_bounding_box ( ) bb_area = bb . extract_from_image ( image ) if self . is_out_of_image ( image , fully = True , partly = False ) : return bb_area xx = self . xx_int yy = self . yy_int xx_mask = xx - np . min ( xx ) yy_mask = yy - np . min ( yy ) height_mask = np . max ( yy_mask ) width_mask = np . max ( xx_mask ) rr_face , cc_face = skimage . draw . polygon ( yy_mask , xx_mask , shape = ( height_mask , width_mask ) ) mask = np . zeros ( ( height_mask , width_mask ) , dtype = np . bool ) mask [ rr_face , cc_face ] = True if image . ndim == 3 : mask = np . tile ( mask [ : , : , np . newaxis ] , ( 1 , 1 , image . shape [ 2 ] ) ) return bb_area * mask",Extract the image pixels within the polygon .
"def change_first_point_by_coords ( self , x , y , max_distance = 1e-4 , raise_if_too_far_away = True ) : if len ( self . exterior ) == 0 : raise Exception ( ""Cannot reorder polygon points, because it contains no points."" ) closest_idx , closest_dist = self . find_closest_point_index ( x = x , y = y , return_distance = True ) if max_distance is not None and closest_dist > max_distance : if not raise_if_too_far_away : return self . deepcopy ( ) closest_point = self . exterior [ closest_idx , : ] raise Exception ( ""Closest found point (%.9f, %.9f) exceeds max_distance of %.9f exceeded"" % ( closest_point [ 0 ] , closest_point [ 1 ] , closest_dist ) ) return self . change_first_point_by_index ( closest_idx )",Set the first point of the exterior to the given point based on its coordinates .
"def change_first_point_by_index ( self , point_idx ) : ia . do_assert ( 0 <= point_idx < len ( self . exterior ) ) if point_idx == 0 : return self . deepcopy ( ) exterior = np . concatenate ( ( self . exterior [ point_idx : , : ] , self . exterior [ : point_idx , : ] ) , axis = 0 ) return self . deepcopy ( exterior = exterior )",Set the first point of the exterior to the given point based on its index .
"def to_shapely_polygon ( self ) : import shapely . geometry return shapely . geometry . Polygon ( [ ( point [ 0 ] , point [ 1 ] ) for point in self . exterior ] )",Convert this polygon to a Shapely polygon .
"def to_shapely_line_string ( self , closed = False , interpolate = 0 ) : return _convert_points_to_shapely_line_string ( self . exterior , closed = closed , interpolate = interpolate )",Convert this polygon to a Shapely LineString object .
"def to_bounding_box ( self ) : from imgaug . augmentables . bbs import BoundingBox xx = self . xx yy = self . yy return BoundingBox ( x1 = min ( xx ) , x2 = max ( xx ) , y1 = min ( yy ) , y2 = max ( yy ) , label = self . label )",Convert this polygon to a bounding box tightly containing the whole polygon .
"def to_keypoints ( self ) : from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = point [ 0 ] , y = point [ 1 ] ) for point in self . exterior ]",Convert this polygon s exterior to Keypoint instances .
"def to_line_string ( self , closed = True ) : from imgaug . augmentables . lines import LineString if not closed or len ( self . exterior ) <= 1 : return LineString ( self . exterior , label = self . label ) return LineString ( np . concatenate ( [ self . exterior , self . exterior [ 0 : 1 , : ] ] , axis = 0 ) , label = self . label )",Convert this polygon s exterior to a LineString instance .
"def from_shapely ( polygon_shapely , label = None ) : import shapely . geometry ia . do_assert ( isinstance ( polygon_shapely , shapely . geometry . Polygon ) ) if polygon_shapely . exterior is None or len ( polygon_shapely . exterior . coords ) == 0 : return Polygon ( [ ] , label = label ) exterior = np . float32 ( [ [ x , y ] for ( x , y ) in polygon_shapely . exterior . coords ] ) return Polygon ( exterior , label = label )",Create a polygon from a Shapely polygon .
"def exterior_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , list ) : other = Polygon ( np . float32 ( other ) ) elif ia . is_np_array ( other ) : other = Polygon ( other ) else : assert isinstance ( other , Polygon ) other = other return self . to_line_string ( closed = True ) . coords_almost_equals ( other . to_line_string ( closed = True ) , max_distance = max_distance , points_per_edge = points_per_edge )",Estimate if this and other polygon s exterior are almost identical .
"def almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if not isinstance ( other , Polygon ) : return False if self . label is not None or other . label is not None : if self . label is None : return False if other . label is None : return False if self . label != other . label : return False return self . exterior_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )",Estimate if this polygon s and another s geometry / labels are similar .
"def copy ( self , exterior = None , label = None ) : return self . deepcopy ( exterior = exterior , label = label )",Create a shallow copy of the Polygon object .
"def deepcopy ( self , exterior = None , label = None ) : return Polygon ( exterior = np . copy ( self . exterior ) if exterior is None else exterior , label = self . label if label is None else label )",Create a deep copy of the Polygon object .
"def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) polygons = [ poly . project ( self . shape , shape ) for poly in self . polygons ] return PolygonsOnImage ( polygons , shape )",Project polygons from one image to a new one .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_face = None , color_lines = None , color_points = None , alpha = 1.0 , alpha_face = None , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , raise_if_out_of_image = False ) : for poly in self . polygons : image = poly . draw_on_image ( image , color = color , color_face = color_face , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_face = alpha_face , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) return image",Draw all polygons onto a given image .
"def remove_out_of_image ( self , fully = True , partly = False ) : polys_clean = [ poly for poly in self . polygons if not poly . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return PolygonsOnImage ( polys_clean , shape = self . shape )",Remove all polygons that are fully or partially outside of the image .
"def clip_out_of_image ( self ) : polys_cut = [ poly . clip_out_of_image ( self . shape ) for poly in self . polygons if poly . is_partly_within_image ( self . shape ) ] polys_cut_flat = [ poly for poly_lst in polys_cut for poly in poly_lst ] return PolygonsOnImage ( polys_cut_flat , shape = self . shape )",Clip off all parts from all polygons that are outside of the image .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : polys_new = [ poly . shift ( top = top , right = right , bottom = bottom , left = left ) for poly in self . polygons ] return PolygonsOnImage ( polys_new , shape = self . shape )",Shift all polygons from one or more image sides i . e . move them on the x / y - axis .
"def deepcopy ( self ) : polys = [ poly . deepcopy ( ) for poly in self . polygons ] return PolygonsOnImage ( polys , tuple ( self . shape ) )",Create a deep copy of the PolygonsOnImage object .
"def from_shapely ( geometry , label = None ) : import shapely . geometry if isinstance ( geometry , shapely . geometry . MultiPolygon ) : return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) elif isinstance ( geometry , shapely . geometry . Polygon ) : return MultiPolygon ( [ Polygon . from_shapely ( geometry , label = label ) ] ) elif isinstance ( geometry , shapely . geometry . collection . GeometryCollection ) : ia . do_assert ( all ( [ isinstance ( poly , shapely . geometry . Polygon ) for poly in geometry . geoms ] ) ) return MultiPolygon ( [ Polygon . from_shapely ( poly , label = label ) for poly in geometry . geoms ] ) else : raise Exception ( ""Unknown datatype '%s'. Expected shapely.geometry.Polygon or "" ""shapely.geometry.MultiPolygon or "" ""shapely.geometry.collections.GeometryCollection."" % ( type ( geometry ) , ) )",Create a MultiPolygon from a Shapely MultiPolygon a Shapely Polygon or a Shapely GeometryCollection .
"def Pad ( px = None , percent = None , pad_mode = ""constant"" , pad_cval = 0 , keep_size = True , sample_independently = True , name = None , deterministic = False , random_state = None ) : def recursive_validate ( v ) : if v is None : return v elif ia . is_single_number ( v ) : ia . do_assert ( v >= 0 ) return v elif isinstance ( v , iap . StochasticParameter ) : return v elif isinstance ( v , tuple ) : return tuple ( [ recursive_validate ( v_ ) for v_ in v ] ) elif isinstance ( v , list ) : return [ recursive_validate ( v_ ) for v_ in v ] else : raise Exception ( ""Expected None or int or float or StochasticParameter or list or tuple, got %s."" % ( type ( v ) , ) ) px = recursive_validate ( px ) percent = recursive_validate ( percent ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) aug = CropAndPad ( px = px , percent = percent , pad_mode = pad_mode , pad_cval = pad_cval , keep_size = keep_size , sample_independently = sample_independently , name = name , deterministic = deterministic , random_state = random_state ) return aug",Augmenter that pads images i . e . adds columns / rows to them .
"def Crop ( px = None , percent = None , keep_size = True , sample_independently = True , name = None , deterministic = False , random_state = None ) : def recursive_negate ( v ) : if v is None : return v elif ia . is_single_number ( v ) : ia . do_assert ( v >= 0 ) return - v elif isinstance ( v , iap . StochasticParameter ) : return iap . Multiply ( v , - 1 ) elif isinstance ( v , tuple ) : return tuple ( [ recursive_negate ( v_ ) for v_ in v ] ) elif isinstance ( v , list ) : return [ recursive_negate ( v_ ) for v_ in v ] else : raise Exception ( ""Expected None or int or float or StochasticParameter or list or tuple, got %s."" % ( type ( v ) , ) ) px = recursive_negate ( px ) percent = recursive_negate ( percent ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) aug = CropAndPad ( px = px , percent = percent , keep_size = keep_size , sample_independently = sample_independently , name = name , deterministic = deterministic , random_state = random_state ) return aug",Augmenter that crops / cuts away pixels at the sides of the image .
"def isect_segments__naive ( segments ) : isect = [ ] if Real is float : segments = [ ( s [ 0 ] , s [ 1 ] ) if s [ 0 ] [ X ] <= s [ 1 ] [ X ] else ( s [ 1 ] , s [ 0 ] ) for s in segments ] else : segments = [ ( ( Real ( s [ 0 ] [ 0 ] ) , Real ( s [ 0 ] [ 1 ] ) ) , ( Real ( s [ 1 ] [ 0 ] ) , Real ( s [ 1 ] [ 1 ] ) ) , ) if ( s [ 0 ] <= s [ 1 ] ) else ( ( Real ( s [ 1 ] [ 0 ] ) , Real ( s [ 1 ] [ 1 ] ) ) , ( Real ( s [ 0 ] [ 0 ] ) , Real ( s [ 0 ] [ 1 ] ) ) , ) for s in segments ] n = len ( segments ) for i in range ( n ) : a0 , a1 = segments [ i ] for j in range ( i + 1 , n ) : b0 , b1 = segments [ j ] if a0 not in ( b0 , b1 ) and a1 not in ( b0 , b1 ) : ix = isect_seg_seg_v2_point ( a0 , a1 , b0 , b1 ) if ix is not None : isect . append ( ix ) return isect",Brute force O ( n2 ) version of isect_segments for test validation .
"def isect_polygon__naive ( points ) : isect = [ ] n = len ( points ) if Real is float : pass else : points = [ ( Real ( p [ 0 ] ) , Real ( p [ 1 ] ) ) for p in points ] for i in range ( n ) : a0 , a1 = points [ i ] , points [ ( i + 1 ) % n ] for j in range ( i + 1 , n ) : b0 , b1 = points [ j ] , points [ ( j + 1 ) % n ] if a0 not in ( b0 , b1 ) and a1 not in ( b0 , b1 ) : ix = isect_seg_seg_v2_point ( a0 , a1 , b0 , b1 ) if ix is not None : if USE_IGNORE_SEGMENT_ENDINGS : if ( ( len_squared_v2v2 ( ix , a0 ) < NUM_EPS_SQ or len_squared_v2v2 ( ix , a1 ) < NUM_EPS_SQ ) and ( len_squared_v2v2 ( ix , b0 ) < NUM_EPS_SQ or len_squared_v2v2 ( ix , b1 ) < NUM_EPS_SQ ) ) : continue isect . append ( ix ) return isect",Brute force O ( n2 ) version of isect_polygon for test validation .
"def get_intersections ( self ) : if Real is float : return list ( self . intersections . keys ( ) ) else : return [ ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) for p in self . intersections . keys ( ) ]",Return a list of unordered intersection points .
"def get_intersections_with_segments ( self ) : if Real is float : return [ ( p , [ event . segment for event in event_set ] ) for p , event_set in self . intersections . items ( ) ] else : return [ ( ( float ( p [ 0 ] ) , float ( p [ 1 ] ) ) , [ ( ( float ( event . segment [ 0 ] [ 0 ] ) , float ( event . segment [ 0 ] [ 1 ] ) ) , ( float ( event . segment [ 1 ] [ 0 ] ) , float ( event . segment [ 1 ] [ 1 ] ) ) ) for event in event_set ] , ) for p , event_set in self . intersections . items ( ) ]",Return a list of unordered intersection ( point segment ) pairs where segments may contain 2 or more values .
"def poll ( self ) : assert ( len ( self . events_scan ) != 0 ) p , events_current = self . events_scan . pop_min ( ) return p , events_current",Get and remove the first ( lowest ) item from this queue .
def clear ( self ) : def _clear ( node ) : if node is not None : _clear ( node . left ) _clear ( node . right ) node . free ( ) _clear ( self . _root ) self . _count = 0 self . _root = None,T . clear () - > None . Remove all items from T .
"def pop_item ( self ) : if self . is_empty ( ) : raise KeyError ( ""pop_item(): tree is empty"" ) node = self . _root while True : if node . left is not None : node = node . left elif node . right is not None : node = node . right else : break key = node . key value = node . value self . remove ( key ) return key , value",T . pop_item () - > ( k v ) remove and return some ( key value ) pair as a 2 - tuple ; but raise KeyError if T is empty .
"def min_item ( self ) : if self . is_empty ( ) : raise ValueError ( ""Tree is empty"" ) node = self . _root while node . left is not None : node = node . left return node . key , node . value",Get item with min key of tree raises ValueError if tree is empty .
"def max_item ( self ) : if self . is_empty ( ) : raise ValueError ( ""Tree is empty"" ) node = self . _root while node . right is not None : node = node . right return node . key , node . value",Get item with max key of tree raises ValueError if tree is empty .
"def succ_item ( self , key , default = _sentinel ) : node = self . _root succ_node = None while node is not None : cmp = self . _cmp ( self . _cmp_data , key , node . key ) if cmp == 0 : break elif cmp < 0 : if ( succ_node is None ) or self . _cmp ( self . _cmp_data , node . key , succ_node . key ) < 0 : succ_node = node node = node . left else : node = node . right if node is None : if default is _sentinel : raise KeyError ( str ( key ) ) return default if node . right is not None : node = node . right while node . left is not None : node = node . left if succ_node is None : succ_node = node elif self . _cmp ( self . _cmp_data , node . key , succ_node . key ) < 0 : succ_node = node elif succ_node is None : if default is _sentinel : raise KeyError ( str ( key ) ) return default return succ_node . key , succ_node . value",Get successor ( k v ) pair of key raises KeyError if key is max key or key does not exist . optimized for pypy .
"def prev_item ( self , key , default = _sentinel ) : node = self . _root prev_node = None while node is not None : cmp = self . _cmp ( self . _cmp_data , key , node . key ) if cmp == 0 : break elif cmp < 0 : node = node . left else : if ( prev_node is None ) or self . _cmp ( self . _cmp_data , prev_node . key , node . key ) < 0 : prev_node = node node = node . right if node is None : if default is _sentinel : raise KeyError ( str ( key ) ) return default if node . left is not None : node = node . left while node . right is not None : node = node . right if prev_node is None : prev_node = node elif self . _cmp ( self . _cmp_data , prev_node . key , node . key ) < 0 : prev_node = node elif prev_node is None : if default is _sentinel : raise KeyError ( str ( key ) ) return default return prev_node . key , prev_node . value",Get predecessor ( k v ) pair of key raises KeyError if key is min key or key does not exist . optimized for pypy .
"def set_default ( self , key , default = None ) : try : return self . get_value ( key ) except KeyError : self . insert ( key , default ) return default",T . set_default ( k [ d ] ) - > T . get ( k d ) also set T [ k ] = d if k not in T
"def pop ( self , key , * args ) : if len ( args ) > 1 : raise TypeError ( ""pop expected at most 2 arguments, got %d"" % ( 1 + len ( args ) ) ) try : value = self . get_value ( key ) self . remove ( key ) return value except KeyError : if len ( args ) == 0 : raise else : return args [ 0 ]",T . pop ( k [ d ] ) - > v remove specified key and return the corresponding value . If key is not found d is returned if given otherwise KeyError is raised
"def prev_key ( self , key , default = _sentinel ) : item = self . prev_item ( key , default ) return default if item is default else item [ 0 ]",Get predecessor to key raises KeyError if key is min key or key does not exist .
"def succ_key ( self , key , default = _sentinel ) : item = self . succ_item ( key , default ) return default if item is default else item [ 0 ]",Get successor to key raises KeyError if key is max key or key does not exist .
"def key_slice ( self , start_key , end_key , reverse = False ) : return ( k for k , v in self . iter_items ( start_key , end_key , reverse = reverse ) )",T . key_slice ( start_key end_key ) - > key iterator : start_key < = key < end_key .
"def iter_items ( self , start_key = None , end_key = None , reverse = False ) : if self . is_empty ( ) : return [ ] if reverse : return self . _iter_items_backward ( start_key , end_key ) else : return self . _iter_items_forward ( start_key , end_key )",Iterates over the ( key value ) items of the associated tree in ascending order if reverse is True iterate in descending order reverse defaults to False
"def insert ( self , key , value ) : if self . _root is None : self . _root = self . _new_node ( key , value ) self . _root . red = False return head = Node ( ) grand_parent = None grand_grand_parent = head parent = None direction = 0 last = 0 grand_grand_parent . right = self . _root node = grand_grand_parent . right while True : if node is None : node = self . _new_node ( key , value ) parent [ direction ] = node elif RBTree . is_red ( node . left ) and RBTree . is_red ( node . right ) : node . red = True node . left . red = False node . right . red = False if RBTree . is_red ( node ) and RBTree . is_red ( parent ) : direction2 = 1 if grand_grand_parent . right is grand_parent else 0 if node is parent [ last ] : grand_grand_parent [ direction2 ] = RBTree . jsw_single ( grand_parent , 1 - last ) else : grand_grand_parent [ direction2 ] = RBTree . jsw_double ( grand_parent , 1 - last ) if self . _cmp ( self . _cmp_data , key , node . key ) == 0 : node . value = value break last = direction direction = 0 if ( self . _cmp ( self . _cmp_data , key , node . key ) < 0 ) else 1 if grand_parent is not None : grand_grand_parent = grand_parent grand_parent = parent parent = node node = node [ direction ] self . _root = head . right self . _root . red = False",T . insert ( key value ) < == > T [ key ] = value insert key value into tree .
"def remove ( self , key ) : if self . _root is None : raise KeyError ( str ( key ) ) head = Node ( ) node = head node . right = self . _root parent = None grand_parent = None found = None direction = 1 while node [ direction ] is not None : last = direction grand_parent = parent parent = node node = node [ direction ] direction = 1 if ( self . _cmp ( self . _cmp_data , node . key , key ) < 0 ) else 0 if self . _cmp ( self . _cmp_data , key , node . key ) == 0 : found = node if not RBTree . is_red ( node ) and not RBTree . is_red ( node [ direction ] ) : if RBTree . is_red ( node [ 1 - direction ] ) : parent [ last ] = RBTree . jsw_single ( node , direction ) parent = parent [ last ] elif not RBTree . is_red ( node [ 1 - direction ] ) : sibling = parent [ 1 - last ] if sibling is not None : if ( not RBTree . is_red ( sibling [ 1 - last ] ) ) and ( not RBTree . is_red ( sibling [ last ] ) ) : parent . red = False sibling . red = True node . red = True else : direction2 = 1 if grand_parent . right is parent else 0 if RBTree . is_red ( sibling [ last ] ) : grand_parent [ direction2 ] = RBTree . jsw_double ( parent , last ) elif RBTree . is_red ( sibling [ 1 - last ] ) : grand_parent [ direction2 ] = RBTree . jsw_single ( parent , last ) grand_parent [ direction2 ] . red = True node . red = True grand_parent [ direction2 ] . left . red = False grand_parent [ direction2 ] . right . red = False if found is not None : found . key = node . key found . value = node . value parent [ int ( parent . right is node ) ] = node [ int ( node . left is None ) ] node . free ( ) self . _count -= 1 self . _root = head . right if self . _root is not None : self . _root . red = False if not found : raise KeyError ( str ( key ) )",T . remove ( key ) < == > del T [ key ] remove item <key > from tree .
"def noise2d ( self , x , y ) : stretch_offset = ( x + y ) * STRETCH_CONSTANT_2D xs = x + stretch_offset ys = y + stretch_offset xsb = floor ( xs ) ysb = floor ( ys ) squish_offset = ( xsb + ysb ) * SQUISH_CONSTANT_2D xb = xsb + squish_offset yb = ysb + squish_offset xins = xs - xsb yins = ys - ysb in_sum = xins + yins dx0 = x - xb dy0 = y - yb value = 0 dx1 = dx0 - 1 - SQUISH_CONSTANT_2D dy1 = dy0 - 0 - SQUISH_CONSTANT_2D attn1 = 2 - dx1 * dx1 - dy1 * dy1 extrapolate = self . _extrapolate2d if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , dx1 , dy1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_2D dy2 = dy0 - 1 - SQUISH_CONSTANT_2D attn2 = 2 - dx2 * dx2 - dy2 * dy2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , dx2 , dy2 ) if in_sum <= 1 : zins = 1 - in_sum if zins > xins or zins > yins : if xins > yins : xsv_ext = xsb + 1 ysv_ext = ysb - 1 dx_ext = dx0 - 1 dy_ext = dy0 + 1 else : xsv_ext = xsb - 1 ysv_ext = ysb + 1 dx_ext = dx0 + 1 dy_ext = dy0 - 1 else : xsv_ext = xsb + 1 ysv_ext = ysb + 1 dx_ext = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 1 - 2 * SQUISH_CONSTANT_2D else : zins = 2 - in_sum if zins < xins or zins < yins : if xins > yins : xsv_ext = xsb + 2 ysv_ext = ysb + 0 dx_ext = dx0 - 2 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 + 0 - 2 * SQUISH_CONSTANT_2D else : xsv_ext = xsb + 0 ysv_ext = ysb + 2 dx_ext = dx0 + 0 - 2 * SQUISH_CONSTANT_2D dy_ext = dy0 - 2 - 2 * SQUISH_CONSTANT_2D else : dx_ext = dx0 dy_ext = dy0 xsv_ext = xsb ysv_ext = ysb xsb += 1 ysb += 1 dx0 = dx0 - 1 - 2 * SQUISH_CONSTANT_2D dy0 = dy0 - 1 - 2 * SQUISH_CONSTANT_2D attn0 = 2 - dx0 * dx0 - dy0 * dy0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb , ysb , dx0 , dy0 ) attn_ext = 2 - dx_ext * dx_ext - dy_ext * dy_ext if attn_ext > 0 : attn_ext *= attn_ext value += attn_ext * attn_ext * extrapolate ( xsv_ext , ysv_ext , dx_ext , dy_ext ) return value / NORM_CONSTANT_2D",Generate 2D OpenSimplex noise from X Y coordinates .
"def noise3d ( self , x , y , z ) : stretch_offset = ( x + y + z ) * STRETCH_CONSTANT_3D xs = x + stretch_offset ys = y + stretch_offset zs = z + stretch_offset xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) squish_offset = ( xsb + ysb + zsb ) * SQUISH_CONSTANT_3D xb = xsb + squish_offset yb = ysb + squish_offset zb = zsb + squish_offset xins = xs - xsb yins = ys - ysb zins = zs - zsb in_sum = xins + yins + zins dx0 = x - xb dy0 = y - yb dz0 = z - zb value = 0 extrapolate = self . _extrapolate3d if in_sum <= 1 : a_point = 0x01 a_score = xins b_point = 0x02 b_score = yins if a_score >= b_score and zins > b_score : b_score = zins b_point = 0x04 elif a_score < b_score and zins > a_score : a_score = zins a_point = 0x04 wins = 1 - in_sum if wins > a_score or wins > b_score : c = b_point if ( b_score > a_score ) else a_point if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 dx_ext1 = dx0 else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 if ( c & 0x01 ) == 0 : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 dz_ext1 = dz0 + 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 else : c = ( a_point | b_point ) if ( c & 0x01 ) == 0 : xsv_ext0 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_3D dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D if ( c & 0x02 ) == 0 : ysv_ext0 = ysb ysv_ext1 = ysb - 1 dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D if ( c & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , dx0 , dy0 , dz0 ) dx1 = dx0 - 1 - SQUISH_CONSTANT_3D dy1 = dy0 - 0 - SQUISH_CONSTANT_3D dz1 = dz0 - 0 - SQUISH_CONSTANT_3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_3D dy2 = dy0 - 1 - SQUISH_CONSTANT_3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) elif in_sum >= 2 : a_point = 0x06 a_score = xins b_point = 0x05 b_score = yins if a_score <= b_score and zins < b_score : b_score = zins b_point = 0x03 elif a_score > b_score and zins < a_score : a_score = zins a_point = 0x03 wins = 3 - in_sum if wins < a_score or wins < b_score : c = b_point if ( b_score < a_score ) else a_point if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_3D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_3D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D if ( c & 0x01 ) != 0 : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_3D if ( c & 0x04 ) != 0 : zsv_ext0 = zsb + 1 zsv_ext1 = zsb + 2 dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 3 * SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_3D else : c = ( a_point & b_point ) if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 1 xsv_ext1 = xsb + 2 dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx0 - SQUISH_CONSTANT_3D dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D if ( c & 0x02 ) != 0 : ysv_ext0 = ysb + 1 ysv_ext1 = ysb + 2 dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy0 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D if ( c & 0x04 ) != 0 : zsv_ext0 = zsb + 1 zsv_ext1 = zsb + 2 dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz0 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D dx3 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dy3 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dz3 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx3 , dy3 , dz3 ) dx2 = dx3 dy2 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D dz2 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx2 , dy2 , dz2 ) dx1 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D dy1 = dy3 dz1 = dz2 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx1 , dy1 , dz1 ) dx0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D dy0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D dz0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , dx0 , dy0 , dz0 ) else : p1 = xins + yins if p1 > 1 : a_score = p1 - 1 a_point = 0x03 a_is_further_side = True else : a_score = 1 - p1 a_point = 0x04 a_is_further_side = False p2 = xins + zins if p2 > 1 : b_score = p2 - 1 b_point = 0x05 b_is_further_side = True else : b_score = 1 - p2 b_point = 0x02 b_is_further_side = False p3 = yins + zins if p3 > 1 : score = p3 - 1 if a_score <= b_score and a_score < score : a_point = 0x06 a_is_further_side = True elif a_score > b_score and b_score < score : b_point = 0x06 b_is_further_side = True else : score = 1 - p3 if a_score <= b_score and a_score < score : a_point = 0x01 a_is_further_side = False elif a_score > b_score and b_score < score : b_point = 0x01 b_is_further_side = False if a_is_further_side == b_is_further_side : if a_is_further_side : dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb + 1 c = ( a_point & b_point ) if ( c & 0x01 ) != 0 : dx_ext1 = dx0 - 2 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb + 2 ysv_ext1 = ysb zsv_ext1 = zsb elif ( c & 0x02 ) != 0 : dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb + 2 zsv_ext1 = zsb else : dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb zsv_ext1 = zsb + 2 else : dx_ext0 = dx0 dy_ext0 = dy0 dz_ext0 = dz0 xsv_ext0 = xsb ysv_ext0 = ysb zsv_ext0 = zsb c = ( a_point | b_point ) if ( c & 0x01 ) == 0 : dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb - 1 ysv_ext1 = ysb + 1 zsv_ext1 = zsb + 1 elif ( c & 0x02 ) == 0 : dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 + 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb + 1 ysv_ext1 = ysb - 1 zsv_ext1 = zsb + 1 else : dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext1 = dz0 + 1 - SQUISH_CONSTANT_3D xsv_ext1 = xsb + 1 ysv_ext1 = ysb + 1 zsv_ext1 = zsb - 1 else : if a_is_further_side : c1 = a_point c2 = b_point else : c1 = b_point c2 = a_point if ( c1 & 0x01 ) == 0 : dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb - 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb + 1 elif ( c1 & 0x02 ) == 0 : dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 + 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 - 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb - 1 zsv_ext0 = zsb + 1 else : dx_ext0 = dx0 - 1 - SQUISH_CONSTANT_3D dy_ext0 = dy0 - 1 - SQUISH_CONSTANT_3D dz_ext0 = dz0 + 1 - SQUISH_CONSTANT_3D xsv_ext0 = xsb + 1 ysv_ext0 = ysb + 1 zsv_ext0 = zsb - 1 dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_3D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_3D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_3D xsv_ext1 = xsb ysv_ext1 = ysb zsv_ext1 = zsb if ( c2 & 0x01 ) != 0 : dx_ext1 -= 2 xsv_ext1 += 2 elif ( c2 & 0x02 ) != 0 : dy_ext1 -= 2 ysv_ext1 += 2 else : dz_ext1 -= 2 zsv_ext1 += 2 dx1 = dx0 - 1 - SQUISH_CONSTANT_3D dy1 = dy0 - 0 - SQUISH_CONSTANT_3D dz1 = dz0 - 0 - SQUISH_CONSTANT_3D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , dx1 , dy1 , dz1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_3D dy2 = dy0 - 1 - SQUISH_CONSTANT_3D dz2 = dz1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , dx2 , dy2 , dz2 ) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_3D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , dx3 , dy3 , dz3 ) dx4 = dx0 - 1 - 2 * SQUISH_CONSTANT_3D dy4 = dy0 - 1 - 2 * SQUISH_CONSTANT_3D dz4 = dz0 - 0 - 2 * SQUISH_CONSTANT_3D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , dx4 , dy4 , dz4 ) dx5 = dx4 dy5 = dy0 - 0 - 2 * SQUISH_CONSTANT_3D dz5 = dz0 - 1 - 2 * SQUISH_CONSTANT_3D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , dx5 , dy5 , dz5 ) dx6 = dx0 - 0 - 2 * SQUISH_CONSTANT_3D dy6 = dy4 dz6 = dz5 attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , dx6 , dy6 , dz6 ) attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0 if attn_ext0 > 0 : attn_ext0 *= attn_ext0 value += attn_ext0 * attn_ext0 * extrapolate ( xsv_ext0 , ysv_ext0 , zsv_ext0 , dx_ext0 , dy_ext0 , dz_ext0 ) attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1 if attn_ext1 > 0 : attn_ext1 *= attn_ext1 value += attn_ext1 * attn_ext1 * extrapolate ( xsv_ext1 , ysv_ext1 , zsv_ext1 , dx_ext1 , dy_ext1 , dz_ext1 ) return value / NORM_CONSTANT_3D",Generate 3D OpenSimplex noise from X Y Z coordinates .
"def noise4d ( self , x , y , z , w ) : stretch_offset = ( x + y + z + w ) * STRETCH_CONSTANT_4D xs = x + stretch_offset ys = y + stretch_offset zs = z + stretch_offset ws = w + stretch_offset xsb = floor ( xs ) ysb = floor ( ys ) zsb = floor ( zs ) wsb = floor ( ws ) squish_offset = ( xsb + ysb + zsb + wsb ) * SQUISH_CONSTANT_4D xb = xsb + squish_offset yb = ysb + squish_offset zb = zsb + squish_offset wb = wsb + squish_offset xins = xs - xsb yins = ys - ysb zins = zs - zsb wins = ws - wsb in_sum = xins + yins + zins + wins dx0 = x - xb dy0 = y - yb dz0 = z - zb dw0 = w - wb value = 0 extrapolate = self . _extrapolate4d if in_sum <= 1 : a_po = 0x01 a_score = xins b_po = 0x02 b_score = yins if a_score >= b_score and zins > b_score : b_score = zins b_po = 0x04 elif a_score < b_score and zins > a_score : a_score = zins a_po = 0x04 if a_score >= b_score and wins > b_score : b_score = wins b_po = 0x08 elif a_score < b_score and wins > a_score : a_score = wins a_po = 0x08 uins = 1 - in_sum if uins > a_score or uins > b_score : c = b_po if ( b_score > a_score ) else a_po if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx0 + 1 dx_ext1 = dx_ext2 = dx0 else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 1 if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy_ext1 = dy_ext2 = dy0 if ( c & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1 if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz_ext1 = dz_ext2 = dz0 if ( c & 0x03 ) != 0 : if ( c & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext2 -= 1 dz_ext2 += 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1 if ( c & 0x08 ) == 0 : wsv_ext0 = wsv_ext1 = wsb wsv_ext2 = wsb - 1 dw_ext0 = dw_ext1 = dw0 dw_ext2 = dw0 + 1 else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1 dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 1 else : c = ( a_po | b_po ) if ( c & 0x01 ) == 0 : xsv_ext0 = xsv_ext2 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext2 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0x01 : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext2 -= 1 dy_ext2 += 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0x03 : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext2 -= 1 dz_ext2 += 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x08 ) == 0 : wsv_ext0 = wsv_ext1 = wsb wsv_ext2 = wsb - 1 dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - SQUISH_CONSTANT_4D dw_ext2 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb + 1 dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw_ext2 = dw0 - 1 - SQUISH_CONSTANT_4D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 0 , dx0 , dy0 , dz0 , dw0 ) dx1 = dx0 - 1 - SQUISH_CONSTANT_4D dy1 = dy0 - 0 - SQUISH_CONSTANT_4D dz1 = dz0 - 0 - SQUISH_CONSTANT_4D dw1 = dw0 - 0 - SQUISH_CONSTANT_4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_4D dy2 = dy0 - 1 - SQUISH_CONSTANT_4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) elif in_sum >= 3 : a_po = 0x0E a_score = xins b_po = 0x0D b_score = yins if a_score <= b_score and zins < b_score : b_score = zins b_po = 0x0B elif a_score > b_score and zins < a_score : a_score = zins a_po = 0x0B if a_score <= b_score and wins < b_score : b_score = wins b_po = 0x07 elif a_score > b_score and wins < a_score : a_score = wins a_po = 0x07 uins = 4 - in_sum if uins < a_score or uins < b_score : c = b_po if ( b_score < a_score ) else a_po if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsv_ext2 = xsb + 1 dx_ext0 = dx0 - 2 - 4 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx_ext1 = dx_ext2 = dx0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D if ( c & 0x01 ) != 0 : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy_ext1 = dy_ext2 = dy0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D if ( c & 0x03 ) != 0x03 : if ( c & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext2 += 1 dz_ext2 -= 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz_ext1 = dz_ext2 = dz0 - 4 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsv_ext1 = wsb + 1 wsv_ext2 = wsb + 2 dw_ext0 = dw_ext1 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 - 4 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb dw_ext0 = dw_ext1 = dw_ext2 = dw0 - 4 * SQUISH_CONSTANT_4D else : c = ( a_po & b_po ) if ( c & 0x01 ) != 0 : xsv_ext0 = xsv_ext2 = xsb + 1 xsv_ext1 = xsb + 2 dx_ext0 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext2 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsv_ext2 = xsb dx_ext0 = dx0 - 2 * SQUISH_CONSTANT_4D dx_ext1 = dx_ext2 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb + 1 dy_ext0 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x01 ) != 0 : ysv_ext2 += 1 dy_ext2 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysv_ext2 = ysb dy_ext0 = dy0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy_ext2 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb + 1 dz_ext0 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x03 ) != 0 : zsv_ext2 += 1 dz_ext2 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsv_ext2 = zsb dz_ext0 = dz0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz_ext2 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsv_ext1 = wsb + 1 wsv_ext2 = wsb + 2 dw_ext0 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsv_ext2 = wsb dw_ext0 = dw0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw_ext2 = dw0 - 3 * SQUISH_CONSTANT_4D dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dw4 = dw0 - 3 * SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH_CONSTANT_4D dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) dx2 = dx4 dy2 = dy0 - 3 * SQUISH_CONSTANT_4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) dx1 = dx0 - 3 * SQUISH_CONSTANT_4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) dx0 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D dy0 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D dz0 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D dw0 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D attn0 = 2 - dx0 * dx0 - dy0 * dy0 - dz0 * dz0 - dw0 * dw0 if attn0 > 0 : attn0 *= attn0 value += attn0 * attn0 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 1 , dx0 , dy0 , dz0 , dw0 ) elif in_sum <= 2 : a_is_bigger_side = True b_is_bigger_side = True if xins + yins > zins + wins : a_score = xins + yins a_po = 0x03 else : a_score = zins + wins a_po = 0x0C if xins + zins > yins + wins : b_score = xins + zins b_po = 0x05 else : b_score = yins + wins b_po = 0x0A if xins + wins > yins + zins : score = xins + wins if a_score >= b_score and score > b_score : b_score = score b_po = 0x09 elif a_score < b_score and score > a_score : a_score = score a_po = 0x09 else : score = yins + zins if a_score >= b_score and score > b_score : b_score = score b_po = 0x06 elif a_score < b_score and score > a_score : a_score = score a_po = 0x06 p1 = 2 - in_sum + xins if a_score >= b_score and p1 > b_score : b_score = p1 b_po = 0x01 b_is_bigger_side = False elif a_score < b_score and p1 > a_score : a_score = p1 a_po = 0x01 a_is_bigger_side = False p2 = 2 - in_sum + yins if a_score >= b_score and p2 > b_score : b_score = p2 b_po = 0x02 b_is_bigger_side = False elif a_score < b_score and p2 > a_score : a_score = p2 a_po = 0x02 a_is_bigger_side = False p3 = 2 - in_sum + zins if a_score >= b_score and p3 > b_score : b_score = p3 b_po = 0x04 b_is_bigger_side = False elif a_score < b_score and p3 > a_score : a_score = p3 a_po = 0x04 a_is_bigger_side = False p4 = 2 - in_sum + wins if a_score >= b_score and p4 > b_score : b_po = 0x08 b_is_bigger_side = False elif a_score < b_score and p4 > a_score : a_po = 0x08 a_is_bigger_side = False if a_is_bigger_side == b_is_bigger_side : if a_is_bigger_side : c1 = ( a_po | b_po ) c2 = ( a_po & b_po ) if ( c1 & 0x01 ) == 0 : xsv_ext0 = xsb xsv_ext1 = xsb - 1 dx_ext0 = dx0 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 + 1 - 2 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x02 ) == 0 : ysv_ext0 = ysb ysv_ext1 = ysb - 1 dy_ext0 = dy0 - 3 * SQUISH_CONSTANT_4D dy_ext1 = dy0 + 1 - 2 * SQUISH_CONSTANT_4D else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dy_ext1 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x04 ) == 0 : zsv_ext0 = zsb zsv_ext1 = zsb - 1 dz_ext0 = dz0 - 3 * SQUISH_CONSTANT_4D dz_ext1 = dz0 + 1 - 2 * SQUISH_CONSTANT_4D else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dz_ext1 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - 2 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) != 0 : xsv_ext2 += 2 dx_ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv_ext2 += 2 dy_ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv_ext2 += 2 dz_ext2 -= 2 else : wsv_ext2 += 2 dw_ext2 -= 2 else : xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 dy_ext2 = dy0 dz_ext2 = dz0 dw_ext2 = dw0 c = ( a_po | b_po ) if ( c & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D else : if a_is_bigger_side : c1 = a_po c2 = b_po else : c1 = b_po c2 = a_po if ( c1 & 0x01 ) == 0 : xsv_ext0 = xsb - 1 xsv_ext1 = xsb dx_ext0 = dx0 + 1 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb + 1 dx_ext0 = dx_ext1 = dx0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x02 ) == 0 : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - SQUISH_CONSTANT_4D if ( c1 & 0x01 ) == 0x01 : ysv_ext0 -= 1 dy_ext0 += 1 else : ysv_ext1 -= 1 dy_ext1 += 1 else : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x04 ) == 0 : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - SQUISH_CONSTANT_4D if ( c1 & 0x03 ) == 0x03 : zsv_ext0 -= 1 dz_ext0 += 1 else : zsv_ext1 -= 1 dz_ext1 += 1 else : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - SQUISH_CONSTANT_4D if ( c1 & 0x08 ) == 0 : wsv_ext0 = wsb wsv_ext1 = wsb - 1 dw_ext0 = dw0 - SQUISH_CONSTANT_4D dw_ext1 = dw0 + 1 - SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb + 1 dw_ext0 = dw_ext1 = dw0 - 1 - SQUISH_CONSTANT_4D xsv_ext2 = xsb ysv_ext2 = ysb zsv_ext2 = zsb wsv_ext2 = wsb dx_ext2 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) != 0 : xsv_ext2 += 2 dx_ext2 -= 2 elif ( c2 & 0x02 ) != 0 : ysv_ext2 += 2 dy_ext2 -= 2 elif ( c2 & 0x04 ) != 0 : zsv_ext2 += 2 dz_ext2 -= 2 else : wsv_ext2 += 2 dw_ext2 -= 2 dx1 = dx0 - 1 - SQUISH_CONSTANT_4D dy1 = dy0 - 0 - SQUISH_CONSTANT_4D dz1 = dz0 - 0 - SQUISH_CONSTANT_4D dw1 = dw0 - 0 - SQUISH_CONSTANT_4D attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 0 , dx1 , dy1 , dz1 , dw1 ) dx2 = dx0 - 0 - SQUISH_CONSTANT_4D dy2 = dy0 - 1 - SQUISH_CONSTANT_4D dz2 = dz1 dw2 = dw1 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 0 , dx2 , dy2 , dz2 , dw2 ) dx3 = dx2 dy3 = dy1 dz3 = dz0 - 1 - SQUISH_CONSTANT_4D dw3 = dw1 attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 0 , dx3 , dy3 , dz3 , dw3 ) dx4 = dx2 dy4 = dy1 dz4 = dz1 dw4 = dw0 - 1 - SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 0 , wsb + 1 , dx4 , dy4 , dz4 , dw4 ) dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) else : a_is_bigger_side = True b_is_bigger_side = True if xins + yins < zins + wins : a_score = xins + yins a_po = 0x0C else : a_score = zins + wins a_po = 0x03 if xins + zins < yins + wins : b_score = xins + zins b_po = 0x0A else : b_score = yins + wins b_po = 0x05 if xins + wins < yins + zins : score = xins + wins if a_score <= b_score and score < b_score : b_score = score b_po = 0x06 elif a_score > b_score and score < a_score : a_score = score a_po = 0x06 else : score = yins + zins if a_score <= b_score and score < b_score : b_score = score b_po = 0x09 elif a_score > b_score and score < a_score : a_score = score a_po = 0x09 p1 = 3 - in_sum + xins if a_score <= b_score and p1 < b_score : b_score = p1 b_po = 0x0E b_is_bigger_side = False elif a_score > b_score and p1 < a_score : a_score = p1 a_po = 0x0E a_is_bigger_side = False p2 = 3 - in_sum + yins if a_score <= b_score and p2 < b_score : b_score = p2 b_po = 0x0D b_is_bigger_side = False elif a_score > b_score and p2 < a_score : a_score = p2 a_po = 0x0D a_is_bigger_side = False p3 = 3 - in_sum + zins if a_score <= b_score and p3 < b_score : b_score = p3 b_po = 0x0B b_is_bigger_side = False elif a_score > b_score and p3 < a_score : a_score = p3 a_po = 0x0B a_is_bigger_side = False p4 = 3 - in_sum + wins if a_score <= b_score and p4 < b_score : b_po = 0x07 b_is_bigger_side = False elif a_score > b_score and p4 < a_score : a_po = 0x07 a_is_bigger_side = False if a_is_bigger_side == b_is_bigger_side : if a_is_bigger_side : c1 = ( a_po & b_po ) c2 = ( a_po | b_po ) xsv_ext0 = xsv_ext1 = xsb ysv_ext0 = ysv_ext1 = ysb zsv_ext0 = zsv_ext1 = zsb wsv_ext0 = wsv_ext1 = wsb dx_ext0 = dx0 - SQUISH_CONSTANT_4D dy_ext0 = dy0 - SQUISH_CONSTANT_4D dz_ext0 = dz0 - SQUISH_CONSTANT_4D dw_ext0 = dw0 - SQUISH_CONSTANT_4D dx_ext1 = dx0 - 2 * SQUISH_CONSTANT_4D dy_ext1 = dy0 - 2 * SQUISH_CONSTANT_4D dz_ext1 = dz0 - 2 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 * SQUISH_CONSTANT_4D if ( c1 & 0x01 ) != 0 : xsv_ext0 += 1 dx_ext0 -= 1 xsv_ext1 += 2 dx_ext1 -= 2 elif ( c1 & 0x02 ) != 0 : ysv_ext0 += 1 dy_ext0 -= 1 ysv_ext1 += 2 dy_ext1 -= 2 elif ( c1 & 0x04 ) != 0 : zsv_ext0 += 1 dz_ext0 -= 1 zsv_ext1 += 2 dz_ext1 -= 2 else : wsv_ext0 += 1 dw_ext0 -= 1 wsv_ext1 += 2 dw_ext1 -= 2 xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) == 0 : xsv_ext2 -= 2 dx_ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv_ext2 -= 2 dy_ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv_ext2 -= 2 dz_ext2 += 2 else : wsv_ext2 -= 2 dw_ext2 += 2 else : xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 4 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 4 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 4 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 4 * SQUISH_CONSTANT_4D c = ( a_po & b_po ) if ( c & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x01 ) == 0 : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c & 0x08 ) != 0 : wsv_ext0 = wsb + 1 wsv_ext1 = wsb + 2 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D else : if a_is_bigger_side : c1 = a_po c2 = b_po else : c1 = b_po c2 = a_po if ( c1 & 0x01 ) != 0 : xsv_ext0 = xsb + 2 xsv_ext1 = xsb + 1 dx_ext0 = dx0 - 2 - 3 * SQUISH_CONSTANT_4D dx_ext1 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D else : xsv_ext0 = xsv_ext1 = xsb dx_ext0 = dx_ext1 = dx0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x02 ) != 0 : ysv_ext0 = ysv_ext1 = ysb + 1 dy_ext0 = dy_ext1 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x01 ) == 0 : ysv_ext0 += 1 dy_ext0 -= 1 else : ysv_ext1 += 1 dy_ext1 -= 1 else : ysv_ext0 = ysv_ext1 = ysb dy_ext0 = dy_ext1 = dy0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x04 ) != 0 : zsv_ext0 = zsv_ext1 = zsb + 1 dz_ext0 = dz_ext1 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x03 ) == 0 : zsv_ext0 += 1 dz_ext0 -= 1 else : zsv_ext1 += 1 dz_ext1 -= 1 else : zsv_ext0 = zsv_ext1 = zsb dz_ext0 = dz_ext1 = dz0 - 3 * SQUISH_CONSTANT_4D if ( c1 & 0x08 ) != 0 : wsv_ext0 = wsb + 1 wsv_ext1 = wsb + 2 dw_ext0 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D dw_ext1 = dw0 - 2 - 3 * SQUISH_CONSTANT_4D else : wsv_ext0 = wsv_ext1 = wsb dw_ext0 = dw_ext1 = dw0 - 3 * SQUISH_CONSTANT_4D xsv_ext2 = xsb + 1 ysv_ext2 = ysb + 1 zsv_ext2 = zsb + 1 wsv_ext2 = wsb + 1 dx_ext2 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy_ext2 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz_ext2 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw_ext2 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D if ( c2 & 0x01 ) == 0 : xsv_ext2 -= 2 dx_ext2 += 2 elif ( c2 & 0x02 ) == 0 : ysv_ext2 -= 2 dy_ext2 += 2 elif ( c2 & 0x04 ) == 0 : zsv_ext2 -= 2 dz_ext2 += 2 else : wsv_ext2 -= 2 dw_ext2 += 2 dx4 = dx0 - 1 - 3 * SQUISH_CONSTANT_4D dy4 = dy0 - 1 - 3 * SQUISH_CONSTANT_4D dz4 = dz0 - 1 - 3 * SQUISH_CONSTANT_4D dw4 = dw0 - 3 * SQUISH_CONSTANT_4D attn4 = 2 - dx4 * dx4 - dy4 * dy4 - dz4 * dz4 - dw4 * dw4 if attn4 > 0 : attn4 *= attn4 value += attn4 * attn4 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 1 , wsb + 0 , dx4 , dy4 , dz4 , dw4 ) dx3 = dx4 dy3 = dy4 dz3 = dz0 - 3 * SQUISH_CONSTANT_4D dw3 = dw0 - 1 - 3 * SQUISH_CONSTANT_4D attn3 = 2 - dx3 * dx3 - dy3 * dy3 - dz3 * dz3 - dw3 * dw3 if attn3 > 0 : attn3 *= attn3 value += attn3 * attn3 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 1 , dx3 , dy3 , dz3 , dw3 ) dx2 = dx4 dy2 = dy0 - 3 * SQUISH_CONSTANT_4D dz2 = dz4 dw2 = dw3 attn2 = 2 - dx2 * dx2 - dy2 * dy2 - dz2 * dz2 - dw2 * dw2 if attn2 > 0 : attn2 *= attn2 value += attn2 * attn2 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 1 , dx2 , dy2 , dz2 , dw2 ) dx1 = dx0 - 3 * SQUISH_CONSTANT_4D dz1 = dz4 dy1 = dy4 dw1 = dw3 attn1 = 2 - dx1 * dx1 - dy1 * dy1 - dz1 * dz1 - dw1 * dw1 if attn1 > 0 : attn1 *= attn1 value += attn1 * attn1 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 1 , dx1 , dy1 , dz1 , dw1 ) dx5 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy5 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz5 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw5 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn5 = 2 - dx5 * dx5 - dy5 * dy5 - dz5 * dz5 - dw5 * dw5 if attn5 > 0 : attn5 *= attn5 value += attn5 * attn5 * extrapolate ( xsb + 1 , ysb + 1 , zsb + 0 , wsb + 0 , dx5 , dy5 , dz5 , dw5 ) dx6 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy6 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz6 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw6 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn6 = 2 - dx6 * dx6 - dy6 * dy6 - dz6 * dz6 - dw6 * dw6 if attn6 > 0 : attn6 *= attn6 value += attn6 * attn6 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 1 , wsb + 0 , dx6 , dy6 , dz6 , dw6 ) dx7 = dx0 - 1 - 2 * SQUISH_CONSTANT_4D dy7 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz7 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw7 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn7 = 2 - dx7 * dx7 - dy7 * dy7 - dz7 * dz7 - dw7 * dw7 if attn7 > 0 : attn7 *= attn7 value += attn7 * attn7 * extrapolate ( xsb + 1 , ysb + 0 , zsb + 0 , wsb + 1 , dx7 , dy7 , dz7 , dw7 ) dx8 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy8 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz8 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw8 = dw0 - 0 - 2 * SQUISH_CONSTANT_4D attn8 = 2 - dx8 * dx8 - dy8 * dy8 - dz8 * dz8 - dw8 * dw8 if attn8 > 0 : attn8 *= attn8 value += attn8 * attn8 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 1 , wsb + 0 , dx8 , dy8 , dz8 , dw8 ) dx9 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy9 = dy0 - 1 - 2 * SQUISH_CONSTANT_4D dz9 = dz0 - 0 - 2 * SQUISH_CONSTANT_4D dw9 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn9 = 2 - dx9 * dx9 - dy9 * dy9 - dz9 * dz9 - dw9 * dw9 if attn9 > 0 : attn9 *= attn9 value += attn9 * attn9 * extrapolate ( xsb + 0 , ysb + 1 , zsb + 0 , wsb + 1 , dx9 , dy9 , dz9 , dw9 ) dx10 = dx0 - 0 - 2 * SQUISH_CONSTANT_4D dy10 = dy0 - 0 - 2 * SQUISH_CONSTANT_4D dz10 = dz0 - 1 - 2 * SQUISH_CONSTANT_4D dw10 = dw0 - 1 - 2 * SQUISH_CONSTANT_4D attn10 = 2 - dx10 * dx10 - dy10 * dy10 - dz10 * dz10 - dw10 * dw10 if attn10 > 0 : attn10 *= attn10 value += attn10 * attn10 * extrapolate ( xsb + 0 , ysb + 0 , zsb + 1 , wsb + 1 , dx10 , dy10 , dz10 , dw10 ) attn_ext0 = 2 - dx_ext0 * dx_ext0 - dy_ext0 * dy_ext0 - dz_ext0 * dz_ext0 - dw_ext0 * dw_ext0 if attn_ext0 > 0 : attn_ext0 *= attn_ext0 value += attn_ext0 * attn_ext0 * extrapolate ( xsv_ext0 , ysv_ext0 , zsv_ext0 , wsv_ext0 , dx_ext0 , dy_ext0 , dz_ext0 , dw_ext0 ) attn_ext1 = 2 - dx_ext1 * dx_ext1 - dy_ext1 * dy_ext1 - dz_ext1 * dz_ext1 - dw_ext1 * dw_ext1 if attn_ext1 > 0 : attn_ext1 *= attn_ext1 value += attn_ext1 * attn_ext1 * extrapolate ( xsv_ext1 , ysv_ext1 , zsv_ext1 , wsv_ext1 , dx_ext1 , dy_ext1 , dz_ext1 , dw_ext1 ) attn_ext2 = 2 - dx_ext2 * dx_ext2 - dy_ext2 * dy_ext2 - dz_ext2 * dz_ext2 - dw_ext2 * dw_ext2 if attn_ext2 > 0 : attn_ext2 *= attn_ext2 value += attn_ext2 * attn_ext2 * extrapolate ( xsv_ext2 , ysv_ext2 , zsv_ext2 , wsv_ext2 , dx_ext2 , dy_ext2 , dz_ext2 , dw_ext2 ) return value / NORM_CONSTANT_4D",Generate 4D OpenSimplex noise from X Y Z W coordinates .
"def adjust_contrast_gamma ( arr , gamma ) : if arr . dtype . name == ""uint8"" : min_value , _center_value , max_value = iadt . get_value_range_of_dtype ( arr . dtype ) dynamic_range = max_value - min_value value_range = np . linspace ( 0 , 1.0 , num = dynamic_range + 1 , dtype = np . float32 ) table = ( min_value + ( value_range ** np . float32 ( gamma ) ) * dynamic_range ) arr_aug = cv2 . LUT ( arr , np . clip ( table , min_value , max_value ) . astype ( arr . dtype ) ) if arr . ndim == 3 and arr_aug . ndim == 2 : return arr_aug [ ... , np . newaxis ] return arr_aug else : return ski_exposure . adjust_gamma ( arr , gamma )",Adjust contrast by scaling each pixel value to 255 * (( I_ij / 255 ) ** gamma ) .
"def adjust_contrast_sigmoid ( arr , gain , cutoff ) : if arr . dtype . name == ""uint8"" : min_value , _center_value , max_value = iadt . get_value_range_of_dtype ( arr . dtype ) dynamic_range = max_value - min_value value_range = np . linspace ( 0 , 1.0 , num = dynamic_range + 1 , dtype = np . float32 ) gain = np . float32 ( gain ) cutoff = np . float32 ( cutoff ) table = min_value + dynamic_range * 1 / ( 1 + np . exp ( gain * ( cutoff - value_range ) ) ) arr_aug = cv2 . LUT ( arr , np . clip ( table , min_value , max_value ) . astype ( arr . dtype ) ) if arr . ndim == 3 and arr_aug . ndim == 2 : return arr_aug [ ... , np . newaxis ] return arr_aug else : return ski_exposure . adjust_sigmoid ( arr , cutoff = cutoff , gain = gain )",Adjust contrast by scaling each pixel value to 255 * 1 / ( 1 + exp ( gain * ( cutoff - I_ij / 255 ))) .
"def adjust_contrast_log ( arr , gain ) : if arr . dtype . name == ""uint8"" : min_value , _center_value , max_value = iadt . get_value_range_of_dtype ( arr . dtype ) dynamic_range = max_value - min_value value_range = np . linspace ( 0 , 1.0 , num = dynamic_range + 1 , dtype = np . float32 ) gain = np . float32 ( gain ) table = min_value + dynamic_range * gain * np . log2 ( 1 + value_range ) arr_aug = cv2 . LUT ( arr , np . clip ( table , min_value , max_value ) . astype ( arr . dtype ) ) if arr . ndim == 3 and arr_aug . ndim == 2 : return arr_aug [ ... , np . newaxis ] return arr_aug else : return ski_exposure . adjust_log ( arr , gain = gain )",Adjust contrast by scaling each pixel value to 255 * gain * log_2 ( 1 + I_ij / 255 ) .
"def adjust_contrast_linear ( arr , alpha ) : if arr . dtype . name == ""uint8"" : min_value , center_value , max_value = iadt . get_value_range_of_dtype ( arr . dtype ) value_range = np . arange ( 0 , 256 , dtype = np . float32 ) alpha = np . float32 ( alpha ) table = center_value + alpha * ( value_range - center_value ) arr_aug = cv2 . LUT ( arr , np . clip ( table , min_value , max_value ) . astype ( arr . dtype ) ) if arr . ndim == 3 and arr_aug . ndim == 2 : return arr_aug [ ... , np . newaxis ] return arr_aug else : input_dtype = arr . dtype _min_value , center_value , _max_value = iadt . get_value_range_of_dtype ( input_dtype ) if input_dtype . kind in [ ""u"" , ""i"" ] : center_value = int ( center_value ) image_aug = center_value + alpha * ( arr . astype ( np . float64 ) - center_value ) image_aug = iadt . restore_dtypes_ ( image_aug , input_dtype ) return image_aug",Adjust contrast by scaling each pixel value to 127 + alpha * ( I_ij - 127 ) .
"def GammaContrast ( gamma = 1 , per_channel = False , name = None , deterministic = False , random_state = None ) : params1d = [ iap . handle_continuous_param ( gamma , ""gamma"" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) ] func = adjust_contrast_gamma return _ContrastFuncWrapper ( func , params1d , per_channel , dtypes_allowed = [ ""uint8"" , ""uint16"" , ""uint32"" , ""uint64"" , ""int8"" , ""int16"" , ""int32"" , ""int64"" , ""float16"" , ""float32"" , ""float64"" ] , dtypes_disallowed = [ ""float96"" , ""float128"" , ""float256"" , ""bool"" ] , name = name if name is not None else ia . caller_name ( ) , deterministic = deterministic , random_state = random_state )",Adjust contrast by scaling each pixel value to 255 * (( I_ij / 255 ) ** gamma ) .
"def SigmoidContrast ( gain = 10 , cutoff = 0.5 , per_channel = False , name = None , deterministic = False , random_state = None ) : params1d = [ iap . handle_continuous_param ( gain , ""gain"" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) , iap . handle_continuous_param ( cutoff , ""cutoff"" , value_range = ( 0 , 1.0 ) , tuple_to_uniform = True , list_to_choice = True ) ] func = adjust_contrast_sigmoid return _ContrastFuncWrapper ( func , params1d , per_channel , dtypes_allowed = [ ""uint8"" , ""uint16"" , ""uint32"" , ""uint64"" , ""int8"" , ""int16"" , ""int32"" , ""int64"" , ""float16"" , ""float32"" , ""float64"" ] , dtypes_disallowed = [ ""float96"" , ""float128"" , ""float256"" , ""bool"" ] , name = name if name is not None else ia . caller_name ( ) , deterministic = deterministic , random_state = random_state )",Adjust contrast by scaling each pixel value to 255 * 1 / ( 1 + exp ( gain * ( cutoff - I_ij / 255 ))) .
"def LogContrast ( gain = 1 , per_channel = False , name = None , deterministic = False , random_state = None ) : params1d = [ iap . handle_continuous_param ( gain , ""gain"" , value_range = ( 0 , None ) , tuple_to_uniform = True , list_to_choice = True ) ] func = adjust_contrast_log return _ContrastFuncWrapper ( func , params1d , per_channel , dtypes_allowed = [ ""uint8"" , ""uint16"" , ""uint32"" , ""uint64"" , ""int8"" , ""int16"" , ""int32"" , ""int64"" , ""float16"" , ""float32"" , ""float64"" ] , dtypes_disallowed = [ ""float96"" , ""float128"" , ""float256"" , ""bool"" ] , name = name if name is not None else ia . caller_name ( ) , deterministic = deterministic , random_state = random_state )",Adjust contrast by scaling each pixel value to 255 * gain * log_2 ( 1 + I_ij / 255 ) .
"def LinearContrast ( alpha = 1 , per_channel = False , name = None , deterministic = False , random_state = None ) : params1d = [ iap . handle_continuous_param ( alpha , ""alpha"" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) ] func = adjust_contrast_linear return _ContrastFuncWrapper ( func , params1d , per_channel , dtypes_allowed = [ ""uint8"" , ""uint16"" , ""uint32"" , ""int8"" , ""int16"" , ""int32"" , ""float16"" , ""float32"" , ""float64"" ] , dtypes_disallowed = [ ""uint64"" , ""int64"" , ""float96"" , ""float128"" , ""float256"" , ""bool"" ] , name = name if name is not None else ia . caller_name ( ) , deterministic = deterministic , random_state = random_state )",Adjust contrast by scaling each pixel value to 127 + alpha * ( I_ij - 127 ) .
"def InColorspace ( to_colorspace , from_colorspace = ""RGB"" , children = None , name = None , deterministic = False , random_state = None ) : return WithColorspace ( to_colorspace , from_colorspace , children , name , deterministic , random_state )",Convert images to another colorspace .
"def Grayscale ( alpha = 0 , from_colorspace = ""RGB"" , name = None , deterministic = False , random_state = None ) : if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return ChangeColorspace ( to_colorspace = ChangeColorspace . GRAY , alpha = alpha , from_colorspace = from_colorspace , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to convert images to their grayscale versions .
def height ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . yy ) - np . min ( self . yy ),Get the height of a bounding box encapsulating the line .
def width ( self ) : if len ( self . coords ) <= 1 : return 0 return np . max ( self . xx ) - np . min ( self . xx ),Get the width of a bounding box encapsulating the line .
"def get_pointwise_inside_image_mask ( self , image ) : if len ( self . coords ) == 0 : return np . zeros ( ( 0 , ) , dtype = bool ) shape = normalize_shape ( image ) height , width = shape [ 0 : 2 ] x_within = np . logical_and ( 0 <= self . xx , self . xx < width ) y_within = np . logical_and ( 0 <= self . yy , self . yy < height ) return np . logical_and ( x_within , y_within )",Get for each point whether it is inside of the given image plane .
"def compute_neighbour_distances ( self ) : if len ( self . coords ) <= 1 : return np . zeros ( ( 0 , ) , dtype = np . float32 ) return np . sqrt ( np . sum ( ( self . coords [ : - 1 , : ] - self . coords [ 1 : , : ] ) ** 2 , axis = 1 ) )",Get the euclidean distance between each two consecutive points .
"def compute_pointwise_distances ( self , other , default = None ) : import shapely . geometry from . kps import Keypoint if isinstance ( other , Keypoint ) : other = shapely . geometry . Point ( ( other . x , other . y ) ) elif isinstance ( other , LineString ) : if len ( other . coords ) == 0 : return default elif len ( other . coords ) == 1 : other = shapely . geometry . Point ( other . coords [ 0 , : ] ) else : other = shapely . geometry . LineString ( other . coords ) elif isinstance ( other , tuple ) : assert len ( other ) == 2 other = shapely . geometry . Point ( other ) else : raise ValueError ( ( ""Expected Keypoint or LineString or tuple (x,y), "" + ""got type %s."" ) % ( type ( other ) , ) ) return [ shapely . geometry . Point ( point ) . distance ( other ) for point in self . coords ]",Compute the minimal distance between each point on self and other .
"def compute_distance ( self , other , default = None ) : distances = self . compute_pointwise_distances ( other , default = [ ] ) if len ( distances ) == 0 : return default return min ( distances )",Compute the minimal distance between the line string and other .
"def contains ( self , other , max_distance = 1e-4 ) : return self . compute_distance ( other , default = np . inf ) < max_distance",Estimate whether the bounding box contains a point .
"def project ( self , from_shape , to_shape ) : coords_proj = project_coords ( self . coords , from_shape , to_shape ) return self . copy ( coords = coords_proj )",Project the line string onto a differently shaped image .
"def is_fully_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default return np . all ( self . get_pointwise_inside_image_mask ( image ) )",Estimate whether the line string is fully inside the image area .
"def is_partly_within_image ( self , image , default = False ) : if len ( self . coords ) == 0 : return default mask = self . get_pointwise_inside_image_mask ( image ) if np . any ( mask ) : return True return len ( self . clip_out_of_image ( image ) ) > 0",Estimate whether the line string is at least partially inside the image .
"def is_out_of_image ( self , image , fully = True , partly = False , default = True ) : if len ( self . coords ) == 0 : return default if self . is_fully_within_image ( image ) : return False elif self . is_partly_within_image ( image ) : return partly else : return fully",Estimate whether the line is partially / fully outside of the image area .
"def clip_out_of_image ( self , image ) : if len ( self . coords ) == 0 : return [ ] inside_image_mask = self . get_pointwise_inside_image_mask ( image ) ooi_mask = ~ inside_image_mask if len ( self . coords ) == 1 : if not np . any ( inside_image_mask ) : return [ ] return [ self . copy ( ) ] if np . all ( inside_image_mask ) : return [ self . copy ( ) ] height , width = normalize_shape ( image ) [ 0 : 2 ] eps = 1e-3 edges = [ LineString ( [ ( 0.0 , 0.0 ) , ( width - eps , 0.0 ) ] ) , LineString ( [ ( width - eps , 0.0 ) , ( width - eps , height - eps ) ] ) , LineString ( [ ( width - eps , height - eps ) , ( 0.0 , height - eps ) ] ) , LineString ( [ ( 0.0 , height - eps ) , ( 0.0 , 0.0 ) ] ) ] intersections = self . find_intersections_with ( edges ) points = [ ] gen = enumerate ( zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] , ooi_mask [ : - 1 ] , ooi_mask [ 1 : ] , intersections ) ) for i , ( line_start , line_end , ooi_start , ooi_end , inter_line ) in gen : points . append ( ( line_start , False , ooi_start ) ) for p_inter in inter_line : points . append ( ( p_inter , True , False ) ) is_last = ( i == len ( self . coords ) - 2 ) if is_last and not ooi_end : points . append ( ( line_end , False , ooi_end ) ) lines = [ ] line = [ ] for i , ( coord , was_added , ooi ) in enumerate ( points ) : if ooi : if len ( line ) > 0 : lines . append ( line ) line = [ ] continue if not was_added : line . append ( coord ) else : is_last_point = ( i == len ( points ) - 1 ) is_next_ooi = ( not is_last_point and bool ( points [ i + 1 ] [ 2 ] ) is True ) p_prev = line [ - 1 ] if len ( line ) > 0 else None p_next = None if not is_last_point and not is_next_ooi : p_next = points [ i + 1 ] [ 0 ] dist_prev = None dist_next = None if p_prev is not None : dist_prev = np . linalg . norm ( np . float32 ( coord ) - np . float32 ( p_prev ) ) if p_next is not None : dist_next = np . linalg . norm ( np . float32 ( coord ) - np . float32 ( p_next ) ) dist_prev_ok = ( dist_prev is None or dist_prev > 1e-2 ) dist_next_ok = ( dist_next is None or dist_next > 1e-2 ) if dist_prev_ok and dist_next_ok : line . append ( coord ) if len ( line ) > 0 : lines . append ( line ) lines = [ line for line in lines if len ( line ) > 0 ] return [ self . deepcopy ( coords = line ) for line in lines ]",Clip off all parts of the line_string that are outside of the image .
"def find_intersections_with ( self , other ) : import shapely . geometry geom = _convert_var_to_shapely_geometry ( other ) result = [ ] for p_start , p_end in zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] ) : ls = shapely . geometry . LineString ( [ p_start , p_end ] ) intersections = ls . intersection ( geom ) intersections = list ( _flatten_shapely_collection ( intersections ) ) intersections_points = [ ] for inter in intersections : if isinstance ( inter , shapely . geometry . linestring . LineString ) : inter_start = ( inter . coords [ 0 ] [ 0 ] , inter . coords [ 0 ] [ 1 ] ) inter_end = ( inter . coords [ - 1 ] [ 0 ] , inter . coords [ - 1 ] [ 1 ] ) intersections_points . extend ( [ inter_start , inter_end ] ) else : assert isinstance ( inter , shapely . geometry . point . Point ) , ( ""Expected to find shapely.geometry.point.Point or "" ""shapely.geometry.linestring.LineString intersection, "" ""actually found %s."" % ( type ( inter ) , ) ) intersections_points . append ( ( inter . x , inter . y ) ) inter_sorted = sorted ( intersections_points , key = lambda p : np . linalg . norm ( np . float32 ( p ) - p_start ) ) result . append ( inter_sorted ) return result",Find all intersection points between the line string and other .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : top = top if top is not None else 0 right = right if right is not None else 0 bottom = bottom if bottom is not None else 0 left = left if left is not None else 0 coords = np . copy ( self . coords ) coords [ : , 0 ] += left - right coords [ : , 1 ] += top - bottom return self . copy ( coords = coords )",Shift / move the line string from one or more image sides .
"def draw_mask ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : heatmap = self . draw_heatmap_array ( image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = size_lines , size_points = size_points , antialiased = False , raise_if_out_of_image = raise_if_out_of_image ) return heatmap > 0.5",Draw this line segment as a binary image mask .
"def draw_lines_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , antialiased = True , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( ""Expected (H,W) or (H,W,1) as image_shape, got %s."" % ( image_shape , ) ) arr = self . draw_lines_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0",Draw the line segments of the line string as a heatmap array .
"def draw_points_heatmap_array ( self , image_shape , alpha = 1.0 , size = 1 , raise_if_out_of_image = False ) : assert len ( image_shape ) == 2 or ( len ( image_shape ) == 3 and image_shape [ - 1 ] == 1 ) , ( ""Expected (H,W) or (H,W,1) as image_shape, got %s."" % ( image_shape , ) ) arr = self . draw_points_on_image ( np . zeros ( image_shape , dtype = np . uint8 ) , color = 255 , alpha = alpha , size = size , raise_if_out_of_image = raise_if_out_of_image ) return arr . astype ( np . float32 ) / 255.0",Draw the points of the line string as a heatmap array .
"def draw_heatmap_array ( self , image_shape , alpha_lines = 1.0 , alpha_points = 1.0 , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : heatmap_lines = self . draw_lines_heatmap_array ( image_shape , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) if size_points <= 0 : return heatmap_lines heatmap_points = self . draw_points_heatmap_array ( image_shape , alpha = alpha_points , size = size_points , raise_if_out_of_image = raise_if_out_of_image ) heatmap = np . dstack ( [ heatmap_lines , heatmap_points ] ) return np . max ( heatmap , axis = 2 )",Draw the line segments and points of the line string as a heatmap array .
"def draw_lines_on_image ( self , image , color = ( 0 , 255 , 0 ) , alpha = 1.0 , size = 3 , antialiased = True , raise_if_out_of_image = False ) : from . . import dtypes as iadt from . . augmenters import blend as blendlib image_was_empty = False if isinstance ( image , tuple ) : image_was_empty = True image = np . zeros ( image , dtype = np . uint8 ) assert image . ndim in [ 2 , 3 ] , ( ( ""Expected image or shape of form (H,W) or (H,W,C), "" + ""got shape %s."" ) % ( image . shape , ) ) if len ( self . coords ) <= 1 or alpha < 0 + 1e-4 or size < 1 : return np . copy ( image ) if raise_if_out_of_image and self . is_out_of_image ( image , partly = False , fully = True ) : raise Exception ( ""Cannot draw line string '%s' on image with shape %s, because "" ""it would be out of bounds."" % ( self . __str__ ( ) , image . shape ) ) if image . ndim == 2 : assert ia . is_single_number ( color ) , ( ""Got a 2D image. Expected then 'color' to be a single number, "" ""but got %s."" % ( str ( color ) , ) ) color = [ color ] elif image . ndim == 3 and ia . is_single_number ( color ) : color = [ color ] * image . shape [ - 1 ] image = image . astype ( np . float32 ) height , width = image . shape [ 0 : 2 ] lines = [ ] for line_start , line_end in zip ( self . coords [ : - 1 ] , self . coords [ 1 : ] ) : lines . append ( ( line_start [ 1 ] , line_start [ 0 ] , line_end [ 1 ] , line_end [ 0 ] ) ) lines = np . round ( np . float32 ( lines ) ) . astype ( np . int32 ) color = np . float32 ( color ) heatmap = np . zeros ( image . shape [ 0 : 2 ] , dtype = np . float32 ) for line in lines : if antialiased : rr , cc , val = skimage . draw . line_aa ( * line ) else : rr , cc = skimage . draw . line ( * line ) val = 1.0 rr_mask = np . logical_and ( 0 <= rr , rr < height ) cc_mask = np . logical_and ( 0 <= cc , cc < width ) mask = np . logical_and ( rr_mask , cc_mask ) if np . any ( mask ) : rr = rr [ mask ] cc = cc [ mask ] val = val [ mask ] if not ia . is_single_number ( val ) else val heatmap [ rr , cc ] = val * alpha if size > 1 : kernel = np . ones ( ( size , size ) , dtype = np . uint8 ) heatmap = cv2 . dilate ( heatmap , kernel ) if image_was_empty : image_blend = image + heatmap * color else : image_color_shape = image . shape [ 0 : 2 ] if image . ndim == 3 : image_color_shape = image_color_shape + ( 1 , ) image_color = np . tile ( color , image_color_shape ) image_blend = blendlib . blend_alpha ( image_color , image , heatmap ) image_blend = iadt . restore_dtypes_ ( image_blend , np . uint8 ) return image_blend",Draw the line segments of the line string on a given image .
"def draw_points_on_image ( self , image , color = ( 0 , 128 , 0 ) , alpha = 1.0 , size = 3 , copy = True , raise_if_out_of_image = False ) : from . kps import KeypointsOnImage kpsoi = KeypointsOnImage . from_xy_array ( self . coords , shape = image . shape ) image = kpsoi . draw_on_image ( image , color = color , alpha = alpha , size = size , copy = copy , raise_if_out_of_image = raise_if_out_of_image ) return image",Draw the points of the line string on a given image .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : assert color is not None assert alpha is not None assert size is not None color_lines = color_lines if color_lines is not None else np . float32 ( color ) color_points = color_points if color_points is not None else np . float32 ( color ) * 0.5 alpha_lines = alpha_lines if alpha_lines is not None else np . float32 ( alpha ) alpha_points = alpha_points if alpha_points is not None else np . float32 ( alpha ) size_lines = size_lines if size_lines is not None else size size_points = size_points if size_points is not None else size * 3 image = self . draw_lines_on_image ( image , color = np . array ( color_lines ) . astype ( np . uint8 ) , alpha = alpha_lines , size = size_lines , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) image = self . draw_points_on_image ( image , color = np . array ( color_points ) . astype ( np . uint8 ) , alpha = alpha_points , size = size_points , copy = False , raise_if_out_of_image = raise_if_out_of_image ) return image",Draw the line string on an image .
"def extract_from_image ( self , image , size = 1 , pad = True , pad_max = None , antialiased = True , prevent_zero_size = True ) : from . bbs import BoundingBox assert image . ndim in [ 2 , 3 ] , ( ""Expected image of shape (H,W,[C]), "" ""got shape %s."" % ( image . shape , ) ) if len ( self . coords ) == 0 or size <= 0 : if prevent_zero_size : return np . zeros ( ( 1 , 1 ) + image . shape [ 2 : ] , dtype = image . dtype ) return np . zeros ( ( 0 , 0 ) + image . shape [ 2 : ] , dtype = image . dtype ) xx = self . xx_int yy = self . yy_int sizeh = ( size - 1 ) / 2 x1 = np . min ( xx ) - sizeh y1 = np . min ( yy ) - sizeh x2 = np . max ( xx ) + 1 + sizeh y2 = np . max ( yy ) + 1 + sizeh bb = BoundingBox ( x1 = x1 , y1 = y1 , x2 = x2 , y2 = y2 ) if len ( self . coords ) == 1 : return bb . extract_from_image ( image , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) heatmap = self . draw_lines_heatmap_array ( image . shape [ 0 : 2 ] , alpha = 1.0 , size = size , antialiased = antialiased ) if image . ndim == 3 : heatmap = np . atleast_3d ( heatmap ) image_masked = image . astype ( np . float32 ) * heatmap extract = bb . extract_from_image ( image_masked , pad = pad , pad_max = pad_max , prevent_zero_size = prevent_zero_size ) return np . clip ( np . round ( extract ) , 0 , 255 ) . astype ( np . uint8 )",Extract the image pixels covered by the line string .
"def concatenate ( self , other ) : if not isinstance ( other , LineString ) : other = LineString ( other ) return self . deepcopy ( coords = np . concatenate ( [ self . coords , other . coords ] , axis = 0 ) )",Concatenate this line string with another one .
"def subdivide ( self , points_per_edge ) : if len ( self . coords ) <= 1 or points_per_edge < 1 : return self . deepcopy ( ) coords = interpolate_points ( self . coords , nb_steps = points_per_edge , closed = False ) return self . deepcopy ( coords = coords )",Adds N interpolated points with uniform spacing to each edge .
"def to_keypoints ( self ) : from imgaug . augmentables . kps import Keypoint return [ Keypoint ( x = x , y = y ) for ( x , y ) in self . coords ]",Convert the line string points to keypoints .
"def to_bounding_box ( self ) : from . bbs import BoundingBox if len ( self . coords ) == 0 : return None return BoundingBox ( x1 = np . min ( self . xx ) , y1 = np . min ( self . yy ) , x2 = np . max ( self . xx ) , y2 = np . max ( self . yy ) , label = self . label )",Generate a bounding box encapsulating the line string .
"def to_polygon ( self ) : from . polys import Polygon return Polygon ( self . coords , label = self . label )",Generate a polygon from the line string points .
"def to_heatmap ( self , image_shape , size_lines = 1 , size_points = 0 , antialiased = True , raise_if_out_of_image = False ) : from . heatmaps import HeatmapsOnImage return HeatmapsOnImage ( self . draw_heatmap_array ( image_shape , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )",Generate a heatmap object from the line string .
"def to_segmentation_map ( self , image_shape , size_lines = 1 , size_points = 0 , raise_if_out_of_image = False ) : from . segmaps import SegmentationMapOnImage return SegmentationMapOnImage ( self . draw_mask ( image_shape , size_lines = size_lines , size_points = size_points , raise_if_out_of_image = raise_if_out_of_image ) , shape = image_shape )",Generate a segmentation map object from the line string .
"def coords_almost_equals ( self , other , max_distance = 1e-6 , points_per_edge = 8 ) : if isinstance ( other , LineString ) : pass elif isinstance ( other , tuple ) : other = LineString ( [ other ] ) else : other = LineString ( other ) if len ( self . coords ) == 0 and len ( other . coords ) == 0 : return True elif 0 in [ len ( self . coords ) , len ( other . coords ) ] : return False self_subd = self . subdivide ( points_per_edge ) other_subd = other . subdivide ( points_per_edge ) dist_self2other = self_subd . compute_pointwise_distances ( other_subd ) dist_other2self = other_subd . compute_pointwise_distances ( self_subd ) dist = max ( np . max ( dist_self2other ) , np . max ( dist_other2self ) ) return dist < max_distance",Compare this and another LineString s coordinates .
"def almost_equals ( self , other , max_distance = 1e-4 , points_per_edge = 8 ) : if self . label != other . label : return False return self . coords_almost_equals ( other , max_distance = max_distance , points_per_edge = points_per_edge )",Compare this and another LineString .
"def copy ( self , coords = None , label = None ) : return LineString ( coords = self . coords if coords is None else coords , label = self . label if label is None else label )",Create a shallow copy of the LineString object .
"def deepcopy ( self , coords = None , label = None ) : return LineString ( coords = np . copy ( self . coords ) if coords is None else coords , label = copylib . deepcopy ( self . label ) if label is None else label )",Create a deep copy of the BoundingBox object .
"def on ( self , image ) : shape = normalize_shape ( image ) if shape [ 0 : 2 ] == self . shape [ 0 : 2 ] : return self . deepcopy ( ) line_strings = [ ls . project ( self . shape , shape ) for ls in self . line_strings ] return self . deepcopy ( line_strings = line_strings , shape = shape )",Project bounding boxes from one image to a new one .
"def from_xy_arrays ( cls , xy , shape ) : lss = [ ] for xy_ls in xy : lss . append ( LineString ( xy_ls ) ) return cls ( lss , shape )",Convert an ( N M 2 ) ndarray to a LineStringsOnImage object .
"def to_xy_arrays ( self , dtype = np . float32 ) : from . . import dtypes as iadt return [ iadt . restore_dtypes_ ( np . copy ( ls . coords ) , dtype ) for ls in self . line_strings ]",Convert this object to an iterable of ( M 2 ) arrays of points .
"def draw_on_image ( self , image , color = ( 0 , 255 , 0 ) , color_lines = None , color_points = None , alpha = 1.0 , alpha_lines = None , alpha_points = None , size = 1 , size_lines = None , size_points = None , antialiased = True , raise_if_out_of_image = False ) : for ls in self . line_strings : image = ls . draw_on_image ( image , color = color , color_lines = color_lines , color_points = color_points , alpha = alpha , alpha_lines = alpha_lines , alpha_points = alpha_points , size = size , size_lines = size_lines , size_points = size_points , antialiased = antialiased , raise_if_out_of_image = raise_if_out_of_image ) return image",Draw all line strings onto a given image .
"def remove_out_of_image ( self , fully = True , partly = False ) : lss_clean = [ ls for ls in self . line_strings if not ls . is_out_of_image ( self . shape , fully = fully , partly = partly ) ] return LineStringsOnImage ( lss_clean , shape = self . shape )",Remove all line strings that are fully / partially outside of the image .
"def clip_out_of_image ( self ) : lss_cut = [ ls_clipped for ls in self . line_strings for ls_clipped in ls . clip_out_of_image ( self . shape ) ] return LineStringsOnImage ( lss_cut , shape = self . shape )",Clip off all parts of the line strings that are outside of the image .
"def shift ( self , top = None , right = None , bottom = None , left = None ) : lss_new = [ ls . shift ( top = top , right = right , bottom = bottom , left = left ) for ls in self . line_strings ] return LineStringsOnImage ( lss_new , shape = self . shape )",Shift / move the line strings from one or more image sides .
"def copy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = lss , shape = shape )",Create a shallow copy of the LineStringsOnImage object .
"def deepcopy ( self , line_strings = None , shape = None ) : lss = self . line_strings if line_strings is None else line_strings shape = self . shape if shape is None else shape return LineStringsOnImage ( line_strings = [ ls . deepcopy ( ) for ls in lss ] , shape = tuple ( shape ) )",Create a deep copy of the LineStringsOnImage object .
"def blend_alpha ( image_fg , image_bg , alpha , eps = 1e-2 ) : assert image_fg . shape == image_bg . shape assert image_fg . dtype . kind == image_bg . dtype . kind assert image_fg . dtype . name not in [ ""float128"" ] assert image_bg . dtype . name not in [ ""float128"" ] input_was_2d = ( len ( image_fg . shape ) == 2 ) if input_was_2d : image_fg = np . atleast_3d ( image_fg ) image_bg = np . atleast_3d ( image_bg ) input_was_bool = False if image_fg . dtype . kind == ""b"" : input_was_bool = True image_fg = image_fg . astype ( np . float32 ) image_bg = image_bg . astype ( np . float32 ) alpha = np . array ( alpha , dtype = np . float64 ) if alpha . size == 1 : pass else : if alpha . ndim == 2 : assert alpha . shape == image_fg . shape [ 0 : 2 ] alpha = alpha . reshape ( ( alpha . shape [ 0 ] , alpha . shape [ 1 ] , 1 ) ) elif alpha . ndim == 3 : assert alpha . shape == image_fg . shape or alpha . shape == image_fg . shape [ 0 : 2 ] + ( 1 , ) else : alpha = alpha . reshape ( ( 1 , 1 , - 1 ) ) if alpha . shape [ 2 ] != image_fg . shape [ 2 ] : alpha = np . tile ( alpha , ( 1 , 1 , image_fg . shape [ 2 ] ) ) if not input_was_bool : if np . all ( alpha >= 1.0 - eps ) : return np . copy ( image_fg ) elif np . all ( alpha <= eps ) : return np . copy ( image_bg ) assert 0 <= alpha . item ( 0 ) <= 1.0 dt_images = iadt . get_minimal_dtype ( [ image_fg , image_bg ] ) isize = dt_images . itemsize * 2 isize = max ( isize , 4 ) dt_blend = np . dtype ( ""f%d"" % ( isize , ) ) if alpha . dtype != dt_blend : alpha = alpha . astype ( dt_blend ) if image_fg . dtype != dt_blend : image_fg = image_fg . astype ( dt_blend ) if image_bg . dtype != dt_blend : image_bg = image_bg . astype ( dt_blend ) image_blend = image_bg + alpha * ( image_fg - image_bg ) if input_was_bool : image_blend = image_blend > 0.5 else : image_blend = iadt . restore_dtypes_ ( image_blend , dt_images , clip = False , round = True ) if input_was_2d : return image_blend [ : , : , 0 ] return image_blend",Blend two images using an alpha blending .
"def SimplexNoiseAlpha ( first = None , second = None , per_channel = False , size_px_max = ( 2 , 16 ) , upscale_method = None , iterations = ( 1 , 3 ) , aggregation_method = ""max"" , sigmoid = True , sigmoid_thresh = None , name = None , deterministic = False , random_state = None ) : upscale_method_default = iap . Choice ( [ ""nearest"" , ""linear"" , ""cubic"" ] , p = [ 0.05 , 0.6 , 0.35 ] ) sigmoid_thresh_default = iap . Normal ( 0.0 , 5.0 ) noise = iap . SimplexNoise ( size_px_max = size_px_max , upscale_method = upscale_method if upscale_method is not None else upscale_method_default ) if iterations != 1 : noise = iap . IterativeNoiseAggregator ( noise , iterations = iterations , aggregation_method = aggregation_method ) if sigmoid is False or ( ia . is_single_number ( sigmoid ) and sigmoid <= 0.01 ) : noise = iap . Sigmoid . create_for_noise ( noise , threshold = sigmoid_thresh if sigmoid_thresh is not None else sigmoid_thresh_default , activated = sigmoid ) if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return AlphaElementwise ( factor = noise , first = first , second = second , per_channel = per_channel , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to alpha - blend two image sources using simplex noise alpha masks .
"def OneOf ( children , name = None , deterministic = False , random_state = None ) : return SomeOf ( n = 1 , children = children , random_order = False , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that always executes exactly one of its children .
"def AssertLambda ( func_images = None , func_heatmaps = None , func_keypoints = None , func_polygons = None , name = None , deterministic = False , random_state = None ) : def func_images_assert ( images , random_state , parents , hooks ) : ia . do_assert ( func_images ( images , random_state , parents , hooks ) , ""Input images did not fulfill user-defined assertion in AssertLambda."" ) return images def func_heatmaps_assert ( heatmaps , random_state , parents , hooks ) : ia . do_assert ( func_heatmaps ( heatmaps , random_state , parents , hooks ) , ""Input heatmaps did not fulfill user-defined assertion in AssertLambda."" ) return heatmaps def func_keypoints_assert ( keypoints_on_images , random_state , parents , hooks ) : ia . do_assert ( func_keypoints ( keypoints_on_images , random_state , parents , hooks ) , ""Input keypoints did not fulfill user-defined assertion in AssertLambda."" ) return keypoints_on_images def func_polygons_assert ( polygons_on_images , random_state , parents , hooks ) : ia . do_assert ( func_polygons ( polygons_on_images , random_state , parents , hooks ) , ""Input polygons did not fulfill user-defined assertion in AssertLambda."" ) return polygons_on_images if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return Lambda ( func_images_assert if func_images is not None else None , func_heatmaps_assert if func_heatmaps is not None else None , func_keypoints_assert if func_keypoints is not None else None , func_polygons_assert if func_polygons is not None else None , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that runs an assert on each batch of input images using a lambda function as condition .
"def AssertShape ( shape , check_images = True , check_heatmaps = True , check_keypoints = True , check_polygons = True , name = None , deterministic = False , random_state = None ) : ia . do_assert ( len ( shape ) == 4 , ""Expected shape to have length 4, got %d with shape: %s."" % ( len ( shape ) , str ( shape ) ) ) def compare ( observed , expected , dimension , image_index ) : if expected is not None : if ia . is_single_integer ( expected ) : ia . do_assert ( observed == expected , ""Expected dim %d (entry index: %s) to have value %d, got %d."" % ( dimension , image_index , expected , observed ) ) elif isinstance ( expected , tuple ) : ia . do_assert ( len ( expected ) == 2 ) ia . do_assert ( expected [ 0 ] <= observed < expected [ 1 ] , ""Expected dim %d (entry index: %s) to have value in range [%d, %d), got %d."" % ( dimension , image_index , expected [ 0 ] , expected [ 1 ] , observed ) ) elif isinstance ( expected , list ) : ia . do_assert ( any ( [ observed == val for val in expected ] ) , ""Expected dim %d (entry index: %s) to have any value of %s, got %d."" % ( dimension , image_index , str ( expected ) , observed ) ) else : raise Exception ( ( ""Invalid datatype for shape entry %d, expected each entry to be an integer, "" + ""a tuple (with two entries) or a list, got %s."" ) % ( dimension , type ( expected ) , ) ) def func_images ( images , _random_state , _parents , _hooks ) : if check_images : if isinstance ( images , list ) : if shape [ 0 ] is not None : compare ( len ( images ) , shape [ 0 ] , 0 , ""ALL"" ) for i in sm . xrange ( len ( images ) ) : image = images [ i ] ia . do_assert ( len ( image . shape ) == 3 , ""Expected image number %d to have a shape of length 3, got %d (shape: %s)."" % ( i , len ( image . shape ) , str ( image . shape ) ) ) for j in sm . xrange ( len ( shape ) - 1 ) : expected = shape [ j + 1 ] observed = image . shape [ j ] compare ( observed , expected , j , i ) else : ia . do_assert ( len ( images . shape ) == 4 , ""Expected image's shape to have length 4, got %d (shape: %s)."" % ( len ( images . shape ) , str ( images . shape ) ) ) for i in range ( 4 ) : expected = shape [ i ] observed = images . shape [ i ] compare ( observed , expected , i , ""ALL"" ) return images def func_heatmaps ( heatmaps , _random_state , _parents , _hooks ) : if check_heatmaps : if shape [ 0 ] is not None : compare ( len ( heatmaps ) , shape [ 0 ] , 0 , ""ALL"" ) for i in sm . xrange ( len ( heatmaps ) ) : heatmaps_i = heatmaps [ i ] for j in sm . xrange ( len ( shape [ 0 : 2 ] ) ) : expected = shape [ j + 1 ] observed = heatmaps_i . arr_0to1 . shape [ j ] compare ( observed , expected , j , i ) return heatmaps def func_keypoints ( keypoints_on_images , _random_state , _parents , _hooks ) : if check_keypoints : if shape [ 0 ] is not None : compare ( len ( keypoints_on_images ) , shape [ 0 ] , 0 , ""ALL"" ) for i in sm . xrange ( len ( keypoints_on_images ) ) : keypoints_on_image = keypoints_on_images [ i ] for j in sm . xrange ( len ( shape [ 0 : 2 ] ) ) : expected = shape [ j + 1 ] observed = keypoints_on_image . shape [ j ] compare ( observed , expected , j , i ) return keypoints_on_images def func_polygons ( polygons_on_images , _random_state , _parents , _hooks ) : if check_polygons : if shape [ 0 ] is not None : compare ( len ( polygons_on_images ) , shape [ 0 ] , 0 , ""ALL"" ) for i in sm . xrange ( len ( polygons_on_images ) ) : polygons_on_image = polygons_on_images [ i ] for j in sm . xrange ( len ( shape [ 0 : 2 ] ) ) : expected = shape [ j + 1 ] observed = polygons_on_image . shape [ j ] compare ( observed , expected , j , i ) return polygons_on_images if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return Lambda ( func_images , func_heatmaps , func_keypoints , func_polygons , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to make assumptions about the shape of input image ( s ) heatmaps and keypoints .
"def shuffle_channels ( image , random_state , channels = None ) : if image . ndim < 3 or image . shape [ 2 ] == 1 : return image nb_channels = image . shape [ 2 ] all_channels = np . arange ( nb_channels ) is_all_channels = ( channels is None or channels == ia . ALL or len ( set ( all_channels ) . difference ( set ( channels ) ) ) == 0 ) if is_all_channels : channels_perm = random_state . permutation ( all_channels ) return image [ ... , channels_perm ] else : channels_perm = random_state . permutation ( channels ) channels_perm_full = all_channels for channel_source , channel_target in zip ( channels , channels_perm ) : channels_perm_full [ channel_source ] = channel_target return image [ ... , channels_perm_full ]",Randomize the order of ( color ) channels in an image .
"def blur_gaussian_ ( image , sigma , ksize = None , backend = ""auto"" , eps = 1e-3 ) : if sigma > 0 + eps : dtype = image . dtype iadt . gate_dtypes ( image , allowed = [ ""bool"" , ""uint8"" , ""uint16"" , ""uint32"" , ""int8"" , ""int16"" , ""int32"" , ""int64"" , ""uint64"" , ""float16"" , ""float32"" , ""float64"" ] , disallowed = [ ""uint128"" , ""uint256"" , ""int128"" , ""int256"" , ""float96"" , ""float128"" , ""float256"" ] , augmenter = None ) dts_not_supported_by_cv2 = [ ""uint32"" , ""uint64"" , ""int64"" , ""float128"" ] backend_to_use = backend if backend == ""auto"" : backend_to_use = ""cv2"" if image . dtype . name not in dts_not_supported_by_cv2 else ""scipy"" elif backend == ""cv2"" : assert image . dtype . name not in dts_not_supported_by_cv2 , ( ""Requested 'cv2' backend, but provided %s input image, which "" + ""cannot be handled by that backend. Choose a different backend or "" + ""set backend to 'auto' or use a different datatype."" ) % ( image . dtype . name , ) elif backend == ""scipy"" : pass if backend_to_use == ""scipy"" : if dtype . name == ""bool"" : image = image . astype ( np . float32 , copy = False ) elif dtype . name == ""float16"" : image = image . astype ( np . float32 , copy = False ) if ksize is not None : warnings . warn ( ""Requested 'scipy' backend or picked it automatically by backend='auto' "" ""in blur_gaussian_(), but also provided 'ksize' argument, which is not understood by "" ""that backend and will be ignored."" ) if image . ndim == 2 : image [ : , : ] = ndimage . gaussian_filter ( image [ : , : ] , sigma , mode = ""mirror"" ) else : nb_channels = image . shape [ 2 ] for channel in sm . xrange ( nb_channels ) : image [ : , : , channel ] = ndimage . gaussian_filter ( image [ : , : , channel ] , sigma , mode = ""mirror"" ) else : if dtype . name == ""bool"" : image = image . astype ( np . float32 , copy = False ) elif dtype . name == ""float16"" : image = image . astype ( np . float32 , copy = False ) elif dtype . name == ""int8"" : image = image . astype ( np . int16 , copy = False ) elif dtype . name == ""int32"" : image = image . astype ( np . float64 , copy = False ) if ksize is None : if sigma < 3.0 : ksize = 3.3 * sigma elif sigma < 5.0 : ksize = 2.9 * sigma else : ksize = 2.6 * sigma ksize = int ( max ( ksize , 5 ) ) else : assert ia . is_single_integer ( ksize ) , ""Expected 'ksize' argument to be a number, got %s."" % ( type ( ksize ) , ) ksize = ksize + 1 if ksize % 2 == 0 else ksize if ksize > 0 : image_warped = cv2 . GaussianBlur ( image , ( ksize , ksize ) , sigmaX = sigma , sigmaY = sigma , borderType = cv2 . BORDER_REFLECT_101 ) image = image_warped [ ... , np . newaxis ] if image . ndim == 3 and image_warped . ndim == 2 else image_warped if dtype . name == ""bool"" : image = image > 0.5 elif dtype . name != image . dtype . name : image = iadt . restore_dtypes_ ( image , dtype ) return image",Blur an image using gaussian blurring .
"def MotionBlur ( k = 5 , angle = ( 0 , 360 ) , direction = ( - 1.0 , 1.0 ) , order = 1 , name = None , deterministic = False , random_state = None ) : k_param = iap . handle_discrete_param ( k , ""k"" , value_range = ( 3 , None ) , tuple_to_uniform = True , list_to_choice = True , allow_floats = False ) angle_param = iap . handle_continuous_param ( angle , ""angle"" , value_range = None , tuple_to_uniform = True , list_to_choice = True ) direction_param = iap . handle_continuous_param ( direction , ""direction"" , value_range = ( - 1.0 - 1e-6 , 1.0 + 1e-6 ) , tuple_to_uniform = True , list_to_choice = True ) def create_matrices ( image , nb_channels , random_state_func ) : from . import geometric as iaa_geometric k_sample = int ( k_param . draw_sample ( random_state = random_state_func ) ) angle_sample = angle_param . draw_sample ( random_state = random_state_func ) direction_sample = direction_param . draw_sample ( random_state = random_state_func ) k_sample = k_sample if k_sample % 2 != 0 else k_sample + 1 direction_sample = np . clip ( direction_sample , - 1.0 , 1.0 ) direction_sample = ( direction_sample + 1.0 ) / 2.0 matrix = np . zeros ( ( k_sample , k_sample ) , dtype = np . float32 ) matrix [ : , k_sample // 2 ] = np . linspace ( float ( direction_sample ) , 1.0 - float ( direction_sample ) , num = k_sample ) rot = iaa_geometric . Affine ( rotate = angle_sample , order = order ) matrix = ( rot . augment_image ( ( matrix * 255 ) . astype ( np . uint8 ) ) / 255.0 ) . astype ( np . float32 ) return [ matrix / np . sum ( matrix ) ] * nb_channels if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return iaa_convolutional . Convolve ( create_matrices , name = name , deterministic = deterministic , random_state = random_state )",Augmenter that sharpens images and overlays the result with the original image .
"def Clouds ( name = None , deterministic = False , random_state = None ) : if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return meta . SomeOf ( ( 1 , 2 ) , children = [ CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.5 , - 2.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.25 , 0.75 ) , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 2.5 , - 2.0 ) , sparsity = ( 0.8 , 1.0 ) , density_multiplier = ( 0.5 , 1.0 ) ) , CloudLayer ( intensity_mean = ( 196 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.0 ) , intensity_coarse_scale = 10 , alpha_min = 0 , alpha_multiplier = ( 0.5 , 1.0 ) , alpha_size_px_max = ( 64 , 128 ) , alpha_freq_exponent = ( - 2.0 , - 1.0 ) , sparsity = ( 1.0 , 1.4 ) , density_multiplier = ( 0.8 , 1.5 ) ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to draw clouds in images .
"def Fog ( name = None , deterministic = False , random_state = None ) : if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) return CloudLayer ( intensity_mean = ( 220 , 255 ) , intensity_freq_exponent = ( - 2.0 , - 1.5 ) , intensity_coarse_scale = 2 , alpha_min = ( 0.7 , 0.9 ) , alpha_multiplier = 0.3 , alpha_size_px_max = ( 2 , 8 ) , alpha_freq_exponent = ( - 4.0 , - 2.0 ) , sparsity = 0.9 , density_multiplier = ( 0.4 , 0.9 ) , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to draw fog in images .
"def Snowflakes ( density = ( 0.005 , 0.075 ) , density_uniformity = ( 0.3 , 0.9 ) , flake_size = ( 0.2 , 0.7 ) , flake_size_uniformity = ( 0.4 , 0.8 ) , angle = ( - 30 , 30 ) , speed = ( 0.007 , 0.03 ) , name = None , deterministic = False , random_state = None ) : if name is None : name = ""Unnamed%s"" % ( ia . caller_name ( ) , ) layer = SnowflakesLayer ( density = density , density_uniformity = density_uniformity , flake_size = flake_size , flake_size_uniformity = flake_size_uniformity , angle = angle , speed = speed , blur_sigma_fraction = ( 0.0001 , 0.001 ) ) return meta . SomeOf ( ( 1 , 3 ) , children = [ layer . deepcopy ( ) for _ in range ( 3 ) ] , random_order = False , name = name , deterministic = deterministic , random_state = random_state )",Augmenter to add falling snowflakes to images .
"def get_arr_int ( self , background_threshold = 0.01 , background_class_id = None ) : if self . input_was [ 0 ] in [ ""bool"" , ""float"" ] : ia . do_assert ( background_class_id is None , ""The background class id may only be changed if the original input to SegmentationMapOnImage "" + ""was an *integer* based segmentation map."" ) if background_class_id is None : background_class_id = 0 channelwise_max_idx = np . argmax ( self . arr , axis = 2 ) if self . input_was [ 0 ] in [ ""bool"" , ""float"" ] : result = 1 + channelwise_max_idx else : result = channelwise_max_idx if background_threshold is not None and background_threshold > 0 : probs = np . amax ( self . arr , axis = 2 ) result [ probs < background_threshold ] = background_class_id return result . astype ( np . int32 )",Get the segmentation map array as an integer array of shape ( H W ) .
"def draw ( self , size = None , background_threshold = 0.01 , background_class_id = None , colors = None , return_foreground_mask = False ) : arr = self . get_arr_int ( background_threshold = background_threshold , background_class_id = background_class_id ) nb_classes = 1 + np . max ( arr ) segmap_drawn = np . zeros ( ( arr . shape [ 0 ] , arr . shape [ 1 ] , 3 ) , dtype = np . uint8 ) if colors is None : colors = SegmentationMapOnImage . DEFAULT_SEGMENT_COLORS ia . do_assert ( nb_classes <= len ( colors ) , ""Can't draw all %d classes as it would exceed the maximum number of %d available colors."" % ( nb_classes , len ( colors ) , ) ) ids_in_map = np . unique ( arr ) for c , color in zip ( sm . xrange ( nb_classes ) , colors ) : if c in ids_in_map : class_mask = ( arr == c ) segmap_drawn [ class_mask ] = color if return_foreground_mask : background_class_id = 0 if background_class_id is None else background_class_id foreground_mask = ( arr != background_class_id ) else : foreground_mask = None if size is not None : segmap_drawn = ia . imresize_single_image ( segmap_drawn , size , interpolation = ""nearest"" ) if foreground_mask is not None : foreground_mask = ia . imresize_single_image ( foreground_mask . astype ( np . uint8 ) , size , interpolation = ""nearest"" ) > 0 if foreground_mask is not None : return segmap_drawn , foreground_mask return segmap_drawn",Render the segmentation map as an RGB image .
"def draw_on_image ( self , image , alpha = 0.75 , resize = ""segmentation_map"" , background_threshold = 0.01 , background_class_id = None , colors = None , draw_background = False ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ ""segmentation_map"" , ""image"" ] ) if resize == ""image"" : image = ia . imresize_single_image ( image , self . arr . shape [ 0 : 2 ] , interpolation = ""cubic"" ) segmap_drawn , foreground_mask = self . draw ( background_threshold = background_threshold , background_class_id = background_class_id , size = image . shape [ 0 : 2 ] if resize == ""segmentation_map"" else None , colors = colors , return_foreground_mask = True ) if draw_background : mix = np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) else : foreground_mask = foreground_mask [ ... , np . newaxis ] mix = np . zeros_like ( image ) mix += ( ~ foreground_mask ) . astype ( np . uint8 ) * image mix += foreground_mask . astype ( np . uint8 ) * np . clip ( ( 1 - alpha ) * image + alpha * segmap_drawn , 0 , 255 ) . astype ( np . uint8 ) return mix",Draw the segmentation map as an overlay over an image .
"def pad ( self , top = 0 , right = 0 , bottom = 0 , left = 0 , mode = ""constant"" , cval = 0.0 ) : arr_padded = ia . pad ( self . arr , top = top , right = right , bottom = bottom , left = left , mode = mode , cval = cval ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was return segmap",Pad the segmentation map on its top / right / bottom / left side .
"def pad_to_aspect_ratio ( self , aspect_ratio , mode = ""constant"" , cval = 0.0 , return_pad_amounts = False ) : arr_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) segmap = SegmentationMapOnImage ( arr_padded , shape = self . shape ) segmap . input_was = self . input_was if return_pad_amounts : return segmap , pad_amounts else : return segmap",Pad the segmentation map on its sides so that its matches a target aspect ratio .
"def resize ( self , sizes , interpolation = ""cubic"" ) : arr_resized = ia . imresize_single_image ( self . arr , sizes , interpolation = interpolation ) arr_resized = np . clip ( arr_resized , 0.0 , 1.0 ) segmap = SegmentationMapOnImage ( arr_resized , shape = self . shape ) segmap . input_was = self . input_was return segmap",Resize the segmentation map array to the provided size given the provided interpolation .
"def to_heatmaps ( self , only_nonempty = False , not_none_if_no_nonempty = False ) : from imgaug . augmentables . heatmaps import HeatmapsOnImage if not only_nonempty : return HeatmapsOnImage . from_0to1 ( self . arr , self . shape , min_value = 0.0 , max_value = 1.0 ) else : nonempty_mask = np . sum ( self . arr , axis = ( 0 , 1 ) ) > 0 + 1e-4 if np . sum ( nonempty_mask ) == 0 : if not_none_if_no_nonempty : nonempty_mask [ 0 ] = True else : return None , [ ] class_indices = np . arange ( self . arr . shape [ 2 ] ) [ nonempty_mask ] channels = self . arr [ ... , class_indices ] return HeatmapsOnImage ( channels , self . shape , min_value = 0.0 , max_value = 1.0 ) , class_indices",Convert segmentation map to heatmaps object .
"def from_heatmaps ( heatmaps , class_indices = None , nb_classes = None ) : if class_indices is None : return SegmentationMapOnImage ( heatmaps . arr_0to1 , shape = heatmaps . shape ) else : ia . do_assert ( nb_classes is not None ) ia . do_assert ( min ( class_indices ) >= 0 ) ia . do_assert ( max ( class_indices ) < nb_classes ) ia . do_assert ( len ( class_indices ) == heatmaps . arr_0to1 . shape [ 2 ] ) arr_0to1 = heatmaps . arr_0to1 arr_0to1_full = np . zeros ( ( arr_0to1 . shape [ 0 ] , arr_0to1 . shape [ 1 ] , nb_classes ) , dtype = np . float32 ) for heatmap_channel , mapped_channel in enumerate ( class_indices ) : arr_0to1_full [ : , : , mapped_channel ] = arr_0to1 [ : , : , heatmap_channel ] return SegmentationMapOnImage ( arr_0to1_full , shape = heatmaps . shape )",Convert heatmaps to segmentation map .
"def deepcopy ( self ) : segmap = SegmentationMapOnImage ( self . arr , shape = self . shape , nb_classes = self . nb_classes ) segmap . input_was = self . input_was return segmap",Create a deep copy of the segmentation map object .
"def offer ( self , p , e : Event ) : existing = self . events_scan . setdefault ( p , ( [ ] , [ ] , [ ] , [ ] ) if USE_VERTICAL else ( [ ] , [ ] , [ ] ) ) '''
        if e.type == Event.Type.END:
            existing.insert(0, e)
        else:
            existing.append(e)
        ''' existing [ e . type ] . append ( e )",Offer a new event s at point p in this queue .
"def get_arr ( self ) : if self . arr_was_2d and self . arr_0to1 . shape [ 2 ] == 1 : arr = self . arr_0to1 [ : , : , 0 ] else : arr = self . arr_0to1 eps = np . finfo ( np . float32 ) . eps min_is_zero = 0.0 - eps < self . min_value < 0.0 + eps max_is_one = 1.0 - eps < self . max_value < 1.0 + eps if min_is_zero and max_is_one : return np . copy ( arr ) else : diff = self . max_value - self . min_value return self . min_value + diff * arr",Get the heatmap s array within the value range originally provided in __init__ () .
"def draw ( self , size = None , cmap = ""jet"" ) : heatmaps_uint8 = self . to_uint8 ( ) heatmaps_drawn = [ ] for c in sm . xrange ( heatmaps_uint8 . shape [ 2 ] ) : heatmap_c = heatmaps_uint8 [ ... , c : c + 1 ] if size is not None : heatmap_c_rs = ia . imresize_single_image ( heatmap_c , size , interpolation = ""nearest"" ) else : heatmap_c_rs = heatmap_c heatmap_c_rs = np . squeeze ( heatmap_c_rs ) . astype ( np . float32 ) / 255.0 if cmap is not None : import matplotlib . pyplot as plt cmap_func = plt . get_cmap ( cmap ) heatmap_cmapped = cmap_func ( heatmap_c_rs ) heatmap_cmapped = np . delete ( heatmap_cmapped , 3 , 2 ) else : heatmap_cmapped = np . tile ( heatmap_c_rs [ ... , np . newaxis ] , ( 1 , 1 , 3 ) ) heatmap_cmapped = np . clip ( heatmap_cmapped * 255 , 0 , 255 ) . astype ( np . uint8 ) heatmaps_drawn . append ( heatmap_cmapped ) return heatmaps_drawn",Render the heatmaps as RGB images .
"def draw_on_image ( self , image , alpha = 0.75 , cmap = ""jet"" , resize = ""heatmaps"" ) : ia . do_assert ( image . ndim == 3 ) ia . do_assert ( image . shape [ 2 ] == 3 ) ia . do_assert ( image . dtype . type == np . uint8 ) ia . do_assert ( 0 - 1e-8 <= alpha <= 1.0 + 1e-8 ) ia . do_assert ( resize in [ ""heatmaps"" , ""image"" ] ) if resize == ""image"" : image = ia . imresize_single_image ( image , self . arr_0to1 . shape [ 0 : 2 ] , interpolation = ""cubic"" ) heatmaps_drawn = self . draw ( size = image . shape [ 0 : 2 ] if resize == ""heatmaps"" else None , cmap = cmap ) mix = [ np . clip ( ( 1 - alpha ) * image + alpha * heatmap_i , 0 , 255 ) . astype ( np . uint8 ) for heatmap_i in heatmaps_drawn ] return mix",Draw the heatmaps as overlays over an image .
"def invert ( self ) : arr_inv = HeatmapsOnImage . from_0to1 ( 1 - self . arr_0to1 , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) arr_inv . arr_was_2d = self . arr_was_2d return arr_inv",Inverts each value in the heatmap shifting low towards high values and vice versa .
"def pad ( self , top = 0 , right = 0 , bottom = 0 , left = 0 , mode = ""constant"" , cval = 0.0 ) : arr_0to1_padded = ia . pad ( self . arr_0to1 , top = top , right = right , bottom = bottom , left = left , mode = mode , cval = cval ) return HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value )",Pad the heatmaps on their top / right / bottom / left side .
"def pad_to_aspect_ratio ( self , aspect_ratio , mode = ""constant"" , cval = 0.0 , return_pad_amounts = False ) : arr_0to1_padded , pad_amounts = ia . pad_to_aspect_ratio ( self . arr_0to1 , aspect_ratio = aspect_ratio , mode = mode , cval = cval , return_pad_amounts = True ) heatmaps = HeatmapsOnImage . from_0to1 ( arr_0to1_padded , shape = self . shape , min_value = self . min_value , max_value = self . max_value ) if return_pad_amounts : return heatmaps , pad_amounts else : return heatmaps",Pad the heatmaps on their sides so that they match a target aspect ratio .
"def avg_pool ( self , block_size ) : arr_0to1_reduced = ia . avg_pool ( self . arr_0to1 , block_size , cval = 0.0 ) return HeatmapsOnImage . from_0to1 ( arr_0to1_reduced , shape = self . shape , min_value = self . min_value , max_value = self . max_value )",Resize the heatmap ( s ) array using average pooling of a given block / kernel size .
"def max_pool ( self , block_size ) : arr_0to1_reduced = ia . max_pool ( self . arr_0to1 , block_size ) return HeatmapsOnImage . from_0to1 ( arr_0to1_reduced , shape = self . shape , min_value = self . min_value , max_value = self . max_value )",Resize the heatmap ( s ) array using max - pooling of a given block / kernel size .
"def resize ( self , sizes , interpolation = ""cubic"" ) : arr_0to1_resized = ia . imresize_single_image ( self . arr_0to1 , sizes , interpolation = interpolation ) arr_0to1_resized = np . clip ( arr_0to1_resized , 0.0 , 1.0 ) return HeatmapsOnImage . from_0to1 ( arr_0to1_resized , shape = self . shape , min_value = self . min_value , max_value = self . max_value )",Resize the heatmap ( s ) array to the provided size given the provided interpolation .
"def to_uint8 ( self ) : arr_0to255 = np . clip ( np . round ( self . arr_0to1 * 255 ) , 0 , 255 ) arr_uint8 = arr_0to255 . astype ( np . uint8 ) return arr_uint8",Convert this heatmaps object to a 0 - to - 255 array .
"def from_uint8 ( arr_uint8 , shape , min_value = 0.0 , max_value = 1.0 ) : arr_0to1 = arr_uint8 . astype ( np . float32 ) / 255.0 return HeatmapsOnImage . from_0to1 ( arr_0to1 , shape , min_value = min_value , max_value = max_value )",Create a heatmaps object from an heatmap array containing values ranging from 0 to 255 .
"def from_0to1 ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) : heatmaps = HeatmapsOnImage ( arr_0to1 , shape , min_value = 0.0 , max_value = 1.0 ) heatmaps . min_value = min_value heatmaps . max_value = max_value return heatmaps",Create a heatmaps object from an heatmap array containing values ranging from 0 . 0 to 1 . 0 .
"def change_normalization ( cls , arr , source , target ) : ia . do_assert ( ia . is_np_array ( arr ) ) if isinstance ( source , HeatmapsOnImage ) : source = ( source . min_value , source . max_value ) else : ia . do_assert ( isinstance ( source , tuple ) ) ia . do_assert ( len ( source ) == 2 ) ia . do_assert ( source [ 0 ] < source [ 1 ] ) if isinstance ( target , HeatmapsOnImage ) : target = ( target . min_value , target . max_value ) else : ia . do_assert ( isinstance ( target , tuple ) ) ia . do_assert ( len ( target ) == 2 ) ia . do_assert ( target [ 0 ] < target [ 1 ] ) eps = np . finfo ( arr . dtype ) . eps mins_same = source [ 0 ] - 10 * eps < target [ 0 ] < source [ 0 ] + 10 * eps maxs_same = source [ 1 ] - 10 * eps < target [ 1 ] < source [ 1 ] + 10 * eps if mins_same and maxs_same : return np . copy ( arr ) min_source , max_source = source min_target , max_target = target diff_source = max_source - min_source diff_target = max_target - min_target arr_0to1 = ( arr - min_source ) / diff_source arr_target = min_target + arr_0to1 * diff_target return arr_target",Change the value range of a heatmap from one min - max to another min - max .
"def deepcopy ( self ) : return HeatmapsOnImage ( self . get_arr ( ) , shape = self . shape , min_value = self . min_value , max_value = self . max_value )",Create a deep copy of the Heatmaps object .
"def setdefault ( self , key : str , value : str ) -> str : set_key = key . lower ( ) . encode ( ""latin-1"" ) set_value = value . encode ( ""latin-1"" ) for idx , ( item_key , item_value ) in enumerate ( self . _list ) : if item_key == set_key : return item_value . decode ( ""latin-1"" ) self . _list . append ( ( set_key , set_value ) ) return value",If the header key does not exist then set it to value . Returns the header value .
"def append ( self , key : str , value : str ) -> None : append_key = key . lower ( ) . encode ( ""latin-1"" ) append_value = value . encode ( ""latin-1"" ) self . _list . append ( ( append_key , append_value ) )",Append a header preserving any duplicate entries .
"def request_response ( func : typing . Callable ) -> ASGIApp : is_coroutine = asyncio . iscoroutinefunction ( func ) async def app ( scope : Scope , receive : Receive , send : Send ) -> None : request = Request ( scope , receive = receive ) if is_coroutine : response = await func ( request ) else : response = await run_in_threadpool ( func , request ) await response ( scope , receive , send ) return app",Takes a function or coroutine func ( request ) - > response and returns an ASGI application .
"def websocket_session ( func : typing . Callable ) -> ASGIApp : async def app ( scope : Scope , receive : Receive , send : Send ) -> None : session = WebSocket ( scope , receive = receive , send = send ) await func ( session ) return app",Takes a coroutine func ( session ) and returns an ASGI application .
"def compile_path ( path : str ) -> typing . Tuple [ typing . Pattern , str , typing . Dict [ str , Convertor ] ] : path_regex = ""^"" path_format = """" idx = 0 param_convertors = { } for match in PARAM_REGEX . finditer ( path ) : param_name , convertor_type = match . groups ( ""str"" ) convertor_type = convertor_type . lstrip ( "":"" ) assert ( convertor_type in CONVERTOR_TYPES ) , f""Unknown path convertor '{convertor_type}'"" convertor = CONVERTOR_TYPES [ convertor_type ] path_regex += path [ idx : match . start ( ) ] path_regex += f""(?P<{param_name}>{convertor.regex})"" path_format += path [ idx : match . start ( ) ] path_format += ""{%s}"" % param_name param_convertors [ param_name ] = convertor idx = match . end ( ) path_regex += path [ idx : ] + ""$"" path_format += path [ idx : ] return re . compile ( path_regex ) , path_format , param_convertors",Given a path string like : / { username : str } return a three - tuple of ( regex format { param_name : convertor } ) .
"def get_endpoints ( self , routes : typing . List [ BaseRoute ] ) -> typing . List [ EndpointInfo ] : endpoints_info : list = [ ] for route in routes : if isinstance ( route , Mount ) : routes = route . routes or [ ] sub_endpoints = [ EndpointInfo ( path = """" . join ( ( route . path , sub_endpoint . path ) ) , http_method = sub_endpoint . http_method , func = sub_endpoint . func , ) for sub_endpoint in self . get_endpoints ( routes ) ] endpoints_info . extend ( sub_endpoints ) elif not isinstance ( route , Route ) or not route . include_in_schema : continue elif inspect . isfunction ( route . endpoint ) or inspect . ismethod ( route . endpoint ) : for method in route . methods or [ ""GET"" ] : if method == ""HEAD"" : continue endpoints_info . append ( EndpointInfo ( route . path , method . lower ( ) , route . endpoint ) ) else : for method in [ ""get"" , ""post"" , ""put"" , ""patch"" , ""delete"" , ""options"" ] : if not hasattr ( route . endpoint , method ) : continue func = getattr ( route . endpoint , method ) endpoints_info . append ( EndpointInfo ( route . path , method . lower ( ) , func ) ) return endpoints_info",Given the routes yields the following information :
"def parse_docstring ( self , func_or_method : typing . Callable ) -> dict : docstring = func_or_method . __doc__ if not docstring : return { } docstring = docstring . split ( ""---"" ) [ - 1 ] parsed = yaml . safe_load ( docstring ) if not isinstance ( parsed , dict ) : return { } return parsed",Given a function parse the docstring as YAML and return a dictionary of info .
"def get_directories ( self , directory : str = None , packages : typing . List [ str ] = None ) -> typing . List [ str ] : directories = [ ] if directory is not None : directories . append ( directory ) for package in packages or [ ] : spec = importlib . util . find_spec ( package ) assert spec is not None , f""Package {package!r} could not be found."" assert ( spec . origin is not None ) , ""Directory 'statics' in package {package!r} could not be found."" directory = os . path . normpath ( os . path . join ( spec . origin , "".."" , ""statics"" ) ) assert os . path . isdir ( directory ) , ""Directory 'statics' in package {package!r} could not be found."" directories . append ( directory ) return directories",Given directory and packages arugments return a list of all the directories that should be used for serving static files from .
"def get_path ( self , scope : Scope ) -> str : return os . path . normpath ( os . path . join ( * scope [ ""path"" ] . split ( ""/"" ) ) )",Given the ASGI scope return the path string to serve up with OS specific path seperators and any .. . components removed .
"async def get_response ( self , path : str , scope : Scope ) -> Response : if scope [ ""method"" ] not in ( ""GET"" , ""HEAD"" ) : return PlainTextResponse ( ""Method Not Allowed"" , status_code = 405 ) if path . startswith ( "".."" ) : return PlainTextResponse ( ""Not Found"" , status_code = 404 ) full_path , stat_result = await self . lookup_path ( path ) if stat_result and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope ) elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : index_path = os . path . join ( path , ""index.html"" ) full_path , stat_result = await self . lookup_path ( index_path ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : if not scope [ ""path"" ] . endswith ( ""/"" ) : url = URL ( scope = scope ) url = url . replace ( path = url . path + ""/"" ) return RedirectResponse ( url = url ) return self . file_response ( full_path , stat_result , scope ) if self . html : full_path , stat_result = await self . lookup_path ( ""404.html"" ) if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : return self . file_response ( full_path , stat_result , scope , status_code = 404 ) return PlainTextResponse ( ""Not Found"" , status_code = 404 )",Returns an HTTP response given the incoming path method and request headers .
"async def check_config ( self ) -> None : if self . directory is None : return try : stat_result = await aio_stat ( self . directory ) except FileNotFoundError : raise RuntimeError ( f""StaticFiles directory '{self.directory}' does not exist."" ) if not ( stat . S_ISDIR ( stat_result . st_mode ) or stat . S_ISLNK ( stat_result . st_mode ) ) : raise RuntimeError ( f""StaticFiles path '{self.directory}' is not a directory."" )",Perform a one - off configuration check that StaticFiles is actually pointed at a directory so that we can raise loud errors rather than just returning 404 responses .
"def is_not_modified ( self , response_headers : Headers , request_headers : Headers ) -> bool : try : if_none_match = request_headers [ ""if-none-match"" ] etag = response_headers [ ""etag"" ] if if_none_match == etag : return True except KeyError : pass try : if_modified_since = parsedate ( request_headers [ ""if-modified-since"" ] ) last_modified = parsedate ( response_headers [ ""last-modified"" ] ) if ( if_modified_since is not None and last_modified is not None and if_modified_since >= last_modified ) : return True except KeyError : pass return False",Given the request and response headers return True if an HTTP Not Modified response could be returned instead .
"def build_environ ( scope : Scope , body : bytes ) -> dict : environ = { ""REQUEST_METHOD"" : scope [ ""method"" ] , ""SCRIPT_NAME"" : scope . get ( ""root_path"" , """" ) , ""PATH_INFO"" : scope [ ""path"" ] , ""QUERY_STRING"" : scope [ ""query_string"" ] . decode ( ""ascii"" ) , ""SERVER_PROTOCOL"" : f""HTTP/{scope['http_version']}"" , ""wsgi.version"" : ( 1 , 0 ) , ""wsgi.url_scheme"" : scope . get ( ""scheme"" , ""http"" ) , ""wsgi.input"" : io . BytesIO ( body ) , ""wsgi.errors"" : sys . stdout , ""wsgi.multithread"" : True , ""wsgi.multiprocess"" : True , ""wsgi.run_once"" : False , } server = scope . get ( ""server"" ) or ( ""localhost"" , 80 ) environ [ ""SERVER_NAME"" ] = server [ 0 ] environ [ ""SERVER_PORT"" ] = server [ 1 ] if scope . get ( ""client"" ) : environ [ ""REMOTE_ADDR"" ] = scope [ ""client"" ] [ 0 ] for name , value in scope . get ( ""headers"" , [ ] ) : name = name . decode ( ""latin1"" ) if name == ""content-length"" : corrected_name = ""CONTENT_LENGTH"" elif name == ""content-type"" : corrected_name = ""CONTENT_TYPE"" else : corrected_name = f""HTTP_{name}"" . upper ( ) . replace ( ""-"" , ""_"" ) value = value . decode ( ""latin1"" ) if corrected_name in environ : value = environ [ corrected_name ] + "","" + value environ [ corrected_name ] = value return environ",Builds a scope and request body into a WSGI environ object .
"async def receive ( self ) -> Message : if self . client_state == WebSocketState . CONNECTING : message = await self . _receive ( ) message_type = message [ ""type"" ] assert message_type == ""websocket.connect"" self . client_state = WebSocketState . CONNECTED return message elif self . client_state == WebSocketState . CONNECTED : message = await self . _receive ( ) message_type = message [ ""type"" ] assert message_type in { ""websocket.receive"" , ""websocket.disconnect"" } if message_type == ""websocket.disconnect"" : self . client_state = WebSocketState . DISCONNECTED return message else : raise RuntimeError ( 'Cannot call ""receive"" once a disconnect message has been received.' )",Receive ASGI websocket messages ensuring valid state transitions .
"async def send ( self , message : Message ) -> None : if self . application_state == WebSocketState . CONNECTING : message_type = message [ ""type"" ] assert message_type in { ""websocket.accept"" , ""websocket.close"" } if message_type == ""websocket.close"" : self . application_state = WebSocketState . DISCONNECTED else : self . application_state = WebSocketState . CONNECTED await self . _send ( message ) elif self . application_state == WebSocketState . CONNECTED : message_type = message [ ""type"" ] assert message_type in { ""websocket.send"" , ""websocket.close"" } if message_type == ""websocket.close"" : self . application_state = WebSocketState . DISCONNECTED await self . _send ( message ) else : raise RuntimeError ( 'Cannot call ""send"" once a close message has been sent.' )",Send ASGI websocket messages ensuring valid state transitions .
"def get_top_long_short_abs ( positions , top = 10 ) : positions = positions . drop ( 'cash' , axis = 'columns' ) df_max = positions . max ( ) df_min = positions . min ( ) df_abs_max = positions . abs ( ) . max ( ) df_top_long = df_max [ df_max > 0 ] . nlargest ( top ) df_top_short = df_min [ df_min < 0 ] . nsmallest ( top ) df_top_abs = df_abs_max . nlargest ( top ) return df_top_long , df_top_short , df_top_abs",Finds the top long short and absolute positions .
"def get_max_median_position_concentration ( positions ) : expos = get_percent_alloc ( positions ) expos = expos . drop ( 'cash' , axis = 1 ) longs = expos . where ( expos . applymap ( lambda x : x > 0 ) ) shorts = expos . where ( expos . applymap ( lambda x : x < 0 ) ) alloc_summary = pd . DataFrame ( ) alloc_summary [ 'max_long' ] = longs . max ( axis = 1 ) alloc_summary [ 'median_long' ] = longs . median ( axis = 1 ) alloc_summary [ 'median_short' ] = shorts . median ( axis = 1 ) alloc_summary [ 'max_short' ] = shorts . min ( axis = 1 ) return alloc_summary",Finds the max and median long and short position concentrations in each time period specified by the index of positions .
"def extract_pos ( positions , cash ) : positions = positions . copy ( ) positions [ 'values' ] = positions . amount * positions . last_sale_price cash . name = 'cash' values = positions . reset_index ( ) . pivot_table ( index = 'index' , columns = 'sid' , values = 'values' ) if ZIPLINE : for asset in values . columns : if type ( asset ) in [ Equity , Future ] : values [ asset ] = values [ asset ] * asset . price_multiplier values = values . join ( cash ) . fillna ( 0 ) values . columns . name = 'sid' return values",Extract position values from backtest object as returned by get_backtest () on the Quantopian research platform .
"def get_sector_exposures ( positions , symbol_sector_map ) : cash = positions [ 'cash' ] positions = positions . drop ( 'cash' , axis = 1 ) unmapped_pos = np . setdiff1d ( positions . columns . values , list ( symbol_sector_map . keys ( ) ) ) if len ( unmapped_pos ) > 0 : warn_message = """"""Warning: Symbols {} have no sector mapping.
        They will not be included in sector allocations"""""" . format ( "", "" . join ( map ( str , unmapped_pos ) ) ) warnings . warn ( warn_message , UserWarning ) sector_exp = positions . groupby ( by = symbol_sector_map , axis = 1 ) . sum ( ) sector_exp [ 'cash' ] = cash return sector_exp",Sum position exposures by sector .
"def get_long_short_pos ( positions ) : pos_wo_cash = positions . drop ( 'cash' , axis = 1 ) longs = pos_wo_cash [ pos_wo_cash > 0 ] . sum ( axis = 1 ) . fillna ( 0 ) shorts = pos_wo_cash [ pos_wo_cash < 0 ] . sum ( axis = 1 ) . fillna ( 0 ) cash = positions . cash net_liquidation = longs + shorts + cash df_pos = pd . DataFrame ( { 'long' : longs . divide ( net_liquidation , axis = 'index' ) , 'short' : shorts . divide ( net_liquidation , axis = 'index' ) } ) df_pos [ 'net exposure' ] = df_pos [ 'long' ] + df_pos [ 'short' ] return df_pos",Determines the long and short allocations in a portfolio .
"def compute_style_factor_exposures ( positions , risk_factor ) : positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) style_factor_exposure = positions_wo_cash . multiply ( risk_factor ) . divide ( gross_exposure , axis = 'index' ) tot_style_factor_exposure = style_factor_exposure . sum ( axis = 'columns' , skipna = True ) return tot_style_factor_exposure",Returns style factor exposure of an algorithm s positions
"def plot_style_factor_exposures ( tot_style_factor_exposure , factor_name = None , ax = None ) : if ax is None : ax = plt . gca ( ) if factor_name is None : factor_name = tot_style_factor_exposure . name ax . plot ( tot_style_factor_exposure . index , tot_style_factor_exposure , label = factor_name ) avg = tot_style_factor_exposure . mean ( ) ax . axhline ( avg , linestyle = '-.' , label = 'Mean = {:.3}' . format ( avg ) ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) _ , _ , y1 , y2 = plt . axis ( ) lim = max ( abs ( y1 ) , abs ( y2 ) ) ax . set ( title = 'Exposure to {}' . format ( factor_name ) , ylabel = '{} \n weighted exposure' . format ( factor_name ) , ylim = ( - lim , lim ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax",Plots DataFrame output of compute_style_factor_exposures as a line graph
"def compute_sector_exposures ( positions , sectors , sector_dict = SECTORS ) : sector_ids = sector_dict . keys ( ) long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) for sector_id in sector_ids : in_sector = positions_wo_cash [ sectors == sector_id ] long_sector = in_sector [ in_sector > 0 ] . sum ( axis = 'columns' ) . divide ( long_exposure ) short_sector = in_sector [ in_sector < 0 ] . sum ( axis = 'columns' ) . divide ( short_exposure ) gross_sector = in_sector . abs ( ) . sum ( axis = 'columns' ) . divide ( gross_exposure ) net_sector = long_sector . subtract ( short_sector ) long_exposures . append ( long_sector ) short_exposures . append ( short_sector ) gross_exposures . append ( gross_sector ) net_exposures . append ( net_sector ) return long_exposures , short_exposures , gross_exposures , net_exposures",Returns arrays of long short and gross sector exposures of an algorithm s positions
"def plot_sector_exposures_longshort ( long_exposures , short_exposures , sector_dict = SECTORS , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( long_exposures [ 0 ] . index , long_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . stackplot ( long_exposures [ 0 ] . index , short_exposures , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Long and short exposures to sectors' , ylabel = 'Proportion of long/short exposure in sectors' ) ax . legend ( loc = 'upper left' , frameon = True , framealpha = 0.5 ) return ax",Plots outputs of compute_sector_exposures as area charts
"def plot_sector_exposures_gross ( gross_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) ax . stackplot ( gross_exposures [ 0 ] . index , gross_exposures , labels = sector_names , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Gross exposure to sectors' , ylabel = 'Proportion of gross exposure \n in sectors' ) return ax",Plots output of compute_sector_exposures as area charts
"def plot_sector_exposures_net ( net_exposures , sector_dict = None , ax = None ) : if ax is None : ax = plt . gca ( ) if sector_dict is None : sector_names = SECTORS . values ( ) else : sector_names = sector_dict . values ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 11 ) ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = sector_names [ i ] ) ax . set ( title = 'Net exposures to sectors' , ylabel = 'Proportion of net exposure \n in sectors' ) return ax",Plots output of compute_sector_exposures as line graphs
"def compute_cap_exposures ( positions , caps ) : long_exposures = [ ] short_exposures = [ ] gross_exposures = [ ] net_exposures = [ ] positions_wo_cash = positions . drop ( 'cash' , axis = 'columns' ) tot_gross_exposure = positions_wo_cash . abs ( ) . sum ( axis = 'columns' ) tot_long_exposure = positions_wo_cash [ positions_wo_cash > 0 ] . sum ( axis = 'columns' ) tot_short_exposure = positions_wo_cash [ positions_wo_cash < 0 ] . abs ( ) . sum ( axis = 'columns' ) for bucket_name , boundaries in CAP_BUCKETS . items ( ) : in_bucket = positions_wo_cash [ ( caps >= boundaries [ 0 ] ) & ( caps <= boundaries [ 1 ] ) ] gross_bucket = in_bucket . abs ( ) . sum ( axis = 'columns' ) . divide ( tot_gross_exposure ) long_bucket = in_bucket [ in_bucket > 0 ] . sum ( axis = 'columns' ) . divide ( tot_long_exposure ) short_bucket = in_bucket [ in_bucket < 0 ] . sum ( axis = 'columns' ) . divide ( tot_short_exposure ) net_bucket = long_bucket . subtract ( short_bucket ) gross_exposures . append ( gross_bucket ) long_exposures . append ( long_bucket ) short_exposures . append ( short_bucket ) net_exposures . append ( net_bucket ) return long_exposures , short_exposures , gross_exposures , net_exposures",Returns arrays of long short and gross market cap exposures of an algorithm s positions
"def plot_cap_exposures_longshort ( long_exposures , short_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) ax . stackplot ( long_exposures [ 0 ] . index , long_exposures , labels = CAP_BUCKETS . keys ( ) , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . stackplot ( long_exposures [ 0 ] . index , short_exposures , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Long and short exposures to market caps' , ylabel = 'Proportion of long/short exposure in market cap buckets' ) ax . legend ( loc = 'upper left' , frameon = True , framealpha = 0.5 ) return ax",Plots outputs of compute_cap_exposures as area charts
"def plot_cap_exposures_gross ( gross_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) ax . stackplot ( gross_exposures [ 0 ] . index , gross_exposures , labels = CAP_BUCKETS . keys ( ) , colors = color_list , alpha = 0.8 , baseline = 'zero' ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Gross exposure to market caps' , ylabel = 'Proportion of gross exposure \n in market cap buckets' ) return ax",Plots outputs of compute_cap_exposures as area charts
"def plot_cap_exposures_net ( net_exposures , ax = None ) : if ax is None : ax = plt . gca ( ) color_list = plt . cm . gist_rainbow ( np . linspace ( 0 , 1 , 5 ) ) cap_names = CAP_BUCKETS . keys ( ) for i in range ( len ( net_exposures ) ) : ax . plot ( net_exposures [ i ] , color = color_list [ i ] , alpha = 0.8 , label = cap_names [ i ] ) ax . axhline ( 0 , color = 'k' , linestyle = '-' ) ax . set ( title = 'Net exposure to market caps' , ylabel = 'Proportion of net exposure \n in market cap buckets' ) return ax",Plots outputs of compute_cap_exposures as line graphs
"def compute_volume_exposures ( shares_held , volumes , percentile ) : shares_held = shares_held . replace ( 0 , np . nan ) shares_longed = shares_held [ shares_held > 0 ] shares_shorted = - 1 * shares_held [ shares_held < 0 ] shares_grossed = shares_held . abs ( ) longed_frac = shares_longed . divide ( volumes ) shorted_frac = shares_shorted . divide ( volumes ) grossed_frac = shares_grossed . divide ( volumes ) longed_threshold = 100 * longed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) shorted_threshold = 100 * shorted_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) grossed_threshold = 100 * grossed_frac . apply ( partial ( np . nanpercentile , q = 100 * percentile ) , axis = 'columns' , ) return longed_threshold , shorted_threshold , grossed_threshold",Returns arrays of pth percentile of long short and gross volume exposures of an algorithm s held shares
"def plot_volume_exposures_longshort ( longed_threshold , shorted_threshold , percentile , ax = None ) : if ax is None : ax = plt . gca ( ) ax . plot ( longed_threshold . index , longed_threshold , color = 'b' , label = 'long' ) ax . plot ( shorted_threshold . index , shorted_threshold , color = 'r' , label = 'short' ) ax . axhline ( 0 , color = 'k' ) ax . set ( title = 'Long and short exposures to illiquidity' , ylabel = '{}th percentile of proportion of volume (%)' . format ( 100 * percentile ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax",Plots outputs of compute_volume_exposures as line graphs
"def plot_volume_exposures_gross ( grossed_threshold , percentile , ax = None ) : if ax is None : ax = plt . gca ( ) ax . plot ( grossed_threshold . index , grossed_threshold , color = 'b' , label = 'gross' ) ax . axhline ( 0 , color = 'k' ) ax . set ( title = 'Gross exposure to illiquidity' , ylabel = '{}th percentile of \n proportion of volume (%)' . format ( 100 * percentile ) ) ax . legend ( frameon = True , framealpha = 0.5 ) return ax",Plots outputs of compute_volume_exposures as line graphs
"def create_full_tear_sheet ( returns , positions = None , transactions = None , market_data = None , benchmark_rets = None , slippage = None , live_start_date = None , sector_mappings = None , bayesian = False , round_trips = False , estimate_intraday = 'infer' , hide_positions = False , cone_std = ( 1.0 , 1.5 , 2.0 ) , bootstrap = False , unadjusted_returns = None , style_factor_panel = None , sectors = None , caps = None , shares_held = None , volumes = None , percentile = None , turnover_denom = 'AGB' , set_context = True , factor_returns = None , factor_loadings = None , pos_in_dollars = True , header_rows = None , factor_partitions = FACTOR_PARTITIONS ) : if ( unadjusted_returns is None ) and ( slippage is not None ) and ( transactions is not None ) : unadjusted_returns = returns . copy ( ) returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , slippage ) positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) create_returns_tear_sheet ( returns , positions = positions , transactions = transactions , live_start_date = live_start_date , cone_std = cone_std , benchmark_rets = benchmark_rets , bootstrap = bootstrap , turnover_denom = turnover_denom , header_rows = header_rows , set_context = set_context ) create_interesting_times_tear_sheet ( returns , benchmark_rets = benchmark_rets , set_context = set_context ) if positions is not None : create_position_tear_sheet ( returns , positions , hide_positions = hide_positions , set_context = set_context , sector_mappings = sector_mappings , estimate_intraday = False ) if transactions is not None : create_txn_tear_sheet ( returns , positions , transactions , unadjusted_returns = unadjusted_returns , estimate_intraday = False , set_context = set_context ) if round_trips : create_round_trip_tear_sheet ( returns = returns , positions = positions , transactions = transactions , sector_mappings = sector_mappings , estimate_intraday = False ) if market_data is not None : create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , last_n_days = 125 , estimate_intraday = False ) if style_factor_panel is not None : create_risk_tear_sheet ( positions , style_factor_panel , sectors , caps , shares_held , volumes , percentile ) if factor_returns is not None and factor_loadings is not None : create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , factor_partitions = factor_partitions ) if bayesian : create_bayesian_tear_sheet ( returns , live_start_date = live_start_date , benchmark_rets = benchmark_rets , set_context = set_context )",Generate a number of tear sheets that are useful for analyzing a strategy s performance .
"def create_simple_tear_sheet ( returns , positions = None , transactions = None , benchmark_rets = None , slippage = None , estimate_intraday = 'infer' , live_start_date = None , turnover_denom = 'AGB' , header_rows = None ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if ( slippage is not None ) and ( transactions is not None ) : returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , slippage ) always_sections = 4 positions_sections = 4 if positions is not None else 0 transactions_sections = 2 if transactions is not None else 0 live_sections = 1 if live_start_date is not None else 0 benchmark_sections = 1 if benchmark_rets is not None else 0 vertical_sections = sum ( [ always_sections , positions_sections , transactions_sections , live_sections , benchmark_sections , ] ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) plotting . show_perf_stats ( returns , benchmark_rets , positions = positions , transactions = transactions , turnover_denom = turnover_denom , live_start_date = live_start_date , header_rows = header_rows ) fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_rolling_returns = plt . subplot ( gs [ : 2 , : ] ) i = 2 if benchmark_rets is not None : ax_rolling_beta = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_rolling_sharpe = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_underwater = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 plotting . plot_rolling_returns ( returns , factor_returns = benchmark_rets , live_start_date = live_start_date , cone_std = ( 1.0 , 1.5 , 2.0 ) , ax = ax_rolling_returns ) ax_rolling_returns . set_title ( 'Cumulative returns' ) if benchmark_rets is not None : plotting . plot_rolling_beta ( returns , benchmark_rets , ax = ax_rolling_beta ) plotting . plot_rolling_sharpe ( returns , ax = ax_rolling_sharpe ) plotting . plot_drawdown_underwater ( returns , ax = ax_underwater ) if positions is not None : ax_exposures = plt . subplot ( gs [ i , : ] ) i += 1 ax_top_positions = plt . subplot ( gs [ i , : ] , sharex = ax_exposures ) i += 1 ax_holdings = plt . subplot ( gs [ i , : ] , sharex = ax_exposures ) i += 1 ax_long_short_holdings = plt . subplot ( gs [ i , : ] ) i += 1 positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = 0 , hide_positions = False , ax = ax_top_positions ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) if transactions is not None : ax_turnover = plt . subplot ( gs [ i , : ] ) i += 1 ax_txn_timings = plt . subplot ( gs [ i , : ] ) i += 1 plotting . plot_turnover ( returns , transactions , positions , ax = ax_turnover ) plotting . plot_txn_time_hist ( transactions , ax = ax_txn_timings ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True )",Simpler version of create_full_tear_sheet ; generates summary performance statistics and important plots as a single image .
"def create_returns_tear_sheet ( returns , positions = None , transactions = None , live_start_date = None , cone_std = ( 1.0 , 1.5 , 2.0 ) , benchmark_rets = None , bootstrap = False , turnover_denom = 'AGB' , header_rows = None , return_fig = False ) : if benchmark_rets is not None : returns = utils . clip_returns_to_benchmark ( returns , benchmark_rets ) plotting . show_perf_stats ( returns , benchmark_rets , positions = positions , transactions = transactions , turnover_denom = turnover_denom , bootstrap = bootstrap , live_start_date = live_start_date , header_rows = header_rows ) plotting . show_worst_drawdown_periods ( returns ) vertical_sections = 11 if live_start_date is not None : vertical_sections += 1 live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) if benchmark_rets is not None : vertical_sections += 1 if bootstrap : vertical_sections += 1 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_rolling_returns = plt . subplot ( gs [ : 2 , : ] ) i = 2 ax_rolling_returns_vol_match = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_rolling_returns_log = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_returns = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 if benchmark_rets is not None : ax_rolling_beta = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_rolling_volatility = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_rolling_sharpe = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_drawdown = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_underwater = plt . subplot ( gs [ i , : ] , sharex = ax_rolling_returns ) i += 1 ax_monthly_heatmap = plt . subplot ( gs [ i , 0 ] ) ax_annual_returns = plt . subplot ( gs [ i , 1 ] ) ax_monthly_dist = plt . subplot ( gs [ i , 2 ] ) i += 1 ax_return_quantiles = plt . subplot ( gs [ i , : ] ) i += 1 plotting . plot_rolling_returns ( returns , factor_returns = benchmark_rets , live_start_date = live_start_date , cone_std = cone_std , ax = ax_rolling_returns ) ax_rolling_returns . set_title ( 'Cumulative returns' ) plotting . plot_rolling_returns ( returns , factor_returns = benchmark_rets , live_start_date = live_start_date , cone_std = None , volatility_match = ( benchmark_rets is not None ) , legend_loc = None , ax = ax_rolling_returns_vol_match ) ax_rolling_returns_vol_match . set_title ( 'Cumulative returns volatility matched to benchmark' ) plotting . plot_rolling_returns ( returns , factor_returns = benchmark_rets , logy = True , live_start_date = live_start_date , cone_std = cone_std , ax = ax_rolling_returns_log ) ax_rolling_returns_log . set_title ( 'Cumulative returns on logarithmic scale' ) plotting . plot_returns ( returns , live_start_date = live_start_date , ax = ax_returns , ) ax_returns . set_title ( 'Returns' ) if benchmark_rets is not None : plotting . plot_rolling_beta ( returns , benchmark_rets , ax = ax_rolling_beta ) plotting . plot_rolling_volatility ( returns , factor_returns = benchmark_rets , ax = ax_rolling_volatility ) plotting . plot_rolling_sharpe ( returns , ax = ax_rolling_sharpe ) plotting . plot_drawdown_periods ( returns , top = 5 , ax = ax_drawdown ) plotting . plot_drawdown_underwater ( returns = returns , ax = ax_underwater ) plotting . plot_monthly_returns_heatmap ( returns , ax = ax_monthly_heatmap ) plotting . plot_annual_returns ( returns , ax = ax_annual_returns ) plotting . plot_monthly_returns_dist ( returns , ax = ax_monthly_dist ) plotting . plot_return_quantiles ( returns , live_start_date = live_start_date , ax = ax_return_quantiles ) if bootstrap and ( benchmark_rets is not None ) : ax_bootstrap = plt . subplot ( gs [ i , : ] ) plotting . plot_perf_stats ( returns , benchmark_rets , ax = ax_bootstrap ) elif bootstrap : raise ValueError ( 'bootstrap requires passing of benchmark_rets.' ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig",Generate a number of plots for analyzing a strategy s returns .
"def create_position_tear_sheet ( returns , positions , show_and_plot_top_pos = 2 , hide_positions = False , return_fig = False , sector_mappings = None , transactions = None , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) if hide_positions : show_and_plot_top_pos = 0 vertical_sections = 7 if sector_mappings is not None else 6 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_exposures = plt . subplot ( gs [ 0 , : ] ) ax_top_positions = plt . subplot ( gs [ 1 , : ] , sharex = ax_exposures ) ax_max_median_pos = plt . subplot ( gs [ 2 , : ] , sharex = ax_exposures ) ax_holdings = plt . subplot ( gs [ 3 , : ] , sharex = ax_exposures ) ax_long_short_holdings = plt . subplot ( gs [ 4 , : ] ) ax_gross_leverage = plt . subplot ( gs [ 5 , : ] , sharex = ax_exposures ) positions_alloc = pos . get_percent_alloc ( positions ) plotting . plot_exposures ( returns , positions , ax = ax_exposures ) plotting . show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = show_and_plot_top_pos , hide_positions = hide_positions , ax = ax_top_positions ) plotting . plot_max_median_position_concentration ( positions , ax = ax_max_median_pos ) plotting . plot_holdings ( returns , positions_alloc , ax = ax_holdings ) plotting . plot_long_short_holdings ( returns , positions_alloc , ax = ax_long_short_holdings ) plotting . plot_gross_leverage ( returns , positions , ax = ax_gross_leverage ) if sector_mappings is not None : sector_exposures = pos . get_sector_exposures ( positions , sector_mappings ) if len ( sector_exposures . columns ) > 1 : sector_alloc = pos . get_percent_alloc ( sector_exposures ) sector_alloc = sector_alloc . drop ( 'cash' , axis = 'columns' ) ax_sector_alloc = plt . subplot ( gs [ 6 , : ] , sharex = ax_exposures ) plotting . plot_sector_allocations ( returns , sector_alloc , ax = ax_sector_alloc ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig",Generate a number of plots for analyzing a strategy s positions and holdings .
"def create_txn_tear_sheet ( returns , positions , transactions , unadjusted_returns = None , estimate_intraday = 'infer' , return_fig = False ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) vertical_sections = 6 if unadjusted_returns is not None else 4 fig = plt . figure ( figsize = ( 14 , vertical_sections * 6 ) ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) ax_turnover = plt . subplot ( gs [ 0 , : ] ) ax_daily_volume = plt . subplot ( gs [ 1 , : ] , sharex = ax_turnover ) ax_turnover_hist = plt . subplot ( gs [ 2 , : ] ) ax_txn_timings = plt . subplot ( gs [ 3 , : ] ) plotting . plot_turnover ( returns , transactions , positions , ax = ax_turnover ) plotting . plot_daily_volume ( returns , transactions , ax = ax_daily_volume ) try : plotting . plot_daily_turnover_hist ( transactions , positions , ax = ax_turnover_hist ) except ValueError : warnings . warn ( 'Unable to generate turnover plot.' , UserWarning ) plotting . plot_txn_time_hist ( transactions , ax = ax_txn_timings ) if unadjusted_returns is not None : ax_slippage_sweep = plt . subplot ( gs [ 4 , : ] ) plotting . plot_slippage_sweep ( unadjusted_returns , positions , transactions , ax = ax_slippage_sweep ) ax_slippage_sensitivity = plt . subplot ( gs [ 5 , : ] ) plotting . plot_slippage_sensitivity ( unadjusted_returns , positions , transactions , ax = ax_slippage_sensitivity ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig",Generate a number of plots for analyzing a strategy s transactions .
"def create_round_trip_tear_sheet ( returns , positions , transactions , sector_mappings = None , estimate_intraday = 'infer' , return_fig = False ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) transactions_closed = round_trips . add_closing_transactions ( positions , transactions ) trades = round_trips . extract_round_trips ( transactions_closed , portfolio_value = positions . sum ( axis = 'columns' ) / ( 1 + returns ) ) if len ( trades ) < 5 : warnings . warn ( """"""Fewer than 5 round-trip trades made.
               Skipping round trip tearsheet."""""" , UserWarning ) return round_trips . print_round_trip_stats ( trades ) plotting . show_profit_attribution ( trades ) if sector_mappings is not None : sector_trades = round_trips . apply_sector_mappings_to_round_trips ( trades , sector_mappings ) plotting . show_profit_attribution ( sector_trades ) fig = plt . figure ( figsize = ( 14 , 3 * 6 ) ) gs = gridspec . GridSpec ( 3 , 2 , wspace = 0.5 , hspace = 0.5 ) ax_trade_lifetimes = plt . subplot ( gs [ 0 , : ] ) ax_prob_profit_trade = plt . subplot ( gs [ 1 , 0 ] ) ax_holding_time = plt . subplot ( gs [ 1 , 1 ] ) ax_pnl_per_round_trip_dollars = plt . subplot ( gs [ 2 , 0 ] ) ax_pnl_per_round_trip_pct = plt . subplot ( gs [ 2 , 1 ] ) plotting . plot_round_trip_lifetimes ( trades , ax = ax_trade_lifetimes ) plotting . plot_prob_profit_trade ( trades , ax = ax_prob_profit_trade ) trade_holding_times = [ x . days for x in trades [ 'duration' ] ] sns . distplot ( trade_holding_times , kde = False , ax = ax_holding_time ) ax_holding_time . set ( xlabel = 'Holding time in days' ) sns . distplot ( trades . pnl , kde = False , ax = ax_pnl_per_round_trip_dollars ) ax_pnl_per_round_trip_dollars . set ( xlabel = 'PnL per round-trip trade in $' ) sns . distplot ( trades . returns . dropna ( ) * 100 , kde = False , ax = ax_pnl_per_round_trip_pct ) ax_pnl_per_round_trip_pct . set ( xlabel = 'Round-trip returns in %' ) gs . tight_layout ( fig ) if return_fig : return fig",Generate a number of figures and plots describing the duration frequency and profitability of trade round trips . A round trip is started when a new long or short position is opened and is only completed when the number of shares in that position returns to or crosses zero .
"def create_interesting_times_tear_sheet ( returns , benchmark_rets = None , legend_loc = 'best' , return_fig = False ) : rets_interesting = timeseries . extract_interesting_date_ranges ( returns ) if not rets_interesting : warnings . warn ( 'Passed returns do not overlap with any' 'interesting times.' , UserWarning ) return utils . print_table ( pd . DataFrame ( rets_interesting ) . describe ( ) . transpose ( ) . loc [ : , [ 'mean' , 'min' , 'max' ] ] * 100 , name = 'Stress Events' , float_format = '{0:.2f}%' . format ) if benchmark_rets is not None : returns = utils . clip_returns_to_benchmark ( returns , benchmark_rets ) bmark_interesting = timeseries . extract_interesting_date_ranges ( benchmark_rets ) num_plots = len ( rets_interesting ) num_rows = int ( ( num_plots + 1 ) / 2.0 ) fig = plt . figure ( figsize = ( 14 , num_rows * 6.0 ) ) gs = gridspec . GridSpec ( num_rows , 2 , wspace = 0.5 , hspace = 0.5 ) for i , ( name , rets_period ) in enumerate ( rets_interesting . items ( ) ) : ax = plt . subplot ( gs [ int ( i / 2.0 ) , i % 2 ] ) ep . cum_returns ( rets_period ) . plot ( ax = ax , color = 'forestgreen' , label = 'algo' , alpha = 0.7 , lw = 2 ) if benchmark_rets is not None : ep . cum_returns ( bmark_interesting [ name ] ) . plot ( ax = ax , color = 'gray' , label = 'benchmark' , alpha = 0.6 ) ax . legend ( [ 'Algo' , 'benchmark' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Algo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( name ) ax . set_ylabel ( 'Returns' ) ax . set_xlabel ( '' ) if return_fig : return fig",Generate a number of returns plots around interesting points in time like the flash crash and 9 / 11 .
"def create_capacity_tear_sheet ( returns , positions , transactions , market_data , liquidation_daily_vol_limit = 0.2 , trade_daily_vol_limit = 0.05 , last_n_days = utils . APPROX_BDAYS_PER_MONTH * 6 , days_to_liquidate_limit = 1 , estimate_intraday = 'infer' ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) print ( ""Max days to liquidation is computed for each traded name "" ""assuming a 20% limit on daily bar consumption \n"" ""and trailing 5 day mean volume as the available bar volume.\n\n"" ""Tickers with >1 day liquidation time at a"" "" constant $1m capital base:"" ) max_days_by_ticker = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 ) max_days_by_ticker . index = ( max_days_by_ticker . index . map ( utils . format_asset ) ) print ( ""Whole backtest:"" ) utils . print_table ( max_days_by_ticker [ max_days_by_ticker . days_to_liquidate > days_to_liquidate_limit ] ) max_days_by_ticker_lnd = capacity . get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = liquidation_daily_vol_limit , capital_base = 1e6 , mean_volume_window = 5 , last_n_days = last_n_days ) max_days_by_ticker_lnd . index = ( max_days_by_ticker_lnd . index . map ( utils . format_asset ) ) print ( ""Last {} trading days:"" . format ( last_n_days ) ) utils . print_table ( max_days_by_ticker_lnd [ max_days_by_ticker_lnd . days_to_liquidate > 1 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data ) llt . index = llt . index . map ( utils . format_asset ) print ( 'Tickers with daily transactions consuming >{}% of daily bar \n' 'all backtest:' . format ( trade_daily_vol_limit * 100 ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) llt = capacity . get_low_liquidity_transactions ( transactions , market_data , last_n_days = last_n_days ) print ( ""Last {} trading days:"" . format ( last_n_days ) ) utils . print_table ( llt [ llt [ 'max_pct_bar_consumed' ] > trade_daily_vol_limit * 100 ] ) bt_starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns . iloc [ 0 ] ) fig , ax_capacity_sweep = plt . subplots ( figsize = ( 14 , 6 ) ) plotting . plot_capacity_sweep ( returns , transactions , market_data , bt_starting_capital , min_pv = 100000 , max_pv = 300000000 , step_size = 1000000 , ax = ax_capacity_sweep )",Generates a report detailing portfolio size constraints set by least liquid tickers . Plots a capacity sweep a curve describing projected sharpe ratio given the slippage penalties that are applied at various capital bases .
"def create_bayesian_tear_sheet ( returns , benchmark_rets = None , live_start_date = None , samples = 2000 , return_fig = False , stoch_vol = False , progressbar = True ) : if not have_bayesian : raise NotImplementedError ( ""Bayesian tear sheet requirements not found.\n"" ""Run 'pip install pyfolio[bayesian]' to install "" ""bayesian requirements."" ) if live_start_date is None : raise NotImplementedError ( 'Bayesian tear sheet requires setting of live_start_date' ) live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) df_train = returns . loc [ returns . index < live_start_date ] df_test = returns . loc [ returns . index >= live_start_date ] print ( ""Running T model"" ) previous_time = time ( ) start_time = previous_time trace_t , ppc_t = bayesian . run_model ( 't' , df_train , returns_test = df_test , samples = samples , ppc = True , progressbar = progressbar ) previous_time = timer ( ""T model"" , previous_time ) print ( ""\nRunning BEST model"" ) trace_best = bayesian . run_model ( 'best' , df_train , returns_test = df_test , samples = samples , progressbar = progressbar ) previous_time = timer ( ""BEST model"" , previous_time ) fig = plt . figure ( figsize = ( 14 , 10 * 2 ) ) gs = gridspec . GridSpec ( 9 , 2 , wspace = 0.3 , hspace = 0.3 ) axs = [ ] row = 0 ax_cone = plt . subplot ( gs [ row , : ] ) bayesian . plot_bayes_cone ( df_train , df_test , ppc_t , ax = ax_cone ) previous_time = timer ( ""plotting Bayesian cone"" , previous_time ) row += 1 axs . append ( plt . subplot ( gs [ row , 0 ] ) ) axs . append ( plt . subplot ( gs [ row , 1 ] ) ) row += 1 axs . append ( plt . subplot ( gs [ row , 0 ] ) ) axs . append ( plt . subplot ( gs [ row , 1 ] ) ) row += 1 axs . append ( plt . subplot ( gs [ row , 0 ] ) ) axs . append ( plt . subplot ( gs [ row , 1 ] ) ) row += 1 axs . append ( plt . subplot ( gs [ row , : ] ) ) bayesian . plot_best ( trace = trace_best , axs = axs ) previous_time = timer ( ""plotting BEST results"" , previous_time ) row += 1 ax_ret_pred_day = plt . subplot ( gs [ row , 0 ] ) ax_ret_pred_week = plt . subplot ( gs [ row , 1 ] ) day_pred = ppc_t [ : , 0 ] p5 = scipy . stats . scoreatpercentile ( day_pred , 5 ) sns . distplot ( day_pred , ax = ax_ret_pred_day ) ax_ret_pred_day . axvline ( p5 , linestyle = '--' , linewidth = 3. ) ax_ret_pred_day . set_xlabel ( 'Predicted returns 1 day' ) ax_ret_pred_day . set_ylabel ( 'Frequency' ) ax_ret_pred_day . text ( 0.4 , 0.9 , 'Bayesian VaR = %.2f' % p5 , verticalalignment = 'bottom' , horizontalalignment = 'right' , transform = ax_ret_pred_day . transAxes ) previous_time = timer ( ""computing Bayesian predictions"" , previous_time ) week_pred = ( np . cumprod ( ppc_t [ : , : 5 ] + 1 , 1 ) - 1 ) [ : , - 1 ] p5 = scipy . stats . scoreatpercentile ( week_pred , 5 ) sns . distplot ( week_pred , ax = ax_ret_pred_week ) ax_ret_pred_week . axvline ( p5 , linestyle = '--' , linewidth = 3. ) ax_ret_pred_week . set_xlabel ( 'Predicted cum returns 5 days' ) ax_ret_pred_week . set_ylabel ( 'Frequency' ) ax_ret_pred_week . text ( 0.4 , 0.9 , 'Bayesian VaR = %.2f' % p5 , verticalalignment = 'bottom' , horizontalalignment = 'right' , transform = ax_ret_pred_week . transAxes ) previous_time = timer ( ""plotting Bayesian VaRs estimate"" , previous_time ) if benchmark_rets is not None : print ( ""\nRunning alpha beta model"" ) benchmark_rets = benchmark_rets . loc [ df_train . index ] trace_alpha_beta = bayesian . run_model ( 'alpha_beta' , df_train , bmark = benchmark_rets , samples = samples , progressbar = progressbar ) previous_time = timer ( ""running alpha beta model"" , previous_time ) row += 1 ax_alpha = plt . subplot ( gs [ row , 0 ] ) ax_beta = plt . subplot ( gs [ row , 1 ] ) sns . distplot ( ( 1 + trace_alpha_beta [ 'alpha' ] [ 100 : ] ) ** 252 - 1 , ax = ax_alpha ) sns . distplot ( trace_alpha_beta [ 'beta' ] [ 100 : ] , ax = ax_beta ) ax_alpha . set_xlabel ( 'Annual Alpha' ) ax_alpha . set_ylabel ( 'Belief' ) ax_beta . set_xlabel ( 'Beta' ) ax_beta . set_ylabel ( 'Belief' ) previous_time = timer ( ""plotting alpha beta model"" , previous_time ) if stoch_vol : returns_cutoff = 400 print ( ""\nRunning stochastic volatility model on "" ""most recent {} days of returns."" . format ( returns_cutoff ) ) if df_train . size > returns_cutoff : df_train_truncated = df_train [ - returns_cutoff : ] _ , trace_stoch_vol = bayesian . model_stoch_vol ( df_train_truncated ) previous_time = timer ( ""running stochastic volatility model"" , previous_time ) row += 1 ax_volatility = plt . subplot ( gs [ row , : ] ) bayesian . plot_stoch_vol ( df_train_truncated , trace = trace_stoch_vol , ax = ax_volatility ) previous_time = timer ( ""plotting stochastic volatility model"" , previous_time ) total_time = time ( ) - start_time print ( ""\nTotal runtime was {:.2f} seconds."" . format ( total_time ) ) gs . tight_layout ( fig ) if return_fig : return fig",Generate a number of Bayesian distributions and a Bayesian cone plot of returns .
"def create_risk_tear_sheet ( positions , style_factor_panel = None , sectors = None , caps = None , shares_held = None , volumes = None , percentile = None , returns = None , transactions = None , estimate_intraday = 'infer' , return_fig = False ) : positions = utils . check_intraday ( estimate_intraday , returns , positions , transactions ) idx = positions . index & style_factor_panel . iloc [ 0 ] . index & sectors . index & caps . index & shares_held . index & volumes . index positions = positions . loc [ idx ] vertical_sections = 0 if style_factor_panel is not None : vertical_sections += len ( style_factor_panel . items ) new_style_dict = { } for item in style_factor_panel . items : new_style_dict . update ( { item : style_factor_panel . loc [ item ] . loc [ idx ] } ) style_factor_panel = pd . Panel ( ) style_factor_panel = style_factor_panel . from_dict ( new_style_dict ) if sectors is not None : vertical_sections += 4 sectors = sectors . loc [ idx ] if caps is not None : vertical_sections += 4 caps = caps . loc [ idx ] if ( shares_held is not None ) & ( volumes is not None ) & ( percentile is not None ) : vertical_sections += 3 shares_held = shares_held . loc [ idx ] volumes = volumes . loc [ idx ] if percentile is None : percentile = 0.1 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 3 , wspace = 0.5 , hspace = 0.5 ) if style_factor_panel is not None : style_axes = [ ] style_axes . append ( plt . subplot ( gs [ 0 , : ] ) ) for i in range ( 1 , len ( style_factor_panel . items ) ) : style_axes . append ( plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) ) j = 0 for name , df in style_factor_panel . iteritems ( ) : sfe = risk . compute_style_factor_exposures ( positions , df ) risk . plot_style_factor_exposures ( sfe , name , style_axes [ j ] ) j += 1 if sectors is not None : i += 1 ax_sector_longshort = plt . subplot ( gs [ i : i + 2 , : ] , sharex = style_axes [ 0 ] ) i += 2 ax_sector_gross = plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) i += 1 ax_sector_net = plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) long_exposures , short_exposures , gross_exposures , net_exposures = risk . compute_sector_exposures ( positions , sectors ) risk . plot_sector_exposures_longshort ( long_exposures , short_exposures , ax = ax_sector_longshort ) risk . plot_sector_exposures_gross ( gross_exposures , ax = ax_sector_gross ) risk . plot_sector_exposures_net ( net_exposures , ax = ax_sector_net ) if caps is not None : i += 1 ax_cap_longshort = plt . subplot ( gs [ i : i + 2 , : ] , sharex = style_axes [ 0 ] ) i += 2 ax_cap_gross = plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) i += 1 ax_cap_net = plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) long_exposures , short_exposures , gross_exposures , net_exposures = risk . compute_cap_exposures ( positions , caps ) risk . plot_cap_exposures_longshort ( long_exposures , short_exposures , ax_cap_longshort ) risk . plot_cap_exposures_gross ( gross_exposures , ax_cap_gross ) risk . plot_cap_exposures_net ( net_exposures , ax_cap_net ) if volumes is not None : i += 1 ax_vol_longshort = plt . subplot ( gs [ i : i + 2 , : ] , sharex = style_axes [ 0 ] ) i += 2 ax_vol_gross = plt . subplot ( gs [ i , : ] , sharex = style_axes [ 0 ] ) longed_threshold , shorted_threshold , grossed_threshold = risk . compute_volume_exposures ( positions , volumes , percentile ) risk . plot_volume_exposures_longshort ( longed_threshold , shorted_threshold , percentile , ax_vol_longshort ) risk . plot_volume_exposures_gross ( grossed_threshold , percentile , ax_vol_gross ) for ax in fig . axes : plt . setp ( ax . get_xticklabels ( ) , visible = True ) if return_fig : return fig",Creates risk tear sheet : computes and plots style factor exposures sector exposures market cap exposures and volume exposures .
"def create_perf_attrib_tear_sheet ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True , return_fig = False , factor_partitions = FACTOR_PARTITIONS ) : portfolio_exposures , perf_attrib_data = perf_attrib . perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars ) display ( Markdown ( ""## Performance Relative to Common Risk Factors"" ) ) perf_attrib . show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars ) vertical_sections = 1 + 2 * max ( len ( factor_partitions ) , 1 ) current_section = 0 fig = plt . figure ( figsize = [ 14 , vertical_sections * 6 ] ) gs = gridspec . GridSpec ( vertical_sections , 1 , wspace = 0.5 , hspace = 0.5 ) perf_attrib . plot_returns ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 if factor_partitions is not None : for factor_type , partitions in factor_partitions . iteritems ( ) : columns_to_select = perf_attrib_data . columns . intersection ( partitions ) perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data [ columns_to_select ] , ax = plt . subplot ( gs [ current_section ] ) , title = ( 'Cumulative common {} returns attribution' ) . format ( factor_type ) ) current_section += 1 for factor_type , partitions in factor_partitions . iteritems ( ) : perf_attrib . plot_risk_exposures ( portfolio_exposures [ portfolio_exposures . columns . intersection ( partitions ) ] , ax = plt . subplot ( gs [ current_section ] ) , title = 'Daily {} factor exposures' . format ( factor_type ) ) current_section += 1 else : perf_attrib . plot_factor_contribution_to_perf ( perf_attrib_data , ax = plt . subplot ( gs [ current_section ] ) ) current_section += 1 perf_attrib . plot_risk_exposures ( portfolio_exposures , ax = plt . subplot ( gs [ current_section ] ) ) gs . tight_layout ( fig ) if return_fig : return fig",Generate plots and tables for analyzing a strategy s performance .
"def daily_txns_with_bar_data ( transactions , market_data ) : transactions . index . name = 'date' txn_daily = pd . DataFrame ( transactions . assign ( amount = abs ( transactions . amount ) ) . groupby ( [ 'symbol' , pd . TimeGrouper ( 'D' ) ] ) . sum ( ) [ 'amount' ] ) txn_daily [ 'price' ] = market_data [ 'price' ] . unstack ( ) txn_daily [ 'volume' ] = market_data [ 'volume' ] . unstack ( ) txn_daily = txn_daily . reset_index ( ) . set_index ( 'date' ) return txn_daily",Sums the absolute value of shares traded in each name on each day . Adds columns containing the closing price and total daily volume for each day - ticker combination .
"def days_to_liquidate_positions ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 ) : DV = market_data [ 'volume' ] * market_data [ 'price' ] roll_mean_dv = DV . rolling ( window = mean_volume_window , center = False ) . mean ( ) . shift ( ) roll_mean_dv = roll_mean_dv . replace ( 0 , np . nan ) positions_alloc = pos . get_percent_alloc ( positions ) positions_alloc = positions_alloc . drop ( 'cash' , axis = 1 ) days_to_liquidate = ( positions_alloc * capital_base ) / ( max_bar_consumption * roll_mean_dv ) return days_to_liquidate . iloc [ mean_volume_window : ]",Compute the number of days that would have been required to fully liquidate each position on each day based on the trailing n day mean daily bar volume and a limit on the proportion of a daily bar that we are allowed to consume .
"def get_max_days_to_liquidate_by_ticker ( positions , market_data , max_bar_consumption = 0.2 , capital_base = 1e6 , mean_volume_window = 5 , last_n_days = None ) : dtlp = days_to_liquidate_positions ( positions , market_data , max_bar_consumption = max_bar_consumption , capital_base = capital_base , mean_volume_window = mean_volume_window ) if last_n_days is not None : dtlp = dtlp . loc [ dtlp . index . max ( ) - pd . Timedelta ( days = last_n_days ) : ] pos_alloc = pos . get_percent_alloc ( positions ) pos_alloc = pos_alloc . drop ( 'cash' , axis = 1 ) liq_desc = pd . DataFrame ( ) liq_desc [ 'days_to_liquidate' ] = dtlp . unstack ( ) liq_desc [ 'pos_alloc_pct' ] = pos_alloc . unstack ( ) * 100 liq_desc . index . levels [ 0 ] . name = 'symbol' liq_desc . index . levels [ 1 ] . name = 'date' worst_liq = liq_desc . reset_index ( ) . sort_values ( 'days_to_liquidate' , ascending = False ) . groupby ( 'symbol' ) . first ( ) return worst_liq",Finds the longest estimated liquidation time for each traded name over the course of backtest ( or last n days of the backtest ) .
"def get_low_liquidity_transactions ( transactions , market_data , last_n_days = None ) : txn_daily_w_bar = daily_txns_with_bar_data ( transactions , market_data ) txn_daily_w_bar . index . name = 'date' txn_daily_w_bar = txn_daily_w_bar . reset_index ( ) if last_n_days is not None : md = txn_daily_w_bar . date . max ( ) - pd . Timedelta ( days = last_n_days ) txn_daily_w_bar = txn_daily_w_bar [ txn_daily_w_bar . date > md ] bar_consumption = txn_daily_w_bar . assign ( max_pct_bar_consumed = ( txn_daily_w_bar . amount / txn_daily_w_bar . volume ) * 100 ) . sort_values ( 'max_pct_bar_consumed' , ascending = False ) max_bar_consumption = bar_consumption . groupby ( 'symbol' ) . first ( ) return max_bar_consumption [ [ 'date' , 'max_pct_bar_consumed' ] ]",For each traded name find the daily transaction total that consumed the greatest proportion of available daily bar volume .
"def apply_slippage_penalty ( returns , txn_daily , simulate_starting_capital , backtest_starting_capital , impact = 0.1 ) : mult = simulate_starting_capital / backtest_starting_capital simulate_traded_shares = abs ( mult * txn_daily . amount ) simulate_traded_dollars = txn_daily . price * simulate_traded_shares simulate_pct_volume_used = simulate_traded_shares / txn_daily . volume penalties = simulate_pct_volume_used ** 2 * impact * simulate_traded_dollars daily_penalty = penalties . resample ( 'D' ) . sum ( ) daily_penalty = daily_penalty . reindex ( returns . index ) . fillna ( 0 ) portfolio_value = ep . cum_returns ( returns , starting_value = backtest_starting_capital ) * mult adj_returns = returns - ( daily_penalty / portfolio_value ) return adj_returns",Applies quadratic volumeshare slippage model to daily returns based on the proportion of the observed historical daily bar dollar volume consumed by the strategy s trades . Scales the size of trades based on the ratio of the starting capital we wish to test to the starting capital of the passed backtest data .
"def map_transaction ( txn ) : if isinstance ( txn [ 'sid' ] , dict ) : sid = txn [ 'sid' ] [ 'sid' ] symbol = txn [ 'sid' ] [ 'symbol' ] else : sid = txn [ 'sid' ] symbol = txn [ 'sid' ] return { 'sid' : sid , 'symbol' : symbol , 'price' : txn [ 'price' ] , 'order_id' : txn [ 'order_id' ] , 'amount' : txn [ 'amount' ] , 'commission' : txn [ 'commission' ] , 'dt' : txn [ 'dt' ] }",Maps a single transaction row to a dictionary .
"def make_transaction_frame ( transactions ) : transaction_list = [ ] for dt in transactions . index : txns = transactions . loc [ dt ] if len ( txns ) == 0 : continue for txn in txns : txn = map_transaction ( txn ) transaction_list . append ( txn ) df = pd . DataFrame ( sorted ( transaction_list , key = lambda x : x [ 'dt' ] ) ) df [ 'txn_dollars' ] = - df [ 'amount' ] * df [ 'price' ] df . index = list ( map ( pd . Timestamp , df . dt . values ) ) return df",Formats a transaction DataFrame .
"def get_txn_vol ( transactions ) : txn_norm = transactions . copy ( ) txn_norm . index = txn_norm . index . normalize ( ) amounts = txn_norm . amount . abs ( ) prices = txn_norm . price values = amounts * prices daily_amounts = amounts . groupby ( amounts . index ) . sum ( ) daily_values = values . groupby ( values . index ) . sum ( ) daily_amounts . name = ""txn_shares"" daily_values . name = ""txn_volume"" return pd . concat ( [ daily_values , daily_amounts ] , axis = 1 )",Extract daily transaction data from set of transaction objects .
"def adjust_returns_for_slippage ( returns , positions , transactions , slippage_bps ) : slippage = 0.0001 * slippage_bps portfolio_value = positions . sum ( axis = 1 ) pnl = portfolio_value * returns traded_value = get_txn_vol ( transactions ) . txn_volume slippage_dollars = traded_value * slippage adjusted_pnl = pnl . add ( - slippage_dollars , fill_value = 0 ) adjusted_returns = returns * adjusted_pnl / pnl return adjusted_returns",Apply a slippage penalty for every dollar traded .
"def get_turnover ( positions , transactions , denominator = 'AGB' ) : txn_vol = get_txn_vol ( transactions ) traded_value = txn_vol . txn_volume if denominator == 'AGB' : AGB = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) denom = AGB . rolling ( 2 ) . mean ( ) denom . iloc [ 0 ] = AGB . iloc [ 0 ] / 2 elif denominator == 'portfolio_value' : denom = positions . sum ( axis = 1 ) else : raise ValueError ( ""Unexpected value for denominator '{}'. The "" ""denominator parameter must be either 'AGB'"" "" or 'portfolio_value'."" . format ( denominator ) ) denom . index = denom . index . normalize ( ) turnover = traded_value . div ( denom , axis = 'index' ) turnover = turnover . fillna ( 0 ) return turnover",- Value of purchases and sales divided by either the actual gross book or the portfolio value for the time step .
"def _groupby_consecutive ( txn , max_delta = pd . Timedelta ( '8h' ) ) : def vwap ( transaction ) : if transaction . amount . sum ( ) == 0 : warnings . warn ( 'Zero transacted shares, setting vwap to nan.' ) return np . nan return ( transaction . amount * transaction . price ) . sum ( ) / transaction . amount . sum ( ) out = [ ] for sym , t in txn . groupby ( 'symbol' ) : t = t . sort_index ( ) t . index . name = 'dt' t = t . reset_index ( ) t [ 'order_sign' ] = t . amount > 0 t [ 'block_dir' ] = ( t . order_sign . shift ( 1 ) != t . order_sign ) . astype ( int ) . cumsum ( ) t [ 'block_time' ] = ( ( t . dt . sub ( t . dt . shift ( 1 ) ) ) > max_delta ) . astype ( int ) . cumsum ( ) grouped_price = ( t . groupby ( ( 'block_dir' , 'block_time' ) ) . apply ( vwap ) ) grouped_price . name = 'price' grouped_rest = t . groupby ( ( 'block_dir' , 'block_time' ) ) . agg ( { 'amount' : 'sum' , 'symbol' : 'first' , 'dt' : 'first' } ) grouped = grouped_rest . join ( grouped_price ) out . append ( grouped ) out = pd . concat ( out ) out = out . set_index ( 'dt' ) return out",Merge transactions of the same direction separated by less than max_delta time duration .
"def extract_round_trips ( transactions , portfolio_value = None ) : transactions = _groupby_consecutive ( transactions ) roundtrips = [ ] for sym , trans_sym in transactions . groupby ( 'symbol' ) : trans_sym = trans_sym . sort_index ( ) price_stack = deque ( ) dt_stack = deque ( ) trans_sym [ 'signed_price' ] = trans_sym . price * np . sign ( trans_sym . amount ) trans_sym [ 'abs_amount' ] = trans_sym . amount . abs ( ) . astype ( int ) for dt , t in trans_sym . iterrows ( ) : if t . price < 0 : warnings . warn ( 'Negative price detected, ignoring for' 'round-trip.' ) continue indiv_prices = [ t . signed_price ] * t . abs_amount if ( len ( price_stack ) == 0 ) or ( copysign ( 1 , price_stack [ - 1 ] ) == copysign ( 1 , t . amount ) ) : price_stack . extend ( indiv_prices ) dt_stack . extend ( [ dt ] * len ( indiv_prices ) ) else : pnl = 0 invested = 0 cur_open_dts = [ ] for price in indiv_prices : if len ( price_stack ) != 0 and ( copysign ( 1 , price_stack [ - 1 ] ) != copysign ( 1 , price ) ) : prev_price = price_stack . popleft ( ) prev_dt = dt_stack . popleft ( ) pnl += - ( price + prev_price ) cur_open_dts . append ( prev_dt ) invested += abs ( prev_price ) else : price_stack . append ( price ) dt_stack . append ( dt ) roundtrips . append ( { 'pnl' : pnl , 'open_dt' : cur_open_dts [ 0 ] , 'close_dt' : dt , 'long' : price < 0 , 'rt_returns' : pnl / invested , 'symbol' : sym , } ) roundtrips = pd . DataFrame ( roundtrips ) roundtrips [ 'duration' ] = roundtrips [ 'close_dt' ] . sub ( roundtrips [ 'open_dt' ] ) if portfolio_value is not None : pv = pd . DataFrame ( portfolio_value , columns = [ 'portfolio_value' ] ) . assign ( date = portfolio_value . index ) roundtrips [ 'date' ] = roundtrips . close_dt . apply ( lambda x : x . replace ( hour = 0 , minute = 0 , second = 0 ) ) tmp = roundtrips . join ( pv , on = 'date' , lsuffix = '_' ) roundtrips [ 'returns' ] = tmp . pnl / tmp . portfolio_value roundtrips = roundtrips . drop ( 'date' , axis = 'columns' ) return roundtrips",Group transactions into round trips . First transactions are grouped by day and directionality . Then long and short transactions are matched to create round - trip round_trips for which PnL duration and returns are computed . Crossings where a position changes from long to short and vice - versa are handled correctly .
"def add_closing_transactions ( positions , transactions ) : closed_txns = transactions [ [ 'symbol' , 'amount' , 'price' ] ] pos_at_end = positions . drop ( 'cash' , axis = 1 ) . iloc [ - 1 ] open_pos = pos_at_end . replace ( 0 , np . nan ) . dropna ( ) end_dt = open_pos . name + pd . Timedelta ( seconds = 1 ) for sym , ending_val in open_pos . iteritems ( ) : txn_sym = transactions [ transactions . symbol == sym ] ending_amount = txn_sym . amount . sum ( ) ending_price = ending_val / ending_amount closing_txn = { 'symbol' : sym , 'amount' : - ending_amount , 'price' : ending_price } closing_txn = pd . DataFrame ( closing_txn , index = [ end_dt ] ) closed_txns = closed_txns . append ( closing_txn ) closed_txns = closed_txns [ closed_txns . amount != 0 ] return closed_txns",Appends transactions that close out all positions at the end of the timespan covered by positions data . Utilizes pricing information in the positions DataFrame to determine closing price .
"def apply_sector_mappings_to_round_trips ( round_trips , sector_mappings ) : sector_round_trips = round_trips . copy ( ) sector_round_trips . symbol = sector_round_trips . symbol . apply ( lambda x : sector_mappings . get ( x , 'No Sector Mapping' ) ) sector_round_trips = sector_round_trips . dropna ( axis = 0 ) return sector_round_trips",Translates round trip symbols to sectors .
"def gen_round_trip_stats ( round_trips ) : stats = { } stats [ 'pnl' ] = agg_all_long_short ( round_trips , 'pnl' , PNL_STATS ) stats [ 'summary' ] = agg_all_long_short ( round_trips , 'pnl' , SUMMARY_STATS ) stats [ 'duration' ] = agg_all_long_short ( round_trips , 'duration' , DURATION_STATS ) stats [ 'returns' ] = agg_all_long_short ( round_trips , 'returns' , RETURN_STATS ) stats [ 'symbols' ] = round_trips . groupby ( 'symbol' ) [ 'returns' ] . agg ( RETURN_STATS ) . T return stats",Generate various round - trip statistics .
"def print_round_trip_stats ( round_trips , hide_pos = False ) : stats = gen_round_trip_stats ( round_trips ) print_table ( stats [ 'summary' ] , float_format = '{:.2f}' . format , name = 'Summary stats' ) print_table ( stats [ 'pnl' ] , float_format = '${:.2f}' . format , name = 'PnL stats' ) print_table ( stats [ 'duration' ] , float_format = '{:.2f}' . format , name = 'Duration stats' ) print_table ( stats [ 'returns' ] * 100 , float_format = '{:.2f}%' . format , name = 'Return stats' ) if not hide_pos : stats [ 'symbols' ] . columns = stats [ 'symbols' ] . columns . map ( format_asset ) print_table ( stats [ 'symbols' ] * 100 , float_format = '{:.2f}%' . format , name = 'Symbol stats' )",Print various round - trip statistics . Tries to pretty - print tables with HTML output if run inside IPython NB .
"def perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : ( returns , positions , factor_returns , factor_loadings ) = _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = transactions , pos_in_dollars = pos_in_dollars ) positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . perf_attrib ( returns , positions , factor_returns , factor_loadings )",Attributes the performance of a returns stream to a set of risk factors .
"def compute_exposures ( positions , factor_loadings , stack_positions = True , pos_in_dollars = True ) : if stack_positions : positions = _stack_positions ( positions , pos_in_dollars = pos_in_dollars ) return ep . compute_exposures ( positions , factor_loadings )",Compute daily risk factor exposures .
"def create_perf_attrib_stats ( perf_attrib , risk_exposures ) : summary = OrderedDict ( ) total_returns = perf_attrib [ 'total_returns' ] specific_returns = perf_attrib [ 'specific_returns' ] common_returns = perf_attrib [ 'common_returns' ] summary [ 'Annualized Specific Return' ] = ep . annual_return ( specific_returns ) summary [ 'Annualized Common Return' ] = ep . annual_return ( common_returns ) summary [ 'Annualized Total Return' ] = ep . annual_return ( total_returns ) summary [ 'Specific Sharpe Ratio' ] = ep . sharpe_ratio ( specific_returns ) summary [ 'Cumulative Specific Return' ] = ep . cum_returns_final ( specific_returns ) summary [ 'Cumulative Common Return' ] = ep . cum_returns_final ( common_returns ) summary [ 'Total Returns' ] = ep . cum_returns_final ( total_returns ) summary = pd . Series ( summary , name = '' ) annualized_returns_by_factor = [ ep . annual_return ( perf_attrib [ c ] ) for c in risk_exposures . columns ] cumulative_returns_by_factor = [ ep . cum_returns_final ( perf_attrib [ c ] ) for c in risk_exposures . columns ] risk_exposure_summary = pd . DataFrame ( data = OrderedDict ( [ ( 'Average Risk Factor Exposure' , risk_exposures . mean ( axis = 'rows' ) ) , ( 'Annualized Return' , annualized_returns_by_factor ) , ( 'Cumulative Return' , cumulative_returns_by_factor ) , ] ) , index = risk_exposures . columns , ) return summary , risk_exposure_summary",Takes perf attribution data over a period of time and computes annualized multifactor alpha multifactor sharpe risk exposures .
"def show_perf_attrib_stats ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : risk_exposures , perf_attrib_data = perf_attrib ( returns , positions , factor_returns , factor_loadings , transactions , pos_in_dollars = pos_in_dollars , ) perf_attrib_stats , risk_exposure_stats = create_perf_attrib_stats ( perf_attrib_data , risk_exposures ) percentage_formatter = '{:.2%}' . format float_formatter = '{:.2f}' . format summary_stats = perf_attrib_stats . loc [ [ 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , 'Specific Sharpe Ratio' ] ] for col_name in ( 'Annualized Specific Return' , 'Annualized Common Return' , 'Annualized Total Return' , ) : summary_stats [ col_name ] = percentage_formatter ( summary_stats [ col_name ] ) summary_stats [ 'Specific Sharpe Ratio' ] = float_formatter ( summary_stats [ 'Specific Sharpe Ratio' ] ) print_table ( summary_stats , name = 'Summary Statistics' ) print_table ( risk_exposure_stats , name = 'Exposures Summary' , formatters = { 'Average Risk Factor Exposure' : float_formatter , 'Annualized Return' : percentage_formatter , 'Cumulative Return' : percentage_formatter , } , )",Calls perf_attrib using inputs and displays outputs using utils . print_table .
"def plot_returns ( perf_attrib_data , cost = None , ax = None ) : if ax is None : ax = plt . gca ( ) returns = perf_attrib_data [ 'total_returns' ] total_returns_label = 'Total returns' cumulative_returns_less_costs = _cumulative_returns_less_costs ( returns , cost ) if cost is not None : total_returns_label += ' (adjusted)' specific_returns = perf_attrib_data [ 'specific_returns' ] common_returns = perf_attrib_data [ 'common_returns' ] ax . plot ( cumulative_returns_less_costs , color = 'b' , label = total_returns_label ) ax . plot ( ep . cum_returns ( specific_returns ) , color = 'g' , label = 'Cumulative specific returns' ) ax . plot ( ep . cum_returns ( common_returns ) , color = 'r' , label = 'Cumulative common returns' ) if cost is not None : ax . plot ( - ep . cum_returns ( cost ) , color = 'k' , label = 'Cumulative cost spent' ) ax . set_title ( 'Time series of cumulative returns' ) ax . set_ylabel ( 'Returns' ) configure_legend ( ax ) return ax",Plot total specific and common returns .
"def plot_alpha_returns ( alpha_returns , ax = None ) : if ax is None : ax = plt . gca ( ) ax . hist ( alpha_returns , color = 'g' , label = 'Multi-factor alpha' ) ax . set_title ( 'Histogram of alphas' ) ax . axvline ( 0 , color = 'k' , linestyle = '--' , label = 'Zero' ) avg = alpha_returns . mean ( ) ax . axvline ( avg , color = 'b' , label = 'Mean = {: 0.5f}' . format ( avg ) ) configure_legend ( ax ) return ax",Plot histogram of daily multi - factor alpha returns ( specific returns ) .
"def plot_factor_contribution_to_perf ( perf_attrib_data , ax = None , title = 'Cumulative common returns attribution' , ) : if ax is None : ax = plt . gca ( ) factors_to_plot = perf_attrib_data . drop ( [ 'total_returns' , 'common_returns' ] , axis = 'columns' , errors = 'ignore' ) factors_cumulative = pd . DataFrame ( ) for factor in factors_to_plot : factors_cumulative [ factor ] = ep . cum_returns ( factors_to_plot [ factor ] ) for col in factors_cumulative : ax . plot ( factors_cumulative [ col ] ) ax . axhline ( 0 , color = 'k' ) configure_legend ( ax , change_colors = True ) ax . set_ylabel ( 'Cumulative returns by factor' ) ax . set_title ( title ) return ax",Plot each factor s contribution to performance .
"def plot_risk_exposures ( exposures , ax = None , title = 'Daily risk factor exposures' ) : if ax is None : ax = plt . gca ( ) for col in exposures : ax . plot ( exposures [ col ] ) configure_legend ( ax , change_colors = True ) ax . set_ylabel ( 'Factor exposures' ) ax . set_title ( title ) return ax",Parameters ---------- exposures : pd . DataFrame df indexed by datetime with factors as columns - Example : momentum reversal dt 2017 - 01 - 01 - 0 . 238655 0 . 077123 2017 - 01 - 02 0 . 821872 1 . 520515
"def _align_and_warn ( returns , positions , factor_returns , factor_loadings , transactions = None , pos_in_dollars = True ) : missing_stocks = positions . columns . difference ( factor_loadings . index . get_level_values ( 1 ) . unique ( ) ) num_stocks = len ( positions . columns ) - 1 missing_stocks = missing_stocks . drop ( 'cash' ) num_stocks_covered = num_stocks - len ( missing_stocks ) missing_ratio = round ( len ( missing_stocks ) / num_stocks , ndigits = 3 ) if num_stocks_covered == 0 : raise ValueError ( ""Could not perform performance attribution. "" ""No factor loadings were available for this "" ""algorithm's positions."" ) if len ( missing_stocks ) > 0 : if len ( missing_stocks ) > 5 : missing_stocks_displayed = ( "" {} assets were missing factor loadings, including: {}..{}"" ) . format ( len ( missing_stocks ) , ', ' . join ( missing_stocks [ : 5 ] . map ( str ) ) , missing_stocks [ - 1 ] ) avg_allocation_msg = ""selected missing assets"" else : missing_stocks_displayed = ( ""The following assets were missing factor loadings: {}."" ) . format ( list ( missing_stocks ) ) avg_allocation_msg = ""missing assets"" missing_stocks_warning_msg = ( ""Could not determine risk exposures for some of this algorithm's "" ""positions. Returns from the missing assets will not be properly "" ""accounted for in performance attribution.\n"" ""\n"" ""{}. "" ""Ignoring for exposure calculation and performance attribution. "" ""Ratio of assets missing: {}. Average allocation of {}:\n"" ""\n"" ""{}.\n"" ) . format ( missing_stocks_displayed , missing_ratio , avg_allocation_msg , positions [ missing_stocks [ : 5 ] . union ( missing_stocks [ [ - 1 ] ] ) ] . mean ( ) , ) warnings . warn ( missing_stocks_warning_msg ) positions = positions . drop ( missing_stocks , axis = 'columns' , errors = 'ignore' ) missing_factor_loadings_index = positions . index . difference ( factor_loadings . index . get_level_values ( 0 ) . unique ( ) ) missing_factor_loadings_index = positions . index . difference ( factor_loadings . index . get_level_values ( 0 ) . unique ( ) ) if len ( missing_factor_loadings_index ) > 0 : if len ( missing_factor_loadings_index ) > 5 : missing_dates_displayed = ( ""(first missing is {}, last missing is {})"" ) . format ( missing_factor_loadings_index [ 0 ] , missing_factor_loadings_index [ - 1 ] ) else : missing_dates_displayed = list ( missing_factor_loadings_index ) warning_msg = ( ""Could not find factor loadings for {} dates: {}. "" ""Truncating date range for performance attribution. "" ) . format ( len ( missing_factor_loadings_index ) , missing_dates_displayed ) warnings . warn ( warning_msg ) positions = positions . drop ( missing_factor_loadings_index , errors = 'ignore' ) returns = returns . drop ( missing_factor_loadings_index , errors = 'ignore' ) factor_returns = factor_returns . drop ( missing_factor_loadings_index , errors = 'ignore' ) if transactions is not None and pos_in_dollars : turnover = get_turnover ( positions , transactions ) . mean ( ) if turnover > PERF_ATTRIB_TURNOVER_THRESHOLD : warning_msg = ( ""This algorithm has relatively high turnover of its "" ""positions. As a result, performance attribution might not be "" ""fully accurate.\n"" ""\n"" ""Performance attribution is calculated based "" ""on end-of-day holdings and does not account for intraday "" ""activity. Algorithms that derive a high percentage of "" ""returns from buying and selling within the same day may "" ""receive inaccurate performance attribution.\n"" ) warnings . warn ( warning_msg ) return ( returns , positions , factor_returns , factor_loadings )",Make sure that all inputs have matching dates and tickers and raise warnings if necessary .
"def _stack_positions ( positions , pos_in_dollars = True ) : if pos_in_dollars : positions = get_percent_alloc ( positions ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . stack ( ) positions . index = positions . index . set_names ( [ 'dt' , 'ticker' ] ) return positions",Convert positions to percentages if necessary and change them to long format .
"def _cumulative_returns_less_costs ( returns , costs ) : if costs is None : return ep . cum_returns ( returns ) return ep . cum_returns ( returns - costs )",Compute cumulative returns less costs .
"def format_asset ( asset ) : try : import zipline . assets except ImportError : return asset if isinstance ( asset , zipline . assets . Asset ) : return asset . symbol else : return asset",If zipline asset objects are used we want to print them out prettily within the tear sheet . This function should only be applied directly before displaying .
"def vectorize ( func ) : def wrapper ( df , * args , * * kwargs ) : if df . ndim == 1 : return func ( df , * args , * * kwargs ) elif df . ndim == 2 : return df . apply ( func , * args , * * kwargs ) return wrapper",Decorator so that functions can be written to work on Series but may still be called with DataFrames .
"def extract_rets_pos_txn_from_zipline ( backtest ) : backtest . index = backtest . index . normalize ( ) if backtest . index . tzinfo is None : backtest . index = backtest . index . tz_localize ( 'UTC' ) returns = backtest . returns raw_positions = [ ] for dt , pos_row in backtest . positions . iteritems ( ) : df = pd . DataFrame ( pos_row ) df . index = [ dt ] * len ( df ) raw_positions . append ( df ) if not raw_positions : raise ValueError ( ""The backtest does not have any positions."" ) positions = pd . concat ( raw_positions ) positions = pos . extract_pos ( positions , backtest . ending_cash ) transactions = txn . make_transaction_frame ( backtest . transactions ) if transactions . index . tzinfo is None : transactions . index = transactions . index . tz_localize ( 'utc' ) return returns , positions , transactions",Extract returns positions transactions and leverage from the backtest data structure returned by zipline . TradingAlgorithm . run () .
"def print_table ( table , name = None , float_format = None , formatters = None , header_rows = None ) : if isinstance ( table , pd . Series ) : table = pd . DataFrame ( table ) if name is not None : table . columns . name = name html = table . to_html ( float_format = float_format , formatters = formatters ) if header_rows is not None : n_cols = html . split ( '<thead>' ) [ 1 ] . split ( '</thead>' ) [ 0 ] . count ( '<th>' ) rows = '' for name , value in header_rows . items ( ) : rows += ( '\n    <tr style=""text-align: right;""><th>%s</th>' + '<td colspan=%d>%s</td></tr>' ) % ( name , n_cols , value ) html = html . replace ( '<thead>' , '<thead>' + rows ) display ( HTML ( html ) )",Pretty print a pandas DataFrame .
"def detect_intraday ( positions , transactions , threshold = 0.25 ) : daily_txn = transactions . copy ( ) daily_txn . index = daily_txn . index . date txn_count = daily_txn . groupby ( level = 0 ) . symbol . nunique ( ) . sum ( ) daily_pos = positions . drop ( 'cash' , axis = 1 ) . replace ( 0 , np . nan ) return daily_pos . count ( axis = 1 ) . sum ( ) / txn_count < threshold",Attempt to detect an intraday strategy . Get the number of positions held at the end of the day and divide that by the number of unique stocks transacted every day . If the average quotient is below a threshold then an intraday strategy is detected .
"def check_intraday ( estimate , returns , positions , transactions ) : if estimate == 'infer' : if positions is not None and transactions is not None : if detect_intraday ( positions , transactions ) : warnings . warn ( 'Detected intraday strategy; inferring positi' + 'ons from transactions. Set estimate_intraday' + '=False to disable.' ) return estimate_intraday ( returns , positions , transactions ) else : return positions else : return positions elif estimate : if positions is not None and transactions is not None : return estimate_intraday ( returns , positions , transactions ) else : raise ValueError ( 'Positions and txns needed to estimate intraday' ) else : return positions",Logic for checking if a strategy is intraday and processing it .
"def estimate_intraday ( returns , positions , transactions , EOD_hour = 23 ) : txn_val = transactions . copy ( ) txn_val . index . names = [ 'date' ] txn_val [ 'value' ] = txn_val . amount * txn_val . price txn_val = txn_val . reset_index ( ) . pivot_table ( index = 'date' , values = 'value' , columns = 'symbol' ) . replace ( np . nan , 0 ) txn_val [ 'date' ] = txn_val . index . date txn_val = txn_val . groupby ( 'date' ) . cumsum ( ) txn_val [ 'exposure' ] = txn_val . abs ( ) . sum ( axis = 1 ) condition = ( txn_val [ 'exposure' ] == txn_val . groupby ( pd . TimeGrouper ( '24H' ) ) [ 'exposure' ] . transform ( max ) ) txn_val = txn_val [ condition ] . drop ( 'exposure' , axis = 1 ) txn_val [ 'cash' ] = - txn_val . sum ( axis = 1 ) positions_shifted = positions . copy ( ) . shift ( 1 ) . fillna ( 0 ) starting_capital = positions . iloc [ 0 ] . sum ( ) / ( 1 + returns [ 0 ] ) positions_shifted . cash [ 0 ] = starting_capital txn_val . index = txn_val . index . normalize ( ) corrected_positions = positions_shifted . add ( txn_val , fill_value = 0 ) corrected_positions . index . name = 'period_close' corrected_positions . columns . name = 'sid' return corrected_positions",Intraday strategies will often not hold positions at the day end . This attempts to find the point in the day that best represents the activity of the strategy on that day and effectively resamples the end - of - day positions with the positions at this point of day . The point of day is found by detecting when our exposure in the market is at its maximum point . Note that this is an estimate .
"def clip_returns_to_benchmark ( rets , benchmark_rets ) : if ( rets . index [ 0 ] < benchmark_rets . index [ 0 ] ) or ( rets . index [ - 1 ] > benchmark_rets . index [ - 1 ] ) : clipped_rets = rets [ benchmark_rets . index ] else : clipped_rets = rets return clipped_rets",Drop entries from rets so that the start and end dates of rets match those of benchmark_rets .
def to_utc ( df ) : try : df . index = df . index . tz_localize ( 'UTC' ) except TypeError : df . index = df . index . tz_convert ( 'UTC' ) return df,For use in tests ; applied UTC timestamp to DataFrame .
"def get_symbol_rets ( symbol , start = None , end = None ) : return SETTINGS [ 'returns_func' ] ( symbol , start = start , end = end )",Calls the currently registered returns_func
"def configure_legend ( ax , autofmt_xdate = True , change_colors = False , rotation = 30 , ha = 'right' ) : chartBox = ax . get_position ( ) ax . set_position ( [ chartBox . x0 , chartBox . y0 , chartBox . width * 0.75 , chartBox . height ] ) handles , labels = ax . get_legend_handles_labels ( ) handles_and_labels_sorted = sorted ( zip ( handles , labels ) , key = lambda x : x [ 0 ] . get_ydata ( ) [ - 1 ] , reverse = True ) handles_sorted = [ h [ 0 ] for h in handles_and_labels_sorted ] labels_sorted = [ h [ 1 ] for h in handles_and_labels_sorted ] if change_colors : for handle , color in zip ( handles_sorted , cycle ( COLORS ) ) : handle . set_color ( color ) ax . legend ( handles = handles_sorted , labels = labels_sorted , frameon = True , framealpha = 0.5 , loc = 'upper left' , bbox_to_anchor = ( 1.05 , 1 ) , fontsize = 'large' ) if autofmt_xdate : for label in ax . get_xticklabels ( ) : label . set_ha ( ha ) label . set_rotation ( rotation )",Format legend for perf attribution plots : - put legend to the right of plot instead of overlapping with it - make legend order match up with graph lines - set colors according to colormap
"def sample_colormap ( cmap_name , n_samples ) : colors = [ ] colormap = cm . cmap_d [ cmap_name ] for i in np . linspace ( 0 , 1 , n_samples ) : colors . append ( colormap ( i ) ) return colors",Sample a colormap from matplotlib
"def customize ( func ) : @ wraps ( func ) def call_w_context ( * args , * * kwargs ) : set_context = kwargs . pop ( 'set_context' , True ) if set_context : with plotting_context ( ) , axes_style ( ) : return func ( * args , * * kwargs ) else : return func ( * args , * * kwargs ) return call_w_context",Decorator to set plotting context and axes style during function call .
"def plotting_context ( context = 'notebook' , font_scale = 1.5 , rc = None ) : if rc is None : rc = { } rc_default = { 'lines.linewidth' : 1.5 } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . plotting_context ( context = context , font_scale = font_scale , rc = rc )",Create pyfolio default plotting style context .
"def axes_style ( style = 'darkgrid' , rc = None ) : if rc is None : rc = { } rc_default = { } for name , val in rc_default . items ( ) : rc . setdefault ( name , val ) return sns . axes_style ( style = style , rc = rc )",Create pyfolio default axes style context .
"def plot_monthly_returns_heatmap ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) monthly_ret_table = monthly_ret_table . unstack ( ) . round ( 3 ) sns . heatmap ( monthly_ret_table . fillna ( 0 ) * 100.0 , annot = True , annot_kws = { ""size"" : 9 } , alpha = 1.0 , center = 0.0 , cbar = False , cmap = matplotlib . cm . RdYlGn , ax = ax , * * kwargs ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Month' ) ax . set_title ( ""Monthly returns (%)"" ) return ax",Plots a heatmap of returns by month .
"def plot_annual_returns ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) ann_ret_df = pd . DataFrame ( ep . aggregate_returns ( returns , 'yearly' ) ) ax . axvline ( 100 * ann_ret_df . values . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 4 , alpha = 0.7 ) ( 100 * ann_ret_df . sort_index ( ascending = False ) ) . plot ( ax = ax , kind = 'barh' , alpha = 0.70 , * * kwargs ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Year' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( ""Annual returns"" ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) return ax",Plots a bar graph of returns by year .
"def plot_monthly_returns_dist ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) x_axis_formatter = FuncFormatter ( utils . percentage ) ax . xaxis . set_major_formatter ( FuncFormatter ( x_axis_formatter ) ) ax . tick_params ( axis = 'x' , which = 'major' ) monthly_ret_table = ep . aggregate_returns ( returns , 'monthly' ) ax . hist ( 100 * monthly_ret_table , color = 'orangered' , alpha = 0.80 , bins = 20 , * * kwargs ) ax . axvline ( 100 * monthly_ret_table . mean ( ) , color = 'gold' , linestyle = '--' , lw = 4 , alpha = 1.0 ) ax . axvline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 , alpha = 0.75 ) ax . legend ( [ 'Mean' ] , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Number of months' ) ax . set_xlabel ( 'Returns' ) ax . set_title ( ""Distribution of monthly returns"" ) return ax",Plots a distribution of monthly returns .
"def plot_holdings ( returns , positions , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . copy ( ) . drop ( 'cash' , axis = 'columns' ) df_holdings = positions . replace ( 0 , np . nan ) . count ( axis = 1 ) df_holdings_by_month = df_holdings . resample ( '1M' ) . mean ( ) df_holdings . plot ( color = 'steelblue' , alpha = 0.6 , lw = 0.5 , ax = ax , * * kwargs ) df_holdings_by_month . plot ( color = 'orangered' , lw = 2 , ax = ax , * * kwargs ) ax . axhline ( df_holdings . values . mean ( ) , color = 'steelblue' , ls = '--' , lw = 3 ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) leg = ax . legend ( [ 'Daily holdings' , 'Average daily holdings, by month' , 'Average daily holdings, overall' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_title ( 'Total holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax",Plots total amount of stocks with an active position either short or long . Displays daily total daily average per month and all - time daily average .
"def plot_long_short_holdings ( returns , positions , legend_loc = 'upper left' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) positions = positions . drop ( 'cash' , axis = 'columns' ) positions = positions . replace ( 0 , np . nan ) df_longs = positions [ positions > 0 ] . count ( axis = 1 ) df_shorts = positions [ positions < 0 ] . count ( axis = 1 ) lf = ax . fill_between ( df_longs . index , 0 , df_longs . values , color = 'g' , alpha = 0.5 , lw = 2.0 ) sf = ax . fill_between ( df_shorts . index , 0 , df_shorts . values , color = 'r' , alpha = 0.5 , lw = 2.0 ) bf = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'darkgoldenrod' ) leg = ax . legend ( [ lf , sf , bf ] , [ 'Long (max: %s, min: %s)' % ( df_longs . max ( ) , df_longs . min ( ) ) , 'Short (max: %s, min: %s)' % ( df_shorts . max ( ) , df_shorts . min ( ) ) , 'Overlap' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( 'Long and short holdings' ) ax . set_ylabel ( 'Holdings' ) ax . set_xlabel ( '' ) return ax",Plots total amount of stocks with an active position breaking out short and long into transparent filled regions .
"def plot_drawdown_periods ( returns , top = 10 , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) df_drawdowns = timeseries . gen_drawdown_table ( returns , top = top ) df_cum_rets . plot ( ax = ax , * * kwargs ) lim = ax . get_ylim ( ) colors = sns . cubehelix_palette ( len ( df_drawdowns ) ) [ : : - 1 ] for i , ( peak , recovery ) in df_drawdowns [ [ 'Peak date' , 'Recovery date' ] ] . iterrows ( ) : if pd . isnull ( recovery ) : recovery = returns . index [ - 1 ] ax . fill_between ( ( peak , recovery ) , lim [ 0 ] , lim [ 1 ] , alpha = .4 , color = colors [ i ] ) ax . set_ylim ( lim ) ax . set_title ( 'Top %i drawdown periods' % top ) ax . set_ylabel ( 'Cumulative returns' ) ax . legend ( [ 'Portfolio' ] , loc = 'upper left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax",Plots cumulative returns highlighting top drawdown periods .
"def plot_drawdown_underwater ( returns , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . percentage ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_cum_rets = ep . cum_returns ( returns , starting_value = 1.0 ) running_max = np . maximum . accumulate ( df_cum_rets ) underwater = - 100 * ( ( running_max - df_cum_rets ) / running_max ) ( underwater ) . plot ( ax = ax , kind = 'area' , color = 'coral' , alpha = 0.7 , * * kwargs ) ax . set_ylabel ( 'Drawdown' ) ax . set_title ( 'Underwater plot' ) ax . set_xlabel ( '' ) return ax",Plots how far underwaterr returns are over time or plots current drawdown vs . date .
"def plot_perf_stats ( returns , factor_returns , ax = None ) : if ax is None : ax = plt . gca ( ) bootstrap_values = timeseries . perf_stats_bootstrap ( returns , factor_returns , return_stats = False ) bootstrap_values = bootstrap_values . drop ( 'Kurtosis' , axis = 'columns' ) sns . boxplot ( data = bootstrap_values , orient = 'h' , ax = ax ) return ax",Create box plot of some performance metrics of the strategy . The width of the box whiskers is determined by a bootstrap .
"def show_perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' , live_start_date = None , bootstrap = False , header_rows = None ) : if bootstrap : perf_func = timeseries . perf_stats_bootstrap else : perf_func = timeseries . perf_stats perf_stats_all = perf_func ( returns , factor_returns = factor_returns , positions = positions , transactions = transactions , turnover_denom = turnover_denom ) date_rows = OrderedDict ( ) if len ( returns . index ) > 0 : date_rows [ 'Start date' ] = returns . index [ 0 ] . strftime ( '%Y-%m-%d' ) date_rows [ 'End date' ] = returns . index [ - 1 ] . strftime ( '%Y-%m-%d' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) returns_is = returns [ returns . index < live_start_date ] returns_oos = returns [ returns . index >= live_start_date ] positions_is = None positions_oos = None transactions_is = None transactions_oos = None if positions is not None : positions_is = positions [ positions . index < live_start_date ] positions_oos = positions [ positions . index >= live_start_date ] if transactions is not None : transactions_is = transactions [ ( transactions . index < live_start_date ) ] transactions_oos = transactions [ ( transactions . index > live_start_date ) ] perf_stats_is = perf_func ( returns_is , factor_returns = factor_returns , positions = positions_is , transactions = transactions_is , turnover_denom = turnover_denom ) perf_stats_oos = perf_func ( returns_oos , factor_returns = factor_returns , positions = positions_oos , transactions = transactions_oos , turnover_denom = turnover_denom ) if len ( returns . index ) > 0 : date_rows [ 'In-sample months' ] = int ( len ( returns_is ) / APPROX_BDAYS_PER_MONTH ) date_rows [ 'Out-of-sample months' ] = int ( len ( returns_oos ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . concat ( OrderedDict ( [ ( 'In-sample' , perf_stats_is ) , ( 'Out-of-sample' , perf_stats_oos ) , ( 'All' , perf_stats_all ) , ] ) , axis = 1 ) else : if len ( returns . index ) > 0 : date_rows [ 'Total months' ] = int ( len ( returns ) / APPROX_BDAYS_PER_MONTH ) perf_stats = pd . DataFrame ( perf_stats_all , columns = [ 'Backtest' ] ) for column in perf_stats . columns : for stat , value in perf_stats [ column ] . iteritems ( ) : if stat in STAT_FUNCS_PCT : perf_stats . loc [ stat , column ] = str ( np . round ( value * 100 , 1 ) ) + '%' if header_rows is None : header_rows = date_rows else : header_rows = OrderedDict ( header_rows ) header_rows . update ( date_rows ) utils . print_table ( perf_stats , float_format = '{0:.2f}' . format , header_rows = header_rows , )",Prints some performance metrics of the strategy .
"def plot_returns ( returns , live_start_date = None , ax = None ) : if ax is None : ax = plt . gca ( ) ax . set_label ( '' ) ax . set_ylabel ( 'Returns' ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_returns = returns . loc [ returns . index < live_start_date ] oos_returns = returns . loc [ returns . index >= live_start_date ] is_returns . plot ( ax = ax , color = 'g' ) oos_returns . plot ( ax = ax , color = 'r' ) else : returns . plot ( ax = ax , color = 'g' ) return ax",Plots raw returns over time .
"def plot_rolling_returns ( returns , factor_returns = None , live_start_date = None , logy = False , cone_std = None , legend_loc = 'best' , volatility_match = False , cone_function = timeseries . forecast_cone_bootstrap , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Cumulative returns' ) ax . set_yscale ( 'log' if logy else 'linear' ) if volatility_match and factor_returns is None : raise ValueError ( 'volatility_match requires passing of ' 'factor_returns.' ) elif volatility_match and factor_returns is not None : bmark_vol = factor_returns . loc [ returns . index ] . std ( ) returns = ( returns / returns . std ( ) ) * bmark_vol cum_rets = ep . cum_returns ( returns , 1.0 ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) if factor_returns is not None : cum_factor_returns = ep . cum_returns ( factor_returns [ cum_rets . index ] , 1.0 ) cum_factor_returns . plot ( lw = 2 , color = 'gray' , label = factor_returns . name , alpha = 0.60 , ax = ax , * * kwargs ) if live_start_date is not None : live_start_date = ep . utils . get_utc_timestamp ( live_start_date ) is_cum_returns = cum_rets . loc [ cum_rets . index < live_start_date ] oos_cum_returns = cum_rets . loc [ cum_rets . index >= live_start_date ] else : is_cum_returns = cum_rets oos_cum_returns = pd . Series ( [ ] ) is_cum_returns . plot ( lw = 3 , color = 'forestgreen' , alpha = 0.6 , label = 'Backtest' , ax = ax , * * kwargs ) if len ( oos_cum_returns ) > 0 : oos_cum_returns . plot ( lw = 4 , color = 'red' , alpha = 0.6 , label = 'Live' , ax = ax , * * kwargs ) if cone_std is not None : if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] is_returns = returns . loc [ returns . index < live_start_date ] cone_bounds = cone_function ( is_returns , len ( oos_cum_returns ) , cone_std = cone_std , starting_value = is_cum_returns [ - 1 ] ) cone_bounds = cone_bounds . set_index ( oos_cum_returns . index ) for std in cone_std : ax . fill_between ( cone_bounds . index , cone_bounds [ float ( std ) ] , cone_bounds [ float ( - std ) ] , color = 'steelblue' , alpha = 0.5 ) if legend_loc is not None : ax . legend ( loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . axhline ( 1.0 , linestyle = '--' , color = 'black' , lw = 2 ) return ax",Plots cumulative rolling returns versus some benchmarks .
"def plot_rolling_beta ( returns , factor_returns , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) ax . set_title ( ""Rolling portfolio beta to "" + str ( factor_returns . name ) ) ax . set_ylabel ( 'Beta' ) rb_1 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) rb_1 . plot ( color = 'steelblue' , lw = 3 , alpha = 0.6 , ax = ax , * * kwargs ) rb_2 = timeseries . rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 12 ) rb_2 . plot ( color = 'grey' , lw = 3 , alpha = 0.4 , ax = ax , * * kwargs ) ax . axhline ( rb_1 . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_xlabel ( '' ) ax . legend ( [ '6-mo' , '12-mo' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_ylim ( ( - 1.0 , 1.0 ) ) return ax",Plots the rolling 6 - month and 12 - month beta versus date .
"def plot_rolling_volatility ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_vol_ts = timeseries . rolling_volatility ( returns , rolling_window ) rolling_vol_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , * * kwargs ) if factor_returns is not None : rolling_vol_ts_factor = timeseries . rolling_volatility ( factor_returns , rolling_window ) rolling_vol_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , * * kwargs ) ax . set_title ( 'Rolling volatility (6-month)' ) ax . axhline ( rolling_vol_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 2 ) ax . set_ylabel ( 'Volatility' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Volatility' , 'Benchmark volatility' , 'Average volatility' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax",Plots the rolling volatility versus date .
"def plot_rolling_sharpe ( returns , factor_returns = None , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) rolling_sharpe_ts = timeseries . rolling_sharpe ( returns , rolling_window ) rolling_sharpe_ts . plot ( alpha = .7 , lw = 3 , color = 'orangered' , ax = ax , * * kwargs ) if factor_returns is not None : rolling_sharpe_ts_factor = timeseries . rolling_sharpe ( factor_returns , rolling_window ) rolling_sharpe_ts_factor . plot ( alpha = .7 , lw = 3 , color = 'grey' , ax = ax , * * kwargs ) ax . set_title ( 'Rolling Sharpe ratio (6-month)' ) ax . axhline ( rolling_sharpe_ts . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 ) ax . axhline ( 0.0 , color = 'black' , linestyle = '-' , lw = 3 ) ax . set_ylabel ( 'Sharpe ratio' ) ax . set_xlabel ( '' ) if factor_returns is None : ax . legend ( [ 'Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) else : ax . legend ( [ 'Sharpe' , 'Benchmark Sharpe' , 'Average' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) return ax",Plots the rolling Sharpe ratio versus date .
"def plot_gross_leverage ( returns , positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) gl = timeseries . gross_lev ( positions ) gl . plot ( lw = 0.5 , color = 'limegreen' , legend = False , ax = ax , * * kwargs ) ax . axhline ( gl . mean ( ) , color = 'g' , linestyle = '--' , lw = 3 ) ax . set_title ( 'Gross leverage' ) ax . set_ylabel ( 'Gross leverage' ) ax . set_xlabel ( '' ) return ax",Plots gross leverage versus date .
"def plot_exposures ( returns , positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) pos_no_cash = positions . drop ( 'cash' , axis = 1 ) l_exp = pos_no_cash [ pos_no_cash > 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) s_exp = pos_no_cash [ pos_no_cash < 0 ] . sum ( axis = 1 ) / positions . sum ( axis = 1 ) net_exp = pos_no_cash . sum ( axis = 1 ) / positions . sum ( axis = 1 ) ax . fill_between ( l_exp . index , 0 , l_exp . values , label = 'Long' , color = 'green' , alpha = 0.5 ) ax . fill_between ( s_exp . index , 0 , s_exp . values , label = 'Short' , color = 'red' , alpha = 0.5 ) ax . plot ( net_exp . index , net_exp . values , label = 'Net' , color = 'black' , linestyle = 'dotted' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_title ( ""Exposure"" ) ax . set_ylabel ( 'Exposure' ) ax . legend ( loc = 'lower left' , frameon = True , framealpha = 0.5 ) ax . set_xlabel ( '' ) return ax",Plots a cake chart of the long and short exposure .
"def show_and_plot_top_positions ( returns , positions_alloc , show_and_plot = 2 , hide_positions = False , legend_loc = 'real_best' , ax = None , * * kwargs ) : positions_alloc = positions_alloc . copy ( ) positions_alloc . columns = positions_alloc . columns . map ( utils . format_asset ) df_top_long , df_top_short , df_top_abs = pos . get_top_long_short_abs ( positions_alloc ) if show_and_plot == 1 or show_and_plot == 2 : utils . print_table ( pd . DataFrame ( df_top_long * 100 , columns = [ 'max' ] ) , float_format = '{0:.2f}%' . format , name = 'Top 10 long positions of all time' ) utils . print_table ( pd . DataFrame ( df_top_short * 100 , columns = [ 'max' ] ) , float_format = '{0:.2f}%' . format , name = 'Top 10 short positions of all time' ) utils . print_table ( pd . DataFrame ( df_top_abs * 100 , columns = [ 'max' ] ) , float_format = '{0:.2f}%' . format , name = 'Top 10 positions of all time' ) if show_and_plot == 0 or show_and_plot == 2 : if ax is None : ax = plt . gca ( ) positions_alloc [ df_top_abs . index ] . plot ( title = 'Portfolio allocation over time, only top 10 holdings' , alpha = 0.5 , ax = ax , * * kwargs ) if legend_loc == 'real_best' : box = ax . get_position ( ) ax . set_position ( [ box . x0 , box . y0 + box . height * 0.1 , box . width , box . height * 0.9 ] ) ax . legend ( loc = 'upper center' , frameon = True , framealpha = 0.5 , bbox_to_anchor = ( 0.5 , - 0.14 ) , ncol = 5 ) else : ax . legend ( loc = legend_loc ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylabel ( 'Exposure by holding' ) if hide_positions : ax . legend_ . remove ( ) return ax",Prints and / or plots the exposures of the top 10 held positions of all time .
"def plot_max_median_position_concentration ( positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) alloc_summary = pos . get_max_median_position_concentration ( positions ) colors = [ 'mediumblue' , 'steelblue' , 'tomato' , 'firebrick' ] alloc_summary . plot ( linewidth = 1 , color = colors , alpha = 0.6 , ax = ax ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) ax . set_ylabel ( 'Exposure' ) ax . set_title ( 'Long/short max and median position concentration' ) return ax",Plots the max and median of long and short position concentrations over the time .
"def plot_sector_allocations ( returns , sector_alloc , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) sector_alloc . plot ( title = 'Sector allocation over time' , alpha = 0.5 , ax = ax , * * kwargs ) box = ax . get_position ( ) ax . set_position ( [ box . x0 , box . y0 + box . height * 0.1 , box . width , box . height * 0.9 ] ) ax . legend ( loc = 'upper center' , frameon = True , framealpha = 0.5 , bbox_to_anchor = ( 0.5 , - 0.14 ) , ncol = 5 ) ax . set_xlim ( ( sector_alloc . index [ 0 ] , sector_alloc . index [ - 1 ] ) ) ax . set_ylabel ( 'Exposure by sector' ) ax . set_xlabel ( '' ) return ax",Plots the sector exposures of the portfolio over time .
"def plot_return_quantiles ( returns , live_start_date = None , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) is_returns = returns if live_start_date is None else returns . loc [ returns . index < live_start_date ] is_weekly = ep . aggregate_returns ( is_returns , 'weekly' ) is_monthly = ep . aggregate_returns ( is_returns , 'monthly' ) sns . boxplot ( data = [ is_returns , is_weekly , is_monthly ] , palette = [ ""#4c72B0"" , ""#55A868"" , ""#CCB974"" ] , ax = ax , * * kwargs ) if live_start_date is not None : oos_returns = returns . loc [ returns . index >= live_start_date ] oos_weekly = ep . aggregate_returns ( oos_returns , 'weekly' ) oos_monthly = ep . aggregate_returns ( oos_returns , 'monthly' ) sns . swarmplot ( data = [ oos_returns , oos_weekly , oos_monthly ] , ax = ax , color = ""red"" , marker = ""d"" , * * kwargs ) red_dots = matplotlib . lines . Line2D ( [ ] , [ ] , color = ""red"" , marker = ""d"" , label = ""Out-of-sample data"" , linestyle = '' ) ax . legend ( handles = [ red_dots ] , frameon = True , framealpha = 0.5 ) ax . set_xticklabels ( [ 'Daily' , 'Weekly' , 'Monthly' ] ) ax . set_title ( 'Return quantiles' ) return ax",Creates a box plot of daily weekly and monthly return distributions .
"def plot_turnover ( returns , transactions , positions , legend_loc = 'best' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) y_axis_formatter = FuncFormatter ( utils . two_dec_places ) ax . yaxis . set_major_formatter ( FuncFormatter ( y_axis_formatter ) ) df_turnover = txn . get_turnover ( positions , transactions ) df_turnover_by_month = df_turnover . resample ( ""M"" ) . mean ( ) df_turnover . plot ( color = 'steelblue' , alpha = 1.0 , lw = 0.5 , ax = ax , * * kwargs ) df_turnover_by_month . plot ( color = 'orangered' , alpha = 0.5 , lw = 2 , ax = ax , * * kwargs ) ax . axhline ( df_turnover . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . legend ( [ 'Daily turnover' , 'Average daily turnover, by month' , 'Average daily turnover, net' ] , loc = legend_loc , frameon = True , framealpha = 0.5 ) ax . set_title ( 'Daily turnover' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylim ( ( 0 , 2 ) ) ax . set_ylabel ( 'Turnover' ) ax . set_xlabel ( '' ) return ax",Plots turnover vs . date .
"def plot_slippage_sweep ( returns , positions , transactions , slippage_params = ( 3 , 8 , 10 , 12 , 15 , 20 , 50 ) , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) slippage_sweep = pd . DataFrame ( ) for bps in slippage_params : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) label = str ( bps ) + "" bps"" slippage_sweep [ label ] = ep . cum_returns ( adj_returns , 1 ) slippage_sweep . plot ( alpha = 1.0 , lw = 0.5 , ax = ax ) ax . set_title ( 'Cumulative returns given additional per-dollar slippage' ) ax . set_ylabel ( '' ) ax . legend ( loc = 'center left' , frameon = True , framealpha = 0.5 ) return ax",Plots equity curves at different per - dollar slippage assumptions .
"def plot_slippage_sensitivity ( returns , positions , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) avg_returns_given_slippage = pd . Series ( ) for bps in range ( 1 , 100 ) : adj_returns = txn . adjust_returns_for_slippage ( returns , positions , transactions , bps ) avg_returns = ep . annual_return ( adj_returns ) avg_returns_given_slippage . loc [ bps ] = avg_returns avg_returns_given_slippage . plot ( alpha = 1.0 , lw = 2 , ax = ax ) ax . set_title ( 'Average annual returns given additional per-dollar slippage' ) ax . set_xticks ( np . arange ( 0 , 100 , 10 ) ) ax . set_ylabel ( 'Average annual return' ) ax . set_xlabel ( 'Per-dollar slippage (bps)' ) return ax",Plots curve relating per - dollar slippage to average annual returns .
"def plot_daily_turnover_hist ( transactions , positions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) turnover = txn . get_turnover ( positions , transactions ) sns . distplot ( turnover , ax = ax , * * kwargs ) ax . set_title ( 'Distribution of daily turnover rates' ) ax . set_xlabel ( 'Turnover rate' ) return ax",Plots a histogram of daily turnover rates .
"def plot_daily_volume ( returns , transactions , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) daily_txn = txn . get_txn_vol ( transactions ) daily_txn . txn_shares . plot ( alpha = 1.0 , lw = 0.5 , ax = ax , * * kwargs ) ax . axhline ( daily_txn . txn_shares . mean ( ) , color = 'steelblue' , linestyle = '--' , lw = 3 , alpha = 1.0 ) ax . set_title ( 'Daily trading volume' ) ax . set_xlim ( ( returns . index [ 0 ] , returns . index [ - 1 ] ) ) ax . set_ylabel ( 'Amount of shares traded' ) ax . set_xlabel ( '' ) return ax",Plots trading volume per day vs . date .
"def plot_txn_time_hist ( transactions , bin_minutes = 5 , tz = 'America/New_York' , ax = None , * * kwargs ) : if ax is None : ax = plt . gca ( ) txn_time = transactions . copy ( ) txn_time . index = txn_time . index . tz_convert ( pytz . timezone ( tz ) ) txn_time . index = txn_time . index . map ( lambda x : x . hour * 60 + x . minute ) txn_time [ 'trade_value' ] = ( txn_time . amount * txn_time . price ) . abs ( ) txn_time = txn_time . groupby ( level = 0 ) . sum ( ) . reindex ( index = range ( 570 , 961 ) ) txn_time . index = ( txn_time . index / bin_minutes ) . astype ( int ) * bin_minutes txn_time = txn_time . groupby ( level = 0 ) . sum ( ) txn_time [ 'time_str' ] = txn_time . index . map ( lambda x : str ( datetime . time ( int ( x / 60 ) , x % 60 ) ) [ : - 3 ] ) trade_value_sum = txn_time . trade_value . sum ( ) txn_time . trade_value = txn_time . trade_value . fillna ( 0 ) / trade_value_sum ax . bar ( txn_time . index , txn_time . trade_value , width = bin_minutes , * * kwargs ) ax . set_xlim ( 570 , 960 ) ax . set_xticks ( txn_time . index [ : : int ( 30 / bin_minutes ) ] ) ax . set_xticklabels ( txn_time . time_str [ : : int ( 30 / bin_minutes ) ] ) ax . set_title ( 'Transaction time distribution' ) ax . set_ylabel ( 'Proportion' ) ax . set_xlabel ( '' ) return ax",Plots a histogram of transaction times binning the times into buckets of a given duration .
"def show_worst_drawdown_periods ( returns , top = 5 ) : drawdown_df = timeseries . gen_drawdown_table ( returns , top = top ) utils . print_table ( drawdown_df . sort_values ( 'Net drawdown in %' , ascending = False ) , name = 'Worst drawdown periods' , float_format = '{0:.2f}' . format , )",Prints information about the worst drawdown periods .
"def plot_monthly_returns_timeseries ( returns , ax = None , * * kwargs ) : def cumulate_returns ( x ) : return ep . cum_returns ( x ) [ - 1 ] if ax is None : ax = plt . gca ( ) monthly_rets = returns . resample ( 'M' ) . apply ( lambda x : cumulate_returns ( x ) ) monthly_rets = monthly_rets . to_period ( ) sns . barplot ( x = monthly_rets . index , y = monthly_rets . values , color = 'steelblue' ) locs , labels = plt . xticks ( ) plt . setp ( labels , rotation = 90 ) xticks_coord = [ ] xticks_label = [ ] count = 0 for i in monthly_rets . index : if i . month == 1 : xticks_label . append ( i ) xticks_coord . append ( count ) ax . axvline ( count , color = 'gray' , ls = '--' , alpha = 0.3 ) count += 1 ax . axhline ( 0.0 , color = 'darkgray' , ls = '-' ) ax . set_xticks ( xticks_coord ) ax . set_xticklabels ( xticks_label ) return ax",Plots monthly returns as a timeseries .
"def plot_round_trip_lifetimes ( round_trips , disp_amount = 16 , lsize = 18 , ax = None ) : if ax is None : ax = plt . subplot ( ) symbols_sample = round_trips . symbol . unique ( ) np . random . seed ( 1 ) sample = np . random . choice ( round_trips . symbol . unique ( ) , replace = False , size = min ( disp_amount , len ( symbols_sample ) ) ) sample_round_trips = round_trips [ round_trips . symbol . isin ( sample ) ] symbol_idx = pd . Series ( np . arange ( len ( sample ) ) , index = sample ) for symbol , sym_round_trips in sample_round_trips . groupby ( 'symbol' ) : for _ , row in sym_round_trips . iterrows ( ) : c = 'b' if row . long else 'r' y_ix = symbol_idx [ symbol ] + 0.05 ax . plot ( [ row [ 'open_dt' ] , row [ 'close_dt' ] ] , [ y_ix , y_ix ] , color = c , linewidth = lsize , solid_capstyle = 'butt' ) ax . set_yticks ( range ( disp_amount ) ) ax . set_yticklabels ( [ utils . format_asset ( s ) for s in sample ] ) ax . set_ylim ( ( - 0.5 , min ( len ( sample ) , disp_amount ) - 0.5 ) ) blue = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'b' , label = 'Long' ) red = patches . Rectangle ( [ 0 , 0 ] , 1 , 1 , color = 'r' , label = 'Short' ) leg = ax . legend ( handles = [ blue , red ] , loc = 'lower left' , frameon = True , framealpha = 0.5 ) leg . get_frame ( ) . set_edgecolor ( 'black' ) ax . grid ( False ) return ax",Plots timespans and directions of a sample of round trip trades .
"def show_profit_attribution ( round_trips ) : total_pnl = round_trips [ 'pnl' ] . sum ( ) pnl_attribution = round_trips . groupby ( 'symbol' ) [ 'pnl' ] . sum ( ) / total_pnl pnl_attribution . name = '' pnl_attribution . index = pnl_attribution . index . map ( utils . format_asset ) utils . print_table ( pnl_attribution . sort_values ( inplace = False , ascending = False , ) , name = 'Profitability (PnL / PnL total) per name' , float_format = '{:.2%}' . format , )",Prints the share of total PnL contributed by each traded name .
"def plot_prob_profit_trade ( round_trips , ax = None ) : x = np . linspace ( 0 , 1. , 500 ) round_trips [ 'profitable' ] = round_trips . pnl > 0 dist = sp . stats . beta ( round_trips . profitable . sum ( ) , ( ~ round_trips . profitable ) . sum ( ) ) y = dist . pdf ( x ) lower_perc = dist . ppf ( .025 ) upper_perc = dist . ppf ( .975 ) lower_plot = dist . ppf ( .001 ) upper_plot = dist . ppf ( .999 ) if ax is None : ax = plt . subplot ( ) ax . plot ( x , y ) ax . axvline ( lower_perc , color = '0.5' ) ax . axvline ( upper_perc , color = '0.5' ) ax . set_xlabel ( 'Probability of making a profitable decision' ) ax . set_ylabel ( 'Belief' ) ax . set_xlim ( lower_plot , upper_plot ) ax . set_ylim ( ( 0 , y . max ( ) + 1. ) ) return ax",Plots a probability distribution for the event of making a profitable trade .
"def plot_cones ( name , bounds , oos_returns , num_samples = 1000 , ax = None , cone_std = ( 1. , 1.5 , 2. ) , random_seed = None , num_strikes = 3 ) : if ax is None : fig = figure . Figure ( figsize = ( 10 , 8 ) ) FigureCanvasAgg ( fig ) axes = fig . add_subplot ( 111 ) else : axes = ax returns = ep . cum_returns ( oos_returns , starting_value = 1. ) bounds_tmp = bounds . copy ( ) returns_tmp = returns . copy ( ) cone_start = returns . index [ 0 ] colors = [ ""green"" , ""orange"" , ""orangered"" , ""darkred"" ] for c in range ( num_strikes + 1 ) : if c > 0 : tmp = returns . loc [ cone_start : ] bounds_tmp = bounds_tmp . iloc [ 0 : len ( tmp ) ] bounds_tmp = bounds_tmp . set_index ( tmp . index ) crossing = ( tmp < bounds_tmp [ float ( - 2. ) ] . iloc [ : len ( tmp ) ] ) if crossing . sum ( ) <= 0 : break cone_start = crossing . loc [ crossing ] . index [ 0 ] returns_tmp = returns . loc [ cone_start : ] bounds_tmp = ( bounds - ( 1 - returns . loc [ cone_start ] ) ) for std in cone_std : x = returns_tmp . index y1 = bounds_tmp [ float ( std ) ] . iloc [ : len ( returns_tmp ) ] y2 = bounds_tmp [ float ( - std ) ] . iloc [ : len ( returns_tmp ) ] axes . fill_between ( x , y1 , y2 , color = colors [ c ] , alpha = 0.5 ) label = 'Cumulative returns = {:.2f}%' . format ( ( returns . iloc [ - 1 ] - 1 ) * 100 ) axes . plot ( returns . index , returns . values , color = 'black' , lw = 3. , label = label ) if name is not None : axes . set_title ( name ) axes . axhline ( 1 , color = 'black' , alpha = 0.2 ) axes . legend ( frameon = True , framealpha = 0.5 ) if ax is None : return fig else : return axes",Plots the upper and lower bounds of an n standard deviation cone of forecasted cumulative returns . Redraws a new cone when cumulative returns fall outside of last cone drawn .
"def var_cov_var_normal ( P , c , mu = 0 , sigma = 1 ) : alpha = sp . stats . norm . ppf ( 1 - c , mu , sigma ) return P - P * ( alpha + 1 )",Variance - covariance calculation of daily Value - at - Risk in a portfolio .
"def sortino_ratio ( returns , required_return = 0 , period = DAILY ) : return ep . sortino_ratio ( returns , required_return = required_return )",Determines the Sortino ratio of a strategy .
"def downside_risk ( returns , required_return = 0 , period = DAILY ) : return ep . downside_risk ( returns , required_return = required_return , period = period )",Determines the downside deviation below a threshold
"def sharpe_ratio ( returns , risk_free = 0 , period = DAILY ) : return ep . sharpe_ratio ( returns , risk_free = risk_free , period = period )",Determines the Sharpe ratio of a strategy .
"def rolling_beta ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 ) : if factor_returns . ndim > 1 : return factor_returns . apply ( partial ( rolling_beta , returns ) , rolling_window = rolling_window ) else : out = pd . Series ( index = returns . index ) for beg , end in zip ( returns . index [ 0 : - rolling_window ] , returns . index [ rolling_window : ] ) : out . loc [ end ] = ep . beta ( returns . loc [ beg : end ] , factor_returns . loc [ beg : end ] ) return out",Determines the rolling beta of a strategy .
"def rolling_regression ( returns , factor_returns , rolling_window = APPROX_BDAYS_PER_MONTH * 6 , nan_threshold = 0.1 ) : ret_no_na = returns . dropna ( ) columns = [ 'alpha' ] + factor_returns . columns . tolist ( ) rolling_risk = pd . DataFrame ( columns = columns , index = ret_no_na . index ) rolling_risk . index . name = 'dt' for beg , end in zip ( ret_no_na . index [ : - rolling_window ] , ret_no_na . index [ rolling_window : ] ) : returns_period = ret_no_na [ beg : end ] factor_returns_period = factor_returns . loc [ returns_period . index ] if np . all ( factor_returns_period . isnull ( ) . mean ( ) ) < nan_threshold : factor_returns_period_dnan = factor_returns_period . dropna ( ) reg = linear_model . LinearRegression ( fit_intercept = True ) . fit ( factor_returns_period_dnan , returns_period . loc [ factor_returns_period_dnan . index ] ) rolling_risk . loc [ end , factor_returns . columns ] = reg . coef_ rolling_risk . loc [ end , 'alpha' ] = reg . intercept_ return rolling_risk",Computes rolling factor betas using a multivariate linear regression ( separate linear regressions is problematic because the factors may be confounded ) .
"def gross_lev ( positions ) : exposure = positions . drop ( 'cash' , axis = 1 ) . abs ( ) . sum ( axis = 1 ) return exposure / positions . sum ( axis = 1 )",Calculates the gross leverage of a strategy .
"def value_at_risk ( returns , period = None , sigma = 2.0 ) : if period is not None : returns_agg = ep . aggregate_returns ( returns , period ) else : returns_agg = returns . copy ( ) value_at_risk = returns_agg . mean ( ) - sigma * returns_agg . std ( ) return value_at_risk",Get value at risk ( VaR ) .
"def perf_stats ( returns , factor_returns = None , positions = None , transactions = None , turnover_denom = 'AGB' ) : stats = pd . Series ( ) for stat_func in SIMPLE_STAT_FUNCS : stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = stat_func ( returns ) if positions is not None : stats [ 'Gross leverage' ] = gross_lev ( positions ) . mean ( ) if transactions is not None : stats [ 'Daily turnover' ] = get_turnover ( positions , transactions , turnover_denom ) . mean ( ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : res = stat_func ( returns , factor_returns ) stats [ STAT_FUNC_NAMES [ stat_func . __name__ ] ] = res return stats",Calculates various performance metrics of a strategy for use in plotting . show_perf_stats .
"def perf_stats_bootstrap ( returns , factor_returns = None , return_stats = True , * * kwargs ) : bootstrap_values = OrderedDict ( ) for stat_func in SIMPLE_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns ) if factor_returns is not None : for stat_func in FACTOR_STAT_FUNCS : stat_name = STAT_FUNC_NAMES [ stat_func . __name__ ] bootstrap_values [ stat_name ] = calc_bootstrap ( stat_func , returns , factor_returns = factor_returns ) bootstrap_values = pd . DataFrame ( bootstrap_values ) if return_stats : stats = bootstrap_values . apply ( calc_distribution_stats ) return stats . T [ [ 'mean' , 'median' , '5%' , '95%' ] ] else : return bootstrap_values",Calculates various bootstrapped performance metrics of a strategy .
"def calc_bootstrap ( func , returns , * args , * * kwargs ) : n_samples = kwargs . pop ( 'n_samples' , 1000 ) out = np . empty ( n_samples ) factor_returns = kwargs . pop ( 'factor_returns' , None ) for i in range ( n_samples ) : idx = np . random . randint ( len ( returns ) , size = len ( returns ) ) returns_i = returns . iloc [ idx ] . reset_index ( drop = True ) if factor_returns is not None : factor_returns_i = factor_returns . iloc [ idx ] . reset_index ( drop = True ) out [ i ] = func ( returns_i , factor_returns_i , * args , * * kwargs ) else : out [ i ] = func ( returns_i , * args , * * kwargs ) return out",Performs a bootstrap analysis on a user - defined function returning a summary statistic .
"def calc_distribution_stats ( x ) : return pd . Series ( { 'mean' : np . mean ( x ) , 'median' : np . median ( x ) , 'std' : np . std ( x ) , '5%' : np . percentile ( x , 5 ) , '25%' : np . percentile ( x , 25 ) , '75%' : np . percentile ( x , 75 ) , '95%' : np . percentile ( x , 95 ) , 'IQR' : np . subtract . reduce ( np . percentile ( x , [ 75 , 25 ] ) ) , } )",Calculate various summary statistics of data .
"def get_max_drawdown_underwater ( underwater ) : valley = np . argmin ( underwater ) peak = underwater [ : valley ] [ underwater [ : valley ] == 0 ] . index [ - 1 ] try : recovery = underwater [ valley : ] [ underwater [ valley : ] == 0 ] . index [ 0 ] except IndexError : recovery = np . nan return peak , valley , recovery",Determines peak valley and recovery dates given an underwater DataFrame .
"def get_max_drawdown ( returns ) : returns = returns . copy ( ) df_cum = cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 return get_max_drawdown_underwater ( underwater )",Determines the maximum drawdown of a strategy .
"def get_top_drawdowns ( returns , top = 10 ) : returns = returns . copy ( ) df_cum = ep . cum_returns ( returns , 1.0 ) running_max = np . maximum . accumulate ( df_cum ) underwater = df_cum / running_max - 1 drawdowns = [ ] for t in range ( top ) : peak , valley , recovery = get_max_drawdown_underwater ( underwater ) if not pd . isnull ( recovery ) : underwater . drop ( underwater [ peak : recovery ] . index [ 1 : - 1 ] , inplace = True ) else : underwater = underwater . loc [ : peak ] drawdowns . append ( ( peak , valley , recovery ) ) if ( len ( returns ) == 0 ) or ( len ( underwater ) == 0 ) : break return drawdowns",Finds top drawdowns sorted by drawdown amount .
"def gen_drawdown_table ( returns , top = 10 ) : df_cum = ep . cum_returns ( returns , 1.0 ) drawdown_periods = get_top_drawdowns ( returns , top = top ) df_drawdowns = pd . DataFrame ( index = list ( range ( top ) ) , columns = [ 'Net drawdown in %' , 'Peak date' , 'Valley date' , 'Recovery date' , 'Duration' ] ) for i , ( peak , valley , recovery ) in enumerate ( drawdown_periods ) : if pd . isnull ( recovery ) : df_drawdowns . loc [ i , 'Duration' ] = np . nan else : df_drawdowns . loc [ i , 'Duration' ] = len ( pd . date_range ( peak , recovery , freq = 'B' ) ) df_drawdowns . loc [ i , 'Peak date' ] = ( peak . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Valley date' ] = ( valley . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) if isinstance ( recovery , float ) : df_drawdowns . loc [ i , 'Recovery date' ] = recovery else : df_drawdowns . loc [ i , 'Recovery date' ] = ( recovery . to_pydatetime ( ) . strftime ( '%Y-%m-%d' ) ) df_drawdowns . loc [ i , 'Net drawdown in %' ] = ( ( df_cum . loc [ peak ] - df_cum . loc [ valley ] ) / df_cum . loc [ peak ] ) * 100 df_drawdowns [ 'Peak date' ] = pd . to_datetime ( df_drawdowns [ 'Peak date' ] ) df_drawdowns [ 'Valley date' ] = pd . to_datetime ( df_drawdowns [ 'Valley date' ] ) df_drawdowns [ 'Recovery date' ] = pd . to_datetime ( df_drawdowns [ 'Recovery date' ] ) return df_drawdowns",Places top drawdowns in a table .
"def rolling_volatility ( returns , rolling_vol_window ) : return returns . rolling ( rolling_vol_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )",Determines the rolling volatility of a strategy .
"def rolling_sharpe ( returns , rolling_sharpe_window ) : return returns . rolling ( rolling_sharpe_window ) . mean ( ) / returns . rolling ( rolling_sharpe_window ) . std ( ) * np . sqrt ( APPROX_BDAYS_PER_YEAR )",Determines the rolling Sharpe ratio of a strategy .
"def simulate_paths ( is_returns , num_days , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = np . empty ( ( num_samples , num_days ) ) seed = np . random . RandomState ( seed = random_seed ) for i in range ( num_samples ) : samples [ i , : ] = is_returns . sample ( num_days , replace = True , random_state = seed ) return samples",Gnerate alternate paths using available values from in - sample returns .
"def summarize_paths ( samples , cone_std = ( 1. , 1.5 , 2. ) , starting_value = 1. ) : cum_samples = ep . cum_returns ( samples . T , starting_value = starting_value ) . T cum_mean = cum_samples . mean ( axis = 0 ) cum_std = cum_samples . std ( axis = 0 ) if isinstance ( cone_std , ( float , int ) ) : cone_std = [ cone_std ] cone_bounds = pd . DataFrame ( columns = pd . Float64Index ( [ ] ) ) for num_std in cone_std : cone_bounds . loc [ : , float ( num_std ) ] = cum_mean + cum_std * num_std cone_bounds . loc [ : , float ( - num_std ) ] = cum_mean - cum_std * num_std return cone_bounds",Gnerate the upper and lower bounds of an n standard deviation cone of forecasted cumulative returns .
"def forecast_cone_bootstrap ( is_returns , num_days , cone_std = ( 1. , 1.5 , 2. ) , starting_value = 1 , num_samples = 1000 , random_seed = None ) : samples = simulate_paths ( is_returns = is_returns , num_days = num_days , starting_value = starting_value , num_samples = num_samples , random_seed = random_seed ) cone_bounds = summarize_paths ( samples = samples , cone_std = cone_std , starting_value = starting_value ) return cone_bounds",Determines the upper and lower bounds of an n standard deviation cone of forecasted cumulative returns . Future cumulative mean and standard devation are computed by repeatedly sampling from the in - sample daily returns ( i . e . bootstrap ) . This cone is non - parametric meaning it does not assume that returns are normally distributed .
"def extract_interesting_date_ranges ( returns ) : returns_dupe = returns . copy ( ) returns_dupe . index = returns_dupe . index . map ( pd . Timestamp ) ranges = OrderedDict ( ) for name , ( start , end ) in PERIODS . items ( ) : try : period = returns_dupe . loc [ start : end ] if len ( period ) == 0 : continue ranges [ name ] = period except BaseException : continue return ranges",Extracts returns based on interesting events . See gen_date_range_interesting .
"def model_returns_t_alpha_beta ( data , bmark , samples = 2000 , progressbar = True ) : data_bmark = pd . concat ( [ data , bmark ] , axis = 1 ) . dropna ( ) with pm . Model ( ) as model : sigma = pm . HalfCauchy ( 'sigma' , beta = 1 ) nu = pm . Exponential ( 'nu_minus_two' , 1. / 10. ) X = data_bmark . iloc [ : , 1 ] y = data_bmark . iloc [ : , 0 ] alpha_reg = pm . Normal ( 'alpha' , mu = 0 , sd = .1 ) beta_reg = pm . Normal ( 'beta' , mu = 0 , sd = 1 ) mu_reg = alpha_reg + beta_reg * X pm . StudentT ( 'returns' , nu = nu + 2 , mu = mu_reg , sd = sigma , observed = y ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace",Run Bayesian alpha - beta - model with T distributed returns .
"def model_returns_normal ( data , samples = 500 , progressbar = True ) : with pm . Model ( ) as model : mu = pm . Normal ( 'mean returns' , mu = 0 , sd = .01 , testval = data . mean ( ) ) sigma = pm . HalfCauchy ( 'volatility' , beta = 1 , testval = data . std ( ) ) returns = pm . Normal ( 'returns' , mu = mu , sd = sigma , observed = data ) pm . Deterministic ( 'annual volatility' , returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'sharpe' , returns . distribution . mean / returns . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace",Run Bayesian model assuming returns are normally distributed .
"def model_best ( y1 , y2 , samples = 1000 , progressbar = True ) : y = np . concatenate ( ( y1 , y2 ) ) mu_m = np . mean ( y ) mu_p = 0.000001 * 1 / np . std ( y ) ** 2 sigma_low = np . std ( y ) / 1000 sigma_high = np . std ( y ) * 1000 with pm . Model ( ) as model : group1_mean = pm . Normal ( 'group1_mean' , mu = mu_m , tau = mu_p , testval = y1 . mean ( ) ) group2_mean = pm . Normal ( 'group2_mean' , mu = mu_m , tau = mu_p , testval = y2 . mean ( ) ) group1_std = pm . Uniform ( 'group1_std' , lower = sigma_low , upper = sigma_high , testval = y1 . std ( ) ) group2_std = pm . Uniform ( 'group2_std' , lower = sigma_low , upper = sigma_high , testval = y2 . std ( ) ) nu = pm . Exponential ( 'nu_minus_two' , 1 / 29. , testval = 4. ) + 2. returns_group1 = pm . StudentT ( 'group1' , nu = nu , mu = group1_mean , lam = group1_std ** - 2 , observed = y1 ) returns_group2 = pm . StudentT ( 'group2' , nu = nu , mu = group2_mean , lam = group2_std ** - 2 , observed = y2 ) diff_of_means = pm . Deterministic ( 'difference of means' , group2_mean - group1_mean ) pm . Deterministic ( 'difference of stds' , group2_std - group1_std ) pm . Deterministic ( 'effect size' , diff_of_means / pm . math . sqrt ( ( group1_std ** 2 + group2_std ** 2 ) / 2 ) ) pm . Deterministic ( 'group1_annual_volatility' , returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_annual_volatility' , returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group1_sharpe' , returns_group1 . distribution . mean / returns_group1 . distribution . variance ** .5 * np . sqrt ( 252 ) ) pm . Deterministic ( 'group2_sharpe' , returns_group2 . distribution . mean / returns_group2 . distribution . variance ** .5 * np . sqrt ( 252 ) ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace",Bayesian Estimation Supersedes the T - Test
"def plot_best ( trace = None , data_train = None , data_test = None , samples = 1000 , burn = 200 , axs = None ) : if trace is None : if ( data_train is not None ) or ( data_test is not None ) : raise ValueError ( 'Either pass trace or data_train and data_test' ) trace = model_best ( data_train , data_test , samples = samples ) trace = trace [ burn : ] if axs is None : fig , axs = plt . subplots ( ncols = 2 , nrows = 3 , figsize = ( 16 , 4 ) ) def distplot_w_perc ( trace , ax ) : sns . distplot ( trace , ax = ax ) ax . axvline ( stats . scoreatpercentile ( trace , 2.5 ) , color = '0.5' , label = '2.5 and 97.5 percentiles' ) ax . axvline ( stats . scoreatpercentile ( trace , 97.5 ) , color = '0.5' ) sns . distplot ( trace [ 'group1_mean' ] , ax = axs [ 0 ] , label = 'Backtest' ) sns . distplot ( trace [ 'group2_mean' ] , ax = axs [ 0 ] , label = 'Forward' ) axs [ 0 ] . legend ( loc = 0 , frameon = True , framealpha = 0.5 ) axs [ 1 ] . legend ( loc = 0 , frameon = True , framealpha = 0.5 ) distplot_w_perc ( trace [ 'difference of means' ] , axs [ 1 ] ) axs [ 0 ] . set ( xlabel = 'Mean' , ylabel = 'Belief' , yticklabels = [ ] ) axs [ 1 ] . set ( xlabel = 'Difference of means' , yticklabels = [ ] ) sns . distplot ( trace [ 'group1_annual_volatility' ] , ax = axs [ 2 ] , label = 'Backtest' ) sns . distplot ( trace [ 'group2_annual_volatility' ] , ax = axs [ 2 ] , label = 'Forward' ) distplot_w_perc ( trace [ 'group2_annual_volatility' ] - trace [ 'group1_annual_volatility' ] , axs [ 3 ] ) axs [ 2 ] . set ( xlabel = 'Annual volatility' , ylabel = 'Belief' , yticklabels = [ ] ) axs [ 2 ] . legend ( loc = 0 , frameon = True , framealpha = 0.5 ) axs [ 3 ] . set ( xlabel = 'Difference of volatility' , yticklabels = [ ] ) sns . distplot ( trace [ 'group1_sharpe' ] , ax = axs [ 4 ] , label = 'Backtest' ) sns . distplot ( trace [ 'group2_sharpe' ] , ax = axs [ 4 ] , label = 'Forward' ) distplot_w_perc ( trace [ 'group2_sharpe' ] - trace [ 'group1_sharpe' ] , axs [ 5 ] ) axs [ 4 ] . set ( xlabel = 'Sharpe' , ylabel = 'Belief' , yticklabels = [ ] ) axs [ 4 ] . legend ( loc = 0 , frameon = True , framealpha = 0.5 ) axs [ 5 ] . set ( xlabel = 'Difference of Sharpes' , yticklabels = [ ] ) sns . distplot ( trace [ 'effect size' ] , ax = axs [ 6 ] ) axs [ 6 ] . axvline ( stats . scoreatpercentile ( trace [ 'effect size' ] , 2.5 ) , color = '0.5' ) axs [ 6 ] . axvline ( stats . scoreatpercentile ( trace [ 'effect size' ] , 97.5 ) , color = '0.5' ) axs [ 6 ] . set ( xlabel = 'Difference of means normalized by volatility' , ylabel = 'Belief' , yticklabels = [ ] )",Plot BEST significance analysis .
"def model_stoch_vol ( data , samples = 2000 , progressbar = True ) : from pymc3 . distributions . timeseries import GaussianRandomWalk with pm . Model ( ) as model : nu = pm . Exponential ( 'nu' , 1. / 10 , testval = 5. ) sigma = pm . Exponential ( 'sigma' , 1. / .02 , testval = .1 ) s = GaussianRandomWalk ( 's' , sigma ** - 2 , shape = len ( data ) ) volatility_process = pm . Deterministic ( 'volatility_process' , pm . math . exp ( - 2 * s ) ) pm . StudentT ( 'r' , nu , lam = volatility_process , observed = data ) trace = pm . sample ( samples , progressbar = progressbar ) return model , trace",Run stochastic volatility model .
"def plot_stoch_vol ( data , trace = None , ax = None ) : if trace is None : trace = model_stoch_vol ( data ) if ax is None : fig , ax = plt . subplots ( figsize = ( 15 , 8 ) ) data . abs ( ) . plot ( ax = ax ) ax . plot ( data . index , np . exp ( trace [ 's' , : : 30 ] . T ) , 'r' , alpha = .03 ) ax . set ( title = 'Stochastic volatility' , xlabel = 'Time' , ylabel = 'Volatility' ) ax . legend ( [ 'Abs returns' , 'Stochastic volatility process' ] , frameon = True , framealpha = 0.5 ) return ax",Generate plot for stochastic volatility model .
"def compute_bayes_cone ( preds , starting_value = 1. ) : def scoreatpercentile ( cum_preds , p ) : return [ stats . scoreatpercentile ( c , p ) for c in cum_preds . T ] cum_preds = np . cumprod ( preds + 1 , 1 ) * starting_value perc = { p : scoreatpercentile ( cum_preds , p ) for p in ( 5 , 25 , 75 , 95 ) } return perc",Compute 5 25 75 and 95 percentiles of cumulative returns used for the Bayesian cone .
"def compute_consistency_score ( returns_test , preds ) : returns_test_cum = cum_returns ( returns_test , starting_value = 1. ) cum_preds = np . cumprod ( preds + 1 , 1 ) q = [ sp . stats . percentileofscore ( cum_preds [ : , i ] , returns_test_cum . iloc [ i ] , kind = 'weak' ) for i in range ( len ( returns_test_cum ) ) ] return 100 - np . abs ( 50 - np . mean ( q ) ) / .5",Compute Bayesian consistency score .
"def run_model ( model , returns_train , returns_test = None , bmark = None , samples = 500 , ppc = False , progressbar = True ) : if model == 'alpha_beta' : model , trace = model_returns_t_alpha_beta ( returns_train , bmark , samples , progressbar = progressbar ) elif model == 't' : model , trace = model_returns_t ( returns_train , samples , progressbar = progressbar ) elif model == 'normal' : model , trace = model_returns_normal ( returns_train , samples , progressbar = progressbar ) elif model == 'best' : model , trace = model_best ( returns_train , returns_test , samples = samples , progressbar = progressbar ) else : raise NotImplementedError ( 'Model {} not found.' 'Use alpha_beta, t, normal, or best.' . format ( model ) ) if ppc : ppc_samples = pm . sample_ppc ( trace , samples = samples , model = model , size = len ( returns_test ) , progressbar = progressbar ) return trace , ppc_samples [ 'returns' ] return trace",Run one of the Bayesian models .
"def plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = 50 , ax = None ) : score = compute_consistency_score ( returns_test , ppc ) ax = _plot_bayes_cone ( returns_train , returns_test , ppc , plot_train_len = plot_train_len , ax = ax ) ax . text ( 0.40 , 0.90 , 'Consistency score: %.1f' % score , verticalalignment = 'bottom' , horizontalalignment = 'right' , transform = ax . transAxes , ) ax . set_ylabel ( 'Cumulative returns' ) return score",Generate cumulative returns plot with Bayesian cone .
"def load_voc_dataset ( path = 'data' , dataset = '2012' , contain_classes_in_person = False ) : path = os . path . join ( path , 'VOC' ) def _recursive_parse_xml_to_dict ( xml ) : """"""Recursively parses XML contents to python dict.

        We assume that `object` tags are the only ones that can appear
        multiple times at the same level of a tree.

        Args:
            xml: xml tree obtained by parsing XML file contents using lxml.etree

        Returns:
            Python dictionary holding XML contents.

        """""" if xml is not None : return { xml . tag : xml . text } result = { } for child in xml : child_result = _recursive_parse_xml_to_dict ( child ) if child . tag != 'object' : result [ child . tag ] = child_result [ child . tag ] else : if child . tag not in result : result [ child . tag ] = [ ] result [ child . tag ] . append ( child_result [ child . tag ] ) return { xml . tag : result } import xml . etree . ElementTree as ET if dataset == ""2012"" : url = ""http://pjreddie.com/media/files/"" tar_filename = ""VOCtrainval_11-May-2012.tar"" extracted_filename = ""VOC2012"" logging . info ( ""    [============= VOC 2012 =============]"" ) elif dataset == ""2012test"" : extracted_filename = ""VOC2012test"" logging . info ( ""    [============= VOC 2012 Test Set =============]"" ) logging . info ( ""    \nAuthor: 2012test only have person annotation, so 2007test is highly recommended for testing !\n"" ) import time time . sleep ( 3 ) if os . path . isdir ( os . path . join ( path , extracted_filename ) ) is False : logging . info ( ""For VOC 2012 Test data - online registration required"" ) logging . info ( "" Please download VOC2012test.tar from:  \n register: http://host.robots.ox.ac.uk:8080 \n voc2012 : http://host.robots.ox.ac.uk:8080/eval/challenges/voc2012/ \ndownload: http://host.robots.ox.ac.uk:8080/eval/downloads/VOC2012test.tar"" ) logging . info ( "" unzip VOC2012test.tar,rename the folder to VOC2012test and put it into %s"" % path ) exit ( ) elif dataset == ""2007"" : url = ""http://pjreddie.com/media/files/"" tar_filename = ""VOCtrainval_06-Nov-2007.tar"" extracted_filename = ""VOC2007"" logging . info ( ""    [============= VOC 2007 =============]"" ) elif dataset == ""2007test"" : url = ""http://pjreddie.com/media/files/"" tar_filename = ""VOCtest_06-Nov-2007.tar"" extracted_filename = ""VOC2007test"" logging . info ( ""    [============= VOC 2007 Test Set =============]"" ) else : raise Exception ( ""Please set the dataset aug to 2012, 2012test or 2007."" ) if dataset != ""2012test"" : from sys import platform as _platform if folder_exists ( os . path . join ( path , extracted_filename ) ) is False : logging . info ( ""[VOC] {} is nonexistent in {}"" . format ( extracted_filename , path ) ) maybe_download_and_extract ( tar_filename , path , url , extract = True ) del_file ( os . path . join ( path , tar_filename ) ) if dataset == ""2012"" : if _platform == ""win32"" : os . system ( ""move {}\VOCdevkit\VOC2012 {}\VOC2012"" . format ( path , path ) ) else : os . system ( ""mv {}/VOCdevkit/VOC2012 {}/VOC2012"" . format ( path , path ) ) elif dataset == ""2007"" : if _platform == ""win32"" : os . system ( ""move {}\VOCdevkit\VOC2007 {}\VOC2007"" . format ( path , path ) ) else : os . system ( ""mv {}/VOCdevkit/VOC2007 {}/VOC2007"" . format ( path , path ) ) elif dataset == ""2007test"" : if _platform == ""win32"" : os . system ( ""move {}\VOCdevkit\VOC2007 {}\VOC2007test"" . format ( path , path ) ) else : os . system ( ""mv {}/VOCdevkit/VOC2007 {}/VOC2007test"" . format ( path , path ) ) del_folder ( os . path . join ( path , 'VOCdevkit' ) ) classes = [ ""aeroplane"" , ""bicycle"" , ""bird"" , ""boat"" , ""bottle"" , ""bus"" , ""car"" , ""cat"" , ""chair"" , ""cow"" , ""diningtable"" , ""dog"" , ""horse"" , ""motorbike"" , ""person"" , ""pottedplant"" , ""sheep"" , ""sofa"" , ""train"" , ""tvmonitor"" ] if contain_classes_in_person : classes_in_person = [ ""head"" , ""hand"" , ""foot"" ] else : classes_in_person = [ ] classes += classes_in_person classes_dict = utils . list_string_to_dict ( classes ) logging . info ( ""[VOC] object classes {}"" . format ( classes_dict ) ) folder_imgs = os . path . join ( path , extracted_filename , ""JPEGImages"" ) imgs_file_list = load_file_list ( path = folder_imgs , regx = '\\.jpg' , printable = False ) logging . info ( ""[VOC] {} images found"" . format ( len ( imgs_file_list ) ) ) imgs_file_list . sort ( key = lambda s : int ( s . replace ( '.' , ' ' ) . replace ( '_' , '' ) . split ( ' ' ) [ - 2 ] ) ) imgs_file_list = [ os . path . join ( folder_imgs , s ) for s in imgs_file_list ] if dataset != ""2012test"" : folder_semseg = os . path . join ( path , extracted_filename , ""SegmentationClass"" ) imgs_semseg_file_list = load_file_list ( path = folder_semseg , regx = '\\.png' , printable = False ) logging . info ( ""[VOC] {} maps for semantic segmentation found"" . format ( len ( imgs_semseg_file_list ) ) ) imgs_semseg_file_list . sort ( key = lambda s : int ( s . replace ( '.' , ' ' ) . replace ( '_' , '' ) . split ( ' ' ) [ - 2 ] ) ) imgs_semseg_file_list = [ os . path . join ( folder_semseg , s ) for s in imgs_semseg_file_list ] folder_insseg = os . path . join ( path , extracted_filename , ""SegmentationObject"" ) imgs_insseg_file_list = load_file_list ( path = folder_insseg , regx = '\\.png' , printable = False ) logging . info ( ""[VOC] {} maps for instance segmentation found"" . format ( len ( imgs_semseg_file_list ) ) ) imgs_insseg_file_list . sort ( key = lambda s : int ( s . replace ( '.' , ' ' ) . replace ( '_' , '' ) . split ( ' ' ) [ - 2 ] ) ) imgs_insseg_file_list = [ os . path . join ( folder_insseg , s ) for s in imgs_insseg_file_list ] else : imgs_semseg_file_list = [ ] imgs_insseg_file_list = [ ] folder_ann = os . path . join ( path , extracted_filename , ""Annotations"" ) imgs_ann_file_list = load_file_list ( path = folder_ann , regx = '\\.xml' , printable = False ) logging . info ( ""[VOC] {} XML annotation files for bounding box and object class found"" . format ( len ( imgs_ann_file_list ) ) ) imgs_ann_file_list . sort ( key = lambda s : int ( s . replace ( '.' , ' ' ) . replace ( '_' , '' ) . split ( ' ' ) [ - 2 ] ) ) imgs_ann_file_list = [ os . path . join ( folder_ann , s ) for s in imgs_ann_file_list ] if dataset == ""2012test"" : imgs_file_list_new = [ ] for ann in imgs_ann_file_list : ann = os . path . split ( ann ) [ - 1 ] . split ( '.' ) [ 0 ] for im in imgs_file_list : if ann in im : imgs_file_list_new . append ( im ) break imgs_file_list = imgs_file_list_new logging . info ( ""[VOC] keep %d images"" % len ( imgs_file_list_new ) ) def convert ( size , box ) : dw = 1. / size [ 0 ] dh = 1. / size [ 1 ] x = ( box [ 0 ] + box [ 1 ] ) / 2.0 y = ( box [ 2 ] + box [ 3 ] ) / 2.0 w = box [ 1 ] - box [ 0 ] h = box [ 3 ] - box [ 2 ] x = x * dw w = w * dw y = y * dh h = h * dh return x , y , w , h def convert_annotation ( file_name ) : """"""Given VOC2012 XML Annotations, returns number of objects and info."""""" in_file = open ( file_name ) out_file = """" tree = ET . parse ( in_file ) root = tree . getroot ( ) size = root . find ( 'size' ) w = int ( size . find ( 'width' ) . text ) h = int ( size . find ( 'height' ) . text ) n_objs = 0 for obj in root . iter ( 'object' ) : if dataset != ""2012test"" : difficult = obj . find ( 'difficult' ) . text cls = obj . find ( 'name' ) . text if cls not in classes or int ( difficult ) == 1 : continue else : cls = obj . find ( 'name' ) . text if cls not in classes : continue cls_id = classes . index ( cls ) xmlbox = obj . find ( 'bndbox' ) b = ( float ( xmlbox . find ( 'xmin' ) . text ) , float ( xmlbox . find ( 'xmax' ) . text ) , float ( xmlbox . find ( 'ymin' ) . text ) , float ( xmlbox . find ( 'ymax' ) . text ) ) bb = convert ( ( w , h ) , b ) out_file += str ( cls_id ) + "" "" + "" "" . join ( [ str ( a ) for a in bb ] ) + '\n' n_objs += 1 if cls in ""person"" : for part in obj . iter ( 'part' ) : cls = part . find ( 'name' ) . text if cls not in classes_in_person : continue cls_id = classes . index ( cls ) xmlbox = part . find ( 'bndbox' ) b = ( float ( xmlbox . find ( 'xmin' ) . text ) , float ( xmlbox . find ( 'xmax' ) . text ) , float ( xmlbox . find ( 'ymin' ) . text ) , float ( xmlbox . find ( 'ymax' ) . text ) ) bb = convert ( ( w , h ) , b ) out_file += str ( cls_id ) + "" "" + "" "" . join ( [ str ( a ) for a in bb ] ) + '\n' n_objs += 1 in_file . close ( ) return n_objs , out_file logging . info ( ""[VOC] Parsing xml annotations files"" ) n_objs_list = [ ] objs_info_list = [ ] objs_info_dicts = { } for idx , ann_file in enumerate ( imgs_ann_file_list ) : n_objs , objs_info = convert_annotation ( ann_file ) n_objs_list . append ( n_objs ) objs_info_list . append ( objs_info ) with tf . gfile . GFile ( ann_file , 'r' ) as fid : xml_str = fid . read ( ) xml = etree . fromstring ( xml_str ) data = _recursive_parse_xml_to_dict ( xml ) [ 'annotation' ] objs_info_dicts . update ( { imgs_file_list [ idx ] : data } ) return imgs_file_list , imgs_semseg_file_list , imgs_insseg_file_list , imgs_ann_file_list , classes , classes_in_person , classes_dict , n_objs_list , objs_info_list , objs_info_dicts",Pascal VOC 2007 / 2012 Dataset .
"def main ( _ ) : if FLAGS . model == ""small"" : init_scale = 0.1 learning_rate = 1.0 max_grad_norm = 5 num_steps = 20 hidden_size = 200 max_epoch = 4 max_max_epoch = 13 keep_prob = 1.0 lr_decay = 0.5 batch_size = 20 vocab_size = 10000 elif FLAGS . model == ""medium"" : init_scale = 0.05 learning_rate = 1.0 max_grad_norm = 5 num_steps = 35 hidden_size = 650 max_epoch = 6 max_max_epoch = 39 keep_prob = 0.5 lr_decay = 0.8 batch_size = 20 vocab_size = 10000 elif FLAGS . model == ""large"" : init_scale = 0.04 learning_rate = 1.0 max_grad_norm = 10 num_steps = 35 hidden_size = 1500 max_epoch = 14 max_max_epoch = 55 keep_prob = 0.35 lr_decay = 1 / 1.15 batch_size = 20 vocab_size = 10000 else : raise ValueError ( ""Invalid model: %s"" , FLAGS . model ) train_data , valid_data , test_data , vocab_size = tl . files . load_ptb_dataset ( ) print ( 'len(train_data) {}' . format ( len ( train_data ) ) ) print ( 'len(valid_data) {}' . format ( len ( valid_data ) ) ) print ( 'len(test_data)  {}' . format ( len ( test_data ) ) ) print ( 'vocab_size      {}' . format ( vocab_size ) ) sess = tf . InteractiveSession ( ) input_data = tf . placeholder ( tf . int32 , [ batch_size , num_steps ] ) targets = tf . placeholder ( tf . int32 , [ batch_size , num_steps ] ) input_data_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) targets_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) def inference ( x , is_training , num_steps , reuse = None ) : """"""If reuse is True, the inferences use the existing parameters,
        then different inferences share the same parameters.

        Note :
        - For DynamicRNNLayer, you can set dropout and the number of RNN layer internally.
        """""" print ( ""\nnum_steps : %d, is_training : %s, reuse : %s"" % ( num_steps , is_training , reuse ) ) init = tf . random_uniform_initializer ( - init_scale , init_scale ) with tf . variable_scope ( ""model"" , reuse = reuse ) : net = tl . layers . EmbeddingInputlayer ( x , vocab_size , hidden_size , init , name = 'embedding' ) net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop1' ) net = tl . layers . RNNLayer ( net , cell_fn = tf . contrib . rnn . BasicLSTMCell , cell_init_args = { 'forget_bias' : 0.0 } , n_hidden = hidden_size , initializer = init , n_steps = num_steps , return_last = False , name = 'basic_lstm_layer1' ) lstm1 = net net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop2' ) net = tl . layers . RNNLayer ( net , cell_fn = tf . contrib . rnn . BasicLSTMCell , cell_init_args = { 'forget_bias' : 0.0 } , n_hidden = hidden_size , initializer = init , n_steps = num_steps , return_last = False , return_seq_2d = True , name = 'basic_lstm_layer2' ) lstm2 = net net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop3' ) net = tl . layers . DenseLayer ( net , vocab_size , W_init = init , b_init = init , act = None , name = 'output' ) return net , lstm1 , lstm2 net , lstm1 , lstm2 = inference ( input_data , is_training = True , num_steps = num_steps , reuse = None ) net_val , lstm1_val , lstm2_val = inference ( input_data , is_training = False , num_steps = num_steps , reuse = True ) net_test , lstm1_test , lstm2_test = inference ( input_data_test , is_training = False , num_steps = 1 , reuse = True ) sess . run ( tf . global_variables_initializer ( ) ) def loss_fn ( outputs , targets ) : loss = tf . contrib . legacy_seq2seq . sequence_loss_by_example ( [ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones_like ( tf . reshape ( targets , [ - 1 ] ) , dtype = tf . float32 ) ] ) cost = tf . reduce_sum ( loss ) / batch_size return cost cost = loss_fn ( net . outputs , targets ) cost_val = loss_fn ( net_val . outputs , targets ) cost_test = loss_fn ( net_test . outputs , targets_test ) with tf . variable_scope ( 'learning_rate' ) : lr = tf . Variable ( 0.0 , trainable = False ) tvars = tf . trainable_variables ( ) grads , _ = tf . clip_by_global_norm ( tf . gradients ( cost , tvars ) , max_grad_norm ) optimizer = tf . train . GradientDescentOptimizer ( lr ) train_op = optimizer . apply_gradients ( zip ( grads , tvars ) ) sess . run ( tf . global_variables_initializer ( ) ) net . print_params ( ) net . print_layers ( ) tl . layers . print_all_variables ( ) print ( ""\nStart learning a language model by using PTB dataset"" ) for i in range ( max_max_epoch ) : new_lr_decay = lr_decay ** max ( i - max_epoch , 0.0 ) sess . run ( tf . assign ( lr , learning_rate * new_lr_decay ) ) print ( ""Epoch: %d/%d Learning rate: %.3f"" % ( i + 1 , max_max_epoch , sess . run ( lr ) ) ) epoch_size = ( ( len ( train_data ) // batch_size ) - 1 ) // num_steps start_time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize_rnn_state ( lstm1 . initial_state ) state2 = tl . layers . initialize_rnn_state ( lstm2 . initial_state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( train_data , batch_size , num_steps ) ) : feed_dict = { input_data : x , targets : y , lstm1 . initial_state : state1 , lstm2 . initial_state : state2 , } feed_dict . update ( net . all_drop ) _cost , state1 , state2 , _ = sess . run ( [ cost , lstm1 . final_state , lstm2 . final_state , train_op ] , feed_dict = feed_dict ) costs += _cost iters += num_steps if step % ( epoch_size // 10 ) == 10 : print ( ""%.3f perplexity: %.3f speed: %.0f wps"" % ( step * 1.0 / epoch_size , np . exp ( costs / iters ) , iters * batch_size / ( time . time ( ) - start_time ) ) ) train_perplexity = np . exp ( costs / iters ) print ( ""Epoch: %d/%d Train Perplexity: %.3f"" % ( i + 1 , max_max_epoch , train_perplexity ) ) start_time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize_rnn_state ( lstm1_val . initial_state ) state2 = tl . layers . initialize_rnn_state ( lstm2_val . initial_state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( valid_data , batch_size , num_steps ) ) : feed_dict = { input_data : x , targets : y , lstm1_val . initial_state : state1 , lstm2_val . initial_state : state2 , } _cost , state1 , state2 , _ = sess . run ( [ cost_val , lstm1_val . final_state , lstm2_val . final_state , tf . no_op ( ) ] , feed_dict = feed_dict ) costs += _cost iters += num_steps valid_perplexity = np . exp ( costs / iters ) print ( ""Epoch: %d/%d Valid Perplexity: %.3f"" % ( i + 1 , max_max_epoch , valid_perplexity ) ) print ( ""Evaluation"" ) start_time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize_rnn_state ( lstm1_test . initial_state ) state2 = tl . layers . initialize_rnn_state ( lstm2_test . initial_state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( test_data , batch_size = 1 , num_steps = 1 ) ) : feed_dict = { input_data_test : x , targets_test : y , lstm1_test . initial_state : state1 , lstm2_test . initial_state : state2 , } _cost , state1 , state2 = sess . run ( [ cost_test , lstm1_test . final_state , lstm2_test . final_state ] , feed_dict = feed_dict ) costs += _cost iters += 1 test_perplexity = np . exp ( costs / iters ) print ( ""Test Perplexity: %.3f took %.2fs"" % ( test_perplexity , time . time ( ) - start_time ) ) print ( ""More example: Text generation using Trump's speech data: https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_generate_text.py -- def main_lstm_generate_text():"" )",The core of the model consists of an LSTM cell that processes one word at a time and computes probabilities of the possible continuations of the sentence . The memory state of the network is initialized with a vector of zeros and gets updated after reading each word . Also for computational reasons we will process data in mini - batches of size batch_size .
"def private_method ( func ) : def func_wrapper ( * args , * * kwargs ) : """"""Decorator wrapper function."""""" outer_frame = inspect . stack ( ) [ 1 ] [ 0 ] if 'self' not in outer_frame . f_locals or outer_frame . f_locals [ 'self' ] is not args [ 0 ] : raise RuntimeError ( '%s.%s is a private method' % ( args [ 0 ] . __class__ . __name__ , func . __name__ ) ) return func ( * args , * * kwargs ) return func_wrapper",Decorator for making an instance method private .
"def protected_method ( func ) : def func_wrapper ( * args , * * kwargs ) : """"""Decorator wrapper function."""""" outer_frame = inspect . stack ( ) [ 1 ] [ 0 ] caller = inspect . getmro ( outer_frame . f_locals [ 'self' ] . __class__ ) [ : - 1 ] target = inspect . getmro ( args [ 0 ] . __class__ ) [ : - 1 ] share_subsclass = False for cls_ in target : if issubclass ( caller [ 0 ] , cls_ ) or caller [ 0 ] is cls_ : share_subsclass = True break if ( 'self' not in outer_frame . f_locals or outer_frame . f_locals [ 'self' ] is not args [ 0 ] ) and ( not share_subsclass ) : raise RuntimeError ( '%s.%s is a protected method' % ( args [ 0 ] . __class__ . __name__ , func . __name__ ) ) return func ( * args , * * kwargs ) return func_wrapper",Decorator for making an instance method private .
"def atrous_conv1d ( prev_layer , n_filter = 32 , filter_size = 2 , stride = 1 , dilation = 1 , act = None , padding = 'SAME' , data_format = 'NWC' , W_init = tf . truncated_normal_initializer ( stddev = 0.02 ) , b_init = tf . constant_initializer ( value = 0.0 ) , W_init_args = None , b_init_args = None , name = 'atrous_1d' , ) : return Conv1dLayer ( prev_layer = prev_layer , act = act , shape = ( filter_size , int ( prev_layer . outputs . get_shape ( ) [ - 1 ] ) , n_filter ) , stride = stride , padding = padding , dilation_rate = dilation , data_format = data_format , W_init = W_init , b_init = b_init , W_init_args = W_init_args , b_init_args = b_init_args , name = name , )",Simplified version of : class : AtrousConv1dLayer .
"def _GetNextLogCountPerToken ( token ) : global _log_counter_per_token _log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) return _log_counter_per_token [ token ]",Wrapper for _log_counter_per_token .
"def log_every_n ( level , msg , n , * args ) : count = _GetNextLogCountPerToken ( _GetFileAndLine ( ) ) log_if ( level , msg , not ( count % n ) , * args )",Log msg % args at level level once per n times .
"def log_if ( level , msg , condition , * args ) : if condition : vlog ( level , msg , * args )",Log msg % args at level level only if condition is fulfilled .
"def _GetFileAndLine ( ) : f = _sys . _getframe ( ) our_file = f . f_code . co_filename f = f . f_back while f : code = f . f_code if code . co_filename != our_file : return ( code . co_filename , f . f_lineno ) f = f . f_back return ( '<unknown>' , 0 )",Returns ( filename linenumber ) for the stack frame .
"def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : global _level_names now = timestamp or _time . time ( ) now_tuple = _time . localtime ( now ) now_microsecond = int ( 1e6 * ( now % 1.0 ) ) ( filename , line ) = file_and_line or _GetFileAndLine ( ) basename = _os . path . basename ( filename ) severity = 'I' if level in _level_names : severity = _level_names [ level ] [ 0 ] s = '%c%02d%02d %02d: %02d: %02d.%06d %5d %s: %d] ' % ( severity , now_tuple [ 1 ] , now_tuple [ 2 ] , now_tuple [ 3 ] , now_tuple [ 4 ] , now_tuple [ 5 ] , now_microsecond , _get_thread_id ( ) , basename , line ) return s",Assemble a logline prefix using the google2 format .
"def load_mpii_pose_dataset ( path = 'data' , is_16_pos_only = False ) : path = os . path . join ( path , 'mpii_human_pose' ) logging . info ( ""Load or Download MPII Human Pose > {}"" . format ( path ) ) url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/"" tar_filename = ""mpii_human_pose_v1_u12_2.zip"" extracted_filename = ""mpii_human_pose_v1_u12_2"" if folder_exists ( os . path . join ( path , extracted_filename ) ) is False : logging . info ( ""[MPII] (annotation) {} is nonexistent in {}"" . format ( extracted_filename , path ) ) maybe_download_and_extract ( tar_filename , path , url , extract = True ) del_file ( os . path . join ( path , tar_filename ) ) url = ""http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/"" tar_filename = ""mpii_human_pose_v1.tar.gz"" extracted_filename2 = ""images"" if folder_exists ( os . path . join ( path , extracted_filename2 ) ) is False : logging . info ( ""[MPII] (images) {} is nonexistent in {}"" . format ( extracted_filename , path ) ) maybe_download_and_extract ( tar_filename , path , url , extract = True ) del_file ( os . path . join ( path , tar_filename ) ) import scipy . io as sio logging . info ( ""reading annotations from mat file ..."" ) ann_train_list = [ ] ann_test_list = [ ] img_train_list = [ ] img_test_list = [ ] def save_joints ( ) : mat = sio . loadmat ( os . path . join ( path , extracted_filename , ""mpii_human_pose_v1_u12_1.mat"" ) ) for _ , ( anno , train_flag ) in enumerate ( zip ( mat [ 'RELEASE' ] [ 'annolist' ] [ 0 , 0 ] [ 0 ] , mat [ 'RELEASE' ] [ 'img_train' ] [ 0 , 0 ] [ 0 ] ) ) : img_fn = anno [ 'image' ] [ 'name' ] [ 0 , 0 ] [ 0 ] train_flag = int ( train_flag ) if train_flag : img_train_list . append ( img_fn ) ann_train_list . append ( [ ] ) else : img_test_list . append ( img_fn ) ann_test_list . append ( [ ] ) head_rect = [ ] if 'x1' in str ( anno [ 'annorect' ] . dtype ) : head_rect = zip ( [ x1 [ 0 , 0 ] for x1 in anno [ 'annorect' ] [ 'x1' ] [ 0 ] ] , [ y1 [ 0 , 0 ] for y1 in anno [ 'annorect' ] [ 'y1' ] [ 0 ] ] , [ x2 [ 0 , 0 ] for x2 in anno [ 'annorect' ] [ 'x2' ] [ 0 ] ] , [ y2 [ 0 , 0 ] for y2 in anno [ 'annorect' ] [ 'y2' ] [ 0 ] ] ) else : head_rect = [ ] if 'annopoints' in str ( anno [ 'annorect' ] . dtype ) : annopoints = anno [ 'annorect' ] [ 'annopoints' ] [ 0 ] head_x1s = anno [ 'annorect' ] [ 'x1' ] [ 0 ] head_y1s = anno [ 'annorect' ] [ 'y1' ] [ 0 ] head_x2s = anno [ 'annorect' ] [ 'x2' ] [ 0 ] head_y2s = anno [ 'annorect' ] [ 'y2' ] [ 0 ] for annopoint , head_x1 , head_y1 , head_x2 , head_y2 in zip ( annopoints , head_x1s , head_y1s , head_x2s , head_y2s ) : if annopoint . size : head_rect = [ float ( head_x1 [ 0 , 0 ] ) , float ( head_y1 [ 0 , 0 ] ) , float ( head_x2 [ 0 , 0 ] ) , float ( head_y2 [ 0 , 0 ] ) ] annopoint = annopoint [ 'point' ] [ 0 , 0 ] j_id = [ str ( j_i [ 0 , 0 ] ) for j_i in annopoint [ 'id' ] [ 0 ] ] x = [ x [ 0 , 0 ] for x in annopoint [ 'x' ] [ 0 ] ] y = [ y [ 0 , 0 ] for y in annopoint [ 'y' ] [ 0 ] ] joint_pos = { } for _j_id , ( _x , _y ) in zip ( j_id , zip ( x , y ) ) : joint_pos [ int ( _j_id ) ] = [ float ( _x ) , float ( _y ) ] if 'is_visible' in str ( annopoint . dtype ) : vis = [ v [ 0 ] if v . size > 0 else [ 0 ] for v in annopoint [ 'is_visible' ] [ 0 ] ] vis = dict ( [ ( k , int ( v [ 0 ] ) ) if len ( v ) > 0 else v for k , v in zip ( j_id , vis ) ] ) else : vis = None if ( ( is_16_pos_only == True ) and ( len ( joint_pos ) == 16 ) ) or ( is_16_pos_only == False ) : data = { 'filename' : img_fn , 'train' : train_flag , 'head_rect' : head_rect , 'is_visible' : vis , 'joint_pos' : joint_pos } if train_flag : ann_train_list [ - 1 ] . append ( data ) else : ann_test_list [ - 1 ] . append ( data ) save_joints ( ) logging . info ( ""reading images list ..."" ) img_dir = os . path . join ( path , extracted_filename2 ) _img_list = load_file_list ( path = os . path . join ( path , extracted_filename2 ) , regx = '\\.jpg' , printable = False ) for i , im in enumerate ( img_train_list ) : if im not in _img_list : print ( 'missing training image {} in {} (remove from img(ann)_train_list)' . format ( im , img_dir ) ) del img_train_list [ i ] del ann_train_list [ i ] for i , im in enumerate ( img_test_list ) : if im not in _img_list : print ( 'missing testing image {} in {} (remove from img(ann)_test_list)' . format ( im , img_dir ) ) del img_train_list [ i ] del ann_train_list [ i ] n_train_images = len ( img_train_list ) n_test_images = len ( img_test_list ) n_images = n_train_images + n_test_images logging . info ( ""n_images: {} n_train_images: {} n_test_images: {}"" . format ( n_images , n_train_images , n_test_images ) ) n_train_ann = len ( ann_train_list ) n_test_ann = len ( ann_test_list ) n_ann = n_train_ann + n_test_ann logging . info ( ""n_ann: {} n_train_ann: {} n_test_ann: {}"" . format ( n_ann , n_train_ann , n_test_ann ) ) n_train_people = len ( sum ( ann_train_list , [ ] ) ) n_test_people = len ( sum ( ann_test_list , [ ] ) ) n_people = n_train_people + n_test_people logging . info ( ""n_people: {} n_train_people: {} n_test_people: {}"" . format ( n_people , n_train_people , n_test_people ) ) for i , value in enumerate ( img_train_list ) : img_train_list [ i ] = os . path . join ( img_dir , value ) for i , value in enumerate ( img_test_list ) : img_test_list [ i ] = os . path . join ( img_dir , value ) return img_train_list , ann_train_list , img_test_list , ann_test_list",Load MPII Human Pose Dataset .
"def transformer ( U , theta , out_size , name = 'SpatialTransformer2dAffine' ) : def _repeat ( x , n_repeats ) : with tf . variable_scope ( '_repeat' ) : rep = tf . transpose ( tf . expand_dims ( tf . ones ( shape = tf . stack ( [ n_repeats , ] ) ) , 1 ) , [ 1 , 0 ] ) rep = tf . cast ( rep , 'int32' ) x = tf . matmul ( tf . reshape ( x , ( - 1 , 1 ) ) , rep ) return tf . reshape ( x , [ - 1 ] ) def _interpolate ( im , x , y , out_size ) : with tf . variable_scope ( '_interpolate' ) : num_batch = tf . shape ( im ) [ 0 ] height = tf . shape ( im ) [ 1 ] width = tf . shape ( im ) [ 2 ] channels = tf . shape ( im ) [ 3 ] x = tf . cast ( x , 'float32' ) y = tf . cast ( y , 'float32' ) height_f = tf . cast ( height , 'float32' ) width_f = tf . cast ( width , 'float32' ) out_height = out_size [ 0 ] out_width = out_size [ 1 ] zero = tf . zeros ( [ ] , dtype = 'int32' ) max_y = tf . cast ( tf . shape ( im ) [ 1 ] - 1 , 'int32' ) max_x = tf . cast ( tf . shape ( im ) [ 2 ] - 1 , 'int32' ) x = ( x + 1.0 ) * ( width_f ) / 2.0 y = ( y + 1.0 ) * ( height_f ) / 2.0 x0 = tf . cast ( tf . floor ( x ) , 'int32' ) x1 = x0 + 1 y0 = tf . cast ( tf . floor ( y ) , 'int32' ) y1 = y0 + 1 x0 = tf . clip_by_value ( x0 , zero , max_x ) x1 = tf . clip_by_value ( x1 , zero , max_x ) y0 = tf . clip_by_value ( y0 , zero , max_y ) y1 = tf . clip_by_value ( y1 , zero , max_y ) dim2 = width dim1 = width * height base = _repeat ( tf . range ( num_batch ) * dim1 , out_height * out_width ) base_y0 = base + y0 * dim2 base_y1 = base + y1 * dim2 idx_a = base_y0 + x0 idx_b = base_y1 + x0 idx_c = base_y0 + x1 idx_d = base_y1 + x1 im_flat = tf . reshape ( im , tf . stack ( [ - 1 , channels ] ) ) im_flat = tf . cast ( im_flat , 'float32' ) Ia = tf . gather ( im_flat , idx_a ) Ib = tf . gather ( im_flat , idx_b ) Ic = tf . gather ( im_flat , idx_c ) Id = tf . gather ( im_flat , idx_d ) x0_f = tf . cast ( x0 , 'float32' ) x1_f = tf . cast ( x1 , 'float32' ) y0_f = tf . cast ( y0 , 'float32' ) y1_f = tf . cast ( y1 , 'float32' ) wa = tf . expand_dims ( ( ( x1_f - x ) * ( y1_f - y ) ) , 1 ) wb = tf . expand_dims ( ( ( x1_f - x ) * ( y - y0_f ) ) , 1 ) wc = tf . expand_dims ( ( ( x - x0_f ) * ( y1_f - y ) ) , 1 ) wd = tf . expand_dims ( ( ( x - x0_f ) * ( y - y0_f ) ) , 1 ) output = tf . add_n ( [ wa * Ia , wb * Ib , wc * Ic , wd * Id ] ) return output def _meshgrid ( height , width ) : with tf . variable_scope ( '_meshgrid' ) : x_t = tf . matmul ( tf . ones ( shape = tf . stack ( [ height , 1 ] ) ) , tf . transpose ( tf . expand_dims ( tf . linspace ( - 1.0 , 1.0 , width ) , 1 ) , [ 1 , 0 ] ) ) y_t = tf . matmul ( tf . expand_dims ( tf . linspace ( - 1.0 , 1.0 , height ) , 1 ) , tf . ones ( shape = tf . stack ( [ 1 , width ] ) ) ) x_t_flat = tf . reshape ( x_t , ( 1 , - 1 ) ) y_t_flat = tf . reshape ( y_t , ( 1 , - 1 ) ) ones = tf . ones_like ( x_t_flat ) grid = tf . concat ( axis = 0 , values = [ x_t_flat , y_t_flat , ones ] ) return grid def _transform ( theta , input_dim , out_size ) : with tf . variable_scope ( '_transform' ) : num_batch = tf . shape ( input_dim ) [ 0 ] num_channels = tf . shape ( input_dim ) [ 3 ] theta = tf . reshape ( theta , ( - 1 , 2 , 3 ) ) theta = tf . cast ( theta , 'float32' ) out_height = out_size [ 0 ] out_width = out_size [ 1 ] grid = _meshgrid ( out_height , out_width ) grid = tf . expand_dims ( grid , 0 ) grid = tf . reshape ( grid , [ - 1 ] ) grid = tf . tile ( grid , tf . stack ( [ num_batch ] ) ) grid = tf . reshape ( grid , tf . stack ( [ num_batch , 3 , - 1 ] ) ) T_g = tf . matmul ( theta , grid ) x_s = tf . slice ( T_g , [ 0 , 0 , 0 ] , [ - 1 , 1 , - 1 ] ) y_s = tf . slice ( T_g , [ 0 , 1 , 0 ] , [ - 1 , 1 , - 1 ] ) x_s_flat = tf . reshape ( x_s , [ - 1 ] ) y_s_flat = tf . reshape ( y_s , [ - 1 ] ) input_transformed = _interpolate ( input_dim , x_s_flat , y_s_flat , out_size ) output = tf . reshape ( input_transformed , tf . stack ( [ num_batch , out_height , out_width , num_channels ] ) ) return output with tf . variable_scope ( name ) : output = _transform ( theta , U , out_size ) return output",Spatial Transformer Layer for 2D Affine Transformation <https : // en . wikipedia . org / wiki / Affine_transformation > __ see : class : SpatialTransformer2dAffineLayer class .
"def batch_transformer ( U , thetas , out_size , name = 'BatchSpatialTransformer2dAffine' ) : with tf . variable_scope ( name ) : num_batch , num_transforms = map ( int , thetas . get_shape ( ) . as_list ( ) [ : 2 ] ) indices = [ [ i ] * num_transforms for i in xrange ( num_batch ) ] input_repeated = tf . gather ( U , tf . reshape ( indices , [ - 1 ] ) ) return transformer ( input_repeated , thetas , out_size )",Batch Spatial Transformer function for 2D Affine Transformation <https : // en . wikipedia . org / wiki / Affine_transformation > __ .
"def create_task_spec_def ( ) : if 'TF_CONFIG' in os . environ : env = json . loads ( os . environ . get ( 'TF_CONFIG' , '{}' ) ) task_data = env . get ( 'task' , None ) or { 'type' : 'master' , 'index' : 0 } cluster_data = env . get ( 'cluster' , None ) or { 'ps' : None , 'worker' : None , 'master' : None } return TaskSpecDef ( task_type = task_data [ 'type' ] , index = task_data [ 'index' ] , trial = task_data [ 'trial' ] if 'trial' in task_data else None , ps_hosts = cluster_data [ 'ps' ] , worker_hosts = cluster_data [ 'worker' ] , master = cluster_data [ 'master' ] if 'master' in cluster_data else None ) elif 'JOB_NAME' in os . environ : return TaskSpecDef ( task_type = os . environ [ 'JOB_NAME' ] , index = os . environ [ 'TASK_INDEX' ] , ps_hosts = os . environ . get ( 'PS_HOSTS' , None ) , worker_hosts = os . environ . get ( 'WORKER_HOSTS' , None ) , master = os . environ . get ( 'MASTER_HOST' , None ) ) else : raise Exception ( 'You need to setup TF_CONFIG or JOB_NAME to define the task.' )",Returns the a : class : TaskSpecDef based on the environment variables for distributed training .
"def create_distributed_session ( task_spec = None , checkpoint_dir = None , scaffold = None , hooks = None , chief_only_hooks = None , save_checkpoint_secs = 600 , save_summaries_steps = object ( ) , save_summaries_secs = object ( ) , config = None , stop_grace_period_secs = 120 , log_step_count_steps = 100 ) : target = task_spec . target ( ) if task_spec is not None else None is_chief = task_spec . is_master ( ) if task_spec is not None else True return tf . train . MonitoredTrainingSession ( master = target , is_chief = is_chief , checkpoint_dir = checkpoint_dir , scaffold = scaffold , save_checkpoint_secs = save_checkpoint_secs , save_summaries_steps = save_summaries_steps , save_summaries_secs = save_summaries_secs , log_step_count_steps = log_step_count_steps , stop_grace_period_secs = stop_grace_period_secs , config = config , hooks = hooks , chief_only_hooks = chief_only_hooks )",Creates a distributed session .
"def validation_metrics ( self ) : if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : raise AttributeError ( 'Validation is not setup.' ) n = 0.0 metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) self . _sess . run ( self . _validation_iterator . initializer ) while True : try : metrics = self . _sess . run ( self . _validation_metrics ) for i , m in enumerate ( metrics ) : metric_sums [ i ] += m n += 1.0 except tf . errors . OutOfRangeError : break for i , m in enumerate ( metric_sums ) : metric_sums [ i ] = metric_sums [ i ] / n return zip ( self . _validation_metrics , metric_sums )",A helper function to compute validation related metrics
"def train_and_validate_to_end ( self , validate_step_size = 50 ) : while not self . _sess . should_stop ( ) : self . train_on_batch ( ) if self . global_step % validate_step_size == 0 : log_str = 'step: %d, ' % self . global_step for n , m in self . validation_metrics : log_str += '%s: %f, ' % ( n . name , m ) logging . info ( log_str )",A helper function that shows how to train and validate a model at the same time .
"def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : path = os . path . join ( path , name ) def load_mnist_images ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) logging . info ( filepath ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) data = data . reshape ( shape ) return data / np . float32 ( 256 ) def load_mnist_labels ( path , filename ) : filepath = maybe_download_and_extract ( filename , path , url ) with gzip . open ( filepath , 'rb' ) as f : data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) return data logging . info ( ""Load or Download {0} > {1}"" . format ( name . upper ( ) , path ) ) X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] X_train = np . asarray ( X_train , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) X_val = np . asarray ( X_val , dtype = np . float32 ) y_val = np . asarray ( y_val , dtype = np . int32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_val , y_val , X_test , y_test",A generic function to load mnist - like dataset .
"def load_cifar10_dataset ( shape = ( - 1 , 32 , 32 , 3 ) , path = 'data' , plotable = False ) : path = os . path . join ( path , 'cifar10' ) logging . info ( ""Load or Download cifar10 > {}"" . format ( path ) ) def unpickle ( file ) : fp = open ( file , 'rb' ) if sys . version_info . major == 2 : data = pickle . load ( fp ) elif sys . version_info . major == 3 : data = pickle . load ( fp , encoding = 'latin-1' ) fp . close ( ) return data filename = 'cifar-10-python.tar.gz' url = 'https://www.cs.toronto.edu/~kriz/' maybe_download_and_extract ( filename , path , url , extract = True ) X_train = None y_train = [ ] for i in range ( 1 , 6 ) : data_dic = unpickle ( os . path . join ( path , 'cifar-10-batches-py/' , ""data_batch_{}"" . format ( i ) ) ) if i == 1 : X_train = data_dic [ 'data' ] else : X_train = np . vstack ( ( X_train , data_dic [ 'data' ] ) ) y_train += data_dic [ 'labels' ] test_data_dic = unpickle ( os . path . join ( path , 'cifar-10-batches-py/' , ""test_batch"" ) ) X_test = test_data_dic [ 'data' ] y_test = np . array ( test_data_dic [ 'labels' ] ) if shape == ( - 1 , 3 , 32 , 32 ) : X_test = X_test . reshape ( shape ) X_train = X_train . reshape ( shape ) elif shape == ( - 1 , 32 , 32 , 3 ) : X_test = X_test . reshape ( shape , order = 'F' ) X_train = X_train . reshape ( shape , order = 'F' ) X_test = np . transpose ( X_test , ( 0 , 2 , 1 , 3 ) ) X_train = np . transpose ( X_train , ( 0 , 2 , 1 , 3 ) ) else : X_test = X_test . reshape ( shape ) X_train = X_train . reshape ( shape ) y_train = np . array ( y_train ) if plotable : logging . info ( '\nCIFAR-10' ) fig = plt . figure ( 1 ) logging . info ( 'Shape of a training image: X_train[0] %s' % X_train [ 0 ] . shape ) plt . ion ( ) count = 1 for _ in range ( 10 ) : for _ in range ( 10 ) : _ = fig . add_subplot ( 10 , 10 , count ) if shape == ( - 1 , 3 , 32 , 32 ) : plt . imshow ( np . transpose ( X_train [ count - 1 ] , ( 1 , 2 , 0 ) ) , interpolation = 'nearest' ) elif shape == ( - 1 , 32 , 32 , 3 ) : plt . imshow ( X_train [ count - 1 ] , interpolation = 'nearest' ) else : raise Exception ( ""Do not support the given 'shape' to plot the image examples"" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 plt . draw ( ) plt . pause ( 3 ) logging . info ( ""X_train: %s"" % X_train . shape ) logging . info ( ""y_train: %s"" % y_train . shape ) logging . info ( ""X_test:  %s"" % X_test . shape ) logging . info ( ""y_test:  %s"" % y_test . shape ) X_train = np . asarray ( X_train , dtype = np . float32 ) X_test = np . asarray ( X_test , dtype = np . float32 ) y_train = np . asarray ( y_train , dtype = np . int32 ) y_test = np . asarray ( y_test , dtype = np . int32 ) return X_train , y_train , X_test , y_test",Load CIFAR - 10 dataset .
"def load_cropped_svhn ( path = 'data' , include_extra = True ) : start_time = time . time ( ) path = os . path . join ( path , 'cropped_svhn' ) logging . info ( ""Load or Download Cropped SVHN > {} | include extra images: {}"" . format ( path , include_extra ) ) url = ""http://ufldl.stanford.edu/housenumbers/"" np_file = os . path . join ( path , ""train_32x32.npz"" ) if file_exists ( np_file ) is False : filename = ""train_32x32.mat"" filepath = maybe_download_and_extract ( filename , path , url ) mat = sio . loadmat ( filepath ) X_train = mat [ 'X' ] / 255.0 X_train = np . transpose ( X_train , ( 3 , 0 , 1 , 2 ) ) y_train = np . squeeze ( mat [ 'y' ] , axis = 1 ) y_train [ y_train == 10 ] = 0 np . savez ( np_file , X = X_train , y = y_train ) del_file ( filepath ) else : v = np . load ( np_file ) X_train = v [ 'X' ] y_train = v [ 'y' ] logging . info ( ""  n_train: {}"" . format ( len ( y_train ) ) ) np_file = os . path . join ( path , ""test_32x32.npz"" ) if file_exists ( np_file ) is False : filename = ""test_32x32.mat"" filepath = maybe_download_and_extract ( filename , path , url ) mat = sio . loadmat ( filepath ) X_test = mat [ 'X' ] / 255.0 X_test = np . transpose ( X_test , ( 3 , 0 , 1 , 2 ) ) y_test = np . squeeze ( mat [ 'y' ] , axis = 1 ) y_test [ y_test == 10 ] = 0 np . savez ( np_file , X = X_test , y = y_test ) del_file ( filepath ) else : v = np . load ( np_file ) X_test = v [ 'X' ] y_test = v [ 'y' ] logging . info ( ""  n_test: {}"" . format ( len ( y_test ) ) ) if include_extra : logging . info ( ""  getting extra 531131 images, please wait ..."" ) np_file = os . path . join ( path , ""extra_32x32.npz"" ) if file_exists ( np_file ) is False : logging . info ( ""  the first time to load extra images will take long time to convert the file format ..."" ) filename = ""extra_32x32.mat"" filepath = maybe_download_and_extract ( filename , path , url ) mat = sio . loadmat ( filepath ) X_extra = mat [ 'X' ] / 255.0 X_extra = np . transpose ( X_extra , ( 3 , 0 , 1 , 2 ) ) y_extra = np . squeeze ( mat [ 'y' ] , axis = 1 ) y_extra [ y_extra == 10 ] = 0 np . savez ( np_file , X = X_extra , y = y_extra ) del_file ( filepath ) else : v = np . load ( np_file ) X_extra = v [ 'X' ] y_extra = v [ 'y' ] logging . info ( ""  adding n_extra {} to n_train {}"" . format ( len ( y_extra ) , len ( y_train ) ) ) t = time . time ( ) X_train = np . concatenate ( ( X_train , X_extra ) , 0 ) y_train = np . concatenate ( ( y_train , y_extra ) , 0 ) logging . info ( ""  added n_extra {} to n_train {} took {}s"" . format ( len ( y_extra ) , len ( y_train ) , time . time ( ) - t ) ) else : logging . info ( ""  no extra images are included"" ) logging . info ( ""  image size: %s n_train: %d n_test: %d"" % ( str ( X_train . shape [ 1 : 4 ] ) , len ( y_train ) , len ( y_test ) ) ) logging . info ( ""  took: {}s"" . format ( int ( time . time ( ) - start_time ) ) ) return X_train , y_train , X_test , y_test",Load Cropped SVHN .
"def load_ptb_dataset ( path = 'data' ) : path = os . path . join ( path , 'ptb' ) logging . info ( ""Load or Download Penn TreeBank (PTB) dataset > {}"" . format ( path ) ) filename = 'simple-examples.tgz' url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/' maybe_download_and_extract ( filename , path , url , extract = True ) data_path = os . path . join ( path , 'simple-examples' , 'data' ) train_path = os . path . join ( data_path , ""ptb.train.txt"" ) valid_path = os . path . join ( data_path , ""ptb.valid.txt"" ) test_path = os . path . join ( data_path , ""ptb.test.txt"" ) word_to_id = nlp . build_vocab ( nlp . read_words ( train_path ) ) train_data = nlp . words_to_word_ids ( nlp . read_words ( train_path ) , word_to_id ) valid_data = nlp . words_to_word_ids ( nlp . read_words ( valid_path ) , word_to_id ) test_data = nlp . words_to_word_ids ( nlp . read_words ( test_path ) , word_to_id ) vocab_size = len ( word_to_id ) return train_data , valid_data , test_data , vocab_size",Load Penn TreeBank ( PTB ) dataset .
"def load_matt_mahoney_text8_dataset ( path = 'data' ) : path = os . path . join ( path , 'mm_test8' ) logging . info ( ""Load or Download matt_mahoney_text8 Dataset> {}"" . format ( path ) ) filename = 'text8.zip' url = 'http://mattmahoney.net/dc/' maybe_download_and_extract ( filename , path , url , expected_bytes = 31344016 ) with zipfile . ZipFile ( os . path . join ( path , filename ) ) as f : word_list = f . read ( f . namelist ( ) [ 0 ] ) . split ( ) for idx , _ in enumerate ( word_list ) : word_list [ idx ] = word_list [ idx ] . decode ( ) return word_list",Load Matt Mahoney s dataset .
"def load_imdb_dataset ( path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , index_from = 3 ) : path = os . path . join ( path , 'imdb' ) filename = ""imdb.pkl"" url = 'https://s3.amazonaws.com/text-datasets/' maybe_download_and_extract ( filename , path , url ) if filename . endswith ( "".gz"" ) : f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) else : f = open ( os . path . join ( path , filename ) , 'rb' ) X , labels = cPickle . load ( f ) f . close ( ) np . random . seed ( seed ) np . random . shuffle ( X ) np . random . seed ( seed ) np . random . shuffle ( labels ) if start_char is not None : X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] elif index_from : X = [ [ w + index_from for w in x ] for x in X ] if maxlen : new_X = [ ] new_labels = [ ] for x , y in zip ( X , labels ) : if len ( x ) < maxlen : new_X . append ( x ) new_labels . append ( y ) X = new_X labels = new_labels if not X : raise Exception ( 'After filtering for sequences shorter than maxlen=' + str ( maxlen ) + ', no sequence was kept. ' 'Increase maxlen.' ) if not nb_words : nb_words = max ( [ max ( x ) for x in X ] ) if oov_char is not None : X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] else : nX = [ ] for x in X : nx = [ ] for w in x : if ( w >= nb_words or w < skip_top ) : nx . append ( w ) nX . append ( nx ) X = nX X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) return X_train , y_train , X_test , y_test",Load IMDB dataset .
"def load_nietzsche_dataset ( path = 'data' ) : logging . info ( ""Load or Download nietzsche dataset > {}"" . format ( path ) ) path = os . path . join ( path , 'nietzsche' ) filename = ""nietzsche.txt"" url = 'https://s3.amazonaws.com/text-datasets/' filepath = maybe_download_and_extract ( filename , path , url ) with open ( filepath , ""r"" ) as f : words = f . read ( ) return words",Load Nietzsche dataset .
"def load_wmt_en_fr_dataset ( path = 'data' ) : path = os . path . join ( path , 'wmt_en_fr' ) _WMT_ENFR_TRAIN_URL = ""http://www.statmt.org/wmt10/"" _WMT_ENFR_DEV_URL = ""http://www.statmt.org/wmt15/"" def gunzip_file ( gz_path , new_path ) : """"""Unzips from gz_path into new_path."""""" logging . info ( ""Unpacking %s to %s"" % ( gz_path , new_path ) ) with gzip . open ( gz_path , ""rb"" ) as gz_file : with open ( new_path , ""wb"" ) as new_file : for line in gz_file : new_file . write ( line ) def get_wmt_enfr_train_set ( path ) : """"""Download the WMT en-fr training corpus to directory unless it's there."""""" filename = ""training-giga-fren.tar"" maybe_download_and_extract ( filename , path , _WMT_ENFR_TRAIN_URL , extract = True ) train_path = os . path . join ( path , ""giga-fren.release2.fixed"" ) gunzip_file ( train_path + "".fr.gz"" , train_path + "".fr"" ) gunzip_file ( train_path + "".en.gz"" , train_path + "".en"" ) return train_path def get_wmt_enfr_dev_set ( path ) : """"""Download the WMT en-fr training corpus to directory unless it's there."""""" filename = ""dev-v2.tgz"" dev_file = maybe_download_and_extract ( filename , path , _WMT_ENFR_DEV_URL , extract = False ) dev_name = ""newstest2013"" dev_path = os . path . join ( path , ""newstest2013"" ) if not ( gfile . Exists ( dev_path + "".fr"" ) and gfile . Exists ( dev_path + "".en"" ) ) : logging . info ( ""Extracting tgz file %s"" % dev_file ) with tarfile . open ( dev_file , ""r:gz"" ) as dev_tar : fr_dev_file = dev_tar . getmember ( ""dev/"" + dev_name + "".fr"" ) en_dev_file = dev_tar . getmember ( ""dev/"" + dev_name + "".en"" ) fr_dev_file . name = dev_name + "".fr"" en_dev_file . name = dev_name + "".en"" dev_tar . extract ( fr_dev_file , path ) dev_tar . extract ( en_dev_file , path ) return dev_path logging . info ( ""Load or Download WMT English-to-French translation > {}"" . format ( path ) ) train_path = get_wmt_enfr_train_set ( path ) dev_path = get_wmt_enfr_dev_set ( path ) return train_path , dev_path",Load WMT 15 English - to - French translation dataset .
"def load_flickr25k_dataset ( tag = 'sky' , path = ""data"" , n_threads = 50 , printable = False ) : path = os . path . join ( path , 'flickr25k' ) filename = 'mirflickr25k.zip' url = 'http://press.liacs.nl/mirflickr/mirflickr25k/' if folder_exists ( os . path . join ( path , ""mirflickr"" ) ) is False : logging . info ( ""[*] Flickr25k is nonexistent in {}"" . format ( path ) ) maybe_download_and_extract ( filename , path , url , extract = True ) del_file ( os . path . join ( path , filename ) ) folder_imgs = os . path . join ( path , ""mirflickr"" ) path_imgs = load_file_list ( path = folder_imgs , regx = '\\.jpg' , printable = False ) path_imgs . sort ( key = natural_keys ) folder_tags = os . path . join ( path , ""mirflickr"" , ""meta"" , ""tags"" ) path_tags = load_file_list ( path = folder_tags , regx = '\\.txt' , printable = False ) path_tags . sort ( key = natural_keys ) if tag is None : logging . info ( ""[Flickr25k] reading all images"" ) else : logging . info ( ""[Flickr25k] reading images with tag: {}"" . format ( tag ) ) images_list = [ ] for idx , _v in enumerate ( path_tags ) : tags = read_file ( os . path . join ( folder_tags , path_tags [ idx ] ) ) . split ( '\n' ) if tag is None or tag in tags : images_list . append ( path_imgs [ idx ] ) images = visualize . read_images ( images_list , folder_imgs , n_threads = n_threads , printable = printable ) return images",Load Flickr25K dataset .
"def load_flickr1M_dataset ( tag = 'sky' , size = 10 , path = ""data"" , n_threads = 50 , printable = False ) : path = os . path . join ( path , 'flickr1M' ) logging . info ( ""[Flickr1M] using {}% of images = {}"" . format ( size * 10 , size * 100000 ) ) images_zip = [ 'images0.zip' , 'images1.zip' , 'images2.zip' , 'images3.zip' , 'images4.zip' , 'images5.zip' , 'images6.zip' , 'images7.zip' , 'images8.zip' , 'images9.zip' ] tag_zip = 'tags.zip' url = 'http://press.liacs.nl/mirflickr/mirflickr1m/' for image_zip in images_zip [ 0 : size ] : image_folder = image_zip . split ( ""."" ) [ 0 ] if folder_exists ( os . path . join ( path , image_folder ) ) is False : logging . info ( ""[Flickr1M] {} is missing in {}"" . format ( image_folder , path ) ) maybe_download_and_extract ( image_zip , path , url , extract = True ) del_file ( os . path . join ( path , image_zip ) ) shutil . move ( os . path . join ( path , 'images' ) , os . path . join ( path , image_folder ) ) else : logging . info ( ""[Flickr1M] {} exists in {}"" . format ( image_folder , path ) ) if folder_exists ( os . path . join ( path , ""tags"" ) ) is False : logging . info ( ""[Flickr1M] tag files is nonexistent in {}"" . format ( path ) ) maybe_download_and_extract ( tag_zip , path , url , extract = True ) del_file ( os . path . join ( path , tag_zip ) ) else : logging . info ( ""[Flickr1M] tags exists in {}"" . format ( path ) ) images_list = [ ] images_folder_list = [ ] for i in range ( 0 , size ) : images_folder_list += load_folder_list ( path = os . path . join ( path , 'images%d' % i ) ) images_folder_list . sort ( key = lambda s : int ( s . split ( '/' ) [ - 1 ] ) ) for folder in images_folder_list [ 0 : size * 10 ] : tmp = load_file_list ( path = folder , regx = '\\.jpg' , printable = False ) tmp . sort ( key = lambda s : int ( s . split ( '.' ) [ - 2 ] ) ) images_list . extend ( [ os . path . join ( folder , x ) for x in tmp ] ) tag_list = [ ] tag_folder_list = load_folder_list ( os . path . join ( path , ""tags"" ) ) tag_folder_list . sort ( key = lambda s : int ( os . path . basename ( s ) ) ) for folder in tag_folder_list [ 0 : size * 10 ] : tmp = load_file_list ( path = folder , regx = '\\.txt' , printable = False ) tmp . sort ( key = lambda s : int ( s . split ( '.' ) [ - 2 ] ) ) tmp = [ os . path . join ( folder , s ) for s in tmp ] tag_list += tmp logging . info ( ""[Flickr1M] searching tag: {}"" . format ( tag ) ) select_images_list = [ ] for idx , _val in enumerate ( tag_list ) : tags = read_file ( tag_list [ idx ] ) . split ( '\n' ) if tag in tags : select_images_list . append ( images_list [ idx ] ) logging . info ( ""[Flickr1M] reading images with tag: {}"" . format ( tag ) ) images = visualize . read_images ( select_images_list , '' , n_threads = n_threads , printable = printable ) return images",Load Flick1M dataset .
"def load_cyclegan_dataset ( filename = 'summer2winter_yosemite' , path = 'data' ) : path = os . path . join ( path , 'cyclegan' ) url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' if folder_exists ( os . path . join ( path , filename ) ) is False : logging . info ( ""[*] {} is nonexistent in {}"" . format ( filename , path ) ) maybe_download_and_extract ( filename + '.zip' , path , url , extract = True ) del_file ( os . path . join ( path , filename + '.zip' ) ) def load_image_from_folder ( path ) : path_imgs = load_file_list ( path = path , regx = '\\.jpg' , printable = False ) return visualize . read_images ( path_imgs , path = path , n_threads = 10 , printable = False ) im_train_A = load_image_from_folder ( os . path . join ( path , filename , ""trainA"" ) ) im_train_B = load_image_from_folder ( os . path . join ( path , filename , ""trainB"" ) ) im_test_A = load_image_from_folder ( os . path . join ( path , filename , ""testA"" ) ) im_test_B = load_image_from_folder ( os . path . join ( path , filename , ""testB"" ) ) def if_2d_to_3d ( images ) : for i , _v in enumerate ( images ) : if len ( images [ i ] . shape ) == 2 : images [ i ] = images [ i ] [ : , : , np . newaxis ] images [ i ] = np . tile ( images [ i ] , ( 1 , 1 , 3 ) ) return images im_train_A = if_2d_to_3d ( im_train_A ) im_train_B = if_2d_to_3d ( im_train_B ) im_test_A = if_2d_to_3d ( im_test_A ) im_test_B = if_2d_to_3d ( im_test_B ) return im_train_A , im_train_B , im_test_A , im_test_B",Load images from CycleGAN s database see this link <https : // people . eecs . berkeley . edu / ~taesung_park / CycleGAN / datasets / > __ .
"def download_file_from_google_drive ( ID , destination ) : def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : total_size = int ( response . headers . get ( 'content-length' , 0 ) ) with open ( destination , ""wb"" ) as f : for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , desc = destination ) : if chunk : f . write ( chunk ) def get_confirm_token ( response ) : for key , value in response . cookies . items ( ) : if key . startswith ( 'download_warning' ) : return value return None URL = ""https://docs.google.com/uc?export=download"" session = requests . Session ( ) response = session . get ( URL , params = { 'id' : ID } , stream = True ) token = get_confirm_token ( response ) if token : params = { 'id' : ID , 'confirm' : token } response = session . get ( URL , params = params , stream = True ) save_response_content ( response , destination )",Download file from Google Drive .
"def load_celebA_dataset ( path = 'data' ) : data_dir = 'celebA' filename , drive_id = ""img_align_celeba.zip"" , ""0B7EVK8r0v71pZjFTYXZWM3FlRnM"" save_path = os . path . join ( path , filename ) image_path = os . path . join ( path , data_dir ) if os . path . exists ( image_path ) : logging . info ( '[*] {} already exists' . format ( save_path ) ) else : exists_or_mkdir ( path ) download_file_from_google_drive ( drive_id , save_path ) zip_dir = '' with zipfile . ZipFile ( save_path ) as zf : zip_dir = zf . namelist ( ) [ 0 ] zf . extractall ( path ) os . remove ( save_path ) os . rename ( os . path . join ( path , zip_dir ) , image_path ) data_files = load_file_list ( path = image_path , regx = '\\.jpg' , printable = False ) for i , _v in enumerate ( data_files ) : data_files [ i ] = os . path . join ( image_path , data_files [ i ] ) return data_files",Load CelebA dataset
"def save_npz ( save_list = None , name = 'model.npz' , sess = None ) : logging . info ( ""[*] Saving TL params into %s"" % name ) if save_list is None : save_list = [ ] save_list_var = [ ] if sess : save_list_var = sess . run ( save_list ) else : try : save_list_var . extend ( [ v . eval ( ) for v in save_list ] ) except Exception : logging . info ( "" Fail to save model, Hint: pass the session into this function, tl.files.save_npz(network.all_params, name='model.npz', sess=sess)"" ) np . savez ( name , params = save_list_var ) save_list_var = None del save_list_var logging . info ( ""[*] Saved"" )",Input parameters and the file name save parameters into . npz file . Use tl . utils . load_npz () to restore .
"def load_npz ( path = '' , name = 'model.npz' ) : d = np . load ( os . path . join ( path , name ) ) return d [ 'params' ]",Load the parameters of a Model saved by tl . files . save_npz () .
"def assign_params ( sess , params , network ) : ops = [ ] for idx , param in enumerate ( params ) : ops . append ( network . all_params [ idx ] . assign ( param ) ) if sess is not None : sess . run ( ops ) return ops",Assign the given parameters to the TensorLayer network .
"def load_and_assign_npz ( sess = None , name = None , network = None ) : if network is None : raise ValueError ( ""network is None."" ) if sess is None : raise ValueError ( ""session is None."" ) if not os . path . exists ( name ) : logging . error ( ""file {} doesn't exist."" . format ( name ) ) return False else : params = load_npz ( name = name ) assign_params ( sess , params , network ) logging . info ( ""[*] Load {} SUCCESS!"" . format ( name ) ) return network",Load model from npz and assign to a network .
"def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( ""session is None."" ) if save_list is None : save_list = [ ] save_list_names = [ tensor . name for tensor in save_list ] save_list_var = sess . run ( save_list ) save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } np . savez ( name , * * save_var_dict ) save_list_var = None save_var_dict = None del save_list_var del save_var_dict logging . info ( ""[*] Model saved in npz_dict %s"" % name )",Input parameters and the file name save parameters as a dictionary into . npz file .
"def load_and_assign_npz_dict ( name = 'model.npz' , sess = None ) : if sess is None : raise ValueError ( ""session is None."" ) if not os . path . exists ( name ) : logging . error ( ""file {} doesn't exist."" . format ( name ) ) return False params = np . load ( name ) if len ( params . keys ( ) ) != len ( set ( params . keys ( ) ) ) : raise Exception ( ""Duplication in model npz_dict %s"" % name ) ops = list ( ) for key in params . keys ( ) : try : varlist = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = key ) if len ( varlist ) > 1 : raise Exception ( ""[!] Multiple candidate variables to be assigned for name %s"" % key ) elif len ( varlist ) == 0 : raise KeyError else : ops . append ( varlist [ 0 ] . assign ( params [ key ] ) ) logging . info ( ""[*] params restored: %s"" % key ) except KeyError : logging . info ( ""[!] Warning: Tensor named %s not found in network."" % key ) sess . run ( ops ) logging . info ( ""[*] Model restored from npz_dict %s"" % name )",Restore the parameters saved by tl . files . save_npz_dict () .
"def save_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False ) : if sess is None : raise ValueError ( ""session is None."" ) if var_list is None : var_list = [ ] ckpt_file = os . path . join ( save_dir , mode_name ) if var_list == [ ] : var_list = tf . global_variables ( ) logging . info ( ""[*] save %s n_params: %d"" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( ""  param {:3}: {:15}   {}"" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) saver = tf . train . Saver ( var_list ) saver . save ( sess , ckpt_file , global_step = global_step )",Save parameters into ckpt file .
"def load_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , is_latest = True , printable = False ) : if sess is None : raise ValueError ( ""session is None."" ) if var_list is None : var_list = [ ] if is_latest : ckpt_file = tf . train . latest_checkpoint ( save_dir ) else : ckpt_file = os . path . join ( save_dir , mode_name ) if not var_list : var_list = tf . global_variables ( ) logging . info ( ""[*] load %s n_params: %d"" % ( ckpt_file , len ( var_list ) ) ) if printable : for idx , v in enumerate ( var_list ) : logging . info ( ""  param {:3}: {:15}   {}"" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) try : saver = tf . train . Saver ( var_list ) saver . restore ( sess , ckpt_file ) except Exception as e : logging . info ( e ) logging . info ( ""[*] load ckpt fail ..."" )",Load parameters from ckpt file .
"def load_npy_to_any ( path = '' , name = 'file.npy' ) : file_path = os . path . join ( path , name ) try : return np . load ( file_path ) . item ( ) except Exception : return np . load ( file_path ) raise Exception ( ""[!] Fail to load %s"" % file_path )",Load . npy file .
"def load_file_list ( path = None , regx = '\.jpg' , printable = True , keep_prefix = False ) : if path is None : path = os . getcwd ( ) file_list = os . listdir ( path ) return_list = [ ] for _ , f in enumerate ( file_list ) : if re . search ( regx , f ) : return_list . append ( f ) if keep_prefix : for i , f in enumerate ( return_list ) : return_list [ i ] = os . path . join ( path , f ) if printable : logging . info ( 'Match file list = %s' % return_list ) logging . info ( 'Number of files = %d' % len ( return_list ) ) return return_list",r Return a file list in a folder by given a path and regular expression .
"def load_folder_list ( path = """" ) : return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ]",Return a folder list in a folder by given a folder path .
"def exists_or_mkdir ( path , verbose = True ) : if not os . path . exists ( path ) : if verbose : logging . info ( ""[*] creates %s ..."" % path ) os . makedirs ( path ) return False else : if verbose : logging . info ( ""[!] %s exists ..."" % path ) return True",Check a folder by given name if not exist create the folder and return False if directory exists return True .
"def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : def _download ( filename , working_directory , url_source ) : progress_bar = progressbar . ProgressBar ( ) def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : if ( totalSize != 0 ) : if not pbar . max_value : totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) pbar . max_value = int ( totalBlocks ) pbar . update ( count , force = True ) filepath = os . path . join ( working_directory , filename ) logging . info ( 'Downloading %s...\n' % filename ) urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) exists_or_mkdir ( working_directory , verbose = False ) filepath = os . path . join ( working_directory , filename ) if not os . path . exists ( filepath ) : _download ( filename , working_directory , url_source ) statinfo = os . stat ( filepath ) logging . info ( 'Succesfully downloaded %s %s bytes.' % ( filename , statinfo . st_size ) ) if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : raise Exception ( 'Failed to verify ' + filename + '. Can you get to it with a browser?' ) if ( extract ) : if tarfile . is_tarfile ( filepath ) : logging . info ( 'Trying to extract tar file' ) tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) logging . info ( '... Success!' ) elif zipfile . is_zipfile ( filepath ) : logging . info ( 'Trying to extract zip file' ) with zipfile . ZipFile ( filepath ) as zf : zf . extractall ( working_directory ) logging . info ( '... Success!' ) else : logging . info ( ""Unknown compression_format only .tar.gz/.tar.bz2/.tar and .zip supported"" ) return filepath",Checks if file exists in working_directory otherwise tries to dowload the file and optionally also tries to extract the file if format is . zip or . tar
"def natural_keys ( text ) : def atoi ( text ) : return int ( text ) if text . isdigit ( ) else text return [ atoi ( c ) for c in re . split ( '(\d+)' , text ) ]",Sort list of string with number in human order .
"def npz_to_W_pdf ( path = None , regx = 'w1pre_[0-9]+\.(npz)' ) : file_list = load_file_list ( path = path , regx = regx ) for f in file_list : W = load_npz ( path , f ) [ 0 ] logging . info ( ""%s --> %s"" % ( f , f . split ( '.' ) [ 0 ] + '.pdf' ) ) visualize . draw_weights ( W , second = 10 , saveable = True , name = f . split ( '.' ) [ 0 ] , fig_idx = 2012 )",r Convert the first weight matrix of . npz file to . pdf by using tl . visualize . W () .
"def threading_data ( data = None , fn = None , thread_count = None , * * kwargs ) : def apply_fn ( results , i , data , kwargs ) : results [ i ] = fn ( data , * * kwargs ) if thread_count is None : results = [ None ] * len ( data ) threads = [ ] for i , d in enumerate ( data ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , d , kwargs ) ) t . start ( ) threads . append ( t ) else : divs = np . linspace ( 0 , len ( data ) , thread_count + 1 ) divs = np . round ( divs ) . astype ( int ) results = [ None ] * thread_count threads = [ ] for i in range ( thread_count ) : t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , data [ divs [ i ] : divs [ i + 1 ] ] , kwargs ) ) t . start ( ) threads . append ( t ) for t in threads : t . join ( ) if thread_count is None : try : return np . asarray ( results ) except Exception : return results else : return np . concatenate ( results )",Process a batch of data by given function by threading .
"def affine_rotation_matrix ( angle = ( - 20 , 20 ) ) : if isinstance ( angle , tuple ) : theta = np . pi / 180 * np . random . uniform ( angle [ 0 ] , angle [ 1 ] ) else : theta = np . pi / 180 * angle rotation_matrix = np . array ( [ [ np . cos ( theta ) , np . sin ( theta ) , 0 ] , [ - np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) return rotation_matrix",Create an affine transform matrix for image rotation . NOTE : In OpenCV x is width and y is height .
"def affine_horizontal_flip_matrix ( prob = 0.5 ) : factor = np . random . uniform ( 0 , 1 ) if prob >= factor : filp_matrix = np . array ( [ [ - 1. , 0. , 0. ] , [ 0. , 1. , 0. ] , [ 0. , 0. , 1. ] ] ) return filp_matrix else : filp_matrix = np . array ( [ [ 1. , 0. , 0. ] , [ 0. , 1. , 0. ] , [ 0. , 0. , 1. ] ] ) return filp_matrix",Create an affine transformation matrix for image horizontal flipping . NOTE : In OpenCV x is width and y is height .
"def affine_vertical_flip_matrix ( prob = 0.5 ) : factor = np . random . uniform ( 0 , 1 ) if prob >= factor : filp_matrix = np . array ( [ [ 1. , 0. , 0. ] , [ 0. , - 1. , 0. ] , [ 0. , 0. , 1. ] ] ) return filp_matrix else : filp_matrix = np . array ( [ [ 1. , 0. , 0. ] , [ 0. , 1. , 0. ] , [ 0. , 0. , 1. ] ] ) return filp_matrix",Create an affine transformation for image vertical flipping . NOTE : In OpenCV x is width and y is height .
"def affine_shift_matrix ( wrg = ( - 0.1 , 0.1 ) , hrg = ( - 0.1 , 0.1 ) , w = 200 , h = 200 ) : if isinstance ( wrg , tuple ) : tx = np . random . uniform ( wrg [ 0 ] , wrg [ 1 ] ) * w else : tx = wrg * w if isinstance ( hrg , tuple ) : ty = np . random . uniform ( hrg [ 0 ] , hrg [ 1 ] ) * h else : ty = hrg * h shift_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) return shift_matrix",Create an affine transform matrix for image shifting . NOTE : In OpenCV x is width and y is height .
"def affine_shear_matrix ( x_shear = ( - 0.1 , 0.1 ) , y_shear = ( - 0.1 , 0.1 ) ) : if isinstance ( x_shear , tuple ) : x_shear = np . random . uniform ( x_shear [ 0 ] , x_shear [ 1 ] ) if isinstance ( y_shear , tuple ) : y_shear = np . random . uniform ( y_shear [ 0 ] , y_shear [ 1 ] ) shear_matrix = np . array ( [ [ 1 , x_shear , 0 ] , [ y_shear , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) return shear_matrix",Create affine transform matrix for image shearing . NOTE : In OpenCV x is width and y is height .
"def affine_zoom_matrix ( zoom_range = ( 0.8 , 1.1 ) ) : if isinstance ( zoom_range , ( float , int ) ) : scale = zoom_range elif isinstance ( zoom_range , tuple ) : scale = np . random . uniform ( zoom_range [ 0 ] , zoom_range [ 1 ] ) else : raise Exception ( ""zoom_range: float or tuple of 2 floats"" ) zoom_matrix = np . array ( [ [ scale , 0 , 0 ] , [ 0 , scale , 0 ] , [ 0 , 0 , 1 ] ] ) return zoom_matrix",Create an affine transform matrix for zooming / scaling an image s height and width . OpenCV format x is width .
"def affine_respective_zoom_matrix ( w_range = 0.8 , h_range = 1.1 ) : if isinstance ( h_range , ( float , int ) ) : zy = h_range elif isinstance ( h_range , tuple ) : zy = np . random . uniform ( h_range [ 0 ] , h_range [ 1 ] ) else : raise Exception ( ""h_range: float or tuple of 2 floats"" ) if isinstance ( w_range , ( float , int ) ) : zx = w_range elif isinstance ( w_range , tuple ) : zx = np . random . uniform ( w_range [ 0 ] , w_range [ 1 ] ) else : raise Exception ( ""w_range: float or tuple of 2 floats"" ) zoom_matrix = np . array ( [ [ zx , 0 , 0 ] , [ 0 , zy , 0 ] , [ 0 , 0 , 1 ] ] ) return zoom_matrix",Get affine transform matrix for zooming / scaling that height and width are changed independently . OpenCV format x is width .
"def transform_matrix_offset_center ( matrix , y , x ) : o_x = ( x - 1 ) / 2.0 o_y = ( y - 1 ) / 2.0 offset_matrix = np . array ( [ [ 1 , 0 , o_x ] , [ 0 , 1 , o_y ] , [ 0 , 0 , 1 ] ] ) reset_matrix = np . array ( [ [ 1 , 0 , - o_x ] , [ 0 , 1 , - o_y ] , [ 0 , 0 , 1 ] ] ) transform_matrix = np . dot ( np . dot ( offset_matrix , matrix ) , reset_matrix ) return transform_matrix",Convert the matrix from Cartesian coordinates ( the origin in the middle of image ) to Image coordinates ( the origin on the top - left of image ) .
"def affine_transform ( x , transform_matrix , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : x = np . rollaxis ( x , channel_index , 0 ) final_affine_matrix = transform_matrix [ : 2 , : 2 ] final_offset = transform_matrix [ : 2 , 2 ] channel_images = [ ndi . interpolation . affine_transform ( x_channel , final_affine_matrix , final_offset , order = order , mode = fill_mode , cval = cval ) for x_channel in x ] x = np . stack ( channel_images , axis = 0 ) x = np . rollaxis ( x , 0 , channel_index + 1 ) return x",Return transformed images by given an affine matrix in Scipy format ( x is height ) .
"def affine_transform_cv2 ( x , transform_matrix , flags = None , border_mode = 'constant' ) : rows , cols = x . shape [ 0 ] , x . shape [ 1 ] if flags is None : flags = cv2 . INTER_AREA if border_mode is 'constant' : border_mode = cv2 . BORDER_CONSTANT elif border_mode is 'replicate' : border_mode = cv2 . BORDER_REPLICATE else : raise Exception ( ""unsupport border_mode, check cv.BORDER_ for more details."" ) return cv2 . warpAffine ( x , transform_matrix [ 0 : 2 , : ] , ( cols , rows ) , flags = flags , borderMode = border_mode )",Return transformed images by given an affine matrix in OpenCV format ( x is width ) . ( Powered by OpenCV2 faster than tl . prepro . affine_transform )
"def affine_transform_keypoints ( coords_list , transform_matrix ) : coords_result_list = [ ] for coords in coords_list : coords = np . asarray ( coords ) coords = coords . transpose ( [ 1 , 0 ] ) coords = np . insert ( coords , 2 , 1 , axis = 0 ) coords_result = np . matmul ( transform_matrix , coords ) coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) coords_result_list . append ( coords_result ) return coords_result_list",Transform keypoint coordinates according to a given affine transform matrix . OpenCV format x is width .
"def projective_transform_by_points ( x , src , dst , map_args = None , output_shape = None , order = 1 , mode = 'constant' , cval = 0.0 , clip = True , preserve_range = False ) : if map_args is None : map_args = { } if isinstance ( src , list ) : src = np . array ( src ) if isinstance ( dst , list ) : dst = np . array ( dst ) if np . max ( x ) > 1 : x = x / 255 m = transform . ProjectiveTransform ( ) m . estimate ( dst , src ) warped = transform . warp ( x , m , map_args = map_args , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , preserve_range = preserve_range ) return warped",Projective transform by given coordinates usually 4 coordinates .
"def rotation ( x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : theta = np . pi / 180 * np . random . uniform ( - rg , rg ) else : theta = np . pi / 180 * rg rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x",Rotate an image randomly or non - randomly .
"def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( ""The size of cropping should smaller than or equal to the original image"" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] else : h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg return x [ h_offset : h_end , w_offset : w_end ]",Randomly or centrally crop an image .
"def crop_multi ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] if ( h < hrg ) or ( w < wrg ) : raise AssertionError ( ""The size of cropping should smaller than or equal to the original image"" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) results = [ ] for data in x : results . append ( data [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] ) return np . asarray ( results ) else : h_offset = ( h - hrg ) / 2 w_offset = ( w - wrg ) / 2 results = [ ] for data in x : results . append ( data [ h_offset : h - h_offset , w_offset : w - w_offset ] ) return np . asarray ( results )",Randomly or centrally crop multiple images .
"def flip_axis ( x , axis = 1 , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x else : return x else : x = np . asarray ( x ) . swapaxes ( axis , 0 ) x = x [ : : - 1 , ... ] x = x . swapaxes ( 0 , axis ) return x",Flip the axis of an image such as flip left and right up and down randomly or non - randomly
"def flip_axis_multi ( x , axis , is_random = False ) : if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results ) else : return np . asarray ( x ) else : results = [ ] for data in x : data = np . asarray ( data ) . swapaxes ( axis , 0 ) data = data [ : : - 1 , ... ] data = data . swapaxes ( 0 , axis ) results . append ( data ) return np . asarray ( results )",Flip the axises of multiple images together such as flip left and right up and down randomly or non - randomly
"def shift ( x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : h , w = x . shape [ row_index ] , x . shape [ col_index ] if is_random : tx = np . random . uniform ( - hrg , hrg ) * h ty = np . random . uniform ( - wrg , wrg ) * w else : tx , ty = hrg * h , wrg * w translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x",Shift an image randomly or non - randomly .
"def shift_multi ( x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] if is_random : tx = np . random . uniform ( - hrg , hrg ) * h ty = np . random . uniform ( - wrg , wrg ) * w else : tx , ty = hrg * h , wrg * w translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix results = [ ] for data in x : results . append ( affine_transform ( data , transform_matrix , channel_index , fill_mode , cval , order ) ) return np . asarray ( results )",Shift images with the same arguments randomly or non - randomly . Usually be used for image segmentation which x = [ X Y ] X and Y should be matched .
"def shear ( x , intensity = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if is_random : shear = np . random . uniform ( - intensity , intensity ) else : shear = intensity shear_matrix = np . array ( [ [ 1 , - np . sin ( shear ) , 0 ] , [ 0 , np . cos ( shear ) , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( shear_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x",Shear an image randomly or non - randomly .
"def shear2 ( x , shear = ( 0.1 , 0.1 ) , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : if len ( shear ) != 2 : raise AssertionError ( ""shear should be tuple of 2 floats, or you want to use tl.prepro.shear rather than tl.prepro.shear2 ?"" ) if isinstance ( shear , tuple ) : shear = list ( shear ) if is_random : shear [ 0 ] = np . random . uniform ( - shear [ 0 ] , shear [ 0 ] ) shear [ 1 ] = np . random . uniform ( - shear [ 1 ] , shear [ 1 ] ) shear_matrix = np . array ( [ [ 1 , shear [ 0 ] , 0 ] , [ shear [ 1 ] , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = x . shape [ row_index ] , x . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( shear_matrix , h , w ) x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) return x",Shear an image randomly or non - randomly .
"def swirl ( x , center = None , strength = 1 , radius = 100 , rotation = 0 , output_shape = None , order = 1 , mode = 'constant' , cval = 0 , clip = True , preserve_range = False , is_random = False ) : if radius == 0 : raise AssertionError ( ""Invalid radius value"" ) rotation = np . pi / 180 * rotation if is_random : center_h = int ( np . random . uniform ( 0 , x . shape [ 0 ] ) ) center_w = int ( np . random . uniform ( 0 , x . shape [ 1 ] ) ) center = ( center_h , center_w ) strength = np . random . uniform ( 0 , strength ) radius = np . random . uniform ( 1e-10 , radius ) rotation = np . random . uniform ( - rotation , rotation ) max_v = np . max ( x ) if max_v > 1 : x = x / max_v swirled = skimage . transform . swirl ( x , center = center , strength = strength , radius = radius , rotation = rotation , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , preserve_range = preserve_range ) if max_v > 1 : swirled = swirled * max_v return swirled",Swirl an image randomly or non - randomly see scikit - image swirl API <http : // scikit - image . org / docs / dev / api / skimage . transform . html#skimage . transform . swirl > __ and example <http : // scikit - image . org / docs / dev / auto_examples / plot_swirl . html > __ .
"def elastic_transform ( x , alpha , sigma , mode = ""constant"" , cval = 0 , is_random = False ) : if is_random is False : random_state = np . random . RandomState ( None ) else : random_state = np . random . RandomState ( int ( time . time ( ) ) ) is_3d = False if len ( x . shape ) == 3 and x . shape [ - 1 ] == 1 : x = x [ : , : , 0 ] is_3d = True elif len ( x . shape ) == 3 and x . shape [ - 1 ] != 1 : raise Exception ( ""Only support greyscale image"" ) if len ( x . shape ) != 2 : raise AssertionError ( ""input should be grey-scale image"" ) shape = x . shape dx = gaussian_filter ( ( random_state . rand ( * shape ) * 2 - 1 ) , sigma , mode = mode , cval = cval ) * alpha dy = gaussian_filter ( ( random_state . rand ( * shape ) * 2 - 1 ) , sigma , mode = mode , cval = cval ) * alpha x_ , y_ = np . meshgrid ( np . arange ( shape [ 0 ] ) , np . arange ( shape [ 1 ] ) , indexing = 'ij' ) indices = np . reshape ( x_ + dx , ( - 1 , 1 ) ) , np . reshape ( y_ + dy , ( - 1 , 1 ) ) if is_3d : return map_coordinates ( x , indices , order = 1 ) . reshape ( ( shape [ 0 ] , shape [ 1 ] , 1 ) ) else : return map_coordinates ( x , indices , order = 1 ) . reshape ( shape )",Elastic transformation for image as described in [ Simard2003 ] <http : // deeplearning . cs . cmu . edu / pdfs / Simard . pdf > __ .
"def zoom ( x , zoom_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : zoom_matrix = affine_zoom_matrix ( zoom_range = zoom_range ) h , w = x . shape [ 0 ] , x . shape [ 1 ] transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) x = affine_transform_cv2 ( x , transform_matrix , flags = flags , border_mode = border_mode ) return x",Zooming / Scaling a single image that height and width are changed together .
"def respective_zoom ( x , h_range = ( 0.9 , 1.1 ) , w_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : zoom_matrix = affine_respective_zoom_matrix ( h_range = h_range , w_range = w_range ) h , w = x . shape [ 0 ] , x . shape [ 1 ] transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) x = affine_transform_cv2 ( x , transform_matrix , flags = flags , border_mode = border_mode ) return x",Zooming / Scaling a single image that height and width are changed independently .
"def zoom_multi ( x , zoom_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : zoom_matrix = affine_zoom_matrix ( zoom_range = zoom_range ) results = [ ] for img in x : h , w = x . shape [ 0 ] , x . shape [ 1 ] transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) results . append ( affine_transform_cv2 ( x , transform_matrix , flags = flags , border_mode = border_mode ) ) return result",Zoom in and out of images with the same arguments randomly or non - randomly . Usually be used for image segmentation which x = [ X Y ] X and Y should be matched .
"def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) x = exposure . adjust_gamma ( x , gamma , gain ) return x",Change the brightness of a single image randomly or non - randomly .
"def brightness_multi ( x , gamma = 1 , gain = 1 , is_random = False ) : if is_random : gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) results = [ ] for data in x : results . append ( exposure . adjust_gamma ( data , gamma , gain ) ) return np . asarray ( results )",Change the brightness of multiply images randomly or non - randomly . Usually be used for image segmentation which x = [ X Y ] X and Y should be matched .
"def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : if is_random : if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : raise AssertionError ( ""if is_random = True, the arguments are (min, max)"" ) illum_settings = np . random . randint ( 0 , 3 ) if illum_settings == 0 : gamma = np . random . uniform ( gamma [ 0 ] , 1.0 ) elif illum_settings == 1 : gamma = np . random . uniform ( 1.0 , gamma [ 1 ] ) else : gamma = 1 im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( np . random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) ) im_ = np . array ( image ) else : im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) image = PIL . Image . fromarray ( im_ ) contrast_adjust = PIL . ImageEnhance . Contrast ( image ) image = contrast_adjust . enhance ( contrast ) saturation_adjust = PIL . ImageEnhance . Color ( image ) image = saturation_adjust . enhance ( saturation ) im_ = np . array ( image ) return np . asarray ( im_ )",Perform illumination augmentation for a single image randomly or non - randomly .
"def rgb_to_hsv ( rgb ) : rgb = rgb . astype ( 'float' ) hsv = np . zeros_like ( rgb ) hsv [ ... , 3 : ] = rgb [ ... , 3 : ] r , g , b = rgb [ ... , 0 ] , rgb [ ... , 1 ] , rgb [ ... , 2 ] maxc = np . max ( rgb [ ... , : 3 ] , axis = - 1 ) minc = np . min ( rgb [ ... , : 3 ] , axis = - 1 ) hsv [ ... , 2 ] = maxc mask = maxc != minc hsv [ mask , 1 ] = ( maxc - minc ) [ mask ] / maxc [ mask ] rc = np . zeros_like ( r ) gc = np . zeros_like ( g ) bc = np . zeros_like ( b ) rc [ mask ] = ( maxc - r ) [ mask ] / ( maxc - minc ) [ mask ] gc [ mask ] = ( maxc - g ) [ mask ] / ( maxc - minc ) [ mask ] bc [ mask ] = ( maxc - b ) [ mask ] / ( maxc - minc ) [ mask ] hsv [ ... , 0 ] = np . select ( [ r == maxc , g == maxc ] , [ bc - gc , 2.0 + rc - bc ] , default = 4.0 + gc - rc ) hsv [ ... , 0 ] = ( hsv [ ... , 0 ] / 6.0 ) % 1.0 return hsv",Input RGB image [ 0~255 ] return HSV image [ 0~1 ] .
"def hsv_to_rgb ( hsv ) : rgb = np . empty_like ( hsv ) rgb [ ... , 3 : ] = hsv [ ... , 3 : ] h , s , v = hsv [ ... , 0 ] , hsv [ ... , 1 ] , hsv [ ... , 2 ] i = ( h * 6.0 ) . astype ( 'uint8' ) f = ( h * 6.0 ) - i p = v * ( 1.0 - s ) q = v * ( 1.0 - s * f ) t = v * ( 1.0 - s * ( 1.0 - f ) ) i = i % 6 conditions = [ s == 0.0 , i == 1 , i == 2 , i == 3 , i == 4 , i == 5 ] rgb [ ... , 0 ] = np . select ( conditions , [ v , q , p , p , t , v ] , default = v ) rgb [ ... , 1 ] = np . select ( conditions , [ v , v , v , q , p , p ] , default = t ) rgb [ ... , 2 ] = np . select ( conditions , [ v , p , t , v , v , q ] , default = p ) return rgb . astype ( 'uint8' )",Input HSV image [ 0~1 ] return RGB image [ 0~255 ] .
"def adjust_hue ( im , hout = 0.66 , is_offset = True , is_clip = True , is_random = False ) : hsv = rgb_to_hsv ( im ) if is_random : hout = np . random . uniform ( - hout , hout ) if is_offset : hsv [ ... , 0 ] += hout else : hsv [ ... , 0 ] = hout if is_clip : hsv [ ... , 0 ] = np . clip ( hsv [ ... , 0 ] , 0 , np . inf ) rgb = hsv_to_rgb ( hsv ) return rgb",Adjust hue of an RGB image .
"def imresize ( x , size = None , interp = 'bicubic' , mode = None ) : if size is None : size = [ 100 , 100 ] if x . shape [ - 1 ] == 1 : x = scipy . misc . imresize ( x [ : , : , 0 ] , size , interp = interp , mode = mode ) return x [ : , : , np . newaxis ] else : return scipy . misc . imresize ( x , size , interp = interp , mode = mode )",Resize an image by given output size and method .
"def pixel_value_scale ( im , val = 0.9 , clip = None , is_random = False ) : clip = clip if clip is not None else ( - np . inf , np . inf ) if is_random : scale = 1 + np . random . uniform ( - val , val ) im = im * scale else : im = im * val if len ( clip ) == 2 : im = np . clip ( im , clip [ 0 ] , clip [ 1 ] ) else : raise Exception ( ""clip : tuple of 2 numbers"" ) return im",Scales each value in the pixels of the image .
"def samplewise_norm ( x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 ) : if rescale : x *= rescale if x . shape [ channel_index ] == 1 : if samplewise_center : x = x - np . mean ( x ) if samplewise_std_normalization : x = x / np . std ( x ) return x elif x . shape [ channel_index ] == 3 : if samplewise_center : x = x - np . mean ( x , axis = channel_index , keepdims = True ) if samplewise_std_normalization : x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) return x else : raise Exception ( ""Unsupported channels %d"" % x . shape [ channel_index ] )",Normalize an image by rescale samplewise centering and samplewise centering in order .
"def featurewise_norm ( x , mean = None , std = None , epsilon = 1e-7 ) : if mean : x = x - mean if std : x = x / ( std + epsilon ) return x",Normalize every pixels by the same given mean and std which are usually compute from all examples .
"def get_zca_whitening_principal_components_img ( X ) : flatX = np . reshape ( X , ( X . shape [ 0 ] , X . shape [ 1 ] * X . shape [ 2 ] * X . shape [ 3 ] ) ) tl . logging . info ( ""zca : computing sigma .."" ) sigma = np . dot ( flatX . T , flatX ) / flatX . shape [ 0 ] tl . logging . info ( ""zca : computing U, S and V .."" ) U , S , _ = linalg . svd ( sigma ) tl . logging . info ( ""zca : computing principal components .."" ) principal_components = np . dot ( np . dot ( U , np . diag ( 1. / np . sqrt ( S + 10e-7 ) ) ) , U . T ) return principal_components",Return the ZCA whitening principal components matrix .
"def zca_whitening ( x , principal_components ) : flatx = np . reshape ( x , ( x . size ) ) whitex = np . dot ( flatx , principal_components ) x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) return x",Apply ZCA whitening on an image by given principal components matrix .
"def channel_shift ( x , intensity , is_random = False , channel_index = 2 ) : if is_random : factor = np . random . uniform ( - intensity , intensity ) else : factor = intensity x = np . rollaxis ( x , channel_index , 0 ) min_x , max_x = np . min ( x ) , np . max ( x ) channel_images = [ np . clip ( x_channel + factor , min_x , max_x ) for x_channel in x ] x = np . stack ( channel_images , axis = 0 ) x = np . rollaxis ( x , 0 , channel_index + 1 ) return x",Shift the channels of an image randomly or non - randomly see numpy . rollaxis <https : // docs . scipy . org / doc / numpy / reference / generated / numpy . rollaxis . html > __ .
"def channel_shift_multi ( x , intensity , is_random = False , channel_index = 2 ) : if is_random : factor = np . random . uniform ( - intensity , intensity ) else : factor = intensity results = [ ] for data in x : data = np . rollaxis ( data , channel_index , 0 ) min_x , max_x = np . min ( data ) , np . max ( data ) channel_images = [ np . clip ( x_channel + factor , min_x , max_x ) for x_channel in x ] data = np . stack ( channel_images , axis = 0 ) data = np . rollaxis ( x , 0 , channel_index + 1 ) results . append ( data ) return np . asarray ( results )",Shift the channels of images with the same arguments randomly or non - randomly see numpy . rollaxis <https : // docs . scipy . org / doc / numpy / reference / generated / numpy . rollaxis . html > __ . Usually be used for image segmentation which x = [ X Y ] X and Y should be matched .
"def drop ( x , keep = 0.5 ) : if len ( x . shape ) == 3 : if x . shape [ - 1 ] == 3 : img_size = x . shape mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) for i in range ( 3 ) : x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) elif x . shape [ - 1 ] == 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( ""Unsupported shape {}"" . format ( x . shape ) ) elif len ( x . shape ) == 2 or 1 : img_size = x . shape x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) else : raise Exception ( ""Unsupported shape {}"" . format ( x . shape ) ) return x",Randomly set some pixels to zero by a given keeping probability .
"def array_to_img ( x , dim_ordering = ( 0 , 1 , 2 ) , scale = True ) : x = x . transpose ( dim_ordering ) if scale : x += max ( - np . min ( x ) , 0 ) x_max = np . max ( x ) if x_max != 0 : x = x / x_max x *= 255 if x . shape [ 2 ] == 3 : return PIL . Image . fromarray ( x . astype ( 'uint8' ) , 'RGB' ) elif x . shape [ 2 ] == 1 : return PIL . Image . fromarray ( x [ : , : , 0 ] . astype ( 'uint8' ) , 'L' ) else : raise Exception ( 'Unsupported channel number: ' , x . shape [ 2 ] )",Converts a numpy array to PIL image object ( uint8 format ) .
"def find_contours ( x , level = 0.8 , fully_connected = 'low' , positive_orientation = 'low' ) : return skimage . measure . find_contours ( x , level , fully_connected = fully_connected , positive_orientation = positive_orientation )",Find iso - valued contours in a 2D array for a given level value returns list of ( n 2 ) - ndarrays see skimage . measure . find_contours <http : // scikit - image . org / docs / dev / api / skimage . measure . html#skimage . measure . find_contours > __ .
"def pt2map ( list_points = None , size = ( 100 , 100 ) , val = 1 ) : if list_points is None : raise Exception ( ""list_points : list of 2 int"" ) i_m = np . zeros ( size ) if len ( list_points ) == 0 : return i_m for xx in list_points : for x in xx : i_m [ int ( np . round ( x [ 0 ] ) ) ] [ int ( np . round ( x [ 1 ] ) ) ] = val return i_m",Inputs a list of points return a 2D image .
"def binary_dilation ( x , radius = 3 ) : mask = disk ( radius ) x = _binary_dilation ( x , selem = mask ) return x",Return fast binary morphological dilation of an image . see skimage . morphology . binary_dilation <http : // scikit - image . org / docs / dev / api / skimage . morphology . html#skimage . morphology . binary_dilation > __ .
"def dilation ( x , radius = 3 ) : mask = disk ( radius ) x = dilation ( x , selem = mask ) return x",Return greyscale morphological dilation of an image see skimage . morphology . dilation <http : // scikit - image . org / docs / dev / api / skimage . morphology . html#skimage . morphology . dilation > __ .
"def binary_erosion ( x , radius = 3 ) : mask = disk ( radius ) x = _binary_erosion ( x , selem = mask ) return x",Return binary morphological erosion of an image see skimage . morphology . binary_erosion <http : // scikit - image . org / docs / dev / api / skimage . morphology . html#skimage . morphology . binary_erosion > __ .
"def erosion ( x , radius = 3 ) : mask = disk ( radius ) x = _erosion ( x , selem = mask ) return x",Return greyscale morphological erosion of an image see skimage . morphology . erosion <http : // scikit - image . org / docs / dev / api / skimage . morphology . html#skimage . morphology . erosion > __ .
"def obj_box_coords_rescale ( coords = None , shape = None ) : if coords is None : coords = [ ] if shape is None : shape = [ 100 , 200 ] imh , imw = shape [ 0 ] , shape [ 1 ] imh = imh * 1.0 imw = imw * 1.0 coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) x = coord [ 0 ] / imw y = coord [ 1 ] / imh w = coord [ 2 ] / imw h = coord [ 3 ] / imh coords_new . append ( [ x , y , w , h ] ) return coords_new",Scale down a list of coordinates from pixel unit to the ratio of image size i . e . in the range of [ 0 1 ] .
"def obj_box_coord_rescale ( coord = None , shape = None ) : if coord is None : coord = [ ] if shape is None : shape = [ 100 , 200 ] return obj_box_coords_rescale ( coords = [ coord ] , shape = shape ) [ 0 ]",Scale down one coordinates from pixel unit to the ratio of image size i . e . in the range of [ 0 1 ] . It is the reverse process of obj_box_coord_scale_to_pixelunit .
"def obj_box_coord_scale_to_pixelunit ( coord , shape = None ) : if shape is None : shape = [ 100 , 100 ] imh , imw = shape [ 0 : 2 ] x = int ( coord [ 0 ] * imw ) x2 = int ( coord [ 2 ] * imw ) y = int ( coord [ 1 ] * imh ) y2 = int ( coord [ 3 ] * imh ) return [ x , y , x2 , y2 ]",Convert one coordinate [ x y w ( or x2 ) h ( or y2 ) ] in ratio format to image coordinate format . It is the reverse process of obj_box_coord_rescale .
"def obj_box_coord_centroid_to_upleft_butright ( coord , to_int = False ) : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) x_center , y_center , w , h = coord x = x_center - w / 2. y = y_center - h / 2. x2 = x + w y2 = y + h if to_int : return [ int ( x ) , int ( y ) , int ( x2 ) , int ( y2 ) ] else : return [ x , y , x2 , y2 ]",Convert one coordinate [ x_center y_center w h ] to [ x1 y1 x2 y2 ] in up - left and botton - right format .
"def obj_box_coord_upleft_butright_to_centroid ( coord ) : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x1, y1, x2, y2]"" ) x1 , y1 , x2 , y2 = coord w = x2 - x1 h = y2 - y1 x_c = x1 + w / 2. y_c = y1 + h / 2. return [ x_c , y_c , w , h ]",Convert one coordinate [ x1 y1 x2 y2 ] to [ x_center y_center w h ] . It is the reverse process of obj_box_coord_centroid_to_upleft_butright .
"def obj_box_coord_centroid_to_upleft ( coord ) : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) x_center , y_center , w , h = coord x = x_center - w / 2. y = y_center - h / 2. return [ x , y , w , h ]",Convert one coordinate [ x_center y_center w h ] to [ x y w h ] . It is the reverse process of obj_box_coord_upleft_to_centroid .
"def parse_darknet_ann_str_to_list ( annotations ) : annotations = annotations . split ( ""\n"" ) ann = [ ] for a in annotations : a = a . split ( ) if len ( a ) == 5 : for i , _v in enumerate ( a ) : if i == 0 : a [ i ] = int ( a [ i ] ) else : a [ i ] = float ( a [ i ] ) ann . append ( a ) return ann",r Input string format of class x y w h return list of list format .
"def parse_darknet_ann_list_to_cls_box ( annotations ) : class_list = [ ] bbox_list = [ ] for ann in annotations : class_list . append ( ann [ 0 ] ) bbox_list . append ( ann [ 1 : ] ) return class_list , bbox_list",Parse darknet annotation format into two lists for class and bounding box .
"def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : if coords is None : coords = [ ] def _flip ( im , coords ) : im = flip_axis ( im , axis = 1 , is_random = False ) coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) if is_rescale : if is_center : x = 1. - coord [ 0 ] else : x = 1. - coord [ 0 ] - coord [ 2 ] else : if is_center : x = im . shape [ 1 ] - coord [ 0 ] else : x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) return im , coords_new if is_random : factor = np . random . uniform ( - 1 , 1 ) if factor > 0 : return _flip ( im , coords ) else : return im , coords else : return _flip ( im , coords )",Left - right flip the image and coordinates for object detection .
"def obj_box_imresize ( im , coords = None , size = None , interp = 'bicubic' , mode = None , is_rescale = False ) : if coords is None : coords = [ ] if size is None : size = [ 100 , 100 ] imh , imw = im . shape [ 0 : 2 ] imh = imh * 1.0 imw = imw * 1.0 im = imresize ( im , size = size , interp = interp , mode = mode ) if is_rescale is False : coords_new = list ( ) for coord in coords : if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) x = int ( coord [ 0 ] * ( size [ 1 ] / imw ) ) y = int ( coord [ 1 ] * ( size [ 0 ] / imh ) ) w = int ( coord [ 2 ] * ( size [ 1 ] / imw ) ) h = int ( coord [ 3 ] * ( size [ 0 ] / imh ) ) coords_new . append ( [ x , y , w , h ] ) return im , coords_new else : return im , coords",Resize an image and compute the new bounding box coordinates .
"def obj_box_crop ( im , classes = None , coords = None , wrg = 100 , hrg = 100 , is_rescale = False , is_center = False , is_random = False , thresh_wh = 0.02 , thresh_wh2 = 12. ) : if classes is None : classes = [ ] if coords is None : coords = [ ] h , w = im . shape [ 0 ] , im . shape [ 1 ] if ( h <= hrg ) or ( w <= wrg ) : raise AssertionError ( ""The size of cropping should smaller than the original image"" ) if is_random : h_offset = int ( np . random . uniform ( 0 , h - hrg ) - 1 ) w_offset = int ( np . random . uniform ( 0 , w - wrg ) - 1 ) h_end = hrg + h_offset w_end = wrg + w_offset im_new = im [ h_offset : h_end , w_offset : w_end ] else : h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) h_end = h_offset + hrg w_end = w_offset + wrg im_new = im [ h_offset : h_end , w_offset : w_end ] def _get_coord ( coord ) : """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.

        """""" if is_center : coord = obj_box_coord_centroid_to_upleft ( coord ) x = coord [ 0 ] - w_offset y = coord [ 1 ] - h_offset w = coord [ 2 ] h = coord [ 3 ] if x < 0 : if x + w <= 0 : return None w = w + x x = 0 elif x > im_new . shape [ 1 ] : return None if y < 0 : if y + h <= 0 : return None h = h + y y = 0 elif y > im_new . shape [ 0 ] : return None if ( x is not None ) and ( x + w > im_new . shape [ 1 ] ) : w = im_new . shape [ 1 ] - x if ( y is not None ) and ( y + h > im_new . shape [ 0 ] ) : h = im_new . shape [ 0 ] - y if ( w / ( h + 1. ) > thresh_wh2 ) or ( h / ( w + 1. ) > thresh_wh2 ) : return None if ( w / ( im_new . shape [ 1 ] * 1. ) < thresh_wh ) or ( h / ( im_new . shape [ 0 ] * 1. ) < thresh_wh ) : return None coord = [ x , y , w , h ] if is_center : coord = obj_box_coord_upleft_to_centroid ( coord ) return coord coords_new = list ( ) classes_new = list ( ) for i , _ in enumerate ( coords ) : coord = coords [ i ] if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) if is_rescale : coord = obj_box_coord_scale_to_pixelunit ( coord , im . shape ) coord = _get_coord ( coord ) if coord is not None : coord = obj_box_coord_rescale ( coord , im_new . shape ) coords_new . append ( coord ) classes_new . append ( classes [ i ] ) else : coord = _get_coord ( coord ) if coord is not None : coords_new . append ( coord ) classes_new . append ( classes [ i ] ) return im_new , classes_new , coords_new",Randomly or centrally crop an image and compute the new bounding box coordinates . Objects outside the cropped image will be removed .
"def obj_box_shift ( im , classes = None , coords = None , wrg = 0.1 , hrg = 0.1 , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 , is_rescale = False , is_center = False , is_random = False , thresh_wh = 0.02 , thresh_wh2 = 12. ) : if classes is None : classes = [ ] if coords is None : coords = [ ] imh , imw = im . shape [ row_index ] , im . shape [ col_index ] if ( hrg >= 1.0 ) and ( hrg <= 0. ) and ( wrg >= 1.0 ) and ( wrg <= 0. ) : raise AssertionError ( ""shift range should be (0, 1)"" ) if is_random : tx = np . random . uniform ( - hrg , hrg ) * imh ty = np . random . uniform ( - wrg , wrg ) * imw else : tx , ty = hrg * imh , wrg * imw translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) transform_matrix = translation_matrix im_new = affine_transform ( im , transform_matrix , channel_index , fill_mode , cval , order ) def _get_coord ( coord ) : """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.

        """""" if is_center : coord = obj_box_coord_centroid_to_upleft ( coord ) x = coord [ 0 ] - ty y = coord [ 1 ] - tx w = coord [ 2 ] h = coord [ 3 ] if x < 0 : if x + w <= 0 : return None w = w + x x = 0 elif x > im_new . shape [ 1 ] : return None if y < 0 : if y + h <= 0 : return None h = h + y y = 0 elif y > im_new . shape [ 0 ] : return None if ( x is not None ) and ( x + w > im_new . shape [ 1 ] ) : w = im_new . shape [ 1 ] - x if ( y is not None ) and ( y + h > im_new . shape [ 0 ] ) : h = im_new . shape [ 0 ] - y if ( w / ( h + 1. ) > thresh_wh2 ) or ( h / ( w + 1. ) > thresh_wh2 ) : return None if ( w / ( im_new . shape [ 1 ] * 1. ) < thresh_wh ) or ( h / ( im_new . shape [ 0 ] * 1. ) < thresh_wh ) : return None coord = [ x , y , w , h ] if is_center : coord = obj_box_coord_upleft_to_centroid ( coord ) return coord coords_new = list ( ) classes_new = list ( ) for i , _ in enumerate ( coords ) : coord = coords [ i ] if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) if is_rescale : coord = obj_box_coord_scale_to_pixelunit ( coord , im . shape ) coord = _get_coord ( coord ) if coord is not None : coord = obj_box_coord_rescale ( coord , im_new . shape ) coords_new . append ( coord ) classes_new . append ( classes [ i ] ) else : coord = _get_coord ( coord ) if coord is not None : coords_new . append ( coord ) classes_new . append ( classes [ i ] ) return im_new , classes_new , coords_new",Shift an image randomly or non - randomly and compute the new bounding box coordinates . Objects outside the cropped image will be removed .
"def obj_box_zoom ( im , classes = None , coords = None , zoom_range = ( 0.9 , 1.1 ) , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 , is_rescale = False , is_center = False , is_random = False , thresh_wh = 0.02 , thresh_wh2 = 12. ) : if classes is None : classes = [ ] if coords is None : coords = [ ] if len ( zoom_range ) != 2 : raise Exception ( 'zoom_range should be a tuple or list of two floats. ' 'Received arg: ' , zoom_range ) if is_random : if zoom_range [ 0 ] == 1 and zoom_range [ 1 ] == 1 : zx , zy = 1 , 1 tl . logging . info ( "" random_zoom : not zoom in/out"" ) else : zx , zy = np . random . uniform ( zoom_range [ 0 ] , zoom_range [ 1 ] , 2 ) else : zx , zy = zoom_range zoom_matrix = np . array ( [ [ zx , 0 , 0 ] , [ 0 , zy , 0 ] , [ 0 , 0 , 1 ] ] ) h , w = im . shape [ row_index ] , im . shape [ col_index ] transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) im_new = affine_transform ( im , transform_matrix , channel_index , fill_mode , cval , order ) def _get_coord ( coord ) : """"""Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.

        """""" if is_center : coord = obj_box_coord_centroid_to_upleft ( coord ) x = ( coord [ 0 ] - im . shape [ 1 ] / 2 ) / zy + im . shape [ 1 ] / 2 y = ( coord [ 1 ] - im . shape [ 0 ] / 2 ) / zx + im . shape [ 0 ] / 2 w = coord [ 2 ] / zy h = coord [ 3 ] / zx if x < 0 : if x + w <= 0 : return None w = w + x x = 0 elif x > im_new . shape [ 1 ] : return None if y < 0 : if y + h <= 0 : return None h = h + y y = 0 elif y > im_new . shape [ 0 ] : return None if ( x is not None ) and ( x + w > im_new . shape [ 1 ] ) : w = im_new . shape [ 1 ] - x if ( y is not None ) and ( y + h > im_new . shape [ 0 ] ) : h = im_new . shape [ 0 ] - y if ( w / ( h + 1. ) > thresh_wh2 ) or ( h / ( w + 1. ) > thresh_wh2 ) : return None if ( w / ( im_new . shape [ 1 ] * 1. ) < thresh_wh ) or ( h / ( im_new . shape [ 0 ] * 1. ) < thresh_wh ) : return None coord = [ x , y , w , h ] if is_center : coord = obj_box_coord_upleft_to_centroid ( coord ) return coord coords_new = list ( ) classes_new = list ( ) for i , _ in enumerate ( coords ) : coord = coords [ i ] if len ( coord ) != 4 : raise AssertionError ( ""coordinate should be 4 values : [x, y, w, h]"" ) if is_rescale : coord = obj_box_coord_scale_to_pixelunit ( coord , im . shape ) coord = _get_coord ( coord ) if coord is not None : coord = obj_box_coord_rescale ( coord , im_new . shape ) coords_new . append ( coord ) classes_new . append ( classes [ i ] ) else : coord = _get_coord ( coord ) if coord is not None : coords_new . append ( coord ) classes_new . append ( classes [ i ] ) return im_new , classes_new , coords_new",Zoom in and out of a single image randomly or non - randomly and compute the new bounding box coordinates . Objects outside the cropped image will be removed .
"def pad_sequences ( sequences , maxlen = None , dtype = 'int32' , padding = 'post' , truncating = 'pre' , value = 0. ) : lengths = [ len ( s ) for s in sequences ] nb_samples = len ( sequences ) if maxlen is None : maxlen = np . max ( lengths ) sample_shape = tuple ( ) for s in sequences : if len ( s ) > 0 : sample_shape = np . asarray ( s ) . shape [ 1 : ] break x = ( np . ones ( ( nb_samples , maxlen ) + sample_shape ) * value ) . astype ( dtype ) for idx , s in enumerate ( sequences ) : if len ( s ) == 0 : continue if truncating == 'pre' : trunc = s [ - maxlen : ] elif truncating == 'post' : trunc = s [ : maxlen ] else : raise ValueError ( 'Truncating type ""%s"" not understood' % truncating ) trunc = np . asarray ( trunc , dtype = dtype ) if trunc . shape [ 1 : ] != sample_shape : raise ValueError ( 'Shape of sample %s of sequence at position %s is different from expected shape %s' % ( trunc . shape [ 1 : ] , idx , sample_shape ) ) if padding == 'post' : x [ idx , : len ( trunc ) ] = trunc elif padding == 'pre' : x [ idx , - len ( trunc ) : ] = trunc else : raise ValueError ( 'Padding type ""%s"" not understood' % padding ) return x . tolist ( )",Pads each sequence to the same length : the length of the longest sequence . If maxlen is provided any sequence longer than maxlen is truncated to maxlen . Truncation happens off either the beginning ( default ) or the end of the sequence . Supports post - padding and pre - padding ( default ) .
"def remove_pad_sequences ( sequences , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , _ in enumerate ( sequences ) : for j in range ( 1 , len ( sequences [ i ] ) ) : if sequences [ i ] [ - j ] != pad_id : sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] break return sequences_out",Remove padding .
"def process_sequences ( sequences , end_id = 0 , pad_val = 0 , is_shorten = True , remain_end_id = False ) : max_length = 0 for _ , seq in enumerate ( sequences ) : is_end = False for i_w , n in enumerate ( seq ) : if n == end_id and is_end == False : is_end = True if max_length < i_w : max_length = i_w if remain_end_id is False : seq [ i_w ] = pad_val elif is_end == True : seq [ i_w ] = pad_val if remain_end_id is True : max_length += 1 if is_shorten : for i , seq in enumerate ( sequences ) : sequences [ i ] = seq [ : max_length ] return sequences",Set all tokens ( ids ) after END token to the padding value and then shorten ( option ) it to the maximum sequence length in this batch .
"def sequences_add_start_id ( sequences , start_id = 0 , remove_last = False ) : sequences_out = [ [ ] for _ in range ( len ( sequences ) ) ] for i , _ in enumerate ( sequences ) : if remove_last : sequences_out [ i ] = [ start_id ] + sequences [ i ] [ : - 1 ] else : sequences_out [ i ] = [ start_id ] + sequences [ i ] return sequences_out",Add special start token ( id ) in the beginning of each sequence .
"def sequences_add_end_id ( sequences , end_id = 888 ) : sequences_out = [ [ ] for _ in range ( len ( sequences ) ) ] for i , _ in enumerate ( sequences ) : sequences_out [ i ] = sequences [ i ] + [ end_id ] return sequences_out",Add special end token ( id ) in the end of each sequence .
"def sequences_add_end_id_after_pad ( sequences , end_id = 888 , pad_id = 0 ) : sequences_out = copy . deepcopy ( sequences ) for i , v in enumerate ( sequences ) : for j , _v2 in enumerate ( v ) : if sequences [ i ] [ j ] == pad_id : sequences_out [ i ] [ j ] = end_id break return sequences_out",Add special end token ( id ) in the end of each sequence .
"def sequences_get_mask ( sequences , pad_val = 0 ) : mask = np . ones_like ( sequences ) for i , seq in enumerate ( sequences ) : for i_w in reversed ( range ( len ( seq ) ) ) : if seq [ i_w ] == pad_val : mask [ i , i_w ] = 0 else : break return mask",Return mask for sequences .
"def keypoint_random_crop ( image , annos , mask = None , size = ( 368 , 368 ) ) : _target_height = size [ 0 ] _target_width = size [ 1 ] target_size = ( _target_width , _target_height ) if len ( np . shape ( image ) ) == 2 : image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) height , width , _ = np . shape ( image ) for _ in range ( 50 ) : x = random . randrange ( 0 , width - target_size [ 0 ] ) if width > target_size [ 0 ] else 0 y = random . randrange ( 0 , height - target_size [ 1 ] ) if height > target_size [ 1 ] else 0 for joint in annos : if x <= joint [ 0 ] [ 0 ] < x + target_size [ 0 ] and y <= joint [ 0 ] [ 1 ] < y + target_size [ 1 ] : break def pose_crop ( image , annos , mask , x , y , w , h ) : target_size = ( w , h ) img = image resized = img [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] , : ] resized_mask = mask [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] ] adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 10 or point [ 1 ] < - 10 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue new_x , new_y = point [ 0 ] - x , point [ 1 ] - y if new_x > w - 1 or new_y > h - 1 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( new_x , new_y ) ) adjust_joint_list . append ( adjust_joint ) return resized , adjust_joint_list , resized_mask return pose_crop ( image , annos , mask , x , y , target_size [ 0 ] , target_size [ 1 ] )",Randomly crop an image and corresponding keypoints without influence scales given by keypoint_random_resize_shortestedge .
"def keypoint_resize_random_crop ( image , annos , mask = None , size = ( 368 , 368 ) ) : if len ( np . shape ( image ) ) == 2 : image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) def resize_image ( image , annos , mask , target_width , target_height ) : """"""Reszie image

        Parameters
        -----------
        image : 3 channel image
            The given image.
        annos : list of list of floats
            Keypoints of people
        mask : single channel image or None
            The mask if available.
        target_width : int
            Expected width of returned image.
        target_height : int
            Expected height of returned image.

        Returns
        ----------
        preprocessed input image, annos, mask

        """""" y , x , _ = np . shape ( image ) ratio_y = target_height / y ratio_x = target_width / x new_joints = [ ] for people in annos : new_keypoints = [ ] for keypoints in people : if keypoints [ 0 ] < 0 or keypoints [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue pts = ( int ( keypoints [ 0 ] * ratio_x + 0.5 ) , int ( keypoints [ 1 ] * ratio_y + 0.5 ) ) if pts [ 0 ] > target_width - 1 or pts [ 1 ] > target_height - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( pts ) new_joints . append ( new_keypoints ) annos = new_joints new_image = cv2 . resize ( image , ( target_width , target_height ) , interpolation = cv2 . INTER_AREA ) if mask is not None : new_mask = cv2 . resize ( mask , ( target_width , target_height ) , interpolation = cv2 . INTER_AREA ) return new_image , annos , new_mask else : return new_image , annos , None _target_height = size [ 0 ] _target_width = size [ 1 ] if len ( np . shape ( image ) ) == 2 : image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) height , width , _ = np . shape ( image ) if height <= width : ratio = _target_height / height new_width = int ( ratio * width ) if height == width : new_width = _target_height image , annos , mask = resize_image ( image , annos , mask , new_width , _target_height ) if new_width > _target_width : crop_range_x = np . random . randint ( 0 , new_width - _target_width ) else : crop_range_x = 0 image = image [ : , crop_range_x : crop_range_x + _target_width , : ] if mask is not None : mask = mask [ : , crop_range_x : crop_range_x + _target_width ] new_joints = [ ] for people in annos : new_keypoints = [ ] for keypoints in people : if keypoints [ 0 ] < - 10 or keypoints [ 1 ] < - 10 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue top = crop_range_x + _target_width - 1 if keypoints [ 0 ] >= crop_range_x and keypoints [ 0 ] <= top : pts = ( int ( keypoints [ 0 ] - crop_range_x ) , int ( keypoints [ 1 ] ) ) else : pts = ( - 1000 , - 1000 ) new_keypoints . append ( pts ) new_joints . append ( new_keypoints ) annos = new_joints if height > width : ratio = _target_width / width new_height = int ( ratio * height ) image , annos , mask = resize_image ( image , annos , mask , _target_width , new_height ) if new_height > _target_height : crop_range_y = np . random . randint ( 0 , new_height - _target_height ) else : crop_range_y = 0 image = image [ crop_range_y : crop_range_y + _target_width , : , : ] if mask is not None : mask = mask [ crop_range_y : crop_range_y + _target_width , : ] new_joints = [ ] for people in annos : new_keypoints = [ ] for keypoints in people : if keypoints [ 0 ] < 0 or keypoints [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue bot = crop_range_y + _target_height - 1 if keypoints [ 1 ] >= crop_range_y and keypoints [ 1 ] <= bot : pts = ( int ( keypoints [ 0 ] ) , int ( keypoints [ 1 ] - crop_range_y ) ) else : pts = ( - 1000 , - 1000 ) new_keypoints . append ( pts ) new_joints . append ( new_keypoints ) annos = new_joints if mask is not None : return image , annos , mask else : return image , annos , None",Reszie the image to make either its width or height equals to the given sizes . Then randomly crop image without influence scales . Resize the image match with the minimum size before cropping this API will change the zoom scale of object .
"def keypoint_random_rotate ( image , annos , mask = None , rg = 15. ) : def _rotate_coord ( shape , newxy , point , angle ) : angle = - 1 * angle / 180.0 * math . pi ox , oy = shape px , py = point ox /= 2 oy /= 2 qx = math . cos ( angle ) * ( px - ox ) - math . sin ( angle ) * ( py - oy ) qy = math . sin ( angle ) * ( px - ox ) + math . cos ( angle ) * ( py - oy ) new_x , new_y = newxy qx += ox - new_x qy += oy - new_y return int ( qx + 0.5 ) , int ( qy + 0.5 ) def _largest_rotated_rect ( w , h , angle ) : """"""
        Get largest rectangle after rotation.
        http://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders
        """""" angle = angle / 180.0 * math . pi if w <= 0 or h <= 0 : return 0 , 0 width_is_longer = w >= h side_long , side_short = ( w , h ) if width_is_longer else ( h , w ) sin_a , cos_a = abs ( math . sin ( angle ) ) , abs ( math . cos ( angle ) ) if side_short <= 2. * sin_a * cos_a * side_long : x = 0.5 * side_short wr , hr = ( x / sin_a , x / cos_a ) if width_is_longer else ( x / cos_a , x / sin_a ) else : cos_2a = cos_a * cos_a - sin_a * sin_a wr , hr = ( w * cos_a - h * sin_a ) / cos_2a , ( h * cos_a - w * sin_a ) / cos_2a return int ( np . round ( wr ) ) , int ( np . round ( hr ) ) img_shape = np . shape ( image ) height = img_shape [ 0 ] width = img_shape [ 1 ] deg = np . random . uniform ( - rg , rg ) img = image center = ( img . shape [ 1 ] * 0.5 , img . shape [ 0 ] * 0.5 ) rot_m = cv2 . getRotationMatrix2D ( ( int ( center [ 0 ] ) , int ( center [ 1 ] ) ) , deg , 1 ) ret = cv2 . warpAffine ( img , rot_m , img . shape [ 1 : : - 1 ] , flags = cv2 . INTER_AREA , borderMode = cv2 . BORDER_CONSTANT ) if img . ndim == 3 and ret . ndim == 2 : ret = ret [ : , : , np . newaxis ] neww , newh = _largest_rotated_rect ( ret . shape [ 1 ] , ret . shape [ 0 ] , deg ) neww = min ( neww , ret . shape [ 1 ] ) newh = min ( newh , ret . shape [ 0 ] ) newx = int ( center [ 0 ] - neww * 0.5 ) newy = int ( center [ 1 ] - newh * 0.5 ) img = ret [ newy : newy + newh , newx : newx + neww ] adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue x , y = _rotate_coord ( ( width , height ) , ( newx , newy ) , point , deg ) if x > neww - 1 or y > newh - 1 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue if x < 0 or y < 0 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( x , y ) ) adjust_joint_list . append ( adjust_joint ) joint_list = adjust_joint_list if mask is not None : msk = mask center = ( msk . shape [ 1 ] * 0.5 , msk . shape [ 0 ] * 0.5 ) rot_m = cv2 . getRotationMatrix2D ( ( int ( center [ 0 ] ) , int ( center [ 1 ] ) ) , deg , 1 ) ret = cv2 . warpAffine ( msk , rot_m , msk . shape [ 1 : : - 1 ] , flags = cv2 . INTER_AREA , borderMode = cv2 . BORDER_CONSTANT ) if msk . ndim == 3 and msk . ndim == 2 : ret = ret [ : , : , np . newaxis ] neww , newh = _largest_rotated_rect ( ret . shape [ 1 ] , ret . shape [ 0 ] , deg ) neww = min ( neww , ret . shape [ 1 ] ) newh = min ( newh , ret . shape [ 0 ] ) newx = int ( center [ 0 ] - neww * 0.5 ) newy = int ( center [ 1 ] - newh * 0.5 ) msk = ret [ newy : newy + newh , newx : newx + neww ] return img , joint_list , msk else : return img , joint_list , None",Rotate an image and corresponding keypoints .
"def keypoint_random_flip ( image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) ) : _prob = np . random . uniform ( 0 , 1.0 ) if _prob < prob : return image , annos , mask _ , width , _ = np . shape ( image ) image = cv2 . flip ( image , 1 ) mask = cv2 . flip ( mask , 1 ) new_joints = [ ] for people in annos : new_keypoints = [ ] for k in flip_list : point = people [ k ] if point [ 0 ] < 0 or point [ 1 ] < 0 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : new_keypoints . append ( ( - 1000 , - 1000 ) ) continue new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) new_joints . append ( new_keypoints ) annos = new_joints return image , annos , mask",Flip an image and corresponding keypoints .
"def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : height = image . shape [ 0 ] width = image . shape [ 1 ] _min , _max = zoom_range scalew = np . random . uniform ( _min , _max ) scaleh = np . random . uniform ( _min , _max ) neww = int ( width * scalew ) newh = int ( height * scaleh ) dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) if mask is not None : mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) adjust_joint_list = [ ] for joint in annos : adjust_joint = [ ] for point in joint : if point [ 0 ] < - 100 or point [ 1 ] < - 100 : adjust_joint . append ( ( - 1000 , - 1000 ) ) continue adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) adjust_joint_list . append ( adjust_joint ) if mask is not None : return dst , adjust_joint_list , mask else : return dst , adjust_joint_list , None",Randomly resize an image and corresponding keypoints . The height and width of image will be changed independently so the scale will be changed .
"def Vgg19 ( rgb ) : start_time = time . time ( ) print ( ""build model started"" ) rgb_scaled = rgb * 255.0 red , green , blue = tf . split ( rgb_scaled , 3 , 3 ) if red . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) if green . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) if blue . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) bgr = tf . concat ( [ blue - VGG_MEAN [ 0 ] , green - VGG_MEAN [ 1 ] , red - VGG_MEAN [ 2 ] , ] , axis = 3 ) if bgr . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 3 ] : raise Exception ( ""image size unmatch"" ) net_in = InputLayer ( bgr , name = 'input' ) net = Conv2dLayer ( net_in , act = tf . nn . relu , shape = [ 3 , 3 , 3 , 64 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv1_1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 64 , 64 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv1_2' ) net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 64 , 128 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv2_1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 128 , 128 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv2_2' ) net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool2' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 128 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_2' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_3' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_4' ) net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool3' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_2' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_3' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_4' ) net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool4' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_1' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_2' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_3' ) net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_4' ) net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool5' ) net = FlattenLayer ( net , name = 'flatten' ) net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc6' ) net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc7' ) net = DenseLayer ( net , n_units = 1000 , act = None , name = 'fc8' ) print ( ""build model finished: %fs"" % ( time . time ( ) - start_time ) ) return net",Build the VGG 19 Model
"def Vgg19_simple_api ( rgb ) : start_time = time . time ( ) print ( ""build model started"" ) rgb_scaled = rgb * 255.0 red , green , blue = tf . split ( rgb_scaled , 3 , 3 ) if red . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) if green . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) if blue . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : raise Exception ( ""image size unmatch"" ) bgr = tf . concat ( [ blue - VGG_MEAN [ 0 ] , green - VGG_MEAN [ 1 ] , red - VGG_MEAN [ 2 ] , ] , axis = 3 ) if bgr . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 3 ] : raise Exception ( ""image size unmatch"" ) net_in = InputLayer ( bgr , name = 'input' ) net = Conv2d ( net_in , 64 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv1_1' ) net = Conv2d ( net , n_filter = 64 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv1_2' ) net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool1' ) net = Conv2d ( net , n_filter = 128 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv2_1' ) net = Conv2d ( net , n_filter = 128 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv2_2' ) net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool2' ) net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_1' ) net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_2' ) net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_3' ) net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_4' ) net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool3' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_1' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_2' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_3' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_4' ) net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool4' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_1' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_2' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_3' ) net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_4' ) net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool5' ) net = FlattenLayer ( net , name = 'flatten' ) net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc6' ) net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc7' ) net = DenseLayer ( net , n_units = 1000 , act = None , name = 'fc8' ) print ( ""build model finished: %fs"" % ( time . time ( ) - start_time ) ) return net",Build the VGG 19 Model
"def prepro ( I ) : I = I [ 35 : 195 ] I = I [ : : 2 , : : 2 , 0 ] I [ I == 144 ] = 0 I [ I == 109 ] = 0 I [ I != 0 ] = 1 return I . astype ( np . float ) . ravel ( )",Prepro 210x160x3 uint8 frame into 6400 ( 80x80 ) 1D float vector .
"def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : if rewards is None : raise Exception ( ""rewards should be a list"" ) discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) running_add = 0 for t in reversed ( xrange ( 0 , rewards . size ) ) : if mode == 0 : if rewards [ t ] != 0 : running_add = 0 running_add = running_add * gamma + rewards [ t ] discounted_r [ t ] = running_add return discounted_r",Take 1D float array of rewards and compute discounted rewards for an episode . When encount a non - zero value consider as the end a of an episode .
"def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) )",Calculate the loss for Policy Gradient Network .
"def log_weight ( probs , weights , name = 'log_weight' ) : with tf . variable_scope ( name ) : exp_v = tf . reduce_mean ( tf . log ( probs ) * weights ) return exp_v",Log weight .
"def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : if action_list is None : n_action = len ( probs ) action_list = np . arange ( n_action ) else : if len ( action_list ) != len ( probs ) : raise Exception ( ""number of actions should equal to number of probabilities."" ) return np . random . choice ( action_list , p = probs )",Choice and return an an action by given the action probability distribution .
"def cross_entropy ( output , target , name = None ) : if name is None : raise Exception ( ""Please give a unique name to tl.cost.cross_entropy for TF1.0+"" ) return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )",Softmax cross - entropy operation returns the TensorFlow expression of cross - entropy for two distributions it implements softmax internally . See tf . nn . sparse_softmax_cross_entropy_with_logits .
"def sigmoid_cross_entropy ( output , target , name = None ) : return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name )",Sigmoid cross - entropy operation see tf . nn . sigmoid_cross_entropy_with_logits .
"def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : return tf . reduce_mean ( tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , name = name )",Binary cross entropy operation .
"def mean_squared_error ( output , target , is_mean = False , name = ""mean_squared_error"" ) : if output . get_shape ( ) . ndims == 2 : if is_mean : mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , 1 ) , name = name ) else : mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , 1 ) , name = name ) elif output . get_shape ( ) . ndims == 3 : if is_mean : mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , [ 1 , 2 ] ) , name = name ) else : mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , [ 1 , 2 ] ) , name = name ) elif output . get_shape ( ) . ndims == 4 : if is_mean : mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , [ 1 , 2 , 3 ] ) , name = name ) else : mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , [ 1 , 2 , 3 ] ) , name = name ) else : raise Exception ( ""Unknow dimension"" ) return mse",Return the TensorFlow expression of mean - square - error ( L2 ) of two batch of data .
"def normalized_mean_square_error ( output , target , name = ""normalized_mean_squared_error_loss"" ) : if output . get_shape ( ) . ndims == 2 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = 1 ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = 1 ) ) elif output . get_shape ( ) . ndims == 3 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 ] ) ) elif output . get_shape ( ) . ndims == 4 : nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 , 3 ] ) ) nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 , 3 ] ) ) nmse = tf . reduce_mean ( nmse_a / nmse_b , name = name ) return nmse",Return the TensorFlow expression of normalized mean - square - error of two distributions .
"def absolute_difference_error ( output , target , is_mean = False , name = ""absolute_difference_error_loss"" ) : if output . get_shape ( ) . ndims == 2 : if is_mean : loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , 1 ) , name = name ) else : loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , 1 ) , name = name ) elif output . get_shape ( ) . ndims == 3 : if is_mean : loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , [ 1 , 2 ] ) , name = name ) else : loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , [ 1 , 2 ] ) , name = name ) elif output . get_shape ( ) . ndims == 4 : if is_mean : loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , [ 1 , 2 , 3 ] ) , name = name ) else : loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , [ 1 , 2 , 3 ] ) , name = name ) else : raise Exception ( ""Unknow dimension"" ) return loss",Return the TensorFlow expression of absolute difference error ( L1 ) of two batch of data .
"def dice_coe ( output , target , loss_type = 'jaccard' , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : inse = tf . reduce_sum ( output * target , axis = axis ) if loss_type == 'jaccard' : l = tf . reduce_sum ( output * output , axis = axis ) r = tf . reduce_sum ( target * target , axis = axis ) elif loss_type == 'sorensen' : l = tf . reduce_sum ( output , axis = axis ) r = tf . reduce_sum ( target , axis = axis ) else : raise Exception ( ""Unknow loss_type"" ) dice = ( 2. * inse + smooth ) / ( l + r + smooth ) dice = tf . reduce_mean ( dice , name = 'dice_coe' ) return dice",Soft dice ( Sørensen or Jaccard ) coefficient for comparing the similarity of two batch of data usually be used for binary image segmentation i . e . labels are binary . The coefficient between 0 to 1 1 means totally match .
"def dice_hard_coe ( output , target , threshold = 0.5 , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : output = tf . cast ( output > threshold , dtype = tf . float32 ) target = tf . cast ( target > threshold , dtype = tf . float32 ) inse = tf . reduce_sum ( tf . multiply ( output , target ) , axis = axis ) l = tf . reduce_sum ( output , axis = axis ) r = tf . reduce_sum ( target , axis = axis ) hard_dice = ( 2. * inse + smooth ) / ( l + r + smooth ) hard_dice = tf . reduce_mean ( hard_dice , name = 'hard_dice' ) return hard_dice",Non - differentiable Sørensen–Dice coefficient for comparing the similarity of two batch of data usually be used for binary image segmentation i . e . labels are binary . The coefficient between 0 to 1 1 if totally match .
"def iou_coe ( output , target , threshold = 0.5 , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : pre = tf . cast ( output > threshold , dtype = tf . float32 ) truth = tf . cast ( target > threshold , dtype = tf . float32 ) inse = tf . reduce_sum ( tf . multiply ( pre , truth ) , axis = axis ) union = tf . reduce_sum ( tf . cast ( tf . add ( pre , truth ) >= 1 , dtype = tf . float32 ) , axis = axis ) batch_iou = ( inse + smooth ) / ( union + smooth ) iou = tf . reduce_mean ( batch_iou , name = 'iou_coe' ) return iou",Non - differentiable Intersection over Union ( IoU ) for comparing the similarity of two batch of data usually be used for evaluating binary image segmentation . The coefficient between 0 to 1 and 1 means totally match .
"def cross_entropy_seq ( logits , target_seqs , batch_size = None ) : sequence_loss_by_example_fn = tf . contrib . legacy_seq2seq . sequence_loss_by_example loss = sequence_loss_by_example_fn ( [ logits ] , [ tf . reshape ( target_seqs , [ - 1 ] ) ] , [ tf . ones_like ( tf . reshape ( target_seqs , [ - 1 ] ) , dtype = tf . float32 ) ] ) cost = tf . reduce_sum ( loss ) if batch_size is not None : cost = cost / batch_size return cost",Returns the expression of cross - entropy of two sequences implement softmax internally . Normally be used for fixed length RNN outputs see PTB example <https : // github . com / tensorlayer / tensorlayer / blob / master / example / tutorial_ptb_lstm_state_is_tuple . py > __ .
"def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : targets = tf . reshape ( target_seqs , [ - 1 ] ) weights = tf . to_float ( tf . reshape ( input_mask , [ - 1 ] ) ) losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights loss = tf . divide ( tf . reduce_sum ( losses ) , tf . reduce_sum ( weights ) , name = ""seq_loss_with_mask"" ) if return_details : return loss , losses , weights , targets else : return loss",Returns the expression of cross - entropy of two sequences implement softmax internally . Normally be used for Dynamic RNN with Synced sequence input and output .
"def cosine_similarity ( v1 , v2 ) : return tf . reduce_sum ( tf . multiply ( v1 , v2 ) , 1 ) / ( tf . sqrt ( tf . reduce_sum ( tf . multiply ( v1 , v1 ) , 1 ) ) * tf . sqrt ( tf . reduce_sum ( tf . multiply ( v2 , v2 ) , 1 ) ) )",Cosine similarity [ - 1 1 ] .
"def li_regularizer ( scale , scope = None ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) if scale >= 1. : raise ValueError ( 'Setting a scale greater than 1 on a regularizer: %g' % scale ) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def li ( weights ) : """"""Applies li regularization to weights."""""" with tf . name_scope ( 'li_regularizer' ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_sum ( standard_ops . sqrt ( standard_ops . reduce_sum ( tf . square ( weights ) , 1 ) ) ) , name = scope ) return li",Li regularization removes the neurons of previous layer . The i represents inputs . Returns a function that can be used to apply group li regularization to weights . The implementation follows TensorFlow contrib <https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / contrib / layers / python / layers / regularizers . py > __ .
"def maxnorm_regularizer ( scale = 1.0 ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn ( weights , name = 'max_regularizer' ) : """"""Applies max-norm regularization to weights."""""" with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) return mn",Max - norm regularization returns a function that can be used to apply max - norm regularization to weights .
"def maxnorm_o_regularizer ( scale ) : if isinstance ( scale , numbers . Integral ) : raise ValueError ( 'scale cannot be an integer: %s' % scale ) if isinstance ( scale , numbers . Real ) : if scale < 0. : raise ValueError ( 'Setting a scale less than 0 on a regularizer: %g' % scale ) if scale == 0. : tl . logging . info ( 'Scale of 0 disables regularizer.' ) return lambda _ , name = None : None def mn_o ( weights , name = 'maxnorm_o_regularizer' ) : """"""Applies max-norm regularization to weights."""""" with tf . name_scope ( name ) as scope : my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) if tf . __version__ <= '0.12' : standard_ops_fn = standard_ops . mul else : standard_ops_fn = standard_ops . multiply return standard_ops_fn ( my_scale , standard_ops . reduce_sum ( standard_ops . reduce_max ( standard_ops . abs ( weights ) , 0 ) ) , name = scope ) return mn_o",Max - norm output regularization removes the neurons of current layer . Returns a function that can be used to apply max - norm regularization to each column of weight matrix . The implementation follows TensorFlow contrib <https : // github . com / tensorflow / tensorflow / blob / master / tensorflow / contrib / layers / python / layers / regularizers . py > __ .
"def ramp ( x , v_min = 0 , v_max = 1 , name = None ) : return tf . clip_by_value ( x , clip_value_min = v_min , clip_value_max = v_max , name = name )",Ramp activation function .
"def leaky_relu ( x , alpha = 0.2 , name = ""leaky_relu"" ) : if not ( 0 < alpha <= 1 ) : raise ValueError ( ""`alpha` value must be in [0, 1]`"" ) with tf . name_scope ( name , ""leaky_relu"" ) as name_scope : x = tf . convert_to_tensor ( x , name = ""features"" ) return tf . maximum ( x , alpha * x , name = name_scope )",leaky_relu can be used through its shortcut : : func : tl . act . lrelu .
"def leaky_relu6 ( x , alpha = 0.2 , name = ""leaky_relu6"" ) : if not isinstance ( alpha , tf . Tensor ) and not ( 0 < alpha <= 1 ) : raise ValueError ( ""`alpha` value must be in [0, 1]`"" ) with tf . name_scope ( name , ""leaky_relu6"" ) as name_scope : x = tf . convert_to_tensor ( x , name = ""features"" ) return tf . minimum ( tf . maximum ( x , alpha * x ) , 6 , name = name_scope )",: func : leaky_relu6 can be used through its shortcut : : func : tl . act . lrelu6 .
"def leaky_twice_relu6 ( x , alpha_low = 0.2 , alpha_high = 0.2 , name = ""leaky_relu6"" ) : if not isinstance ( alpha_high , tf . Tensor ) and not ( 0 < alpha_high <= 1 ) : raise ValueError ( ""`alpha_high` value must be in [0, 1]`"" ) if not isinstance ( alpha_low , tf . Tensor ) and not ( 0 < alpha_low <= 1 ) : raise ValueError ( ""`alpha_low` value must be in [0, 1]`"" ) with tf . name_scope ( name , ""leaky_twice_relu6"" ) as name_scope : x = tf . convert_to_tensor ( x , name = ""features"" ) x_is_above_0 = tf . minimum ( x , 6 * ( 1 - alpha_high ) + alpha_high * x ) x_is_below_0 = tf . minimum ( alpha_low * x , 0 ) return tf . maximum ( x_is_above_0 , x_is_below_0 , name = name_scope )",: func : leaky_twice_relu6 can be used through its shortcut : : func : : func : tl . act . ltrelu6 .
"def swish ( x , name = 'swish' ) : with tf . name_scope ( name ) : x = tf . nn . sigmoid ( x ) * x return x",Swish function .
"def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : with tf . name_scope ( name ) : return tf . nn . softmax ( x )",Return the softmax outputs of images every pixels have multiple label the sum of a pixel is 1 .
"def _conv_linear ( args , filter_size , num_features , bias , bias_start = 0.0 , scope = None ) : total_arg_size_depth = 0 shapes = [ a . get_shape ( ) . as_list ( ) for a in args ] for shape in shapes : if len ( shape ) != 4 : raise ValueError ( ""Linear is expecting 4D arguments: %s"" % str ( shapes ) ) if not shape [ 3 ] : raise ValueError ( ""Linear expects shape[4] of arguments: %s"" % str ( shapes ) ) else : total_arg_size_depth += shape [ 3 ] dtype = [ a . dtype for a in args ] [ 0 ] with tf . variable_scope ( scope or ""Conv"" ) : matrix = tf . get_variable ( ""Matrix"" , [ filter_size [ 0 ] , filter_size [ 1 ] , total_arg_size_depth , num_features ] , dtype = dtype ) if len ( args ) == 1 : res = tf . nn . conv2d ( args [ 0 ] , matrix , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' ) else : res = tf . nn . conv2d ( tf . concat ( args , 3 ) , matrix , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' ) if not bias : return res bias_term = tf . get_variable ( ""Bias"" , [ num_features ] , dtype = dtype , initializer = tf . constant_initializer ( bias_start , dtype = dtype ) ) return res + bias_term",convolution :
"def advanced_indexing_op ( inputs , index ) : batch_size = tf . shape ( inputs ) [ 0 ] max_length = tf . shape ( inputs ) [ 1 ] dim_size = int ( inputs . get_shape ( ) [ 2 ] ) index = tf . range ( 0 , batch_size ) * max_length + ( index - 1 ) flat = tf . reshape ( inputs , [ - 1 , dim_size ] ) relevant = tf . gather ( flat , index ) return relevant",Advanced Indexing for Sequences returns the outputs by given sequence lengths . When return the last output : class : DynamicRNNLayer uses it to get the last outputs with the sequence lengths .
"def retrieve_seq_length_op ( data ) : with tf . name_scope ( 'GetLength' ) : used = tf . sign ( tf . reduce_max ( tf . abs ( data ) , 2 ) ) length = tf . reduce_sum ( used , 1 ) return tf . cast ( length , tf . int32 )",An op to compute the length of a sequence from input shape of [ batch_size n_step ( max ) n_features ] it can be used when the features of padding ( on right hand side ) are all zeros .
"def retrieve_seq_length_op2 ( data ) : return tf . reduce_sum ( tf . cast ( tf . greater ( data , tf . zeros_like ( data ) ) , tf . int32 ) , 1 )",An op to compute the length of a sequence from input shape of [ batch_size n_step ( max ) ] it can be used when the features of padding ( on right hand side ) are all zeros .
"def retrieve_seq_length_op3 ( data , pad_val = 0 ) : data_shape_size = data . get_shape ( ) . ndims if data_shape_size == 3 : return tf . reduce_sum ( tf . cast ( tf . reduce_any ( tf . not_equal ( data , pad_val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 2 : return tf . reduce_sum ( tf . cast ( tf . not_equal ( data , pad_val ) , dtype = tf . int32 ) , 1 ) elif data_shape_size == 1 : raise ValueError ( ""retrieve_seq_length_op3: data has wrong shape!"" ) else : raise ValueError ( ""retrieve_seq_length_op3: handling data_shape_size %s hasn't been implemented!"" % ( data_shape_size ) )",Return tensor for sequence length if input is tf . string .
"def zero_state ( self , batch_size , dtype = LayersConfig . tf_dtype ) : shape = self . shape num_features = self . num_features zeros = tf . zeros ( [ batch_size , shape [ 0 ] , shape [ 1 ] , num_features * 2 ] , dtype = dtype ) return zeros",Return zero - filled state tensor ( s ) . Args : batch_size : int float or unit Tensor representing the batch size . Returns : tensor of shape [ batch_size x shape [ 0 ] x shape [ 1 ] x num_features ] filled with zeros
"def state_size ( self ) : return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units )",State size of the LSTMStateTuple .
"def _to_bc_h_w ( self , x , x_shape ) : x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) x = tf . reshape ( x , ( - 1 , x_shape [ 1 ] , x_shape [ 2 ] ) ) return x",( b h w c ) - > ( b * c h w )
"def _to_b_h_w_n_c ( self , x , x_shape ) : x = tf . reshape ( x , ( - 1 , x_shape [ 4 ] , x_shape [ 1 ] , x_shape [ 2 ] , x_shape [ 3 ] ) ) x = tf . transpose ( x , [ 0 , 2 , 3 , 4 , 1 ] ) return x",( b * c h w n ) - > ( b h w n c )
"def _tf_repeat ( self , a , repeats ) : if len ( a . get_shape ( ) ) != 1 : raise AssertionError ( ""This is not a 1D Tensor"" ) a = tf . expand_dims ( a , - 1 ) a = tf . tile ( a , [ 1 , repeats ] ) a = self . tf_flatten ( a ) return a",Tensorflow version of np . repeat for 1D
"def _tf_batch_map_coordinates ( self , inputs , coords ) : input_shape = inputs . get_shape ( ) coords_shape = coords . get_shape ( ) batch_channel = tf . shape ( inputs ) [ 0 ] input_h = int ( input_shape [ 1 ] ) input_w = int ( input_shape [ 2 ] ) kernel_n = int ( coords_shape [ 3 ] ) n_coords = input_h * input_w * kernel_n coords_lt = tf . cast ( tf . floor ( coords ) , 'int32' ) coords_rb = tf . cast ( tf . ceil ( coords ) , 'int32' ) coords_lb = tf . stack ( [ coords_lt [ : , : , : , : , 0 ] , coords_rb [ : , : , : , : , 1 ] ] , axis = - 1 ) coords_rt = tf . stack ( [ coords_rb [ : , : , : , : , 0 ] , coords_lt [ : , : , : , : , 1 ] ] , axis = - 1 ) idx = self . _tf_repeat ( tf . range ( batch_channel ) , n_coords ) vals_lt = self . _get_vals_by_coords ( inputs , coords_lt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rb = self . _get_vals_by_coords ( inputs , coords_rb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_lb = self . _get_vals_by_coords ( inputs , coords_lb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) vals_rt = self . _get_vals_by_coords ( inputs , coords_rt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) coords_offset_lt = coords - tf . cast ( coords_lt , 'float32' ) vals_t = vals_lt + ( vals_rt - vals_lt ) * coords_offset_lt [ : , : , : , : , 0 ] vals_b = vals_lb + ( vals_rb - vals_lb ) * coords_offset_lt [ : , : , : , : , 0 ] mapped_vals = vals_t + ( vals_b - vals_t ) * coords_offset_lt [ : , : , : , : , 1 ] return mapped_vals",Batch version of tf_map_coordinates
"def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : input_shape = inputs . get_shape ( ) batch_size = tf . shape ( inputs ) [ 0 ] kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) input_h = input_shape [ 1 ] input_w = input_shape [ 2 ] channel = input_shape [ 3 ] inputs = self . _to_bc_h_w ( inputs , input_shape ) offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) coords = tf . expand_dims ( grid_offset , 0 ) coords = tf . tile ( coords , [ batch_size , 1 , 1 , 1 , 1 ] ) + offsets coords = tf . stack ( [ tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) ] , axis = - 1 ) coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) return mapped_vals",Batch map offsets into input
"def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( ""The length of inputs and targets should be equal"" ) if shuffle : indices = np . arange ( len ( inputs ) ) np . random . shuffle ( indices ) for start_idx in range ( 0 , len ( inputs ) , batch_size ) : end_idx = start_idx + batch_size if end_idx > len ( inputs ) : if allow_dynamic_batch_size : end_idx = len ( inputs ) else : break if shuffle : excerpt = indices [ start_idx : end_idx ] else : excerpt = slice ( start_idx , end_idx ) if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] else : yield inputs [ excerpt ] , targets [ excerpt ]",Generate a generator that input a group of example in numpy . array and their labels return the examples and labels by the given batch size .
"def seq_minibatches ( inputs , targets , batch_size , seq_length , stride = 1 ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( ""The length of inputs and targets should be equal"" ) n_loads = ( batch_size * stride ) + ( seq_length - stride ) for start_idx in range ( 0 , len ( inputs ) - n_loads + 1 , ( batch_size * stride ) ) : seq_inputs = np . zeros ( ( batch_size , seq_length ) + inputs . shape [ 1 : ] , dtype = inputs . dtype ) seq_targets = np . zeros ( ( batch_size , seq_length ) + targets . shape [ 1 : ] , dtype = targets . dtype ) for b_idx in xrange ( batch_size ) : start_seq_idx = start_idx + ( b_idx * stride ) end_seq_idx = start_seq_idx + seq_length seq_inputs [ b_idx ] = inputs [ start_seq_idx : end_seq_idx ] seq_targets [ b_idx ] = targets [ start_seq_idx : end_seq_idx ] flatten_inputs = seq_inputs . reshape ( ( - 1 , ) + inputs . shape [ 1 : ] ) flatten_targets = seq_targets . reshape ( ( - 1 , ) + targets . shape [ 1 : ] ) yield flatten_inputs , flatten_targets",Generate a generator that return a batch of sequence inputs and targets . If batch_size = 100 and seq_length = 5 one return will have 500 rows ( examples ) .
"def seq_minibatches2 ( inputs , targets , batch_size , num_steps ) : if len ( inputs ) != len ( targets ) : raise AssertionError ( ""The length of inputs and targets should be equal"" ) data_len = len ( inputs ) batch_len = data_len // batch_size data = np . zeros ( ( batch_size , batch_len ) + inputs . shape [ 1 : ] , dtype = inputs . dtype ) data2 = np . zeros ( [ batch_size , batch_len ] ) for i in range ( batch_size ) : data [ i ] = inputs [ batch_len * i : batch_len * ( i + 1 ) ] data2 [ i ] = targets [ batch_len * i : batch_len * ( i + 1 ) ] epoch_size = ( batch_len - 1 ) // num_steps if epoch_size == 0 : raise ValueError ( ""epoch_size == 0, decrease batch_size or num_steps"" ) for i in range ( epoch_size ) : x = data [ : , i * num_steps : ( i + 1 ) * num_steps ] x2 = data2 [ : , i * num_steps : ( i + 1 ) * num_steps ] yield ( x , x2 )",Generate a generator that iterates on two list of words . Yields ( Returns ) the source contexts and the target context by the given batch_size and num_steps ( sequence_length ) . In TensorFlow s tutorial this generates the batch_size pointers into the raw PTB data and allows minibatch iteration along these pointers .
"def ptb_iterator ( raw_data , batch_size , num_steps ) : raw_data = np . array ( raw_data , dtype = np . int32 ) data_len = len ( raw_data ) batch_len = data_len // batch_size data = np . zeros ( [ batch_size , batch_len ] , dtype = np . int32 ) for i in range ( batch_size ) : data [ i ] = raw_data [ batch_len * i : batch_len * ( i + 1 ) ] epoch_size = ( batch_len - 1 ) // num_steps if epoch_size == 0 : raise ValueError ( ""epoch_size == 0, decrease batch_size or num_steps"" ) for i in range ( epoch_size ) : x = data [ : , i * num_steps : ( i + 1 ) * num_steps ] y = data [ : , i * num_steps + 1 : ( i + 1 ) * num_steps + 1 ] yield ( x , y )",Generate a generator that iterates on a list of words see PTB example <https : // github . com / tensorlayer / tensorlayer / blob / master / example / tutorial_ptb_lstm_state_is_tuple . py > __ . Yields the source contexts and the target context by the given batch_size and num_steps ( sequence_length ) .
"def deconv2d_bilinear_upsampling_initializer ( shape ) : if shape [ 0 ] != shape [ 1 ] : raise Exception ( 'deconv2d_bilinear_upsampling_initializer only supports symmetrical filter sizes' ) if shape [ 3 ] < shape [ 2 ] : raise Exception ( 'deconv2d_bilinear_upsampling_initializer behaviour is not defined for num_in_channels < num_out_channels ' ) filter_size = shape [ 0 ] num_out_channels = shape [ 2 ] num_in_channels = shape [ 3 ] bilinear_kernel = np . zeros ( [ filter_size , filter_size ] , dtype = np . float32 ) scale_factor = ( filter_size + 1 ) // 2 if filter_size % 2 == 1 : center = scale_factor - 1 else : center = scale_factor - 0.5 for x in range ( filter_size ) : for y in range ( filter_size ) : bilinear_kernel [ x , y ] = ( 1 - abs ( x - center ) / scale_factor ) * ( 1 - abs ( y - center ) / scale_factor ) weights = np . zeros ( ( filter_size , filter_size , num_out_channels , num_in_channels ) ) for i in range ( num_out_channels ) : weights [ : , : , i , i ] = bilinear_kernel return tf . constant_initializer ( value = weights , dtype = LayersConfig . tf_dtype )",Returns the initializer that can be passed to DeConv2dLayer for initializing the weights in correspondence to channel - wise bilinear up - sampling . Used in segmentation approaches such as [ FCN ] ( https : // arxiv . org / abs / 1605 . 06211 )
"def save_model ( self , network = None , model_name = 'model' , * * kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) params = network . get_all_params ( ) s = time . time ( ) kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) try : params_id = self . model_fs . put ( self . _serialization ( params ) ) kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) self . db . Model . insert_one ( kwargs ) print ( ""[Database] Save model: SUCCESS, took: {}s"" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( ""{}  {}  {}  {}  {}"" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( ""[Database] Save model: FAIL"" ) return False",Save model architecture and parameters into database timestamp will be added automatically .
"def find_top_model ( self , sess , sort = None , model_name = 'model' , * * kwargs ) : kwargs . update ( { 'model_name' : model_name } ) self . _fill_project_info ( kwargs ) s = time . time ( ) d = self . db . Model . find_one ( filter = kwargs , sort = sort ) _temp_file_name = '_find_one_model_ztemp_file' if d is not None : params_id = d [ 'params_id' ] graphs = d [ 'architecture' ] _datetime = d [ 'time' ] exists_or_mkdir ( _temp_file_name , False ) with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) else : print ( ""[Database] FAIL! Cannot find model: {}"" . format ( kwargs ) ) return False try : params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) network = load_graph_and_params ( name = _temp_file_name , sess = sess ) del_folder ( _temp_file_name ) pc = self . db . Model . find ( kwargs ) print ( ""[Database] Find one model SUCCESS. kwargs:{} sort:{} save time:{} took: {}s"" . format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) ) for key in d : network . __dict__ . update ( { ""_%s"" % key : d [ key ] } ) params_id_list = pc . distinct ( 'params_id' ) n_params = len ( params_id_list ) if n_params != 1 : print ( ""     Note that there are {} models match the kwargs"" . format ( n_params ) ) return network except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( ""{}  {}  {}  {}  {}"" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False",Finds and returns a model architecture and its parameters from the database which matches the requirement .
"def delete_model ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Model . delete_many ( kwargs ) logging . info ( ""[Database] Delete Model SUCCESS"" )",Delete model .
"def save_dataset ( self , dataset = None , dataset_name = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( ""dataset_name is None, please give a dataset name"" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) try : dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) self . db . Dataset . insert_one ( kwargs ) print ( ""[Database] Save dataset: SUCCESS, took: {}s"" . format ( round ( time . time ( ) - s , 2 ) ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( ""{}  {}  {}  {}  {}"" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) print ( ""[Database] Save dataset: FAIL"" ) return False",Saves one dataset into database timestamp will be added automatically .
"def find_top_dataset ( self , dataset_name = None , sort = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( ""dataset_name is None, please give a dataset name"" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) d = self . db . Dataset . find_one ( filter = kwargs , sort = sort ) if d is not None : dataset_id = d [ 'dataset_id' ] else : print ( ""[Database] FAIL! Cannot find dataset: {}"" . format ( kwargs ) ) return False try : dataset = self . _deserialization ( self . dataset_fs . get ( dataset_id ) . read ( ) ) pc = self . db . Dataset . find ( kwargs ) print ( ""[Database] Find one dataset SUCCESS, {} took: {}s"" . format ( kwargs , round ( time . time ( ) - s , 2 ) ) ) dataset_id_list = pc . distinct ( 'dataset_id' ) n_dataset = len ( dataset_id_list ) if n_dataset != 1 : print ( ""     Note that there are {} datasets match the requirement"" . format ( n_dataset ) ) return dataset except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( ""{}  {}  {}  {}  {}"" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) return False",Finds and returns a dataset from the database which matches the requirement .
"def find_datasets ( self , dataset_name = None , * * kwargs ) : self . _fill_project_info ( kwargs ) if dataset_name is None : raise Exception ( ""dataset_name is None, please give a dataset name"" ) kwargs . update ( { 'dataset_name' : dataset_name } ) s = time . time ( ) pc = self . db . Dataset . find ( kwargs ) if pc is not None : dataset_id_list = pc . distinct ( 'dataset_id' ) dataset_list = [ ] for dataset_id in dataset_id_list : tmp = self . dataset_fs . get ( dataset_id ) . read ( ) dataset_list . append ( self . _deserialization ( tmp ) ) else : print ( ""[Database] FAIL! Cannot find any dataset: {}"" . format ( kwargs ) ) return False print ( ""[Database] Find {} datasets SUCCESS, took: {}s"" . format ( len ( dataset_list ) , round ( time . time ( ) - s , 2 ) ) ) return dataset_list",Finds and returns all datasets from the database which matches the requirement . In some case the data in a dataset can be stored separately for better management .
"def delete_datasets ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Dataset . delete_many ( kwargs ) logging . info ( ""[Database] Delete Dataset SUCCESS"" )",Delete datasets .
"def save_training_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . TrainLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( ""[Database] train log: "" + _log )",Saves the training log timestamp will be added automatically .
"def save_validation_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) _result = self . db . ValidLog . insert_one ( kwargs ) _log = self . _print_dict ( kwargs ) logging . info ( ""[Database] valid log: "" + _log )",Saves the validation log timestamp will be added automatically .
"def delete_training_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . TrainLog . delete_many ( kwargs ) logging . info ( ""[Database] Delete TrainLog SUCCESS"" )",Deletes training log .
"def delete_validation_log ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . ValidLog . delete_many ( kwargs ) logging . info ( ""[Database] Delete ValidLog SUCCESS"" )",Deletes validation log .
"def create_task ( self , task_name = None , script = None , hyper_parameters = None , saved_result_keys = None , * * kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( ""task_name should be string"" ) if not isinstance ( script , str ) : raise Exception ( ""script should be string"" ) if hyper_parameters is None : hyper_parameters = { } if saved_result_keys is None : saved_result_keys = [ ] self . _fill_project_info ( kwargs ) kwargs . update ( { 'time' : datetime . utcnow ( ) } ) kwargs . update ( { 'hyper_parameters' : hyper_parameters } ) kwargs . update ( { 'saved_result_keys' : saved_result_keys } ) _script = open ( script , 'rb' ) . read ( ) kwargs . update ( { 'status' : 'pending' , 'script' : _script , 'result' : { } } ) self . db . Task . insert_one ( kwargs ) logging . info ( ""[Database] Saved Task - task_name: {} script: {}"" . format ( task_name , script ) )",Uploads a task to the database timestamp will be added automatically .
"def run_top_task ( self , task_name = None , sort = None , * * kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( ""task_name should be string"" ) self . _fill_project_info ( kwargs ) kwargs . update ( { 'status' : 'pending' } ) task = self . db . Task . find_one_and_update ( kwargs , { '$set' : { 'status' : 'running' } } , sort = sort ) try : if task is None : logging . info ( ""[Database] Find Task FAIL: key: {} sort: {}"" . format ( task_name , sort ) ) return False else : logging . info ( ""[Database] Find Task SUCCESS: key: {} sort: {}"" . format ( task_name , sort ) ) _datetime = task [ 'time' ] _script = task [ 'script' ] _id = task [ '_id' ] _hyper_parameters = task [ 'hyper_parameters' ] _saved_result_keys = task [ 'saved_result_keys' ] logging . info ( ""  hyper parameters:"" ) for key in _hyper_parameters : globals ( ) [ key ] = _hyper_parameters [ key ] logging . info ( ""    {}: {}"" . format ( key , _hyper_parameters [ key ] ) ) s = time . time ( ) logging . info ( ""[Database] Start Task: key: {} sort: {} push time: {}"" . format ( task_name , sort , _datetime ) ) _script = _script . decode ( 'utf-8' ) with tf . Graph ( ) . as_default ( ) : exec ( _script , globals ( ) ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'finished' } } ) __result = { } for _key in _saved_result_keys : logging . info ( ""  result: {}={} {}"" . format ( _key , globals ( ) [ _key ] , type ( globals ( ) [ _key ] ) ) ) __result . update ( { ""%s"" % _key : globals ( ) [ _key ] } ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'result' : __result } } , return_document = pymongo . ReturnDocument . AFTER ) logging . info ( ""[Database] Finished Task: task_name - {} sort: {} push time: {} took: {}s"" . format ( task_name , sort , _datetime , time . time ( ) - s ) ) return True except Exception as e : exc_type , exc_obj , exc_tb = sys . exc_info ( ) fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] logging . info ( ""{}  {}  {}  {}  {}"" . format ( exc_type , exc_obj , fname , exc_tb . tb_lineno , e ) ) logging . info ( ""[Database] Fail to run task"" ) _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'pending' } } ) return False",Finds and runs a pending task that in the first of the sorting list .
"def delete_tasks ( self , * * kwargs ) : self . _fill_project_info ( kwargs ) self . db . Task . delete_many ( kwargs ) logging . info ( ""[Database] Delete Task SUCCESS"" )",Delete tasks .
"def check_unfinished_task ( self , task_name = None , * * kwargs ) : if not isinstance ( task_name , str ) : raise Exception ( ""task_name should be string"" ) self . _fill_project_info ( kwargs ) kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) task = self . db . Task . find ( kwargs ) task_id_list = task . distinct ( '_id' ) n_task = len ( task_id_list ) if n_task == 0 : logging . info ( ""[Database] No unfinished task - task_name: {}"" . format ( task_name ) ) return False else : logging . info ( ""[Database] Find {} unfinished task - task_name: {}"" . format ( n_task , task_name ) ) return True",Finds and runs a pending task .
"def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : def get_ngrams ( n ) : return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) def hash_ngram ( ngram ) : bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) return unigram_vocab_size + hash_ % n_buckets return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ]",Augment unigram features with hashed n - gram features .
"def load_and_preprocess_imdb_data ( n_gram = None ) : X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) if n_gram is not None : X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) return X_train , y_train , X_test , y_test",Load IMDb data and augment with hashed n - gram features .
"def read_image ( image , path = '' ) : return imageio . imread ( os . path . join ( path , image ) )",Read one image .
"def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : imgs = [ ] for idx in range ( 0 , len ( img_list ) , n_threads ) : b_imgs_list = img_list [ idx : idx + n_threads ] b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) imgs . extend ( b_imgs ) if printable : tl . logging . info ( 'read %d from %s' % ( len ( imgs ) , path ) ) return imgs",Returns all images in list by given path and name of each image file .
"def save_image ( image , image_path = '_temp.png' ) : try : imageio . imwrite ( image_path , image ) except Exception : imageio . imwrite ( image_path , image [ : , : , 0 ] )",Save a image .
"def save_images ( images , size , image_path = '_temp.png' ) : if len ( images . shape ) == 3 : images = images [ : , : , : , np . newaxis ] def merge ( images , size ) : h , w = images . shape [ 1 ] , images . shape [ 2 ] img = np . zeros ( ( h * size [ 0 ] , w * size [ 1 ] , 3 ) , dtype = images . dtype ) for idx , image in enumerate ( images ) : i = idx % size [ 1 ] j = idx // size [ 1 ] img [ j * h : j * h + h , i * w : i * w + w , : ] = image return img def imsave ( images , size , path ) : if np . max ( images ) <= 1 and ( - 1 <= np . min ( images ) < 0 ) : images = ( ( images + 1 ) * 127.5 ) . astype ( np . uint8 ) elif np . max ( images ) <= 1 and np . min ( images ) >= 0 : images = ( images * 255 ) . astype ( np . uint8 ) return imageio . imwrite ( path , merge ( images , size ) ) if len ( images ) > size [ 0 ] * size [ 1 ] : raise AssertionError ( ""number of images should be equal or less than size[0] * size[1] {}"" . format ( len ( images ) ) ) return imsave ( images , size , image_path )",Save multiple images into one single image .
"def draw_boxes_and_labels_to_image ( image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None ) : if len ( coords ) != len ( classes ) : raise AssertionError ( ""number of coordinates and classes are equal"" ) if len ( scores ) > 0 and len ( scores ) != len ( classes ) : raise AssertionError ( ""number of scores and classes are equal"" ) image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) for i , _v in enumerate ( coords ) : if is_center : x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) else : x , y , x2 , y2 = coords [ i ] if is_rescale : x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) cv2 . rectangle ( image , ( int ( x ) , int ( y ) ) , ( int ( x2 ) , int ( y2 ) ) , [ 0 , 255 , 0 ] , thick ) cv2 . putText ( image , classes_list [ classes [ i ] ] + ( ( "" %.2f"" % ( scores [ i ] ) ) if ( len ( scores ) != 0 ) else "" "" ) , ( int ( x ) , int ( y ) ) , 0 , 1.5e-3 * imh , [ 0 , 0 , 256 ] , int ( thick / 2 ) + 1 ) if save_name is not None : save_image ( image , save_name ) return image",Draw bboxes and class labels on image . Return or save the image with bboxes example in the docs of tl . prepro .
"def draw_mpii_pose_to_image ( image , poses , save_name = 'image.png' ) : image = image . copy ( ) imh , imw = image . shape [ 0 : 2 ] thick = int ( ( imh + imw ) // 430 ) radius = int ( thick * 1.5 ) if image . max ( ) < 1 : image = image * 255 for people in poses : joint_pos = people [ 'joint_pos' ] lines = [ [ ( 0 , 1 ) , [ 100 , 255 , 100 ] ] , [ ( 1 , 2 ) , [ 50 , 255 , 50 ] ] , [ ( 2 , 6 ) , [ 0 , 255 , 0 ] ] , [ ( 3 , 4 ) , [ 100 , 100 , 255 ] ] , [ ( 4 , 5 ) , [ 50 , 50 , 255 ] ] , [ ( 6 , 3 ) , [ 0 , 0 , 255 ] ] , [ ( 6 , 7 ) , [ 255 , 255 , 100 ] ] , [ ( 7 , 8 ) , [ 255 , 150 , 50 ] ] , [ ( 8 , 9 ) , [ 255 , 200 , 100 ] ] , [ ( 10 , 11 ) , [ 255 , 100 , 255 ] ] , [ ( 11 , 12 ) , [ 255 , 50 , 255 ] ] , [ ( 12 , 8 ) , [ 255 , 0 , 255 ] ] , [ ( 8 , 13 ) , [ 0 , 255 , 255 ] ] , [ ( 13 , 14 ) , [ 100 , 255 , 255 ] ] , [ ( 14 , 15 ) , [ 200 , 255 , 255 ] ] ] for line in lines : start , end = line [ 0 ] if ( start in joint_pos ) and ( end in joint_pos ) : cv2 . line ( image , ( int ( joint_pos [ start ] [ 0 ] ) , int ( joint_pos [ start ] [ 1 ] ) ) , ( int ( joint_pos [ end ] [ 0 ] ) , int ( joint_pos [ end ] [ 1 ] ) ) , line [ 1 ] , thick ) for pos in joint_pos . items ( ) : _ , pos_loc = pos pos_loc = ( int ( pos_loc [ 0 ] ) , int ( pos_loc [ 1 ] ) ) cv2 . circle ( image , center = pos_loc , radius = radius , color = ( 200 , 200 , 200 ) , thickness = - 1 ) head_rect = people [ 'head_rect' ] if head_rect : cv2 . rectangle ( image , ( int ( head_rect [ 0 ] ) , int ( head_rect [ 1 ] ) ) , ( int ( head_rect [ 2 ] ) , int ( head_rect [ 3 ] ) ) , [ 0 , 180 , 0 ] , thick ) if save_name is not None : save_image ( image , save_name ) return image",Draw people ( s ) into image using MPII dataset format as input return or save the result image .
"def frame ( I = None , second = 5 , saveable = True , name = 'frame' , cmap = None , fig_idx = 12836 ) : import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) plt . figure ( fig_idx ) if len ( I . shape ) and I . shape [ - 1 ] == 1 : I = I [ : , : , 0 ] plt . imshow ( I , cmap ) plt . title ( name ) if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )",Display a frame . Make sure OpenAI Gym render () is disable before using it .
"def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : import matplotlib . pyplot as plt n_mask = CNN . shape [ 3 ] n_row = CNN . shape [ 0 ] n_col = CNN . shape [ 1 ] n_color = CNN . shape [ 2 ] row = int ( np . sqrt ( n_mask ) ) col = int ( np . ceil ( n_mask / row ) ) plt . ion ( ) fig = plt . figure ( fig_idx ) count = 1 for _ir in range ( 1 , row + 1 ) : for _ic in range ( 1 , col + 1 ) : if count > n_mask : break fig . add_subplot ( col , row , count ) if n_color == 1 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = ""nearest"" ) elif n_color == 3 : plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = ""nearest"" ) else : raise Exception ( ""Unknown n_color"" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )",Display a group of RGB or Greyscale CNN masks .
"def tsne_embedding ( embeddings , reverse_dictionary , plot_only = 500 , second = 5 , saveable = False , name = 'tsne' , fig_idx = 9862 ) : import matplotlib . pyplot as plt def plot_with_labels ( low_dim_embs , labels , figsize = ( 18 , 18 ) , second = 5 , saveable = True , name = 'tsne' , fig_idx = 9862 ) : if low_dim_embs . shape [ 0 ] < len ( labels ) : raise AssertionError ( ""More labels than embeddings"" ) if saveable is False : plt . ion ( ) plt . figure ( fig_idx ) plt . figure ( figsize = figsize ) for i , label in enumerate ( labels ) : x , y = low_dim_embs [ i , : ] plt . scatter ( x , y ) plt . annotate ( label , xy = ( x , y ) , xytext = ( 5 , 2 ) , textcoords = 'offset points' , ha = 'right' , va = 'bottom' ) if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second ) try : from sklearn . manifold import TSNE from six . moves import xrange tsne = TSNE ( perplexity = 30 , n_components = 2 , init = 'pca' , n_iter = 5000 ) low_dim_embs = tsne . fit_transform ( embeddings [ : plot_only , : ] ) labels = [ reverse_dictionary [ i ] for i in xrange ( plot_only ) ] plot_with_labels ( low_dim_embs , labels , second = second , saveable = saveable , name = name , fig_idx = fig_idx ) except ImportError : _err = ""Please install sklearn and matplotlib to visualize embeddings."" tl . logging . error ( _err ) raise ImportError ( _err )",Visualize the embeddings by using t - SNE .
"def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : if shape is None : shape = [ 28 , 28 ] import matplotlib . pyplot as plt if saveable is False : plt . ion ( ) fig = plt . figure ( fig_idx ) n_units = W . shape [ 1 ] num_r = int ( np . sqrt ( n_units ) ) num_c = int ( np . ceil ( n_units / num_r ) ) count = int ( 1 ) for _row in range ( 1 , num_r + 1 ) : for _col in range ( 1 , num_c + 1 ) : if count > n_units : break fig . add_subplot ( num_r , num_c , count ) feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) plt . imshow ( np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = ""nearest"" ) plt . gca ( ) . xaxis . set_major_locator ( plt . NullLocator ( ) ) plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) count = count + 1 if saveable : plt . savefig ( name + '.pdf' , format = 'pdf' ) else : plt . draw ( ) plt . pause ( second )",Visualize every columns of the weight matrix to a group of Greyscale img .
"def data_to_tfrecord ( images , labels , filename ) : if os . path . isfile ( filename ) : print ( ""%s exists"" % filename ) return print ( ""Converting data into %s ..."" % filename ) writer = tf . python_io . TFRecordWriter ( filename ) for index , img in enumerate ( images ) : img_raw = img . tobytes ( ) label = int ( labels [ index ] ) example = tf . train . Example ( features = tf . train . Features ( feature = { ""label"" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , } ) ) writer . write ( example . SerializeToString ( ) ) writer . close ( )",Save data into TFRecord .
"def read_and_decode ( filename , is_train = None ) : filename_queue = tf . train . string_input_producer ( [ filename ] ) reader = tf . TFRecordReader ( ) _ , serialized_example = reader . read ( filename_queue ) features = tf . parse_single_example ( serialized_example , features = { 'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , } ) img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) img = tf . reshape ( img , [ 32 , 32 , 3 ] ) if is_train == True : img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) img = tf . image . random_flip_left_right ( img ) img = tf . image . random_brightness ( img , max_delta = 63 ) img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) img = tf . image . per_image_standardization ( img ) elif is_train == False : img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) img = tf . image . per_image_standardization ( img ) elif is_train == None : img = img label = tf . cast ( features [ 'label' ] , tf . int32 ) return img , label",Return tensor to read from TFRecord .
"def print_params ( self , details = True , session = None ) : for i , p in enumerate ( self . all_params ) : if details : try : val = p . eval ( session = session ) logging . info ( ""  param {:3}: {:20} {:15}    {} (mean: {:<18}, median: {:<18}, std: {:<18})   "" . format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) ) except Exception as e : logging . info ( str ( e ) ) raise Exception ( ""Hint: print params details after tl.layers.initialize_global_variables(sess) "" ""or use network.print_params(False)."" ) else : logging . info ( ""  param {:3}: {:20} {:15}    {}"" . format ( i , p . name , str ( p . get_shape ( ) ) , p . dtype . name ) ) logging . info ( ""  num of params: %d"" % self . count_params ( ) )",Print all info of parameters in the network
"def print_layers ( self ) : for i , layer in enumerate ( self . all_layers ) : logging . info ( ""  layer {:3}: {:20} {:15}    {}"" . format ( i , layer . name , str ( layer . get_shape ( ) ) , layer . dtype . name ) )",Print all info of layers in the network .
"def count_params ( self ) : n_params = 0 for _i , p in enumerate ( self . all_params ) : n = 1 for s in p . get_shape ( ) : try : s = int ( s ) except Exception : s = 1 if s : n = n * s n_params = n_params + n return n_params",Returns the number of parameters in the network .
"def get_all_params ( self , session = None ) : _params = [ ] for p in self . all_params : if session is None : _params . append ( p . eval ( ) ) else : _params . append ( session . run ( p ) ) return _params",Return the parameters in a list of array .
"def _get_init_args ( self , skip = 4 ) : stack = inspect . stack ( ) if len ( stack ) < skip + 1 : raise ValueError ( ""The length of the inspection stack is shorter than the requested start position."" ) args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) params = { } for arg in args : if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : val = values [ arg ] if inspect . isfunction ( val ) : params [ arg ] = { ""module_path"" : val . __module__ , ""func_name"" : val . __name__ } elif arg . endswith ( 'init' ) : continue else : params [ arg ] = val return params",Get all arguments of current layer for saving the graph .
"def roi_pooling ( input , rois , pool_height , pool_width ) : out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) output , argmax_output = out [ 0 ] , out [ 1 ] return output",returns a tensorflow operation for computing the Region of Interest Pooling
def _int64_feature ( value ) : return tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ value ] ) ),Wrapper for inserting an int64 Feature into a SequenceExample proto e . g An integer label .
def _bytes_feature ( value ) : return tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ value ] ) ),Wrapper for inserting a bytes Feature into a SequenceExample proto e . g an image in byte
def _int64_feature_list ( values ) : return tf . train . FeatureList ( feature = [ _int64_feature ( v ) for v in values ] ),Wrapper for inserting an int64 FeatureList into a SequenceExample proto e . g sentence in list of ints
def _bytes_feature_list ( values ) : return tf . train . FeatureList ( feature = [ _bytes_feature ( v ) for v in values ] ),Wrapper for inserting a bytes FeatureList into a SequenceExample proto e . g sentence in list of bytes
"def distort_image ( image , thread_id ) : with tf . name_scope ( ""flip_horizontal"" ) : image = tf . image . random_flip_left_right ( image ) color_ordering = thread_id % 2 with tf . name_scope ( ""distort_color"" ) : if color_ordering == 0 : image = tf . image . random_brightness ( image , max_delta = 32. / 255. ) image = tf . image . random_saturation ( image , lower = 0.5 , upper = 1.5 ) image = tf . image . random_hue ( image , max_delta = 0.032 ) image = tf . image . random_contrast ( image , lower = 0.5 , upper = 1.5 ) elif color_ordering == 1 : image = tf . image . random_brightness ( image , max_delta = 32. / 255. ) image = tf . image . random_contrast ( image , lower = 0.5 , upper = 1.5 ) image = tf . image . random_saturation ( image , lower = 0.5 , upper = 1.5 ) image = tf . image . random_hue ( image , max_delta = 0.032 ) image = tf . clip_by_value ( image , 0.0 , 1.0 ) return image",Perform random distortions on an image . Args : image : A float32 Tensor of shape [ height width 3 ] with values in [ 0 1 ) . thread_id : Preprocessing thread id used to select the ordering of color distortions . There should be a multiple of 2 preprocessing threads . Returns : distorted_image : A float32 Tensor of shape [ height width 3 ] with values in [ 0 1 ] .
"def prefetch_input_data ( reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , num_reader_threads = 1 , shard_queue_name = ""filename_queue"" , value_queue_name = ""input_queue"" ) : data_files = [ ] for pattern in file_pattern . split ( "","" ) : data_files . extend ( tf . gfile . Glob ( pattern ) ) if not data_files : tl . logging . fatal ( ""Found no input files matching %s"" , file_pattern ) else : tl . logging . info ( ""Prefetching values from %d files matching %s"" , len ( data_files ) , file_pattern ) if is_training : print ( ""   is_training == True : RandomShuffleQueue"" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) min_queue_examples = values_per_shard * input_queue_capacity_factor capacity = min_queue_examples + 100 * batch_size values_queue = tf . RandomShuffleQueue ( capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , name = ""random_"" + value_queue_name ) else : print ( ""   is_training == False : FIFOQueue"" ) filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) capacity = values_per_shard + 3 * batch_size values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = ""fifo_"" + value_queue_name ) enqueue_ops = [ ] for _ in range ( num_reader_threads ) : _ , value = reader . read ( filename_queue ) enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) tf . summary . scalar ( ""queue/%s/fraction_of_%d_full"" % ( values_queue . name , capacity ) , tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) ) return values_queue",Prefetches string values from disk into an input queue .
"def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : enqueue_list = [ ] for image , caption in images_and_captions : caption_length = tf . shape ( caption ) [ 0 ] input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) input_seq = tf . slice ( caption , [ 0 ] , input_length ) target_seq = tf . slice ( caption , [ 1 ] , input_length ) indicator = tf . ones ( input_length , dtype = tf . int32 ) enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) images , input_seqs , target_seqs , mask = tf . train . batch_join ( enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = ""batch_and_pad"" ) if add_summaries : lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) tf . summary . scalar ( ""caption_length/batch_min"" , tf . reduce_min ( lengths ) ) tf . summary . scalar ( ""caption_length/batch_max"" , tf . reduce_max ( lengths ) ) tf . summary . scalar ( ""caption_length/batch_mean"" , tf . reduce_mean ( lengths ) ) return images , input_seqs , target_seqs , mask",Batches input images and captions .
"def _to_channel_first_bias ( b ) : channel_size = int ( b . shape [ 0 ] ) new_shape = ( channel_size , 1 , 1 ) return tf . reshape ( b , new_shape )",Reshape [ c ] to [ c 1 1 ] .
"def _bias_scale ( x , b , data_format ) : if data_format == 'NHWC' : return x * b elif data_format == 'NCHW' : return x * _to_channel_first_bias ( b ) else : raise ValueError ( 'invalid data_format: %s' % data_format )",The multiplication counter part of tf . nn . bias_add .
"def _bias_add ( x , b , data_format ) : if data_format == 'NHWC' : return tf . add ( x , b ) elif data_format == 'NCHW' : return tf . add ( x , _to_channel_first_bias ( b ) ) else : raise ValueError ( 'invalid data_format: %s' % data_format )",Alternative implementation of tf . nn . bias_add which is compatiable with tensorRT .
"def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : inv = math_ops . rsqrt ( variance + variance_epsilon ) if scale is not None : inv *= scale a = math_ops . cast ( inv , x . dtype ) b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] )",Data Format aware version of tf . nn . batch_normalization .
"def compute_alpha ( x ) : threshold = _compute_threshold ( x ) alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) alpha_array_abs = tf . abs ( alpha_array ) alpha_array_abs1 = tf . where ( tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , tf . zeros_like ( alpha_array_abs , tf . float32 ) ) alpha_sum = tf . reduce_sum ( alpha_array_abs ) n = tf . reduce_sum ( alpha_array_abs1 ) alpha = tf . div ( alpha_sum , n ) return alpha",Computing the scale parameter .
"def flatten_reshape ( variable , name = 'flatten' ) : dim = 1 for d in variable . get_shape ( ) [ 1 : ] . as_list ( ) : dim *= d return tf . reshape ( variable , shape = [ - 1 , dim ] , name = name )",Reshapes a high - dimension vector input .
"def get_layers_with_name ( net , name = """" , verbose = False ) : logging . info ( ""  [*] geting layers with %s"" % name ) layers = [ ] i = 0 for layer in net . all_layers : if name in layer . name : layers . append ( layer ) if verbose : logging . info ( ""  got {:3}: {:15}   {}"" . format ( i , layer . name , str ( layer . get_shape ( ) ) ) ) i = i + 1 return layers",Get a list of layers output in a network by a given name scope .
"def get_variables_with_name ( name = None , train_only = True , verbose = False ) : if name is None : raise Exception ( ""please input a name"" ) logging . info ( ""  [*] geting variables with %s"" % name ) if train_only : t_vars = tf . trainable_variables ( ) else : t_vars = tf . global_variables ( ) d_vars = [ var for var in t_vars if name in var . name ] if verbose : for idx , v in enumerate ( d_vars ) : logging . info ( ""  got {:3}: {:15}   {}"" . format ( idx , v . name , str ( v . get_shape ( ) ) ) ) return d_vars",Get a list of TensorFlow variables by a given name scope .
"def initialize_rnn_state ( state , feed_dict = None ) : if isinstance ( state , LSTMStateTuple ) : c = state . c . eval ( feed_dict = feed_dict ) h = state . h . eval ( feed_dict = feed_dict ) return c , h else : new_state = state . eval ( feed_dict = feed_dict ) return new_state",Returns the initialized RNN state . The inputs are LSTMStateTuple or State of RNNCells and an optional feed_dict .
def list_remove_repeat ( x ) : y = [ ] for i in x : if i not in y : y . append ( i ) return y,Remove the repeated items in a list and return the processed list . You may need it to create merged layer like Concat Elementwise and etc .
"def merge_networks ( layers = None ) : if layers is None : raise Exception ( ""layers should be a list of TensorLayer's Layers."" ) layer = layers [ 0 ] all_params = [ ] all_layers = [ ] all_drop = { } for l in layers : all_params . extend ( l . all_params ) all_layers . extend ( l . all_layers ) all_drop . update ( l . all_drop ) layer . all_params = list ( all_params ) layer . all_layers = list ( all_layers ) layer . all_drop = dict ( all_drop ) layer . all_layers = list_remove_repeat ( layer . all_layers ) layer . all_params = list_remove_repeat ( layer . all_params ) return layer",Merge all parameters layers and dropout probabilities to a : class : Layer . The output of return network is the first network in the list .
"def print_all_variables ( train_only = False ) : if train_only : t_vars = tf . trainable_variables ( ) logging . info ( ""  [*] printing trainable variables"" ) else : t_vars = tf . global_variables ( ) logging . info ( ""  [*] printing global variables"" ) for idx , v in enumerate ( t_vars ) : logging . info ( ""  var {:3}: {:15}   {}"" . format ( idx , str ( v . get_shape ( ) ) , v . name ) )",Print information of trainable or all variables without tl . layers . initialize_global_variables ( sess ) .
"def ternary_operation ( x ) : g = tf . get_default_graph ( ) with g . gradient_override_map ( { ""Sign"" : ""Identity"" } ) : threshold = _compute_threshold ( x ) x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) return x",Ternary operation use threshold computed with weights .
"def _compute_threshold ( x ) : x_sum = tf . reduce_sum ( tf . abs ( x ) , reduction_indices = None , keepdims = False , name = None ) threshold = tf . div ( x_sum , tf . cast ( tf . size ( x ) , tf . float32 ) , name = None ) threshold = tf . multiply ( 0.7 , threshold , name = None ) return threshold",ref : https : // github . com / XJTUWYD / TWN Computing the threshold .
"def freeze_graph ( graph_path , checkpoint_path , output_path , end_node_names , is_binary_graph ) : _freeze_graph ( input_graph = graph_path , input_saver = '' , input_binary = is_binary_graph , input_checkpoint = checkpoint_path , output_graph = output_path , output_node_names = end_node_names , restore_op_name = 'save/restore_all' , filename_tensor_name = 'save/Const:0' , clear_devices = True , initializer_nodes = None )",Reimplementation of the TensorFlow official freeze_graph function to freeze the graph and checkpoint together :
"def convert_model_to_onnx ( frozen_graph_path , end_node_names , onnx_output_path ) : with tf . gfile . GFile ( frozen_graph_path , ""rb"" ) as f : graph_def = tf . GraphDef ( ) graph_def . ParseFromString ( f . read ( ) ) onnx_model = tensorflow_graph_to_onnx_model ( graph_def , end_node_names , opset = 6 ) file = open ( onnx_output_path , ""wb"" ) file . write ( onnx_model . SerializeToString ( ) ) file . close ( )",Reimplementation of the TensorFlow - onnx official tutorial convert the proto buff to onnx file :
"def convert_onnx_to_model ( onnx_input_path ) : model = onnx . load ( onnx_input_path ) tf_rep = prepare ( model ) img = np . load ( ""./assets/image.npz"" ) output = tf_rep . run ( img . reshape ( [ 1 , 784 ] ) ) print ( ""The digit is classified as "" , np . argmax ( output ) )",Reimplementation of the TensorFlow - onnx official tutorial convert the onnx file to specific : model
"def _add_deprecated_function_notice_to_docstring ( doc , date , instructions ) : if instructions : deprecation_message = """"""
            .. warning::
                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.
                *Instructions for updating:* %s.
        """""" % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) , instructions ) else : deprecation_message = """"""
            .. warning::
                **THIS FUNCTION IS DEPRECATED:** It will be removed after %s.
        """""" % ( ( 'in a future version' if date is None else ( 'after %s' % date ) ) ) main_text = [ deprecation_message ] return _add_notice_to_docstring ( doc , 'DEPRECATED FUNCTION' , main_text )",Adds a deprecation notice to a docstring for deprecated functions .
"def _add_notice_to_docstring ( doc , no_doc_str , notice ) : if not doc : lines = [ no_doc_str ] else : lines = _normalize_docstring ( doc ) . splitlines ( ) notice = [ '' ] + notice if len ( lines ) > 1 : if lines [ 1 ] . strip ( ) : notice . append ( '' ) lines [ 1 : 1 ] = notice else : lines += notice return '\n' . join ( lines )",Adds a deprecation notice to a docstring .
"def alphas ( shape , alpha_value , name = None ) : with ops . name_scope ( name , ""alphas"" , [ shape ] ) as name : alpha_tensor = convert_to_tensor ( alpha_value ) alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype if not isinstance ( shape , ops . Tensor ) : try : shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) except ( TypeError , ValueError ) : shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) if not shape . _shape_tuple ( ) : shape = reshape ( shape , [ - 1 ] ) try : output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) except ( TypeError , ValueError ) : output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) if output . dtype . base_dtype != alpha_dtype : raise AssertionError ( ""Dtypes do not corresponds: %s and %s"" % ( output . dtype . base_dtype , alpha_dtype ) ) return output",Creates a tensor with all elements set to alpha_value . This operation returns a tensor of type dtype with shape shape and all elements set to alpha .
"def alphas_like ( tensor , alpha_value , name = None , optimize = True ) : with ops . name_scope ( name , ""alphas_like"" , [ tensor ] ) as name : tensor = ops . convert_to_tensor ( tensor , name = ""tensor"" ) if context . in_eager_mode ( ) : ret = alphas ( shape_internal ( tensor , optimize = optimize ) , alpha_value = alpha_value , name = name ) else : if ( optimize and tensor . shape . is_fully_defined ( ) ) : ret = alphas ( tensor . shape , alpha_value = alpha_value , name = name ) else : ret = alphas ( shape_internal ( tensor , optimize = optimize ) , alpha_value = alpha_value , name = name ) ret . set_shape ( tensor . get_shape ( ) ) return ret",Creates a tensor with all elements set to alpha_value . Given a single tensor ( tensor ) this operation returns a tensor of the same type and shape as tensor with all elements set to alpha_value .
"def example1 ( ) : st = time . time ( ) for _ in range ( 100 ) : xx = tl . prepro . rotation ( image , rg = - 20 , is_random = False ) xx = tl . prepro . flip_axis ( xx , axis = 1 , is_random = False ) xx = tl . prepro . shear2 ( xx , shear = ( 0. , - 0.2 ) , is_random = False ) xx = tl . prepro . zoom ( xx , zoom_range = 1 / 0.8 ) xx = tl . prepro . shift ( xx , wrg = - 0.1 , hrg = 0 , is_random = False ) print ( ""apply transforms one-by-one took %fs for each image"" % ( ( time . time ( ) - st ) / 100 ) ) tl . vis . save_image ( xx , '_result_slow.png' )",Example 1 : Applying transformation one - by - one is very SLOW !
"def example2 ( ) : st = time . time ( ) for _ in range ( 100 ) : transform_matrix = create_transformation_matrix ( ) result = tl . prepro . affine_transform_cv2 ( image , transform_matrix ) print ( ""apply all transforms once took %fs for each image"" % ( ( time . time ( ) - st ) / 100 ) ) tl . vis . save_image ( result , '_result_fast.png' )",Example 2 : Applying all transforms in one is very FAST !
"def example3 ( ) : n_data = 100 imgs_file_list = [ 'tiger.jpeg' ] * n_data train_targets = [ np . ones ( 1 ) ] * n_data def generator ( ) : if len ( imgs_file_list ) != len ( train_targets ) : raise RuntimeError ( 'len(imgs_file_list) != len(train_targets)' ) for _input , _target in zip ( imgs_file_list , train_targets ) : yield _input , _target def _data_aug_fn ( image ) : transform_matrix = create_transformation_matrix ( ) result = tl . prepro . affine_transform_cv2 ( image , transform_matrix ) return result def _map_fn ( image_path , target ) : image = tf . read_file ( image_path ) image = tf . image . decode_jpeg ( image , channels = 3 ) image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) image = tf . py_func ( _data_aug_fn , [ image ] , [ tf . float32 ] ) target = tf . reshape ( target , ( ) ) return image , target n_epoch = 10 batch_size = 5 dataset = tf . data . Dataset ( ) . from_generator ( generator , output_types = ( tf . string , tf . int64 ) ) dataset = dataset . shuffle ( buffer_size = 4096 ) dataset = dataset . repeat ( n_epoch ) dataset = dataset . map ( _map_fn , num_parallel_calls = multiprocessing . cpu_count ( ) ) dataset = dataset . batch ( batch_size ) dataset = dataset . prefetch ( 1 ) iterator = dataset . make_one_shot_iterator ( ) one_element = iterator . get_next ( ) sess = tf . Session ( ) n_step = round ( n_epoch * n_data / batch_size ) st = time . time ( ) for _ in range ( n_step ) : _images , _targets = sess . run ( one_element ) print ( ""dataset APIs took %fs for each image"" % ( ( time . time ( ) - st ) / batch_size / n_step ) )",Example 3 : Using TF dataset API to load and process image for training
"def example4 ( ) : transform_matrix = create_transformation_matrix ( ) result = tl . prepro . affine_transform_cv2 ( image , transform_matrix ) coords = [ [ ( 50 , 100 ) , ( 100 , 100 ) , ( 100 , 50 ) , ( 200 , 200 ) ] , [ ( 250 , 50 ) , ( 200 , 50 ) , ( 200 , 100 ) ] ] coords_result = tl . prepro . affine_transform_keypoints ( coords , transform_matrix ) def imwrite ( image , coords_list , name ) : coords_list_ = [ ] for coords in coords_list : coords = np . array ( coords , np . int32 ) coords = coords . reshape ( ( - 1 , 1 , 2 ) ) coords_list_ . append ( coords ) image = cv2 . polylines ( image , coords_list_ , True , ( 0 , 255 , 255 ) , 3 ) cv2 . imwrite ( name , image [ ... , : : - 1 ] ) imwrite ( image , coords , '_with_keypoints_origin.png' ) imwrite ( result , coords_result , '_with_keypoints_result.png' )",Example 4 : Transforming coordinates using affine matrix .
"def distort_fn ( x , is_train = False ) : x = tl . prepro . crop ( x , 24 , 24 , is_random = is_train ) if is_train : x = tl . prepro . flip_axis ( x , axis = 1 , is_random = True ) x = tl . prepro . brightness ( x , gamma = 0.1 , gain = 1 , is_random = True ) x = ( x - np . mean ( x ) ) / max ( np . std ( x ) , 1e-5 ) return x",The images are processed as follows : .. They are cropped to 24 x 24 pixels centrally for evaluation or randomly for training . .. They are approximately whitened to make the model insensitive to dynamic range . For training we additionally apply a series of random distortions to artificially increase the data set size : .. Randomly flip the image from left to right . .. Randomly distort the image brightness .
"def fit ( sess , network , train_op , cost , X_train , y_train , x , y_ , acc = None , batch_size = 100 , n_epoch = 100 , print_freq = 5 , X_val = None , y_val = None , eval_train = True , tensorboard_dir = None , tensorboard_epoch_freq = 5 , tensorboard_weight_histograms = True , tensorboard_graph_vis = True ) : if X_train . shape [ 0 ] < batch_size : raise AssertionError ( ""Number of training examples should be bigger than the batch size"" ) if tensorboard_dir is not None : tl . logging . info ( ""Setting up tensorboard ..."" ) tl . files . exists_or_mkdir ( tensorboard_dir ) if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'FileWriter' ) : if tensorboard_graph_vis : train_writer = tf . summary . FileWriter ( tensorboard_dir + '/train' , sess . graph ) val_writer = tf . summary . FileWriter ( tensorboard_dir + '/validation' , sess . graph ) else : train_writer = tf . summary . FileWriter ( tensorboard_dir + '/train' ) val_writer = tf . summary . FileWriter ( tensorboard_dir + '/validation' ) if ( tensorboard_weight_histograms ) : for param in network . all_params : if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'histogram' ) : tl . logging . info ( 'Param name %s' % param . name ) tf . summary . histogram ( param . name , param ) if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'histogram' ) : tf . summary . scalar ( 'cost' , cost ) merged = tf . summary . merge_all ( ) tl . layers . initialize_global_variables ( sess ) tl . logging . info ( ""Finished! use `tensorboard --logdir=%s/` to start tensorboard"" % tensorboard_dir ) tl . logging . info ( ""Start training the network ..."" ) start_time_begin = time . time ( ) tensorboard_train_index , tensorboard_val_index = 0 , 0 for epoch in range ( n_epoch ) : start_time = time . time ( ) loss_ep = 0 n_step = 0 for X_train_a , y_train_a in tl . iterate . minibatches ( X_train , y_train , batch_size , shuffle = True ) : feed_dict = { x : X_train_a , y_ : y_train_a } feed_dict . update ( network . all_drop ) loss , _ = sess . run ( [ cost , train_op ] , feed_dict = feed_dict ) loss_ep += loss n_step += 1 loss_ep = loss_ep / n_step if tensorboard_dir is not None and hasattr ( tf , 'summary' ) : if epoch + 1 == 1 or ( epoch + 1 ) % tensorboard_epoch_freq == 0 : for X_train_a , y_train_a in tl . iterate . minibatches ( X_train , y_train , batch_size , shuffle = True ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_train_a , y_ : y_train_a } feed_dict . update ( dp_dict ) result = sess . run ( merged , feed_dict = feed_dict ) train_writer . add_summary ( result , tensorboard_train_index ) tensorboard_train_index += 1 if ( X_val is not None ) and ( y_val is not None ) : for X_val_a , y_val_a in tl . iterate . minibatches ( X_val , y_val , batch_size , shuffle = True ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_val_a , y_ : y_val_a } feed_dict . update ( dp_dict ) result = sess . run ( merged , feed_dict = feed_dict ) val_writer . add_summary ( result , tensorboard_val_index ) tensorboard_val_index += 1 if epoch + 1 == 1 or ( epoch + 1 ) % print_freq == 0 : if ( X_val is not None ) and ( y_val is not None ) : tl . logging . info ( ""Epoch %d of %d took %fs"" % ( epoch + 1 , n_epoch , time . time ( ) - start_time ) ) if eval_train is True : train_loss , train_acc , n_batch = 0 , 0 , 0 for X_train_a , y_train_a in tl . iterate . minibatches ( X_train , y_train , batch_size , shuffle = True ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_train_a , y_ : y_train_a } feed_dict . update ( dp_dict ) if acc is not None : err , ac = sess . run ( [ cost , acc ] , feed_dict = feed_dict ) train_acc += ac else : err = sess . run ( cost , feed_dict = feed_dict ) train_loss += err n_batch += 1 tl . logging . info ( ""   train loss: %f"" % ( train_loss / n_batch ) ) if acc is not None : tl . logging . info ( ""   train acc: %f"" % ( train_acc / n_batch ) ) val_loss , val_acc , n_batch = 0 , 0 , 0 for X_val_a , y_val_a in tl . iterate . minibatches ( X_val , y_val , batch_size , shuffle = True ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_val_a , y_ : y_val_a } feed_dict . update ( dp_dict ) if acc is not None : err , ac = sess . run ( [ cost , acc ] , feed_dict = feed_dict ) val_acc += ac else : err = sess . run ( cost , feed_dict = feed_dict ) val_loss += err n_batch += 1 tl . logging . info ( ""   val loss: %f"" % ( val_loss / n_batch ) ) if acc is not None : tl . logging . info ( ""   val acc: %f"" % ( val_acc / n_batch ) ) else : tl . logging . info ( ""Epoch %d of %d took %fs, loss %f"" % ( epoch + 1 , n_epoch , time . time ( ) - start_time , loss_ep ) ) tl . logging . info ( ""Total training time: %fs"" % ( time . time ( ) - start_time_begin ) )",Training a given non time - series network by the given cost function training data batch_size n_epoch etc .
"def predict ( sess , network , X , x , y_op , batch_size = None ) : if batch_size is None : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X , } feed_dict . update ( dp_dict ) return sess . run ( y_op , feed_dict = feed_dict ) else : result = None for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X_a , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) if result is None : result = result_a else : result = np . concatenate ( ( result , result_a ) ) if result is None : if len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = result_a else : if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : dp_dict = dict_to_one ( network . all_drop ) feed_dict = { x : X [ - ( len ( X ) % batch_size ) : , : ] , } feed_dict . update ( dp_dict ) result_a = sess . run ( y_op , feed_dict = feed_dict ) result = np . concatenate ( ( result , result_a ) ) return result",Return the predict results of given non time - series network .
"def evaluation ( y_test = None , y_predict = None , n_classes = None ) : c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) acc = accuracy_score ( y_test , y_predict ) tl . logging . info ( 'confusion matrix: \n%s' % c_mat ) tl . logging . info ( 'f1-score        : %s' % f1 ) tl . logging . info ( 'f1-score(macro) : %f' % f1_macro ) tl . logging . info ( 'accuracy-score  : %f' % acc ) return c_mat , f1 , acc , f1_macro",Input the predicted results targets results and the number of class return the confusion matrix F1 - score of each class accuracy and macro F1 - score .
"def class_balancing_oversample ( X_train = None , y_train = None , printable = True ) : if printable : tl . logging . info ( ""Classes balancing for training examples..."" ) c = Counter ( y_train ) if printable : tl . logging . info ( 'the occurrence number of each stage: %s' % c . most_common ( ) ) tl . logging . info ( 'the least stage is Label %s have %s instances' % c . most_common ( ) [ - 1 ] ) tl . logging . info ( 'the most stage is  Label %s have %s instances' % c . most_common ( 1 ) [ 0 ] ) most_num = c . most_common ( 1 ) [ 0 ] [ 1 ] if printable : tl . logging . info ( 'most num is %d, all classes tend to be this num' % most_num ) locations = { } number = { } for lab , num in c . most_common ( ) : number [ lab ] = num locations [ lab ] = np . where ( np . array ( y_train ) == lab ) [ 0 ] if printable : tl . logging . info ( 'convert list(np.array) to dict format' ) X = { } for lab , num in number . items ( ) : X [ lab ] = X_train [ locations [ lab ] ] if printable : tl . logging . info ( 'start oversampling' ) for key in X : temp = X [ key ] while True : if len ( X [ key ] ) >= most_num : break X [ key ] = np . vstack ( ( X [ key ] , temp ) ) if printable : tl . logging . info ( 'first features of label 0 > %d' % len ( X [ 0 ] [ 0 ] ) ) tl . logging . info ( 'the occurrence num of each stage after oversampling' ) for key in X : tl . logging . info ( ""%s %d"" % ( key , len ( X [ key ] ) ) ) if printable : tl . logging . info ( 'make each stage have same num of instances' ) for key in X : X [ key ] = X [ key ] [ 0 : most_num , : ] tl . logging . info ( ""%s %d"" % ( key , len ( X [ key ] ) ) ) if printable : tl . logging . info ( 'convert from dict to list format' ) y_train = [ ] X_train = np . empty ( shape = ( 0 , len ( X [ 0 ] [ 0 ] ) ) ) for key in X : X_train = np . vstack ( ( X_train , X [ key ] ) ) y_train . extend ( [ key for i in range ( len ( X [ key ] ) ) ] ) c = Counter ( y_train ) if printable : tl . logging . info ( 'the occurrence number of each stage after oversampling: %s' % c . most_common ( ) ) return X_train , y_train",Input the features and labels return the features and labels after oversampling .
"def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : rnd = random . Random ( ) if seed : rnd = random . Random ( seed ) return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ]",Return a list of random integer by the given range and quantity .
"def list_string_to_dict ( string ) : dictionary = { } for idx , c in enumerate ( string ) : dictionary . update ( { c : idx } ) return dictionary",Inputs [ a b c ] returns { a : 0 b : 1 c : 2 } .
"def exit_tensorflow ( sess = None , port = 6006 ) : text = ""[TL] Close tensorboard and nvidia-process if available"" text2 = ""[TL] Close tensorboard and nvidia-process not yet supported by this function (tl.ops.exit_tf) on "" if sess is not None : sess . close ( ) if _platform == ""linux"" or _platform == ""linux2"" : tl . logging . info ( 'linux: %s' % text ) os . system ( 'nvidia-smi' ) os . system ( 'fuser ' + port + '/tcp -k' ) os . system ( ""nvidia-smi | grep python |awk '{print $3}'|xargs kill"" ) _exit ( ) elif _platform == ""darwin"" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( ""lsof -i tcp:"" + str ( port ) + ""  | grep -v PID | awk '{print $2}' | xargs kill"" , shell = True ) elif _platform == ""win32"" : raise NotImplementedError ( ""this function is not supported on the Windows platform"" ) else : tl . logging . info ( text2 + _platform )",Close TensorFlow session TensorBoard and Nvidia - process if available .
"def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : text = ""[TL] Open tensorboard, go to localhost:"" + str ( port ) + "" to access"" text2 = "" not yet supported by this function (tl.ops.open_tb)"" if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : tl . logging . info ( ""[TL] Log reportory was created at %s"" % log_dir ) if _platform == ""linux"" or _platform == ""linux2"" : raise NotImplementedError ( ) elif _platform == ""darwin"" : tl . logging . info ( 'OS X: %s' % text ) subprocess . Popen ( sys . prefix + "" | python -m tensorflow.tensorboard --logdir="" + log_dir + "" --port="" + str ( port ) , shell = True ) elif _platform == ""win32"" : raise NotImplementedError ( ""this function is not supported on the Windows platform"" ) else : tl . logging . info ( _platform + text2 )",Open Tensorboard .
"def clear_all_placeholder_variables ( printable = True ) : tl . logging . info ( 'clear all .....................................' ) gl = globals ( ) . copy ( ) for var in gl : if var [ 0 ] == '_' : continue if 'func' in str ( globals ( ) [ var ] ) : continue if 'module' in str ( globals ( ) [ var ] ) : continue if 'class' in str ( globals ( ) [ var ] ) : continue if printable : tl . logging . info ( "" clear_all ------- %s"" % str ( globals ( ) [ var ] ) ) del globals ( ) [ var ]",Clears all the placeholder variables of keep prob including keeping probabilities of all dropout denoising dropconnect etc .
"def set_gpu_fraction ( gpu_fraction = 0.3 ) : tl . logging . info ( ""[TL]: GPU MEM Fraction %f"" % gpu_fraction ) gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) return sess",Set the GPU memory fraction for the application .
"def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : if batch_size % num_skips != 0 : raise Exception ( ""batch_size should be able to be divided by num_skips."" ) if num_skips > 2 * skip_window : raise Exception ( ""num_skips <= 2 * skip_window"" ) batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) span = 2 * skip_window + 1 buffer = collections . deque ( maxlen = span ) for _ in range ( span ) : buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) for i in range ( batch_size // num_skips ) : target = skip_window targets_to_avoid = [ skip_window ] for j in range ( num_skips ) : while target in targets_to_avoid : target = random . randint ( 0 , span - 1 ) targets_to_avoid . append ( target ) batch [ i * num_skips + j ] = buffer [ skip_window ] labels [ i * num_skips + j , 0 ] = buffer [ target ] buffer . append ( data [ data_index ] ) data_index = ( data_index + 1 ) % len ( data ) return batch , labels , data_index",Generate a training batch for the Skip - Gram model .
"def sample ( a = None , temperature = 1.0 ) : if a is None : raise Exception ( ""a : list of float"" ) b = np . copy ( a ) try : if temperature == 1 : return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) if temperature is None : return np . argmax ( a ) else : a = np . log ( a ) / temperature a = np . exp ( a ) / np . sum ( np . exp ( a ) ) return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) except Exception : message = ""For large vocabulary_size, choice a higher temperature\
         to avoid log error. Hint : use ``sample_top``. "" warnings . warn ( message , Warning ) return np . argmax ( np . random . multinomial ( 1 , b , 1 ) )",Sample an index from a probability array .
"def sample_top ( a = None , top_k = 10 ) : if a is None : a = [ ] idx = np . argpartition ( a , - top_k ) [ - top_k : ] probs = a [ idx ] probs = probs / np . sum ( probs ) choice = np . random . choice ( idx , p = probs ) return choice",Sample from top_k probabilities .
"def process_sentence ( sentence , start_word = ""<S>"" , end_word = ""</S>"" ) : if start_word is not None : process_sentence = [ start_word ] else : process_sentence = [ ] process_sentence . extend ( nltk . tokenize . word_tokenize ( sentence . lower ( ) ) ) if end_word is not None : process_sentence . append ( end_word ) return process_sentence",Seperate a sentence string into a list of string words add start_word and end_word see create_vocab () and tutorial_tfrecord3 . py .
"def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : tl . logging . info ( ""Creating vocabulary."" ) counter = Counter ( ) for c in sentences : counter . update ( c ) tl . logging . info ( ""    Total words: %d"" % len ( counter ) ) word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) word_counts = [ ( ""<PAD>"" , 0 ) ] + word_counts tl . logging . info ( ""    Words in vocabulary: %d"" % len ( word_counts ) ) with tf . gfile . FastGFile ( word_counts_output_file , ""w"" ) as f : f . write ( ""\n"" . join ( [ ""%s %d"" % ( w , c ) for w , c in word_counts ] ) ) tl . logging . info ( ""    Wrote vocabulary file: %s"" % word_counts_output_file ) reverse_vocab = [ x [ 0 ] for x in word_counts ] unk_id = len ( reverse_vocab ) vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) vocab = SimpleVocabulary ( vocab_dict , unk_id ) return vocab",Creates the vocabulary of word to word_id .
"def read_words ( filename = ""nietzsche.txt"" , replace = None ) : if replace is None : replace = [ '\n' , '<eos>' ] with tf . gfile . GFile ( filename , ""r"" ) as f : try : context_list = f . read ( ) . replace ( * replace ) . split ( ) except Exception : f . seek ( 0 ) replace = [ x . encode ( 'utf-8' ) for x in replace ] context_list = f . read ( ) . replace ( * replace ) . split ( ) return context_list",Read list format context from a file .
"def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : if word2id is None : word2id = { } questions = [ ] questions_skipped = 0 with open ( eval_file , ""rb"" ) as analogy_f : for line in analogy_f : if line . startswith ( b"":"" ) : continue words = line . strip ( ) . lower ( ) . split ( b"" "" ) ids = [ word2id . get ( w . strip ( ) ) for w in words ] if None in ids or len ( ids ) != 4 : questions_skipped += 1 else : questions . append ( np . array ( ids ) ) tl . logging . info ( ""Eval analogy file: %s"" % eval_file ) tl . logging . info ( ""Questions: %d"" , len ( questions ) ) tl . logging . info ( ""Skipped: %d"" , questions_skipped ) analogy_questions = np . array ( questions , dtype = np . int32 ) return analogy_questions",Reads through an analogy question file return its id format .
"def build_reverse_dictionary ( word_to_id ) : reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) return reverse_dictionary",Given a dictionary that maps word to integer id . Returns a reverse dictionary that maps a id to word .
"def build_words_dataset ( words = None , vocabulary_size = 50000 , printable = True , unk_key = 'UNK' ) : if words is None : raise Exception ( ""words : list of str or byte"" ) count = [ [ unk_key , - 1 ] ] count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 ) ) dictionary = dict ( ) for word , _ in count : dictionary [ word ] = len ( dictionary ) data = list ( ) unk_count = 0 for word in words : if word in dictionary : index = dictionary [ word ] else : index = 0 unk_count += 1 data . append ( index ) count [ 0 ] [ 1 ] = unk_count reverse_dictionary = dict ( zip ( dictionary . values ( ) , dictionary . keys ( ) ) ) if printable : tl . logging . info ( 'Real vocabulary size    %d' % len ( collections . Counter ( words ) . keys ( ) ) ) tl . logging . info ( 'Limited vocabulary size {}' . format ( vocabulary_size ) ) if len ( collections . Counter ( words ) . keys ( ) ) < vocabulary_size : raise Exception ( ""len(collections.Counter(words).keys()) >= vocabulary_size , the limited vocabulary_size must be less than or equal to the read vocabulary_size"" ) return data , count , dictionary , reverse_dictionary",Build the words dictionary and replace rare words with UNK token . The most common word has the smallest integer id .
"def words_to_word_ids ( data = None , word_to_id = None , unk_key = 'UNK' ) : if data is None : raise Exception ( ""data : list of string or byte"" ) if word_to_id is None : raise Exception ( ""word_to_id : a dictionary"" ) word_ids = [ ] for word in data : if word_to_id . get ( word ) is not None : word_ids . append ( word_to_id [ word ] ) else : word_ids . append ( word_to_id [ unk_key ] ) return word_ids",Convert a list of string ( words ) to IDs .
"def save_vocab ( count = None , name = 'vocab.txt' ) : if count is None : count = [ ] pwd = os . getcwd ( ) vocabulary_size = len ( count ) with open ( os . path . join ( pwd , name ) , ""w"" ) as f : for i in xrange ( vocabulary_size ) : f . write ( ""%s %d\n"" % ( tf . compat . as_text ( count [ i ] [ 0 ] ) , count [ i ] [ 1 ] ) ) tl . logging . info ( ""%d vocab saved to %s in %s"" % ( vocabulary_size , name , pwd ) )",Save the vocabulary to a file so the model can be reloaded .
"def basic_tokenizer ( sentence , _WORD_SPLIT = re . compile ( b""([.,!?\""':;)(])"" ) ) : words = [ ] sentence = tf . compat . as_bytes ( sentence ) for space_separated_fragment in sentence . strip ( ) . split ( ) : words . extend ( re . split ( _WORD_SPLIT , space_separated_fragment ) ) return [ w for w in words if w ]",Very basic tokenizer : split the sentence into a list of tokens .
"def create_vocabulary ( vocabulary_path , data_path , max_vocabulary_size , tokenizer = None , normalize_digits = True , _DIGIT_RE = re . compile ( br""\d"" ) , _START_VOCAB = None ) : if _START_VOCAB is None : _START_VOCAB = [ b""_PAD"" , b""_GO"" , b""_EOS"" , b""_UNK"" ] if not gfile . Exists ( vocabulary_path ) : tl . logging . info ( ""Creating vocabulary %s from data %s"" % ( vocabulary_path , data_path ) ) vocab = { } with gfile . GFile ( data_path , mode = ""rb"" ) as f : counter = 0 for line in f : counter += 1 if counter % 100000 == 0 : tl . logging . info ( ""  processing line %d"" % counter ) tokens = tokenizer ( line ) if tokenizer else basic_tokenizer ( line ) for w in tokens : word = re . sub ( _DIGIT_RE , b""0"" , w ) if normalize_digits else w if word in vocab : vocab [ word ] += 1 else : vocab [ word ] = 1 vocab_list = _START_VOCAB + sorted ( vocab , key = vocab . get , reverse = True ) if len ( vocab_list ) > max_vocabulary_size : vocab_list = vocab_list [ : max_vocabulary_size ] with gfile . GFile ( vocabulary_path , mode = ""wb"" ) as vocab_file : for w in vocab_list : vocab_file . write ( w + b""\n"" ) else : tl . logging . info ( ""Vocabulary %s from data %s exists"" % ( vocabulary_path , data_path ) )",r Create vocabulary file ( if it does not exist yet ) from data file .
"def initialize_vocabulary ( vocabulary_path ) : if gfile . Exists ( vocabulary_path ) : rev_vocab = [ ] with gfile . GFile ( vocabulary_path , mode = ""rb"" ) as f : rev_vocab . extend ( f . readlines ( ) ) rev_vocab = [ tf . compat . as_bytes ( line . strip ( ) ) for line in rev_vocab ] vocab = dict ( [ ( x , y ) for ( y , x ) in enumerate ( rev_vocab ) ] ) return vocab , rev_vocab else : raise ValueError ( ""Vocabulary file %s not found."" , vocabulary_path )",Initialize vocabulary from file return the word_to_id ( dictionary ) and id_to_word ( list ) .
"def sentence_to_token_ids ( sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br""\d"" ) ) : if tokenizer : words = tokenizer ( sentence ) else : words = basic_tokenizer ( sentence ) if not normalize_digits : return [ vocabulary . get ( w , UNK_ID ) for w in words ] return [ vocabulary . get ( re . sub ( _DIGIT_RE , b""0"" , w ) , UNK_ID ) for w in words ]",Convert a string to list of integers representing token - ids .
"def data_to_token_ids ( data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br""\d"" ) ) : if not gfile . Exists ( target_path ) : tl . logging . info ( ""Tokenizing data in %s"" % data_path ) vocab , _ = initialize_vocabulary ( vocabulary_path ) with gfile . GFile ( data_path , mode = ""rb"" ) as data_file : with gfile . GFile ( target_path , mode = ""w"" ) as tokens_file : counter = 0 for line in data_file : counter += 1 if counter % 100000 == 0 : tl . logging . info ( ""  tokenizing line %d"" % counter ) token_ids = sentence_to_token_ids ( line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE ) tokens_file . write ( "" "" . join ( [ str ( tok ) for tok in token_ids ] ) + ""\n"" ) else : tl . logging . info ( ""Target path %s exists"" % target_path )",Tokenize data file and turn into token - ids using given vocabulary file .
"def moses_multi_bleu ( hypotheses , references , lowercase = False ) : if np . size ( hypotheses ) == 0 : return np . float32 ( 0.0 ) try : multi_bleu_path , _ = urllib . request . urlretrieve ( ""https://raw.githubusercontent.com/moses-smt/mosesdecoder/"" ""master/scripts/generic/multi-bleu.perl"" ) os . chmod ( multi_bleu_path , 0o755 ) except Exception : tl . logging . info ( ""Unable to fetch multi-bleu.perl script, using local."" ) metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) bin_dir = os . path . abspath ( os . path . join ( metrics_dir , "".."" , "".."" , ""bin"" ) ) multi_bleu_path = os . path . join ( bin_dir , ""tools/multi-bleu.perl"" ) hypothesis_file = tempfile . NamedTemporaryFile ( ) hypothesis_file . write ( ""\n"" . join ( hypotheses ) . encode ( ""utf-8"" ) ) hypothesis_file . write ( b""\n"" ) hypothesis_file . flush ( ) reference_file = tempfile . NamedTemporaryFile ( ) reference_file . write ( ""\n"" . join ( references ) . encode ( ""utf-8"" ) ) reference_file . write ( b""\n"" ) reference_file . flush ( ) with open ( hypothesis_file . name , ""r"" ) as read_pred : bleu_cmd = [ multi_bleu_path ] if lowercase : bleu_cmd += [ ""-lc"" ] bleu_cmd += [ reference_file . name ] try : bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) bleu_out = bleu_out . decode ( ""utf-8"" ) bleu_score = re . search ( r""BLEU = (.+?),"" , bleu_out ) . group ( 1 ) bleu_score = float ( bleu_score ) except subprocess . CalledProcessError as error : if error . output is not None : tl . logging . warning ( ""multi-bleu.perl script returned non-zero exit code"" ) tl . logging . warning ( error . output ) bleu_score = np . float32 ( 0.0 ) hypothesis_file . close ( ) reference_file . close ( ) return np . float32 ( bleu_score )",Calculate the bleu score for hypotheses and references using the MOSES ulti - bleu . perl script .
"def word_to_id ( self , word ) : if word in self . _vocab : return self . _vocab [ word ] else : return self . _unk_id",Returns the integer id of a word string .
"def word_to_id ( self , word ) : if word in self . vocab : return self . vocab [ word ] else : return self . unk_id",Returns the integer word id of a word string .
"def id_to_word ( self , word_id ) : if word_id >= len ( self . reverse_vocab ) : return self . reverse_vocab [ self . unk_id ] else : return self . reverse_vocab [ word_id ]",Returns the word string of an integer word id .
"def basic_clean_str ( string ) : string = re . sub ( r""\n"" , "" "" , string ) string = re . sub ( r""\'s"" , "" \'s"" , string ) string = re . sub ( r""\’s"",  "" \'s"",  s ring) string = re . sub ( r""\'ve"" , "" have"" , string ) string = re . sub ( r""\’ve"",  "" have"",  s ring) string = re . sub ( r""\'t"" , "" not"" , string ) string = re . sub ( r""\’t"",  "" not"",  s ring) string = re . sub ( r""\'re"" , "" are"" , string ) string = re . sub ( r""\’re"",  "" are"",  s ring) string = re . sub ( r""\'d"" , """" , string ) string = re . sub ( r""\’d"",  "" ,  s ring) string = re . sub ( r""\'ll"" , "" will"" , string ) string = re . sub ( r""\’ll"",  "" will"",  s ring) string = re . sub ( r""\“"",  ""  "",  s ring)    “a”       --> “ a ” string = re . sub ( r""\”"",  ""  "",  s ring) string = re . sub ( r""\"""" , ""  "" , string ) string = re . sub ( r""\'"" , ""  "" , string ) string = re . sub ( r""\’"",  ""  "",  s ring)    they’     --> they ’ string = re . sub ( r""\."" , "" . "" , string ) string = re . sub ( r""\,"" , "" , "" , string ) string = re . sub ( r""\!"" , "" ! "" , string ) string = re . sub ( r""\-"" , ""  "" , string ) string = re . sub ( r""\("" , ""  "" , string ) string = re . sub ( r""\)"" , ""  "" , string ) string = re . sub ( r""\]"" , ""  "" , string ) string = re . sub ( r""\["" , ""  "" , string ) string = re . sub ( r""\?"" , ""  "" , string ) string = re . sub ( r""\>"" , ""  "" , string ) string = re . sub ( r""\<"" , ""  "" , string ) string = re . sub ( r""\="" , ""  "" , string ) string = re . sub ( r""\;"" , ""  "" , string ) string = re . sub ( r""\;"" , ""  "" , string ) string = re . sub ( r""\:"" , ""  "" , string ) string = re . sub ( r""\"""" , ""  "" , string ) string = re . sub ( r""\$"" , ""  "" , string ) string = re . sub ( r""\_"" , ""  "" , string ) string = re . sub ( r""\s{2,}"" , "" "" , string ) return string . strip ( ) . lower ( )",Tokenization / string cleaning for a datasets .
"def main_restore_embedding_layer ( ) : vocabulary_size = 50000 embedding_size = 128 model_file_name = ""model_word2vec_50k_128"" batch_size = None print ( ""Load existing embedding matrix and dictionaries"" ) all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) data = all_var [ 'data' ] count = all_var [ 'count' ] dictionary = all_var [ 'dictionary' ] reverse_dictionary = all_var [ 'reverse_dictionary' ] tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) del all_var , data , count load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) sess . run ( tf . global_variables_initializer ( ) ) tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) emb_net . print_params ( ) emb_net . print_layers ( ) word = b'hello' word_id = dictionary [ word ] print ( 'word_id:' , word_id ) words = [ b'i' , b'am' , b'tensor' , b'layer' ] word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) print ( 'word_ids:' , word_ids ) print ( 'context:' , context ) vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) print ( 'vector:' , vector . shape ) vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) print ( 'vectors:' , vectors . shape )",How to use Embedding layer and how to convert IDs to vector IDs to words etc .
"def main_lstm_generate_text ( ) : init_scale = 0.1 learning_rate = 1.0 max_grad_norm = 5 sequence_length = 20 hidden_size = 200 max_epoch = 4 max_max_epoch = 100 lr_decay = 0.9 batch_size = 20 top_k_list = [ 1 , 3 , 5 , 10 ] print_length = 30 model_file_name = ""model_generate_text.npz"" words = customized_read_words ( input_fpath = ""data/trump/trump_text.txt"" ) vocab = tl . nlp . create_vocab ( [ words ] , word_counts_output_file = 'vocab.txt' , min_word_count = 1 ) vocab = tl . nlp . Vocabulary ( 'vocab.txt' , unk_word = ""<UNK>"" ) vocab_size = vocab . unk_id + 1 train_data = [ vocab . word_to_id ( word ) for word in words ] seed = ""it is a"" seed = nltk . tokenize . word_tokenize ( seed ) print ( 'seed : %s' % seed ) sess = tf . InteractiveSession ( ) input_data = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) targets = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) input_data_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) def inference ( x , is_train , sequence_length , reuse = None ) : """"""If reuse is True, the inferences use the existing parameters,
        then different inferences share the same parameters.
        """""" print ( ""\nsequence_length: %d, is_train: %s, reuse: %s"" % ( sequence_length , is_train , reuse ) ) rnn_init = tf . random_uniform_initializer ( - init_scale , init_scale ) with tf . variable_scope ( ""model"" , reuse = reuse ) : network = EmbeddingInputlayer ( x , vocab_size , hidden_size , rnn_init , name = 'embedding' ) network = RNNLayer ( network , cell_fn = tf . contrib . rnn . BasicLSTMCell , cell_init_args = { 'forget_bias' : 0.0 , 'state_is_tuple' : True } , n_hidden = hidden_size , initializer = rnn_init , n_steps = sequence_length , return_last = False , return_seq_2d = True , name = 'lstm1' ) lstm1 = network network = DenseLayer ( network , vocab_size , W_init = rnn_init , b_init = rnn_init , act = None , name = 'output' ) return network , lstm1 network , lstm1 = inference ( input_data , is_train = True , sequence_length = sequence_length , reuse = None ) network_test , lstm1_test = inference ( input_data_test , is_train = False , sequence_length = 1 , reuse = True ) y_linear = network_test . outputs y_soft = tf . nn . softmax ( y_linear ) def loss_fn ( outputs , targets , batch_size , sequence_length ) : loss = tf . contrib . legacy_seq2seq . sequence_loss_by_example ( [ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones ( [ batch_size * sequence_length ] ) ] ) cost = tf . reduce_sum ( loss ) / batch_size return cost cost = loss_fn ( network . outputs , targets , batch_size , sequence_length ) with tf . variable_scope ( 'learning_rate' ) : lr = tf . Variable ( 0.0 , trainable = False ) tvars = network . all_params grads , _ = tf . clip_by_global_norm ( tf . gradients ( cost , tvars ) , max_grad_norm ) optimizer = tf . train . GradientDescentOptimizer ( lr ) train_op = optimizer . apply_gradients ( zip ( grads , tvars ) ) sess . run ( tf . global_variables_initializer ( ) ) print ( ""\nStart learning a model to generate text"" ) for i in range ( max_max_epoch ) : new_lr_decay = lr_decay ** max ( i - max_epoch , 0.0 ) sess . run ( tf . assign ( lr , learning_rate * new_lr_decay ) ) print ( ""Epoch: %d/%d Learning rate: %.8f"" % ( i + 1 , max_max_epoch , sess . run ( lr ) ) ) epoch_size = ( ( len ( train_data ) // batch_size ) - 1 ) // sequence_length start_time = time . time ( ) costs = 0.0 iters = 0 state1 = tl . layers . initialize_rnn_state ( lstm1 . initial_state ) for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( train_data , batch_size , sequence_length ) ) : _cost , state1 , _ = sess . run ( [ cost , lstm1 . final_state , train_op ] , feed_dict = { input_data : x , targets : y , lstm1 . initial_state : state1 } ) costs += _cost iters += sequence_length if step % ( epoch_size // 10 ) == 1 : print ( ""%.3f perplexity: %.3f speed: %.0f wps"" % ( step * 1.0 / epoch_size , np . exp ( costs / iters ) , iters * batch_size / ( time . time ( ) - start_time ) ) ) train_perplexity = np . exp ( costs / iters ) print ( ""Epoch: %d/%d Train Perplexity: %.3f"" % ( i + 1 , max_max_epoch , train_perplexity ) ) for top_k in top_k_list : state1 = tl . layers . initialize_rnn_state ( lstm1_test . initial_state ) outs_id = [ vocab . word_to_id ( w ) for w in seed ] for ids in outs_id [ : - 1 ] : a_id = np . asarray ( ids ) . reshape ( 1 , 1 ) state1 = sess . run ( [ lstm1_test . final_state ] , feed_dict = { input_data_test : a_id , lstm1_test . initial_state : state1 } ) a_id = outs_id [ - 1 ] for _ in range ( print_length ) : a_id = np . asarray ( a_id ) . reshape ( 1 , 1 ) out , state1 = sess . run ( [ y_soft , lstm1_test . final_state ] , feed_dict = { input_data_test : a_id , lstm1_test . initial_state : state1 } ) a_id = tl . nlp . sample_top ( out [ 0 ] , top_k = top_k ) outs_id . append ( a_id ) sentence = [ vocab . id_to_word ( w ) for w in outs_id ] sentence = "" "" . join ( sentence ) print ( top_k , ':' , sentence ) print ( ""Save model"" ) tl . files . save_npz ( network_test . all_params , name = model_file_name )",Generate text by Synced sequence input and output .
"def createAndStartSwarm ( client , clientInfo = """" , clientKey = """" , params = """" , minimumWorkers = None , maximumWorkers = None , alreadyRunning = False ) : if minimumWorkers is None : minimumWorkers = Configuration . getInt ( ""nupic.hypersearch.minWorkersPerSwarm"" ) if maximumWorkers is None : maximumWorkers = Configuration . getInt ( ""nupic.hypersearch.maxWorkersPerSwarm"" ) return ClientJobsDAO . get ( ) . jobInsert ( client = client , cmdLine = ""$HYPERSEARCH"" , clientInfo = clientInfo , clientKey = clientKey , alreadyRunning = alreadyRunning , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = ClientJobsDAO . JOB_TYPE_HS )",Create and start a swarm job .
"def getSwarmModelParams ( modelID ) : cjDAO = ClientJobsDAO . get ( ) ( jobID , description ) = cjDAO . modelsGetFields ( modelID , [ ""jobId"" , ""genDescription"" ] ) ( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ ""genBaseDescription"" ] ) descriptionDirectory = tempfile . mkdtemp ( ) try : baseDescriptionFilePath = os . path . join ( descriptionDirectory , ""base.py"" ) with open ( baseDescriptionFilePath , mode = ""wb"" ) as f : f . write ( baseDescription ) descriptionFilePath = os . path . join ( descriptionDirectory , ""description.py"" ) with open ( descriptionFilePath , mode = ""wb"" ) as f : f . write ( description ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) return json . dumps ( dict ( modelConfig = expIface . getModelDescription ( ) , inferenceArgs = expIface . getModelControl ( ) . get ( ""inferenceArgs"" , None ) ) ) finally : shutil . rmtree ( descriptionDirectory , ignore_errors = True )",Retrieve the Engine - level model params from a Swarm model
"def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : global g_max_concurrency , g_max_concurrency_raise_exception assert maxConcurrency >= 0 g_max_concurrency = maxConcurrency g_max_concurrency_raise_exception = raiseException return",Enable the diagnostic feature for debugging unexpected concurrency in acquiring ConnectionWrapper instances .
"def _getCommonSteadyDBArgsDict ( ) : return dict ( creator = pymysql , host = Configuration . get ( 'nupic.cluster.database.host' ) , port = int ( Configuration . get ( 'nupic.cluster.database.port' ) ) , user = Configuration . get ( 'nupic.cluster.database.user' ) , passwd = Configuration . get ( 'nupic.cluster.database.passwd' ) , charset = 'utf8' , use_unicode = True , setsession = [ 'SET AUTOCOMMIT = 1' ] )",Returns a dictionary of arguments for DBUtils . SteadyDB . SteadyDBConnection constructor .
"def _getLogger ( cls , logLevel = None ) : logger = logging . getLogger ( ""."" . join ( [ 'com.numenta' , _MODULE_NAME , cls . __name__ ] ) ) if logLevel is not None : logger . setLevel ( logLevel ) return logger",Gets a logger for the given class in this module
"def get ( cls ) : if cls . _connectionPolicy is None : logger = _getLogger ( cls ) logger . info ( ""Creating db connection policy via provider %r"" , cls . _connectionPolicyInstanceProvider ) cls . _connectionPolicy = cls . _connectionPolicyInstanceProvider ( ) logger . debug ( ""Created connection policy: %r"" , cls . _connectionPolicy ) return cls . _connectionPolicy . acquireConnection ( )",Acquire a ConnectionWrapper instance that represents a connection to the SQL server per nupic . cluster . database . * configuration settings .
"def _createDefaultPolicy ( cls ) : logger = _getLogger ( cls ) logger . debug ( ""Creating database connection policy: platform=%r; pymysql.VERSION=%r"" , platform . system ( ) , pymysql . VERSION ) if platform . system ( ) == ""Java"" : policy = SingleSharedConnectionPolicy ( ) else : policy = PooledConnectionPolicy ( ) return policy",[ private ] Create the default database connection policy instance
"def release ( self ) : self . _logger . debug ( ""Releasing: %r"" , self ) if self . _addedToInstanceSet : try : self . _clsOutstandingInstances . remove ( self ) except : self . _logger . exception ( ""Failed to remove self from _clsOutstandingInstances: %r;"" , self ) raise self . _releaser ( dbConn = self . dbConn , cursor = self . cursor ) self . __class__ . _clsNumOutstanding -= 1 assert self . _clsNumOutstanding >= 0 , ""_clsNumOutstanding=%r"" % ( self . _clsNumOutstanding , ) self . _releaser = None self . cursor = None self . dbConn = None self . _creationTracebackString = None self . _addedToInstanceSet = False self . _logger = None return",Release the database connection and cursor
"def _trackInstanceAndCheckForConcurrencyViolation ( self ) : global g_max_concurrency , g_max_concurrency_raise_exception assert g_max_concurrency is not None assert self not in self . _clsOutstandingInstances , repr ( self ) self . _creationTracebackString = traceback . format_stack ( ) if self . _clsNumOutstanding >= g_max_concurrency : errorMsg = ( ""With numOutstanding=%r, exceeded concurrency limit=%r "" ""when requesting %r. OTHER TRACKED UNRELEASED "" ""INSTANCES (%s): %r"" ) % ( self . _clsNumOutstanding , g_max_concurrency , self , len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) self . _logger . error ( errorMsg ) if g_max_concurrency_raise_exception : raise ConcurrencyExceededError ( errorMsg ) self . _clsOutstandingInstances . add ( self ) self . _addedToInstanceSet = True return",Check for concurrency violation and add self to _clsOutstandingInstances .
"def close ( self ) : self . _logger . info ( ""Closing"" ) if self . _conn is not None : self . _conn . close ( ) self . _conn = None else : self . _logger . warning ( ""close() called, but connection policy was alredy closed"" ) return",Close the policy instance and its shared database connection .
"def acquireConnection ( self ) : self . _logger . debug ( ""Acquiring connection"" ) self . _conn . _ping_check ( ) connWrap = ConnectionWrapper ( dbConn = self . _conn , cursor = self . _conn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap",Get a Connection instance .
"def close ( self ) : self . _logger . info ( ""Closing"" ) if self . _pool is not None : self . _pool . close ( ) self . _pool = None else : self . _logger . warning ( ""close() called, but connection policy was alredy closed"" ) return",Close the policy instance and its database connection pool .
"def acquireConnection ( self ) : self . _logger . debug ( ""Acquiring connection"" ) dbConn = self . _pool . connection ( shareable = False ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap",Get a connection from the pool .
"def close ( self ) : self . _logger . info ( ""Closing"" ) if self . _opened : self . _opened = False else : self . _logger . warning ( ""close() called, but connection policy was alredy closed"" ) return",Close the policy instance .
"def acquireConnection ( self ) : self . _logger . debug ( ""Acquiring connection"" ) dbConn = SteadyDB . connect ( * * _getCommonSteadyDBArgsDict ( ) ) connWrap = ConnectionWrapper ( dbConn = dbConn , cursor = dbConn . cursor ( ) , releaser = self . _releaseConnection , logger = self . _logger ) return connWrap",Create a Connection instance .
"def _releaseConnection ( self , dbConn , cursor ) : self . _logger . debug ( ""Releasing connection"" ) cursor . close ( ) dbConn . close ( ) return",Release database connection and cursor ; passed as a callback to ConnectionWrapper
"def getSpec ( cls ) : ns = dict ( description = KNNAnomalyClassifierRegion . __doc__ , singleNodeOnly = True , inputs = dict ( spBottomUpOut = dict ( description = """"""The output signal generated from the bottom-up inputs
                            from lower levels."""""" , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , tpTopDownOut = dict ( description = """"""The top-down inputsignal, generated from
                          feedback from upper levels"""""" , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , tpLrnActiveStateT = dict ( description = """"""Active cells in the learn state at time T from TM.
                        This is used to classify on."""""" , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , sequenceIdIn = dict ( description = ""Sequence ID"" , dataType = 'UInt64' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , ) , outputs = dict ( ) , parameters = dict ( trainRecords = dict ( description = 'Number of records to wait for training' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , anomalyThreshold = dict ( description = 'Threshold used to classify anomalies.' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , cacheSize = dict ( description = 'Number of records to store in cache.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , classificationVectorType = dict ( description = """"""Vector type to use when classifying.
              1 - Vector Column with Difference (TM and SP)
            """""" , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 1 , accessMode = 'ReadWrite' ) , activeColumnCount = dict ( description = """"""Number of active columns in a given step. Typically
            equivalent to SP.numActiveColumnsPerInhArea"""""" , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 40 , accessMode = 'ReadWrite' ) , classificationMaxDist = dict ( description = """"""Maximum distance a sample can be from an anomaly
            in the classifier to be labeled as an anomaly.

            Ex: With rawOverlap distance, a value of 0.65 means that the points
            must be at most a distance 0.65 apart from each other. This
            translates to they must be at least 35% similar."""""" , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0.65 , accessMode = 'Create' ) ) , commands = dict ( getLabels = dict ( description = ""Returns a list of label dicts with properties ROWID and labels."" ""ROWID corresponds to the records id and labels is a list of "" ""strings representing the records labels.  Takes additional "" ""integer properties start and end representing the range that "" ""will be returned."" ) , addLabel = dict ( description = ""Takes parameters start, end and labelName. Adds the label "" ""labelName to the records from start to end. This will recalculate "" ""labels from end to the most recent record."" ) , removeLabels = dict ( description = ""Takes additional parameters start, end, labelFilter.  Start and "" ""end correspond to range to remove the label. Remove labels from "" ""each record with record ROWID in range from start to end, "" ""noninclusive of end. Removes all records if labelFilter is None, "" ""otherwise only removes the labels eqaul to labelFilter."" ) ) ) ns [ 'parameters' ] . update ( KNNClassifierRegion . getSpec ( ) [ 'parameters' ] ) return ns",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getSpec .
"def getParameter ( self , name , index = - 1 ) : if name == ""trainRecords"" : return self . trainRecords elif name == ""anomalyThreshold"" : return self . anomalyThreshold elif name == ""activeColumnCount"" : return self . _activeColumnCount elif name == ""classificationMaxDist"" : return self . _classificationMaxDist else : return PyRegion . getParameter ( self , name , index )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getParameter .
"def setParameter ( self , name , index , value ) : if name == ""trainRecords"" : if not ( isinstance ( value , float ) or isinstance ( value , int ) ) : raise HTMPredictionModelInvalidArgument ( ""Invalid argument type \'%s\'. threshold "" ""must be a number."" % ( type ( value ) ) ) if len ( self . _recordsCache ) > 0 and value < self . _recordsCache [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( ""Invalid value. autoDetectWaitRecord "" ""value must be valid record within output stream. Current minimum "" "" ROWID in output stream is %d."" % ( self . _recordsCache [ 0 ] . ROWID ) ) self . trainRecords = value self . _deleteRangeFromKNN ( 0 , self . _recordsCache [ 0 ] . ROWID ) self . _classifyStates ( ) elif name == ""anomalyThreshold"" : if not ( isinstance ( value , float ) or isinstance ( value , int ) ) : raise HTMPredictionModelInvalidArgument ( ""Invalid argument type \'%s\'. threshold "" ""must be a number."" % ( type ( value ) ) ) self . anomalyThreshold = value self . _classifyStates ( ) elif name == ""classificationMaxDist"" : if not ( isinstance ( value , float ) or isinstance ( value , int ) ) : raise HTMPredictionModelInvalidArgument ( ""Invalid argument type \'%s\'. "" ""classificationMaxDist must be a number."" % ( type ( value ) ) ) self . _classificationMaxDist = value self . _classifyStates ( ) elif name == ""activeColumnCount"" : self . _activeColumnCount = value else : return PyRegion . setParameter ( self , name , index , value )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . setParameter .
"def compute ( self , inputs , outputs ) : record = self . _constructClassificationRecord ( inputs ) if record . ROWID >= self . getParameter ( 'trainRecords' ) : self . _classifyState ( record ) self . _recordsCache . append ( record ) while len ( self . _recordsCache ) > self . cacheSize : self . _recordsCache . pop ( 0 ) self . labelResults = record . anomalyLabel self . _iteration += 1",Process one input sample . This method is called by the runtime engine .
"def _classifyState ( self , state ) : if state . ROWID < self . getParameter ( 'trainRecords' ) : if not state . setByUser : state . anomalyLabel = [ ] self . _deleteRecordsFromKNN ( [ state ] ) return label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG newCategory = self . _recomputeRecordFromKNN ( state ) labelList = self . _categoryToLabelList ( newCategory ) if state . setByUser : if label in state . anomalyLabel : state . anomalyLabel . remove ( label ) if autoLabel in state . anomalyLabel : state . anomalyLabel . remove ( autoLabel ) labelList . extend ( state . anomalyLabel ) if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : labelList . append ( label ) elif label in labelList : ind = labelList . index ( label ) labelList [ ind ] = autoLabel labelList = list ( set ( labelList ) ) if label in labelList and autoLabel in labelList : labelList . remove ( autoLabel ) if state . anomalyLabel == labelList : return state . anomalyLabel = labelList if state . anomalyLabel == [ ] : self . _deleteRecordsFromKNN ( [ state ] ) else : self . _addRecordToKNN ( state )",Reclassifies given state .
"def _constructClassificationRecord ( self , inputs ) : allSPColumns = inputs [ ""spBottomUpOut"" ] activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] score = anomaly . computeRawAnomalyScore ( activeSPColumns , self . _prevPredictedColumns ) spSize = len ( allSPColumns ) allTPCells = inputs [ 'tpTopDownOut' ] tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) classificationVector = numpy . array ( [ ] ) if self . classificationVectorType == 1 : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = inputs [ ""tpLrnActiveStateT"" ] . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . classificationVectorType == 2 : classificationVector = numpy . zeros ( spSize + spSize ) if activeSPColumns . shape [ 0 ] > 0 : classificationVector [ activeSPColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeSPColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( ""Classification vector type must be either 'tpc' or"" "" 'sp_tpe', current value is %s"" % ( self . classificationVectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = allTPCells . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = self . _iteration , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result",Construct a _HTMClassificationRecord based on the state of the model passed in through the inputs .
"def _addRecordToKNN ( self , record ) : knn = self . _knnclassifier . _knn prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )",Adds the record to the KNN classifier .
"def _deleteRecordsFromKNN ( self , recordsToDelete ) : prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) idsToDelete = ( [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] ) nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )",Removes the given records from the classifier .
"def _deleteRangeFromKNN ( self , start = 0 , end = None ) : prototype_idx = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = self . _knnclassifier . _knn . _numPatterns self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete )",Removes any stored records within the range from start to end . Noninclusive of end .
"def _recomputeRecordFromKNN ( self , record ) : inputs = { ""categoryIn"" : [ None ] , ""bottomUpIn"" : self . _getStateAnomalyVector ( record ) , } outputs = { ""categoriesOut"" : numpy . zeros ( ( 1 , ) ) , ""bestPrototypeIndices"" : numpy . zeros ( ( 1 , ) ) , ""categoryProbabilitiesOut"" : numpy . zeros ( ( 1 , ) ) } classifier_indexes = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) self . _knnclassifier . setParameter ( 'learningMode' , None , False ) self . _knnclassifier . compute ( inputs , outputs ) self . _knnclassifier . setParameter ( 'learningMode' , None , True ) classifier_distances = self . _knnclassifier . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = self . _knnclassifier . getCategoryList ( ) [ indexID ] return category return None",returns the classified labeling of record
"def _labelToCategoryNumber ( self , label ) : if label not in self . saved_categories : self . saved_categories . append ( label ) return pow ( 2 , self . saved_categories . index ( label ) )",Since the KNN Classifier stores categories as numbers we must store each label as a number . This method converts from a label to a unique number . Each label is assigned a unique bit so multiple labels may be assigned to a single record .
"def _labelListToCategoryNumber ( self , labelList ) : categoryNumber = 0 for label in labelList : categoryNumber += self . _labelToCategoryNumber ( label ) return categoryNumber",This method takes a list of labels and returns a unique category number . This enables this class to store a list of categories for each point since the KNN classifier only stores a single number category for each record .
"def _categoryToLabelList ( self , category ) : if category is None : return [ ] labelList = [ ] labelNum = 0 while category > 0 : if category % 2 == 1 : labelList . append ( self . saved_categories [ labelNum ] ) labelNum += 1 category = category >> 1 return labelList",Converts a category number into a list of labels
"def _getStateAnomalyVector ( self , state ) : vector = numpy . zeros ( self . _anomalyVectorLength ) vector [ state . anomalyVector ] = 1 return vector",Returns a state s anomaly vertor converting it from spare to dense
"def getLabels ( self , start = None , end = None ) : if len ( self . _recordsCache ) == 0 : return { 'isProcessing' : False , 'recordLabels' : [ ] } try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID if end <= start : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for 'getLabels'."" , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'numRecordsStored' : len ( self . _recordsCache ) } ) results = { 'isProcessing' : False , 'recordLabels' : [ ] } ROWIDX = numpy . array ( self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) categories = self . _knnclassifier . getCategoryList ( ) for idx in validIdx : row = dict ( ROWID = int ( ROWIDX [ idx ] ) , labels = self . _categoryToLabelList ( categories [ idx ] ) ) results [ 'recordLabels' ] . append ( row ) return results",Get the labels on classified points within range start to end . Not inclusive of end .
"def addLabel ( self , start , end , labelName ) : if len ( self . _recordsCache ) == 0 : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for 'addLabel'. "" ""Model has no saved records."" ) try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = int ( self . _recordsCache [ - 1 ] . ROWID ) startID = self . _recordsCache [ 0 ] . ROWID clippedStart = max ( 0 , start - startID ) clippedEnd = max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for 'addLabel'."" , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . _recordsCache [ len ( self . _recordsCache ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . _recordsCache ) } ) for state in self . _recordsCache [ clippedStart : clippedEnd ] : if labelName not in state . anomalyLabel : state . anomalyLabel . append ( labelName ) state . setByUser = True self . _addRecordToKNN ( state ) assert len ( self . saved_categories ) > 0 for state in self . _recordsCache [ clippedEnd : ] : self . _classifyState ( state )",Add the label labelName to each record with record ROWID in range from start to end noninclusive of end .
"def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . _recordsCache ) == 0 : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for "" ""'removeLabels'. Model has no saved records."" ) try : start = int ( start ) except Exception : start = 0 try : end = int ( end ) except Exception : end = self . _recordsCache [ - 1 ] . ROWID startID = self . _recordsCache [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . _recordsCache ) if end is None else max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for "" ""'removeLabels'."" , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . _recordsCache [ len ( self . _recordsCache ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . _recordsCache ) } ) recordsToDelete = [ ] for state in self . _recordsCache [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . _recordsCache [ clippedEnd : ] : self . _classifyState ( state )",Remove labels from each record with record ROWID in range from start to end noninclusive of end . Removes all records if labelFilter is None otherwise only removes the labels equal to labelFilter .
"def match ( self , record ) : for field , meta in self . filterDict . iteritems ( ) : index = meta [ 'index' ] categories = meta [ 'categories' ] for category in categories : if not record : continue if record [ index ] . find ( category ) != - 1 : '''
          This field contains the string we're searching for
          so we'll keep the records
          ''' return True return False",Returns True if the record matches any of the provided filters
"def replace ( self , columnIndex , bitmap ) : return super ( _SparseMatrixCorticalColumnAdapter , self ) . replaceSparseRow ( columnIndex , bitmap )",Wraps replaceSparseRow ()
"def update ( self , columnIndex , vector ) : return super ( _SparseMatrixCorticalColumnAdapter , self ) . setRowFromDense ( columnIndex , vector )",Wraps setRowFromDense ()
"def setLocalAreaDensity ( self , localAreaDensity ) : assert ( localAreaDensity > 0 and localAreaDensity <= 1 ) self . _localAreaDensity = localAreaDensity self . _numActiveColumnsPerInhArea = 0",Sets the local area density . Invalidates the numActiveColumnsPerInhArea parameter : param localAreaDensity : ( float ) value to set
"def getPotential ( self , columnIndex , potential ) : assert ( columnIndex < self . _numColumns ) potential [ : ] = self . _potentialPools [ columnIndex ]",: param columnIndex : ( int ) column index to get potential for . : param potential : ( list ) will be overwritten with column potentials . Must match the number of inputs .
"def setPotential ( self , columnIndex , potential ) : assert ( columnIndex < self . _numColumns ) potentialSparse = numpy . where ( potential > 0 ) [ 0 ] if len ( potentialSparse ) < self . _stimulusThreshold : raise Exception ( ""This is likely due to a "" + ""value of stimulusThreshold that is too large relative "" + ""to the input size."" ) self . _potentialPools . replace ( columnIndex , potentialSparse )",Sets the potential mapping for a given column . potential size must match the number of inputs and must be greater than stimulusThreshold . : param columnIndex : ( int ) column index to set potential for . : param potential : ( list ) value to set .
"def getPermanence ( self , columnIndex , permanence ) : assert ( columnIndex < self . _numColumns ) permanence [ : ] = self . _permanences [ columnIndex ]",Returns the permanence values for a given column . permanence size must match the number of inputs . : param columnIndex : ( int ) column index to get permanence for . : param permanence : ( list ) will be overwritten with permanences .
"def setPermanence ( self , columnIndex , permanence ) : assert ( columnIndex < self . _numColumns ) self . _updatePermanencesForColumn ( permanence , columnIndex , raisePerm = False )",Sets the permanence values for a given column . permanence size must match the number of inputs . : param columnIndex : ( int ) column index to set permanence for . : param permanence : ( list ) value to set .
"def getConnectedSynapses ( self , columnIndex , connectedSynapses ) : assert ( columnIndex < self . _numColumns ) connectedSynapses [ : ] = self . _connectedSynapses [ columnIndex ]",: param connectedSynapses : ( list ) will be overwritten : returns : ( iter ) the connected synapses for a given column . connectedSynapses size must match the number of inputs
"def stripUnlearnedColumns ( self , activeArray ) : neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] activeArray [ neverLearned ] = 0",Removes the set of columns who have never been active from the set of active columns selected in the inhibition round . Such columns cannot represent learned pattern and are therefore meaningless if only inference is required . This should not be done when using a random unlearned SP since you would end up with no active columns .
def _updateMinDutyCycles ( self ) : if self . _globalInhibition or self . _inhibitionRadius > self . _numInputs : self . _updateMinDutyCyclesGlobal ( ) else : self . _updateMinDutyCyclesLocal ( ),Updates the minimum duty cycles defining normal activity for a column . A column with activity duty cycle below this minimum threshold is boosted .
def _updateMinDutyCyclesGlobal ( self ) : self . _minOverlapDutyCycles . fill ( self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) ),Updates the minimum duty cycles in a global fashion . Sets the minimum duty cycles for the overlap all columns to be a percent of the maximum in the region specified by minPctOverlapDutyCycle . Functionality it is equivalent to _updateMinDutyCyclesLocal but this function exploits the globality of the computation to perform it in a straightforward and efficient manner .
def _updateMinDutyCyclesLocal ( self ) : for column in xrange ( self . _numColumns ) : neighborhood = self . _getColumnNeighborhood ( column ) maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * self . _minPctOverlapDutyCycles ),Updates the minimum duty cycles . The minimum duty cycles are determined locally . Each column s minimum duty cycles are set to be a percent of the maximum duty cycles in the column s neighborhood . Unlike _updateMinDutyCyclesGlobal here the values can be quite different for different columns .
"def _updateDutyCycles ( self , overlaps , activeColumns ) : overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) overlapArray [ overlaps > 0 ] = 1 activeArray [ activeColumns ] = 1 period = self . _dutyCyclePeriod if ( period > self . _iterationNum ) : period = self . _iterationNum self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( self . _overlapDutyCycles , overlapArray , period ) self . _activeDutyCycles = self . _updateDutyCyclesHelper ( self . _activeDutyCycles , activeArray , period )",Updates the duty cycles for each column . The OVERLAP duty cycle is a moving average of the number of inputs which overlapped with the each column . The ACTIVITY duty cycles is a moving average of the frequency of activation for each column .
"def _updateInhibitionRadius ( self ) : if self . _globalInhibition : self . _inhibitionRadius = int ( self . _columnDimensions . max ( ) ) return avgConnectedSpan = numpy . average ( [ self . _avgConnectedSpanForColumnND ( i ) for i in xrange ( self . _numColumns ) ] ) columnsPerInput = self . _avgColumnsPerInput ( ) diameter = avgConnectedSpan * columnsPerInput radius = ( diameter - 1 ) / 2.0 radius = max ( 1.0 , radius ) self . _inhibitionRadius = int ( radius + 0.5 )",Update the inhibition radius . The inhibition radius is a measure of the square ( or hypersquare ) of columns that each a column is connected to on average . Since columns are are not connected to each other directly we determine this quantity by first figuring out how many * inputs * a column is connected to and then multiplying it by the total number of columns that exist for each input . For multiple dimension the aforementioned calculations are averaged over all dimensions of inputs and columns . This value is meaningless if global inhibition is enabled .
"def _avgColumnsPerInput ( self ) : numDim = max ( self . _columnDimensions . size , self . _inputDimensions . size ) colDim = numpy . ones ( numDim ) colDim [ : self . _columnDimensions . size ] = self . _columnDimensions inputDim = numpy . ones ( numDim ) inputDim [ : self . _inputDimensions . size ] = self . _inputDimensions columnsPerInput = colDim . astype ( realDType ) / inputDim return numpy . average ( columnsPerInput )",The average number of columns per input taking into account the topology of the inputs and columns . This value is used to calculate the inhibition radius . This function supports an arbitrary number of dimensions . If the number of column dimensions does not match the number of input dimensions we treat the missing or phantom dimensions as ones .
"def _avgConnectedSpanForColumn1D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 1 ) connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] if connected . size == 0 : return 0 else : return max ( connected ) - min ( connected ) + 1",The range of connected synapses for column . This is used to calculate the inhibition radius . This variation of the function only supports a 1 dimensional column topology .
"def _avgConnectedSpanForColumn2D ( self , columnIndex ) : assert ( self . _inputDimensions . size == 2 ) connected = self . _connectedSynapses [ columnIndex ] ( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) if rows . size == 0 and cols . size == 0 : return 0 rowSpan = rows . max ( ) - rows . min ( ) + 1 colSpan = cols . max ( ) - cols . min ( ) + 1 return numpy . average ( [ rowSpan , colSpan ] )",The range of connectedSynapses per column averaged for each dimension . This value is used to calculate the inhibition radius . This variation of the function only supports a 2 dimensional column topology .
"def _bumpUpWeakColumns ( self ) : weakColumns = numpy . where ( self . _overlapDutyCycles < self . _minOverlapDutyCycles ) [ 0 ] for columnIndex in weakColumns : perm = self . _permanences [ columnIndex ] . astype ( realDType ) maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] perm [ maskPotential ] += self . _synPermBelowStimulusInc self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False )",This method increases the permanence values of synapses of columns whose activity level has been too low . Such columns are identified by having an overlap duty cycle that drops too much below those of their peers . The permanence values for such columns are increased .
"def _raisePermanenceToThreshold ( self , perm , mask ) : if len ( mask ) < self . _stimulusThreshold : raise Exception ( ""This is likely due to a "" + ""value of stimulusThreshold that is too large relative "" + ""to the input size. [len(mask) < self._stimulusThreshold]"" ) numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) while True : numConnected = numpy . nonzero ( perm > self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] . size if numConnected >= self . _stimulusThreshold : return perm [ mask ] += self . _synPermBelowStimulusInc",This method ensures that each column has enough connections to input bits to allow it to become active . Since a column must have at least self . _stimulusThreshold overlaps in order to be considered during the inhibition phase columns without such minimal number of connections even if all the input bits they are connected to turn on have no chance of obtaining the minimum threshold . For such columns the permanence values are increased until the minimum number of connections are formed .
"def _updatePermanencesForColumn ( self , perm , columnIndex , raisePerm = True ) : maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] if raisePerm : self . _raisePermanenceToThreshold ( perm , maskPotential ) perm [ perm < self . _synPermTrimThreshold ] = 0 numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) newConnected = numpy . where ( perm >= self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] self . _permanences . update ( columnIndex , perm ) self . _connectedSynapses . replace ( columnIndex , newConnected ) self . _connectedCounts [ columnIndex ] = newConnected . size",This method updates the permanence matrix with a column s new permanence values . The column is identified by its index which reflects the row in the matrix and the permanence is given in dense form i . e . a full array containing all the zeros as well as the non - zero values . It is in charge of implementing clipping - ensuring that the permanence values are always between 0 and 1 - and trimming - enforcing sparsity by zeroing out all permanence values below _synPermTrimThreshold . It also maintains the consistency between self . _permanences ( the matrix storing the permanence values ) self . _connectedSynapses ( the matrix storing the bits each column is connected to ) and self . _connectedCounts ( an array storing the number of input bits each column is connected to ) . Every method wishing to modify the permanence matrix should do so through this method .
def _initPermConnected ( self ) : p = self . _synPermConnected + ( self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p,Returns a randomly generated permanence value for a synapses that is initialized in a connected state . The basic idea here is to initialize permanence values very close to synPermConnected so that a small number of learning steps could make it disconnected or connected .
def _initPermNonConnected ( self ) : p = self . _synPermConnected * self . _random . getReal64 ( ) p = int ( p * 100000 ) / 100000.0 return p,Returns a randomly generated permanence value for a synapses that is to be initialized in a non - connected state .
"def _initPermanence ( self , potential , connectedPct ) : perm = numpy . zeros ( self . _numInputs , dtype = realDType ) for i in xrange ( self . _numInputs ) : if ( potential [ i ] < 1 ) : continue if ( self . _random . getReal64 ( ) <= connectedPct ) : perm [ i ] = self . _initPermConnected ( ) else : perm [ i ] = self . _initPermNonConnected ( ) perm [ perm < self . _synPermTrimThreshold ] = 0 return perm",Initializes the permanences of a column . The method returns a 1 - D array the size of the input where each entry in the array represents the initial permanence value between the input bit at the particular index in the array and the column represented by the index parameter .
"def _mapColumn ( self , index ) : columnCoords = numpy . unravel_index ( index , self . _columnDimensions ) columnCoords = numpy . array ( columnCoords , dtype = realDType ) ratios = columnCoords / self . _columnDimensions inputCoords = self . _inputDimensions * ratios inputCoords += 0.5 * self . _inputDimensions / self . _columnDimensions inputCoords = inputCoords . astype ( int ) inputIndex = numpy . ravel_multi_index ( inputCoords , self . _inputDimensions ) return inputIndex",Maps a column to its respective input index keeping to the topology of the region . It takes the index of the column as an argument and determines what is the index of the flattened input vector that is to be the center of the column s potential pool . It distributes the columns over the inputs uniformly . The return value is an integer representing the index of the input bit . Examples of the expected output of this method : * If the topology is one dimensional and the column index is 0 this method will return the input index 0 . If the column index is 1 and there are 3 columns over 7 inputs this method will return the input index 3 . * If the topology is two dimensional with column dimensions [ 3 5 ] and input dimensions [ 7 11 ] and the column index is 3 the method returns input index 8 .
"def _mapPotential ( self , index ) : centerInput = self . _mapColumn ( index ) columnInputs = self . _getInputNeighborhood ( centerInput ) . astype ( uintType ) numPotential = int ( columnInputs . size * self . _potentialPct + 0.5 ) selectedInputs = numpy . empty ( numPotential , dtype = uintType ) self . _random . sample ( columnInputs , selectedInputs ) potential = numpy . zeros ( self . _numInputs , dtype = uintType ) potential [ selectedInputs ] = 1 return potential",Maps a column to its input bits . This method encapsulates the topology of the region . It takes the index of the column as an argument and determines what are the indices of the input vector that are located within the column s potential pool . The return value is a list containing the indices of the input bits . The current implementation of the base class only supports a 1 dimensional topology of columns with a 1 dimensional topology of inputs . To extend this class to support 2 - D topology you will need to override this method . Examples of the expected output of this method : * If the potentialRadius is greater than or equal to the largest input dimension then each column connects to all of the inputs . * If the topology is one dimensional the input space is divided up evenly among the columns and each column is centered over its share of the inputs . If the potentialRadius is 5 then each column connects to the input it is centered above as well as the 5 inputs to the left of that input and the five inputs to the right of that input wrapping around if wrapAround = True . * If the topology is two dimensional the input space is again divided up evenly among the columns and each column is centered above its share of the inputs . If the potentialRadius is 5 the column connects to a square that has 11 inputs on a side and is centered on the input that the column is centered above .
"def _updateBoostFactorsGlobal ( self ) : if ( self . _localAreaDensity > 0 ) : targetDensity = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea targetDensity = min ( targetDensity , 0.5 ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )",Update boost factors when global inhibition is used
"def _updateBoostFactorsLocal ( self ) : targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) for i in xrange ( self . _numColumns ) : maskNeighbors = self . _getColumnNeighborhood ( i ) targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) self . _boostFactors = numpy . exp ( ( targetDensity - self . _activeDutyCycles ) * self . _boostStrength )",Update boost factors when local inhibition is used
"def _calculateOverlap ( self , inputVector ) : overlaps = numpy . zeros ( self . _numColumns , dtype = realDType ) self . _connectedSynapses . rightVecSumAtNZ_fast ( inputVector . astype ( realDType ) , overlaps ) return overlaps",This function determines each column s overlap with the current input vector . The overlap of a column is the number of synapses for that column that are connected ( permanence value is greater than _synPermConnected ) to input bits which are turned on . The implementation takes advantage of the SparseBinaryMatrix class to perform this calculation efficiently .
"def _inhibitColumns ( self , overlaps ) : if ( self . _localAreaDensity > 0 ) : density = self . _localAreaDensity else : inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) ** self . _columnDimensions . size ) inhibitionArea = min ( self . _numColumns , inhibitionArea ) density = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea density = min ( density , 0.5 ) if self . _globalInhibition or self . _inhibitionRadius > max ( self . _columnDimensions ) : return self . _inhibitColumnsGlobal ( overlaps , density ) else : return self . _inhibitColumnsLocal ( overlaps , density )",Performs inhibition . This method calculates the necessary values needed to actually perform inhibition and then delegates the task of picking the active columns to helper functions .
"def _inhibitColumnsGlobal ( self , overlaps , density ) : numActive = int ( density * self . _numColumns ) sortedWinnerIndices = numpy . argsort ( overlaps , kind = 'mergesort' ) start = len ( sortedWinnerIndices ) - numActive while start < len ( sortedWinnerIndices ) : i = sortedWinnerIndices [ start ] if overlaps [ i ] >= self . _stimulusThreshold : break else : start += 1 return sortedWinnerIndices [ start : ] [ : : - 1 ]",Perform global inhibition . Performing global inhibition entails picking the top numActive columns with the highest overlap score in the entire region . At most half of the columns in a local neighborhood are allowed to be active . Columns with an overlap score below the stimulusThreshold are always inhibited .
"def _inhibitColumnsLocal ( self , overlaps , density ) : activeArray = numpy . zeros ( self . _numColumns , dtype = ""bool"" ) for column , overlap in enumerate ( overlaps ) : if overlap >= self . _stimulusThreshold : neighborhood = self . _getColumnNeighborhood ( column ) neighborhoodOverlaps = overlaps [ neighborhood ] numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) ties = numpy . where ( neighborhoodOverlaps == overlap ) tiedNeighbors = neighborhood [ ties ] numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) numActive = int ( 0.5 + density * len ( neighborhood ) ) if numBigger + numTiesLost < numActive : activeArray [ column ] = True return activeArray . nonzero ( ) [ 0 ]",Performs local inhibition . Local inhibition is performed on a column by column basis . Each column observes the overlaps of its neighbors and is selected if its overlap score is within the top numActive in its local neighborhood . At most half of the columns in a local neighborhood are allowed to be active . Columns with an overlap score below the stimulusThreshold are always inhibited .
"def _getColumnNeighborhood ( self , centerColumn ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions ) else : return topology . neighborhood ( centerColumn , self . _inhibitionRadius , self . _columnDimensions )",Gets a neighborhood of columns .
"def _getInputNeighborhood ( self , centerInput ) : if self . _wrapAround : return topology . wrappingNeighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions ) else : return topology . neighborhood ( centerInput , self . _potentialRadius , self . _inputDimensions )",Gets a neighborhood of inputs .
"def _seed ( self , seed = - 1 ) : if seed != - 1 : self . _random = NupicRandom ( seed ) else : self . _random = NupicRandom ( )",Initialize the random seed
"def handleGetValue ( self , topContainer ) : value = self . __referenceDict if self . __referenceDict is not None else topContainer for key in self . __dictKeyChain : value = value [ key ] return value",This method overrides ValueGetterBase s pure virtual method . It returns the referenced value . The derived class is NOT responsible for fully resolving the reference d value in the event the value resolves to another ValueGetterBase - based instance -- this is handled automatically within ValueGetterBase implementation .
"def Array ( dtype , size = None , ref = False ) : def getArrayType ( self ) : """"""A little function to replace the getType() method of arrays

    It returns a string representation of the array element type instead of the
    integer value (NTA_BasicType enum) returned by the origianl array
    """""" return self . _dtype if ref : assert size is None index = basicTypes . index ( dtype ) if index == - 1 : raise Exception ( 'Invalid data type: ' + dtype ) if size and size <= 0 : raise Exception ( 'Array size must be positive' ) suffix = 'ArrayRef' if ref else 'Array' arrayFactory = getattr ( engine_internal , dtype + suffix ) arrayFactory . getType = getArrayType if size : a = arrayFactory ( size ) else : a = arrayFactory ( ) a . _dtype = basicTypes [ index ] return a",Factory function that creates typed Array or ArrayRef objects
def getInputNames ( self ) : inputs = self . getSpec ( ) . inputs return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ],Returns list of input names in spec .
def getOutputNames ( self ) : outputs = self . getSpec ( ) . outputs return [ outputs . getByIndex ( i ) [ 0 ] for i in xrange ( outputs . getCount ( ) ) ],Returns list of output names in spec .
"def _getParameterMethods ( self , paramName ) : if paramName in self . _paramTypeCache : return self . _paramTypeCache [ paramName ] try : paramSpec = self . getSpec ( ) . parameters . getByName ( paramName ) except : return ( None , None ) dataType = paramSpec . dataType dataTypeName = basicTypes [ dataType ] count = paramSpec . count if count == 1 : x = 'etParameter' + dataTypeName try : g = getattr ( self , 'g' + x ) s = getattr ( self , 's' + x ) except AttributeError : raise Exception ( ""Internal error: unknown parameter type %s"" % dataTypeName ) info = ( s , g ) else : if dataTypeName == ""Byte"" : info = ( self . setParameterString , self . getParameterString ) else : helper = _ArrayParameterHelper ( self , dataType ) info = ( self . setParameterArray , helper . getParameterArray ) self . _paramTypeCache [ paramName ] = info return info",Returns functions to set / get the parameter . These are the strongly typed functions get / setParameterUInt32 etc . The return value is a pair : setfunc getfunc If the parameter is not available on this region setfunc / getfunc are None .
"def getParameter ( self , paramName ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if getter is None : import exceptions raise exceptions . Exception ( ""getParameter -- parameter name '%s' does not exist in region %s of type %s"" % ( paramName , self . name , self . type ) ) return getter ( paramName )",Get parameter value
"def setParameter ( self , paramName , value ) : ( setter , getter ) = self . _getParameterMethods ( paramName ) if setter is None : import exceptions raise exceptions . Exception ( ""setParameter -- parameter name '%s' does not exist in region %s of type %s"" % ( paramName , self . name , self . type ) ) setter ( paramName , value )",Set parameter value
"def _getRegions ( self ) : def makeRegion ( name , r ) : """"""Wrap a engine region with a nupic.engine_internal.Region

      Also passes the containing nupic.engine_internal.Network network in _network. This
      function is passed a value wrapper to the CollectionWrapper
      """""" r = Region ( r , self ) return r regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) return regions",Get the collection of regions in a network
"def getRegionsByType ( self , regionClass ) : regions = [ ] for region in self . regions . values ( ) : if type ( region . getSelf ( ) ) is regionClass : regions . append ( region ) return regions",Gets all region instances of a given class ( for example nupic . regions . sp_region . SPRegion ) .
"def getSpec ( cls ) : ns = dict ( description = SDRClassifierRegion . __doc__ , singleNodeOnly = True , inputs = dict ( actValueIn = dict ( description = ""Actual value of the field to predict. Only taken "" ""into account if the input has no category field."" , dataType = ""Real32"" , count = 0 , required = False , regionLevel = False , isDefaultInput = False , requireSplitterMap = False ) , bucketIdxIn = dict ( description = ""Active index of the encoder bucket for the "" ""actual value of the field to predict. Only taken "" ""into account if the input has no category field."" , dataType = ""UInt64"" , count = 0 , required = False , regionLevel = False , isDefaultInput = False , requireSplitterMap = False ) , categoryIn = dict ( description = 'Vector of categories of the input sample' , dataType = 'Real32' , count = 0 , required = True , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , bottomUpIn = dict ( description = 'Belief values over children\'s groups' , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , predictedActiveCells = dict ( description = ""The cells that are active and predicted"" , dataType = 'Real32' , count = 0 , required = True , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , sequenceIdIn = dict ( description = ""Sequence ID"" , dataType = 'UInt64' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , ) , outputs = dict ( categoriesOut = dict ( description = 'Classification results' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False , requireSplitterMap = False ) , actualValues = dict ( description = 'Classification results' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False , requireSplitterMap = False ) , probabilities = dict ( description = 'Classification results' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False , requireSplitterMap = False ) , ) , parameters = dict ( learningMode = dict ( description = 'Boolean (0/1) indicating whether or not a region ' 'is in learning mode.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 1 , accessMode = 'ReadWrite' ) , inferenceMode = dict ( description = 'Boolean (0/1) indicating whether or not a region ' 'is in inference mode.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , maxCategoryCount = dict ( description = 'The maximal number of categories the ' 'classifier will distinguish between.' , dataType = 'UInt32' , required = True , count = 1 , constraints = '' , defaultValue = 2000 , accessMode = 'Create' ) , steps = dict ( description = 'Comma separated list of the desired steps of ' 'prediction that the classifier should learn' , dataType = ""Byte"" , count = 0 , constraints = '' , defaultValue = '0' , accessMode = 'Create' ) , alpha = dict ( description = 'The alpha is the learning rate of the classifier.' 'lower alpha results in longer term memory and slower ' 'learning' , dataType = ""Real32"" , count = 1 , constraints = '' , defaultValue = 0.001 , accessMode = 'Create' ) , implementation = dict ( description = 'The classifier implementation to use.' , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = 'enum: py, cpp' ) , verbosity = dict ( description = 'An integer that controls the verbosity level, ' '0 means no verbose output, increasing integers ' 'provide more verbosity.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'ReadWrite' ) , ) , commands = dict ( ) ) return ns",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getSpec .
"def initialize ( self ) : if self . _sdrClassifier is None : self . _sdrClassifier = SDRClassifierFactory . create ( steps = self . stepsList , alpha = self . alpha , verbosity = self . verbosity , implementation = self . implementation , )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . initialize .
"def setParameter ( self , name , index , value ) : if name == ""learningMode"" : self . learningMode = bool ( int ( value ) ) elif name == ""inferenceMode"" : self . inferenceMode = bool ( int ( value ) ) else : return PyRegion . setParameter ( self , name , index , value )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . setParameter .
"def writeToProto ( self , proto ) : proto . implementation = self . implementation proto . steps = self . steps proto . alpha = self . alpha proto . verbosity = self . verbosity proto . maxCategoryCount = self . maxCategoryCount proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . recordNum = self . recordNum self . _sdrClassifier . write ( proto . sdrClassifier )",Write state to proto object .
"def readFromProto ( cls , proto ) : instance = cls ( ) instance . implementation = proto . implementation instance . steps = proto . steps instance . stepsList = [ int ( i ) for i in proto . steps . split ( "","" ) ] instance . alpha = proto . alpha instance . verbosity = proto . verbosity instance . maxCategoryCount = proto . maxCategoryCount instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . recordNum = proto . recordNum return instance",Read state from proto object .
"def compute ( self , inputs , outputs ) : self . _computeFlag = True patternNZ = inputs [ ""bottomUpIn"" ] . nonzero ( ) [ 0 ] if self . learningMode : categories = [ category for category in inputs [ ""categoryIn"" ] if category >= 0 ] if len ( categories ) > 0 : bucketIdxList = [ ] actValueList = [ ] for category in categories : bucketIdxList . append ( int ( category ) ) if ""actValueIn"" not in inputs : actValueList . append ( int ( category ) ) else : actValueList . append ( float ( inputs [ ""actValueIn"" ] ) ) classificationIn = { ""bucketIdx"" : bucketIdxList , ""actValue"" : actValueList } else : if ""bucketIdxIn"" not in inputs : raise KeyError ( ""Network link missing: bucketIdxOut -> bucketIdxIn"" ) if ""actValueIn"" not in inputs : raise KeyError ( ""Network link missing: actValueOut -> actValueIn"" ) classificationIn = { ""bucketIdx"" : int ( inputs [ ""bucketIdxIn"" ] ) , ""actValue"" : float ( inputs [ ""actValueIn"" ] ) } else : classificationIn = { ""actValue"" : 0 , ""bucketIdx"" : 0 } clResults = self . _sdrClassifier . compute ( recordNum = self . recordNum , patternNZ = patternNZ , classification = classificationIn , learn = self . learningMode , infer = self . inferenceMode ) if clResults is not None and len ( clResults ) > 0 : outputs [ 'actualValues' ] [ : len ( clResults [ ""actualValues"" ] ) ] = clResults [ ""actualValues"" ] for step in self . stepsList : stepIndex = self . stepsList . index ( step ) categoryOut = clResults [ ""actualValues"" ] [ clResults [ step ] . argmax ( ) ] outputs [ 'categoriesOut' ] [ stepIndex ] = categoryOut stepProbabilities = clResults [ step ] for categoryIndex in xrange ( self . maxCategoryCount ) : flatIndex = categoryIndex + stepIndex * self . maxCategoryCount if categoryIndex < len ( stepProbabilities ) : outputs [ 'probabilities' ] [ flatIndex ] = stepProbabilities [ categoryIndex ] else : outputs [ 'probabilities' ] [ flatIndex ] = 0.0 self . recordNum += 1",Process one input sample . This method is called by the runtime engine .
"def customCompute ( self , recordNum , patternNZ , classification ) : if not hasattr ( self , ""_computeFlag"" ) : self . _computeFlag = False if self . _computeFlag : warnings . simplefilter ( 'error' , DeprecationWarning ) warnings . warn ( ""The customCompute() method should not be "" ""called at the same time as the compute() "" ""method. The compute() method is called "" ""whenever network.run() is called."" , DeprecationWarning ) return self . _sdrClassifier . compute ( recordNum , patternNZ , classification , self . learningMode , self . inferenceMode )",Just return the inference value from one input sample . The actual learning happens in compute () -- if and only if learning is enabled -- which is called when you run the network .
"def getOutputElementCount ( self , outputName ) : if outputName == ""categoriesOut"" : return len ( self . stepsList ) elif outputName == ""probabilities"" : return len ( self . stepsList ) * self . maxCategoryCount elif outputName == ""actualValues"" : return self . maxCategoryCount else : raise ValueError ( ""Unknown output {}."" . format ( outputName ) )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getOutputElementCount .
"def run ( self ) : descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( self . _experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) expIface . normalizeStreamSources ( ) modelDescription = expIface . getModelDescription ( ) self . _modelControl = expIface . getModelControl ( ) streamDef = self . _modelControl [ 'dataset' ] from nupic . data . stream_reader import StreamReader readTimeout = 0 self . _inputSource = StreamReader ( streamDef , isBlocking = False , maxTimeout = readTimeout ) fieldStats = self . _getFieldStats ( ) self . _model = ModelFactory . create ( modelDescription ) self . _model . setFieldStatistics ( fieldStats ) self . _model . enableLearning ( ) self . _model . enableInference ( self . _modelControl . get ( ""inferenceArgs"" , None ) ) self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , self . _model . getFieldInfo ( ) , self . _model . getInferenceType ( ) ) self . __loggedMetricPatterns = self . _modelControl . get ( ""loggedMetrics"" , [ ] ) self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , self . _getMetricLabels ( ) ) self . _periodic = self . _initPeriodicActivities ( ) numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) learningOffAt = None iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : self . _model . disableLearning ( ) elif iterationCountInferOnly > 0 : assert numIters > iterationCountInferOnly , ""when iterationCountInferOnly "" ""is specified, iterationCount must be greater than "" ""iterationCountInferOnly."" learningOffAt = numIters - iterationCountInferOnly self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) self . _finalize ( ) return ( self . _cmpReason , None )",Runs the OPF Model
"def __runTaskMainLoop ( self , numIters , learningOffAt = None ) : self . _model . resetSequenceStates ( ) self . _currentRecordIndex = - 1 while True : if self . _isKilled : break if self . _isCanceled : break if self . _isInterrupted . isSet ( ) : self . __setAsOrphaned ( ) break if self . _isMature : if not self . _isBestModel : self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED break else : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF if learningOffAt is not None and self . _currentRecordIndex == learningOffAt : self . _model . disableLearning ( ) try : inputRecord = self . _inputSource . getNextRecordDict ( ) if self . _currentRecordIndex < 0 : self . _inputSource . setTimeout ( 10 ) except Exception , e : raise utils . JobFailException ( ErrorCodes . streamReading , str ( e . args ) , traceback . format_exc ( ) ) if inputRecord is None : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF break if inputRecord : self . _currentRecordIndex += 1 result = self . _model . run ( inputRecord = inputRecord ) result . metrics = self . __metricMgr . update ( result ) if not result . metrics : result . metrics = self . __metricMgr . getMetrics ( ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) result . sensorInput . dataEncodings = None self . _writePrediction ( result ) self . _periodic . tick ( ) if numIters >= 0 and self . _currentRecordIndex >= numIters - 1 : break else : raise ValueError ( ""Got an empty record from FileSource: %r"" % inputRecord )",Main loop of the OPF Model Runner .
"def _finalize ( self ) : self . _logger . info ( ""Finished: modelID=%r; %r records processed. Performing final activities"" , self . _modelID , self . _currentRecordIndex + 1 ) self . _updateModelDBResults ( ) if not self . _isKilled : self . __updateJobResults ( ) else : self . __deleteOutputCache ( self . _modelID ) if self . _predictionLogger : self . _predictionLogger . close ( ) if self . _inputSource : self . _inputSource . close ( )",Run final activities after a model has run . These include recording and logging the final score
"def __createModelCheckpoint ( self ) : if self . _model is None or self . _modelCheckpointGUID is None : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) predictions = StringIO . StringIO ( ) self . _predictionLogger . checkpoint ( checkpointSink = predictions , maxRows = int ( Configuration . get ( 'nupic.model.checkpoint.maxPredictionRows' ) ) ) self . _model . save ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : str ( self . _modelCheckpointGUID ) } , ignoreUnchanged = True ) self . _logger . info ( ""Checkpointed Hypersearch Model: modelID: %r, "" ""checkpointID: %r"" , self . _modelID , checkpointID ) return",Create a checkpoint from the current model and store it in a dir named after checkpoint GUID and finally store the GUID in the Models DB
"def __deleteModelCheckpoint ( self , modelID ) : checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] if checkpointID is None : return try : shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) except : self . _logger . warn ( ""Failed to delete model checkpoint %s. "" ""Assuming that another worker has already deleted it"" , checkpointID ) return self . _jobsDAO . modelSetFields ( modelID , { 'modelCheckpointId' : None } , ignoreUnchanged = True ) return",Delete the stored checkpoint for the specified modelID . This function is called if the current model is now the best model making the old model s checkpoint obsolete
"def _createPredictionLogger ( self ) : self . _predictionLogger = BasicPredictionLogger ( fields = self . _model . getFieldInfo ( ) , experimentDir = self . _experimentDir , label = ""hypersearch-worker"" , inferenceType = self . _model . getInferenceType ( ) ) if self . __loggedMetricPatterns : metricLabels = self . __metricMgr . getMetricLabels ( ) loggedMetrics = matchPatterns ( self . __loggedMetricPatterns , metricLabels ) self . _predictionLogger . setLoggedMetrics ( loggedMetrics )",Creates the model s PredictionLogger object which is an interface to write model results to a permanent storage location
"def __getOptimizedMetricLabel ( self ) : matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , self . _getMetricLabels ( ) ) if len ( matchingKeys ) == 0 : raise Exception ( ""None of the generated metrics match the specified "" ""optimization pattern: %s. Available metrics are %s"" % ( self . _optimizeKeyPattern , self . _getMetricLabels ( ) ) ) elif len ( matchingKeys ) > 1 : raise Exception ( ""The specified optimization pattern '%s' matches more "" ""than one metric: %s"" % ( self . _optimizeKeyPattern , matchingKeys ) ) return matchingKeys [ 0 ]",Get the label for the metric being optimized . This function also caches the label in the instance variable self . _optimizedMetricLabel
def _getFieldStats ( self ) : fieldStats = dict ( ) fieldNames = self . _inputSource . getFieldNames ( ) for field in fieldNames : curStats = dict ( ) curStats [ 'min' ] = self . _inputSource . getFieldMin ( field ) curStats [ 'max' ] = self . _inputSource . getFieldMax ( field ) fieldStats [ field ] = curStats return fieldStats,Method which returns a dictionary of field statistics received from the input source .
"def _updateModelDBResults ( self ) : metrics = self . _getMetrics ( ) reportDict = dict ( [ ( k , metrics [ k ] ) for k in self . _reportMetricLabels ] ) metrics = self . _getMetrics ( ) optimizeDict = dict ( ) if self . _optimizeKeyPattern is not None : optimizeDict [ self . _optimizedMetricLabel ] = metrics [ self . _optimizedMetricLabel ] results = json . dumps ( ( metrics , optimizeDict ) ) self . _jobsDAO . modelUpdateResults ( self . _modelID , results = results , metricValue = optimizeDict . values ( ) [ 0 ] , numRecords = ( self . _currentRecordIndex + 1 ) ) self . _logger . debug ( ""Model Results: modelID=%s; numRecords=%s; results=%s"" % ( self . _modelID , self . _currentRecordIndex + 1 , results ) ) return",Retrieves the current results and updates the model s record in the Model database .
"def __updateJobResultsPeriodic ( self ) : if self . _isBestModelStored and not self . _isBestModel : return while True : jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is None : jobResults = { } else : self . _isBestModelStored = True if not self . _isBestModel : return jobResults = json . loads ( jobResultsStr ) bestModel = jobResults . get ( 'bestModel' , None ) bestMetric = jobResults . get ( 'bestValue' , None ) isSaved = jobResults . get ( 'saved' , False ) if ( bestModel is not None ) and ( self . _modelID != bestModel ) : self . _isBestModel = False return self . __flushPredictionCache ( ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) metrics = self . _getMetrics ( ) jobResults [ 'bestModel' ] = self . _modelID jobResults [ 'bestValue' ] = metrics [ self . _optimizedMetricLabel ] jobResults [ 'metrics' ] = metrics jobResults [ 'saved' ] = False newResults = json . dumps ( jobResults ) isUpdated = self . _jobsDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = newResults ) if isUpdated or ( not isUpdated and newResults == jobResultsStr ) : self . _isBestModel = True break",Periodic check to see if this is the best model . This should only have an effect if this is the * first * model to report its progress
"def __checkIfBestCompletedModel ( self ) : jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is None : jobResults = { } else : jobResults = json . loads ( jobResultsStr ) isSaved = jobResults . get ( 'saved' , False ) bestMetric = jobResults . get ( 'bestValue' , None ) currentMetric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _isBestModel = ( not isSaved ) or ( currentMetric < bestMetric ) return self . _isBestModel , jobResults , jobResultsStr",Reads the current best model for the job and returns whether or not the current model is better than the best model stored for the job
"def __updateJobResults ( self ) : isSaved = False while True : self . _isBestModel , jobResults , jobResultsStr = self . __checkIfBestCompletedModel ( ) if self . _isBestModel : if not isSaved : self . __flushPredictionCache ( ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) self . __createModelCheckpoint ( ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) isSaved = True prevBest = jobResults . get ( 'bestModel' , None ) prevWasSaved = jobResults . get ( 'saved' , False ) if prevBest == self . _modelID : assert not prevWasSaved metrics = self . _getMetrics ( ) jobResults [ 'bestModel' ] = self . _modelID jobResults [ 'bestValue' ] = metrics [ self . _optimizedMetricLabel ] jobResults [ 'metrics' ] = metrics jobResults [ 'saved' ] = True isUpdated = self . _jobsDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : if prevWasSaved : self . __deleteOutputCache ( prevBest ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) self . __deleteModelCheckpoint ( prevBest ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) self . _logger . info ( ""Model %d chosen as best model"" , self . _modelID ) break else : self . __deleteOutputCache ( self . _modelID ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) self . __deleteModelCheckpoint ( self . _modelID ) self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) break",Check if this is the best model If so : 1 ) Write it s checkpoint 2 ) Record this model as the best 3 ) Delete the previous best s output cache Otherwise : 1 ) Delete our output cache
"def _writePrediction ( self , result ) : self . __predictionCache . append ( result ) if self . _isBestModel : self . __flushPredictionCache ( )",Writes the results of one iteration of a model . The results are written to this ModelRunner s in - memory cache unless this model is the best model for the job . If this model is the best model the predictions are written out to a permanent store via a prediction output stream instance
"def __flushPredictionCache ( self ) : if not self . __predictionCache : return if self . _predictionLogger is None : self . _createPredictionLogger ( ) startTime = time . time ( ) self . _predictionLogger . writeRecords ( self . __predictionCache , progressCB = self . __writeRecordsCallback ) self . _logger . info ( ""Flushed prediction cache; numrows=%s; elapsed=%s sec."" , len ( self . __predictionCache ) , time . time ( ) - startTime ) self . __predictionCache . clear ( )",Writes the contents of this model s in - memory prediction cache to a permanent store via the prediction output stream instance
"def __deleteOutputCache ( self , modelID ) : if modelID == self . _modelID and self . _predictionLogger is not None : self . _predictionLogger . close ( ) del self . __predictionCache self . _predictionLogger = None self . __predictionCache = None",Delete s the output cache associated with the given modelID . This actually clears up the resources associated with the cache rather than deleting al the records in the cache
"def _initPeriodicActivities ( self ) : updateModelDBResults = PeriodicActivityRequest ( repeating = True , period = 100 , cb = self . _updateModelDBResults ) updateJobResults = PeriodicActivityRequest ( repeating = True , period = 100 , cb = self . __updateJobResultsPeriodic ) checkCancelation = PeriodicActivityRequest ( repeating = True , period = 50 , cb = self . __checkCancelation ) checkMaturity = PeriodicActivityRequest ( repeating = True , period = 10 , cb = self . __checkMaturity ) updateJobResultsFirst = PeriodicActivityRequest ( repeating = False , period = 2 , cb = self . __updateJobResultsPeriodic ) periodicActivities = [ updateModelDBResults , updateJobResultsFirst , updateJobResults , checkCancelation ] if self . _isMaturityEnabled : periodicActivities . append ( checkMaturity ) return PeriodicActivityMgr ( requestedActivities = periodicActivities )",Creates and returns a PeriodicActivityMgr instance initialized with our periodic activities
"def __checkCancelation ( self ) : print >> sys . stderr , ""reporter:counter:HypersearchWorker,numRecords,50"" jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isCanceled = True self . _logger . info ( ""Model %s canceled because Job %s was stopped."" , self . _modelID , self . _jobID ) else : stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] if stopReason is None : pass elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED self . _isKilled = True self . _logger . info ( ""Model %s canceled because it was killed by hypersearch"" , self . _modelID ) elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isCanceled = True self . _logger . info ( ""Model %s stopped because hypersearch ended"" , self . _modelID ) else : raise RuntimeError ( ""Unexpected stop reason encountered: %s"" % ( stopReason ) )",Check if the cancelation flag has been set for this model in the Model DB
"def __checkMaturity ( self ) : if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : return if self . _isMature : return metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : self . _jobsDAO . modelSetFields ( self . _modelID , { 'engMatured' : True } ) self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED self . _isMature = True self . _logger . info ( ""Model %d has matured (pctChange=%s, n=%d). \n"" ""Scores = %s\n"" ""Stopping execution"" , self . _modelID , pctChange , self . _MATURITY_NUM_POINTS , self . _metricRegression . _window )",Save the current metric value and see if the model s performance has leveled off . We do this by looking at some number of previous number of recordings
"def __setAsOrphaned ( self ) : cmplReason = ClientJobsDAO . CMPL_REASON_ORPHAN cmplMessage = ""Killed by Scheduler"" self . _jobsDAO . modelSetCompleted ( self . _modelID , cmplReason , cmplMessage )",Sets the current model as orphaned . This is called when the scheduler is about to kill the process to reallocate the worker to a different process .
"def readStateFromDB ( self ) : self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] if self . _priorStateJSON is None : swarms = dict ( ) if self . _hsObj . _fixedFields is not None : print self . _hsObj . _fixedFields encoderSet = [ ] for field in self . _hsObj . _fixedFields : if field == '_classifierInput' : continue encoderName = self . getEncoderKeyFromName ( field ) assert encoderName in self . _hsObj . _encoderNames , ""The field '%s' "" "" specified in the fixedFields list is not present in this "" "" model."" % ( field ) encoderSet . append ( encoderName ) encoderSet . sort ( ) swarms [ '.' . join ( encoderSet ) ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . temporal : for encoderName in self . _hsObj . _encoderNames : swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . classification : for encoderName in self . _hsObj . _encoderNames : if encoderName == self . _hsObj . _predictedFieldEncoder : continue swarms [ encoderName ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : swarms [ self . _hsObj . _predictedFieldEncoder ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : 0 , } else : raise RuntimeError ( ""Unsupported search type: %s"" % ( self . _hsObj . _searchType ) ) self . _state = dict ( lastUpdateTime = time . time ( ) , lastGoodSprint = None , searchOver = False , activeSwarms = swarms . keys ( ) , swarms = swarms , sprints = [ { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ] , blackListedEncoders = [ ] , ) self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] assert ( self . _priorStateJSON is not None ) self . _state = json . loads ( self . _priorStateJSON ) self . _dirty = False",Set our state to that obtained from the engWorkerState field of the job record .
"def writeStateToDB ( self ) : if not self . _dirty : return True self . _state [ 'lastUpdateTime' ] = time . time ( ) newStateJSON = json . dumps ( self . _state ) success = self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 'engWorkerState' , str ( newStateJSON ) , str ( self . _priorStateJSON ) ) if success : self . logger . debug ( ""Success changing hsState to: \n%s "" % ( pprint . pformat ( self . _state , indent = 4 ) ) ) self . _priorStateJSON = newStateJSON else : self . logger . debug ( ""Failed to change hsState to: \n%s "" % ( pprint . pformat ( self . _state , indent = 4 ) ) ) self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] self . _state = json . loads ( self . _priorStateJSON ) self . logger . info ( ""New hsState has been set by some other worker to: "" "" \n%s"" % ( pprint . pformat ( self . _state , indent = 4 ) ) ) return success",Update the state in the job record with our local changes ( if any ) . If we don t have the latest state in our priorStateJSON then re - load in the latest state and return False . If we were successful writing out our changes return True
"def getFieldContributions ( self ) : if self . _hsObj . _fixedFields is not None : return dict ( ) , dict ( ) predictedEncoderName = self . _hsObj . _predictedFieldEncoder fieldScores = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : encodersUsed = swarmId . split ( '.' ) if len ( encodersUsed ) != 1 : continue field = self . getEncoderNameFromKey ( encodersUsed [ 0 ] ) bestScore = info [ 'bestErrScore' ] if bestScore is None : ( _modelId , bestScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) fieldScores . append ( ( bestScore , field ) ) if self . _hsObj . _searchType == HsSearchType . legacyTemporal : assert ( len ( fieldScores ) == 1 ) ( baseErrScore , baseField ) = fieldScores [ 0 ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : encodersUsed = swarmId . split ( '.' ) if len ( encodersUsed ) != 2 : continue fields = [ self . getEncoderNameFromKey ( name ) for name in encodersUsed ] fields . remove ( baseField ) fieldScores . append ( ( info [ 'bestErrScore' ] , fields [ 0 ] ) ) else : fieldScores . sort ( reverse = True ) if self . _hsObj . _maxBranching > 0 and len ( fieldScores ) > self . _hsObj . _maxBranching : baseErrScore = fieldScores [ - self . _hsObj . _maxBranching - 1 ] [ 0 ] else : baseErrScore = fieldScores [ 0 ] [ 0 ] pctFieldContributionsDict = dict ( ) absFieldContributionsDict = dict ( ) if baseErrScore is not None : if abs ( baseErrScore ) < 0.00001 : baseErrScore = 0.00001 for ( errScore , field ) in fieldScores : if errScore is not None : pctBetter = ( baseErrScore - errScore ) * 100.0 / baseErrScore else : pctBetter = 0.0 errScore = baseErrScore pctFieldContributionsDict [ field ] = pctBetter absFieldContributionsDict [ field ] = baseErrScore - errScore self . logger . debug ( ""FieldContributions: %s"" % ( pctFieldContributionsDict ) ) return pctFieldContributionsDict , absFieldContributionsDict",Return the field contributions statistics .
"def getAllSwarms ( self , sprintIdx ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'sprintIdx' ] == sprintIdx : swarmIds . append ( swarmId ) return swarmIds",Return the list of all swarms in the given sprint .
"def getCompletedSwarms ( self ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'status' ] == 'completed' : swarmIds . append ( swarmId ) return swarmIds",Return the list of all completed swarms .
"def getCompletingSwarms ( self ) : swarmIds = [ ] for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : if info [ 'status' ] == 'completing' : swarmIds . append ( swarmId ) return swarmIds",Return the list of all completing swarms .
"def bestModelInSprint ( self , sprintIdx ) : swarms = self . getAllSwarms ( sprintIdx ) bestModelId = None bestErrScore = numpy . inf for swarmId in swarms : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) if errScore < bestErrScore : bestModelId = modelId bestErrScore = errScore return ( bestModelId , bestErrScore )",Return the best model ID and it s errScore from the given sprint which may still be in progress . This returns the best score from all models in the sprint which have matured so far .
"def setSwarmState ( self , swarmId , newStatus ) : assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) swarmInfo = self . _state [ 'swarms' ] [ swarmId ] if swarmInfo [ 'status' ] == newStatus : return if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : return self . _dirty = True swarmInfo [ 'status' ] = newStatus if newStatus == 'completed' : ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) swarmInfo [ 'bestModelId' ] = modelId swarmInfo [ 'bestErrScore' ] = errScore if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : self . _state [ 'activeSwarms' ] . remove ( swarmId ) if newStatus == 'killed' : self . _hsObj . killSwarmParticles ( swarmId ) sprintIdx = swarmInfo [ 'sprintIdx' ] self . isSprintActive ( sprintIdx ) sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) bestModelIds = [ ] bestErrScores = [ ] for info in self . _state [ 'swarms' ] . itervalues ( ) : if info [ 'sprintIdx' ] != sprintIdx : continue statusCounts [ info [ 'status' ] ] += 1 if info [ 'status' ] == 'completed' : bestModelIds . append ( info [ 'bestModelId' ] ) bestErrScores . append ( info [ 'bestErrScore' ] ) if statusCounts [ 'active' ] > 0 : sprintStatus = 'active' elif statusCounts [ 'completing' ] > 0 : sprintStatus = 'completing' else : sprintStatus = 'completed' sprintInfo [ 'status' ] = sprintStatus if sprintStatus == 'completed' : if len ( bestErrScores ) > 0 : whichIdx = numpy . array ( bestErrScores ) . argmin ( ) sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] else : sprintInfo [ 'bestModelId' ] = 0 sprintInfo [ 'bestErrScore' ] = numpy . inf bestPrior = numpy . inf for idx in range ( sprintIdx ) : if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) if errScore is None : errScore = numpy . inf else : errScore = numpy . inf if errScore < bestPrior : bestPrior = errScore if sprintInfo [ 'bestErrScore' ] >= bestPrior : self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : self . _state [ 'searchOver' ] = True",Change the given swarm s state to newState . If newState is completed then bestModelId and bestErrScore must be provided .
def anyGoodSprintsActive ( self ) : if self . _state [ 'lastGoodSprint' ] is not None : goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] else : goodSprints = self . _state [ 'sprints' ] for sprint in goodSprints : if sprint [ 'status' ] == 'active' : anyActiveSprints = True break else : anyActiveSprints = False return anyActiveSprints,Return True if there are any more good sprints still being explored . A good sprint is one that is earlier than where we detected an increase in error from sprint to subsequent sprint .
"def isSprintCompleted ( self , sprintIdx ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx >= numExistingSprints : return False return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' )",Return True if the given sprint has completed .
"def killUselessSwarms ( self ) : numExistingSprints = len ( self . _state [ 'sprints' ] ) if self . _hsObj . _searchType == HsSearchType . legacyTemporal : if numExistingSprints <= 2 : return else : if numExistingSprints <= 1 : return completedSwarms = self . getCompletedSwarms ( ) completedSwarms = [ ( swarm , self . _state [ ""swarms"" ] [ swarm ] , self . _state [ ""swarms"" ] [ swarm ] [ ""bestErrScore"" ] ) for swarm in completedSwarms ] completedMatrix = [ [ ] for i in range ( numExistingSprints ) ] for swarm in completedSwarms : completedMatrix [ swarm [ 1 ] [ ""sprintIdx"" ] ] . append ( swarm ) for sprint in completedMatrix : sprint . sort ( key = itemgetter ( 2 ) ) activeSwarms = self . getActiveSwarms ( ) activeSwarms . extend ( self . getCompletingSwarms ( ) ) activeSwarms = [ ( swarm , self . _state [ ""swarms"" ] [ swarm ] , self . _state [ ""swarms"" ] [ swarm ] [ ""bestErrScore"" ] ) for swarm in activeSwarms ] activeMatrix = [ [ ] for i in range ( numExistingSprints ) ] for swarm in activeSwarms : activeMatrix [ swarm [ 1 ] [ ""sprintIdx"" ] ] . append ( swarm ) for sprint in activeMatrix : sprint . sort ( key = itemgetter ( 2 ) ) toKill = [ ] for i in range ( 1 , numExistingSprints ) : for swarm in activeMatrix [ i ] : curSwarmEncoders = swarm [ 0 ] . split ( ""."" ) if ( len ( activeMatrix [ i - 1 ] ) == 0 ) : if i == 2 and ( self . _hsObj . _tryAll3FieldCombinations or self . _hsObj . _tryAll3FieldCombinationsWTimestamps ) : pass else : bestInPrevious = completedMatrix [ i - 1 ] [ 0 ] bestEncoders = bestInPrevious [ 0 ] . split ( '.' ) for encoder in bestEncoders : if not encoder in curSwarmEncoders : toKill . append ( swarm ) if len ( toKill ) > 0 : print ""ParseMe: Killing encoders:"" + str ( toKill ) for swarm in toKill : self . setSwarmState ( swarm [ 0 ] , ""killed"" ) return",See if we can kill off some speculative swarms . If an earlier sprint has finally completed we can now tell which fields should * really * be present in the sprints we ve already started due to speculation and kill off the swarms that should not have been included .
"def isSprintActive ( self , sprintIdx ) : while True : numExistingSprints = len ( self . _state [ 'sprints' ] ) if sprintIdx <= numExistingSprints - 1 : if not self . _hsObj . _speculativeParticles : active = ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'active' ) return ( active , False ) else : active = ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'active' ) if not active : return ( active , False ) activeSwarmIds = self . getActiveSwarms ( sprintIdx ) swarmSizes = [ self . _hsObj . _resultsDB . getParticleInfos ( swarmId , matured = False ) [ 0 ] for swarmId in activeSwarmIds ] notFullSwarms = [ len ( swarm ) for swarm in swarmSizes if len ( swarm ) < self . _hsObj . _minParticlesPerSwarm ] if len ( notFullSwarms ) > 0 : return ( True , False ) if self . _state [ 'lastGoodSprint' ] is not None : return ( False , True ) if self . _hsObj . _fixedFields is not None : return ( False , True ) if sprintIdx > 0 and self . _state [ 'sprints' ] [ sprintIdx - 1 ] [ 'status' ] == 'completed' : ( bestModelId , _ ) = self . bestModelInCompletedSprint ( sprintIdx - 1 ) ( particleState , _ , _ , _ , _ ) = self . _hsObj . _resultsDB . getParticleInfo ( bestModelId ) bestSwarmId = particleState [ 'swarmId' ] baseEncoderSets = [ bestSwarmId . split ( '.' ) ] else : bestSwarmId = None particleState = None baseEncoderSets = [ ] for swarmId in self . getNonKilledSwarms ( sprintIdx - 1 ) : baseEncoderSets . append ( swarmId . split ( '.' ) ) encoderAddSet = [ ] limitFields = False if self . _hsObj . _maxBranching > 0 or self . _hsObj . _minFieldContribution >= 0 : if self . _hsObj . _searchType == HsSearchType . temporal or self . _hsObj . _searchType == HsSearchType . classification : if sprintIdx >= 1 : limitFields = True baseSprintIdx = 0 elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : if sprintIdx >= 2 : limitFields = True baseSprintIdx = 1 else : raise RuntimeError ( ""Unimplemented search type %s"" % ( self . _hsObj . _searchType ) ) if limitFields : pctFieldContributions , absFieldContributions = self . getFieldContributions ( ) toRemove = [ ] self . logger . debug ( ""FieldContributions min: %s"" % ( self . _hsObj . _minFieldContribution ) ) for fieldname in pctFieldContributions : if pctFieldContributions [ fieldname ] < self . _hsObj . _minFieldContribution : self . logger . debug ( ""FieldContributions removing: %s"" % ( fieldname ) ) toRemove . append ( self . getEncoderKeyFromName ( fieldname ) ) else : self . logger . debug ( ""FieldContributions keeping: %s"" % ( fieldname ) ) swarms = self . _state [ ""swarms"" ] sprintSwarms = [ ( swarm , swarms [ swarm ] [ ""bestErrScore"" ] ) for swarm in swarms if swarms [ swarm ] [ ""sprintIdx"" ] == baseSprintIdx ] sprintSwarms = sorted ( sprintSwarms , key = itemgetter ( 1 ) ) if self . _hsObj . _maxBranching > 0 : sprintSwarms = sprintSwarms [ 0 : self . _hsObj . _maxBranching ] for swarm in sprintSwarms : swarmEncoders = swarm [ 0 ] . split ( ""."" ) for encoder in swarmEncoders : if not encoder in encoderAddSet : encoderAddSet . append ( encoder ) encoderAddSet = [ encoder for encoder in encoderAddSet if not str ( encoder ) in toRemove ] else : encoderAddSet = self . _hsObj . _encoderNames newSwarmIds = set ( ) if ( self . _hsObj . _searchType == HsSearchType . temporal or self . _hsObj . _searchType == HsSearchType . legacyTemporal ) and sprintIdx == 2 and ( self . _hsObj . _tryAll3FieldCombinations or self . _hsObj . _tryAll3FieldCombinationsWTimestamps ) : if self . _hsObj . _tryAll3FieldCombinations : newEncoders = set ( self . _hsObj . _encoderNames ) if self . _hsObj . _predictedFieldEncoder in newEncoders : newEncoders . remove ( self . _hsObj . _predictedFieldEncoder ) else : newEncoders = set ( encoderAddSet ) if self . _hsObj . _predictedFieldEncoder in newEncoders : newEncoders . remove ( self . _hsObj . _predictedFieldEncoder ) for encoder in self . _hsObj . _encoderNames : if encoder . endswith ( '_timeOfDay' ) or encoder . endswith ( '_weekend' ) or encoder . endswith ( '_dayOfWeek' ) : newEncoders . add ( encoder ) allCombos = list ( itertools . combinations ( newEncoders , 2 ) ) for combo in allCombos : newSet = list ( combo ) newSet . append ( self . _hsObj . _predictedFieldEncoder ) newSet . sort ( ) newSwarmId = '.' . join ( newSet ) if newSwarmId not in self . _state [ 'swarms' ] : newSwarmIds . add ( newSwarmId ) if ( len ( self . getActiveSwarms ( sprintIdx - 1 ) ) > 0 ) : break else : for baseEncoderSet in baseEncoderSets : for encoder in encoderAddSet : if encoder not in self . _state [ 'blackListedEncoders' ] and encoder not in baseEncoderSet : newSet = list ( baseEncoderSet ) newSet . append ( encoder ) newSet . sort ( ) newSwarmId = '.' . join ( newSet ) if newSwarmId not in self . _state [ 'swarms' ] : newSwarmIds . add ( newSwarmId ) if ( len ( self . getActiveSwarms ( sprintIdx - 1 ) ) > 0 ) : break newSwarmIds = sorted ( newSwarmIds ) if len ( newSwarmIds ) == 0 : if len ( self . getAllSwarms ( sprintIdx ) ) > 0 : return ( True , False ) else : return ( False , True ) self . _dirty = True if len ( self . _state [ ""sprints"" ] ) == sprintIdx : self . _state [ 'sprints' ] . append ( { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None } ) for swarmId in newSwarmIds : self . _state [ 'swarms' ] [ swarmId ] = { 'status' : 'active' , 'bestModelId' : None , 'bestErrScore' : None , 'sprintIdx' : sprintIdx } self . _state [ 'activeSwarms' ] = self . getActiveSwarms ( ) success = self . writeStateToDB ( ) if success : return ( True , False )",If the given sprint exists and is active return active = True .
"def addEncoder ( self , name , encoder ) : self . encoders . append ( ( name , encoder , self . width ) ) for d in encoder . getDescription ( ) : self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) self . width += encoder . getWidth ( )",Adds one encoder .
"def invariant ( self ) : assert isinstance ( self . description , str ) assert isinstance ( self . singleNodeOnly , bool ) assert isinstance ( self . inputs , dict ) assert isinstance ( self . outputs , dict ) assert isinstance ( self . parameters , dict ) assert isinstance ( self . commands , dict ) hasDefaultInput = False for k , v in self . inputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , InputSpec ) v . invariant ( ) if v . isDefaultInput : assert not hasDefaultInput hasDefaultInput = True hasDefaultOutput = False for k , v in self . outputs . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , OutputSpec ) v . invariant ( ) if v . isDefaultOutput : assert not hasDefaultOutput hasDefaultOutput = True for k , v in self . parameters . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , ParameterSpec ) v . invariant ( ) for k , v in self . commands . items ( ) : assert isinstance ( k , str ) assert isinstance ( v , CommandSpec ) v . invariant ( )",Verify the validity of the node spec object
"def toDict ( self ) : def items2dict ( items ) : """"""Convert a dict of node spec items to a plain dict

      Each node spec item object will be converted to a dict of its
      attributes. The entire items dict will become a dict of dicts (same keys).
      """""" d = { } for k , v in items . items ( ) : d [ k ] = v . __dict__ return d self . invariant ( ) return dict ( description = self . description , singleNodeOnly = self . singleNodeOnly , inputs = items2dict ( self . inputs ) , outputs = items2dict ( self . outputs ) , parameters = items2dict ( self . parameters ) , commands = items2dict ( self . commands ) )",Convert the information of the node spec to a plain dict of basic types
"def updateResultsForJob ( self , forceUpdate = True ) : updateInterval = time . time ( ) - self . _lastUpdateAttemptTime if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : return self . logger . info ( ""Attempting model selection for jobID=%d: time=%f"" ""  lastUpdate=%f"" % ( self . _jobID , time . time ( ) , self . _lastUpdateAttemptTime ) ) timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , self . _MIN_UPDATE_INTERVAL ) if not timestampUpdated : self . logger . info ( ""Unable to update selection sweep timestamp: jobID=%d"" "" updateTime=%f"" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) if not forceUpdate : return self . _lastUpdateAttemptTime = time . time ( ) self . logger . info ( ""Succesfully updated selection sweep timestamp jobid=%d updateTime=%f"" % ( self . _jobID , self . _lastUpdateAttemptTime ) ) minUpdateRecords = self . _MIN_UPDATE_THRESHOLD jobResults = self . _getJobResults ( ) if forceUpdate or jobResults is None : minUpdateRecords = 0 candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) self . logger . info ( ""Candidate models=%s, metric=%s, jobID=%s"" % ( candidateIDs , bestMetric , self . _jobID ) ) if len ( candidateIDs ) == 0 : return self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults )",Chooses the best model for a given job .
"def createEncoder ( ) : consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = ""consumption"" , clipInput = True ) time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = ""timestamp_timeOfDay"" ) encoder = MultiEncoder ( ) encoder . addEncoder ( ""consumption"" , consumption_encoder ) encoder . addEncoder ( ""timestamp"" , time_encoder ) return encoder",Create the encoder instance for our test and return it .
"def createNetwork ( dataSource ) : network = Network ( ) network . addRegion ( ""sensor"" , ""py.RecordSensor"" , json . dumps ( { ""verbosity"" : _VERBOSITY } ) ) sensor = network . regions [ ""sensor"" ] . getSelf ( ) sensor . encoder = createEncoder ( ) sensor . dataSource = dataSource SP_PARAMS [ ""inputWidth"" ] = sensor . encoder . getWidth ( ) network . addRegion ( ""spatialPoolerRegion"" , ""py.SPRegion"" , json . dumps ( SP_PARAMS ) ) network . link ( ""sensor"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" ) network . link ( ""sensor"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" , srcOutput = ""resetOut"" , destInput = ""resetIn"" ) network . link ( ""spatialPoolerRegion"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""spatialTopDownOut"" , destInput = ""spatialTopDownIn"" ) network . link ( ""spatialPoolerRegion"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""temporalTopDownOut"" , destInput = ""temporalTopDownIn"" ) network . addRegion ( ""temporalPoolerRegion"" , ""py.TMRegion"" , json . dumps ( TM_PARAMS ) ) network . link ( ""spatialPoolerRegion"" , ""temporalPoolerRegion"" , ""UniformLink"" , """" ) network . link ( ""temporalPoolerRegion"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" , srcOutput = ""topDownOut"" , destInput = ""topDownIn"" ) network . addRegion ( ""anomalyLikelihoodRegion"" , ""py.AnomalyLikelihoodRegion"" , json . dumps ( { } ) ) network . link ( ""temporalPoolerRegion"" , ""anomalyLikelihoodRegion"" , ""UniformLink"" , """" , srcOutput = ""anomalyScore"" , destInput = ""rawAnomalyScore"" ) network . link ( ""sensor"" , ""anomalyLikelihoodRegion"" , ""UniformLink"" , """" , srcOutput = ""sourceOut"" , destInput = ""metricValue"" ) spatialPoolerRegion = network . regions [ ""spatialPoolerRegion"" ] spatialPoolerRegion . setParameter ( ""learningMode"" , True ) spatialPoolerRegion . setParameter ( ""anomalyMode"" , False ) temporalPoolerRegion = network . regions [ ""temporalPoolerRegion"" ] temporalPoolerRegion . setParameter ( ""topDownMode"" , True ) temporalPoolerRegion . setParameter ( ""learningMode"" , True ) temporalPoolerRegion . setParameter ( ""inferenceMode"" , True ) temporalPoolerRegion . setParameter ( ""anomalyMode"" , True ) return network",Create the Network instance .
"def runNetwork ( network , writer ) : sensorRegion = network . regions [ ""sensor"" ] spatialPoolerRegion = network . regions [ ""spatialPoolerRegion"" ] temporalPoolerRegion = network . regions [ ""temporalPoolerRegion"" ] anomalyLikelihoodRegion = network . regions [ ""anomalyLikelihoodRegion"" ] prevPredictedColumns = [ ] for i in xrange ( _NUM_RECORDS ) : network . run ( 1 ) consumption = sensorRegion . getOutputData ( ""sourceOut"" ) [ 0 ] anomalyScore = temporalPoolerRegion . getOutputData ( ""anomalyScore"" ) [ 0 ] anomalyLikelihood = anomalyLikelihoodRegion . getOutputData ( ""anomalyLikelihood"" ) [ 0 ] writer . writerow ( ( i , consumption , anomalyScore , anomalyLikelihood ) )",Run the network and write output to writer .
"def __validateExperimentControl ( self , control ) : taskList = control . get ( 'tasks' , None ) if taskList is not None : taskLabelsList = [ ] for task in taskList : validateOpfJsonValue ( task , ""opfTaskSchema.json"" ) validateOpfJsonValue ( task [ 'taskControl' ] , ""opfTaskControlSchema.json"" ) taskLabel = task [ 'taskLabel' ] assert isinstance ( taskLabel , types . StringTypes ) , ""taskLabel type: %r"" % type ( taskLabel ) assert len ( taskLabel ) > 0 , ""empty string taskLabel not is allowed"" taskLabelsList . append ( taskLabel . lower ( ) ) taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , taskLabelsList ) assert len ( taskLabelDuplicates ) == 0 , ""Duplcate task labels are not allowed: %s"" % taskLabelDuplicates return",Validates control dictionary for the experiment context
"def normalizeStreamSource ( self , stream ) : source = stream [ 'source' ] [ len ( FILE_SCHEME ) : ] if os . path . isabs ( source ) : sourcePath = source else : sourcePath = resource_filename ( ""nupic.datafiles"" , source ) if not os . path . exists ( sourcePath ) : sourcePath = os . path . join ( os . getcwd ( ) , source ) stream [ 'source' ] = FILE_SCHEME + sourcePath",TODO : document : param stream :
def normalizeStreamSources ( self ) : task = dict ( self . __control ) if 'dataset' in task : for stream in task [ 'dataset' ] [ 'streams' ] : self . normalizeStreamSource ( stream ) else : for subtask in task [ 'tasks' ] : for stream in subtask [ 'dataset' ] [ 'streams' ] : self . normalizeStreamSource ( stream ),TODO : document
"def convertNupicEnvToOPF ( self ) : task = dict ( self . __control ) task . pop ( 'environment' ) inferenceArgs = task . pop ( 'inferenceArgs' ) task [ 'taskLabel' ] = 'DefaultTask' iterationCount = task . get ( 'iterationCount' , - 1 ) iterationCountInferOnly = task . pop ( 'iterationCountInferOnly' , 0 ) if iterationCountInferOnly == - 1 : iterationCycle = [ IterationPhaseSpecInferOnly ( 1000 , inferenceArgs = inferenceArgs ) ] elif iterationCountInferOnly > 0 : assert iterationCount > 0 , ""When iterationCountInferOnly is specified, "" ""iterationCount must also be specified and not be -1"" iterationCycle = [ IterationPhaseSpecLearnAndInfer ( iterationCount - iterationCountInferOnly , inferenceArgs = inferenceArgs ) , IterationPhaseSpecInferOnly ( iterationCountInferOnly , inferenceArgs = inferenceArgs ) ] else : iterationCycle = [ IterationPhaseSpecLearnAndInfer ( 1000 , inferenceArgs = inferenceArgs ) ] taskControl = dict ( metrics = task . pop ( 'metrics' ) , loggedMetrics = task . pop ( 'loggedMetrics' ) , iterationCycle = iterationCycle ) task [ 'taskControl' ] = taskControl self . __control = dict ( environment = OpfEnvironment . Nupic , tasks = [ task ] )",TODO : document
"def createNetwork ( dataSource ) : network = Network ( ) network . addRegion ( ""sensor"" , ""py.RecordSensor"" , json . dumps ( { ""verbosity"" : _VERBOSITY } ) ) sensor = network . regions [ ""sensor"" ] . getSelf ( ) sensor . encoder = createEncoder ( ) sensor . dataSource = dataSource sys . path . append ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) from custom_region . identity_region import IdentityRegion Network . registerRegion ( IdentityRegion ) network . addRegion ( ""identityRegion"" , ""py.IdentityRegion"" , json . dumps ( { ""dataWidth"" : sensor . encoder . getWidth ( ) , } ) ) network . link ( ""sensor"" , ""identityRegion"" , ""UniformLink"" , """" ) network . initialize ( ) return network",Create the Network instance .
"def runNetwork ( network , writer ) : identityRegion = network . regions [ ""identityRegion"" ] for i in xrange ( _NUM_RECORDS ) : network . run ( 1 ) encoding = identityRegion . getOutputData ( ""out"" ) writer . writerow ( ( i , encoding ) )",Run the network and write output to writer .
"def _appendReportKeys ( keys , prefix , results ) : allKeys = results . keys ( ) allKeys . sort ( ) for key in allKeys : if hasattr ( results [ key ] , 'keys' ) : _appendReportKeys ( keys , ""%s%s:"" % ( prefix , key ) , results [ key ] ) else : keys . add ( ""%s%s"" % ( prefix , key ) )",Generate a set of possible report keys for an experiment s results . A report key is a string of key names separated by colons each key being one level deeper into the experiment results dict . For example key1 : key2 .
"def _matchReportKeys ( reportKeyREs = [ ] , allReportKeys = [ ] ) : matchingReportKeys = [ ] for keyRE in reportKeyREs : matchObj = re . compile ( keyRE ) found = False for keyName in allReportKeys : match = matchObj . match ( keyName ) if match and match . end ( ) == len ( keyName ) : matchingReportKeys . append ( keyName ) found = True if not found : raise _BadKeyError ( keyRE ) return matchingReportKeys",Extract all items from the allKeys list whose key matches one of the regular expressions passed in reportKeys .
"def _getReportItem ( itemName , results ) : subKeys = itemName . split ( ':' ) subResults = results for subKey in subKeys : subResults = subResults [ subKey ] return subResults",Get a specific item by name out of the results dict .
"def filterResults ( allResults , reportKeys , optimizeKey = None ) : optimizeDict = dict ( ) allReportKeys = set ( ) _appendReportKeys ( keys = allReportKeys , prefix = '' , results = allResults ) matchingKeys = _matchReportKeys ( reportKeys , allReportKeys ) reportDict = dict ( ) for keyName in matchingKeys : value = _getReportItem ( keyName , allResults ) reportDict [ keyName ] = value if optimizeKey is not None : matchingKeys = _matchReportKeys ( [ optimizeKey ] , allReportKeys ) if len ( matchingKeys ) == 0 : raise _BadKeyError ( optimizeKey ) elif len ( matchingKeys ) > 1 : raise _BadOptimizeKeyError ( optimizeKey , matchingKeys ) optimizeKeyFullName = matchingKeys [ 0 ] value = _getReportItem ( optimizeKeyFullName , allResults ) optimizeDict [ optimizeKeyFullName ] = value reportDict [ optimizeKeyFullName ] = value return ( reportDict , optimizeDict )",Given the complete set of results generated by an experiment ( passed in results ) filter out and return only the ones the caller wants as specified through reportKeys and optimizeKey .
"def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) : msg = StringIO . StringIO ( ) print >> msg , ""Exception occurred while running model %s: %r (%s)"" % ( modelID , e , type ( e ) ) traceback . print_exc ( None , msg ) completionReason = jobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) logger . error ( completionMsg ) if type ( e ) is not InvalidConnectionException : jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) if type ( e ) == JobFailException : workerCmpReason = jobsDAO . jobGetFields ( jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = "": "" . join ( str ( i ) for i in e . args ) ) , useConnectionID = False , ignoreUnchanged = True ) return ( completionReason , completionMsg )",Perform standard handling of an exception that occurs while running a model .
"def runModelGivenBaseAndParams ( modelID , jobID , baseDescription , params , predictedField , reportKeys , optimizeKey , jobsDAO , modelCheckpointGUID , logLevel = None , predictionCacheMaxRecords = None ) : from nupic . swarming . ModelRunner import OPFModelRunner logger = logging . getLogger ( 'com.numenta.nupic.hypersearch.utils' ) experimentDir = tempfile . mkdtemp ( ) try : logger . info ( ""Using experiment directory: %s"" % ( experimentDir ) ) paramsFilePath = os . path . join ( experimentDir , 'description.py' ) paramsFile = open ( paramsFilePath , 'wb' ) paramsFile . write ( _paramsFileHead ( ) ) items = params . items ( ) items . sort ( ) for ( key , value ) in items : quotedKey = _quoteAndEscape ( key ) if isinstance ( value , basestring ) : paramsFile . write ( ""  %s : '%s',\n"" % ( quotedKey , value ) ) else : paramsFile . write ( ""  %s : %s,\n"" % ( quotedKey , value ) ) paramsFile . write ( _paramsFileTail ( ) ) paramsFile . close ( ) baseParamsFile = open ( os . path . join ( experimentDir , 'base.py' ) , 'wb' ) baseParamsFile . write ( baseDescription ) baseParamsFile . close ( ) fd = open ( paramsFilePath ) expDescription = fd . read ( ) fd . close ( ) jobsDAO . modelSetFields ( modelID , { 'genDescription' : expDescription } ) try : runner = OPFModelRunner ( modelID = modelID , jobID = jobID , predictedField = predictedField , experimentDir = experimentDir , reportKeyPatterns = reportKeys , optimizeKeyPattern = optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = predictionCacheMaxRecords ) signal . signal ( signal . SIGINT , runner . handleWarningSignal ) ( completionReason , completionMsg ) = runner . run ( ) except InvalidConnectionException : raise except Exception , e : ( completionReason , completionMsg ) = _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , e ) finally : shutil . rmtree ( experimentDir ) signal . signal ( signal . SIGINT , signal . default_int_handler ) return ( completionReason , completionMsg )",This creates an experiment directory with a base . py description file created from baseDescription and a description . py generated from the given params dict and then runs the experiment .
"def rCopy ( d , f = identityConversion , discardNoneKeys = True , deepCopy = True ) : if deepCopy : d = copy . deepcopy ( d ) newDict = { } toCopy = [ ( k , v , newDict , ( ) ) for k , v in d . iteritems ( ) ] while len ( toCopy ) > 0 : k , v , d , prevKeys = toCopy . pop ( ) prevKeys = prevKeys + ( k , ) if isinstance ( v , dict ) : d [ k ] = dict ( ) toCopy [ 0 : 0 ] = [ ( innerK , innerV , d [ k ] , prevKeys ) for innerK , innerV in v . iteritems ( ) ] else : newV = f ( v , prevKeys ) if not discardNoneKeys or newV is not None : d [ k ] = newV return newDict",Recursively copies a dict and returns the result .
"def rApply ( d , f ) : remainingDicts = [ ( d , ( ) ) ] while len ( remainingDicts ) > 0 : current , prevKeys = remainingDicts . pop ( ) for k , v in current . iteritems ( ) : keys = prevKeys + ( k , ) if isinstance ( v , dict ) : remainingDicts . insert ( 0 , ( v , keys ) ) else : f ( v , keys )",Recursively applies f to the values in dict d .
"def clippedObj ( obj , maxElementSize = 64 ) : if hasattr ( obj , '_asdict' ) : obj = obj . _asdict ( ) if isinstance ( obj , dict ) : objOut = dict ( ) for key , val in obj . iteritems ( ) : objOut [ key ] = clippedObj ( val ) elif hasattr ( obj , '__iter__' ) : objOut = [ ] for val in obj : objOut . append ( clippedObj ( val ) ) else : objOut = str ( obj ) if len ( objOut ) > maxElementSize : objOut = objOut [ 0 : maxElementSize ] + '...' return objOut",Return a clipped version of obj suitable for printing This is useful when generating log messages by printing data structures but don t want the message to be too long .
"def validate ( value , * * kwds ) : assert len ( kwds . keys ( ) ) >= 1 assert 'schemaPath' in kwds or 'schemaDict' in kwds schemaDict = None if 'schemaPath' in kwds : schemaPath = kwds . pop ( 'schemaPath' ) schemaDict = loadJsonValueFromFile ( schemaPath ) elif 'schemaDict' in kwds : schemaDict = kwds . pop ( 'schemaDict' ) try : validictory . validate ( value , schemaDict , * * kwds ) except validictory . ValidationError as e : raise ValidationError ( e )",Validate a python value against json schema : validate ( value schemaPath ) validate ( value schemaDict )
def loadJsonValueFromFile ( inputFilePath ) : with open ( inputFilePath ) as fileObj : value = json . load ( fileObj ) return value,Loads a json value from a file and converts it to the corresponding python object .
"def sortedJSONDumpS ( obj ) : itemStrs = [ ] if isinstance ( obj , dict ) : items = obj . items ( ) items . sort ( ) for key , value in items : itemStrs . append ( '%s: %s' % ( json . dumps ( key ) , sortedJSONDumpS ( value ) ) ) return '{%s}' % ( ', ' . join ( itemStrs ) ) elif hasattr ( obj , '__iter__' ) : for val in obj : itemStrs . append ( sortedJSONDumpS ( val ) ) return '[%s]' % ( ', ' . join ( itemStrs ) ) else : return json . dumps ( obj )",Return a JSON representation of obj with sorted keys on any embedded dicts . This insures that the same object will always be represented by the same string even if it contains dicts ( where the sort order of the keys is normally undefined ) .
def tick ( self ) : for act in self . __activities : if not act . iteratorHolder [ 0 ] : continue try : next ( act . iteratorHolder [ 0 ] ) except StopIteration : act . cb ( ) if act . repeating : act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) else : act . iteratorHolder [ 0 ] = None return True,Activity tick handler ; services all activities
"def rUpdate ( original , updates ) : dictPairs = [ ( original , updates ) ] while len ( dictPairs ) > 0 : original , updates = dictPairs . pop ( ) for k , v in updates . iteritems ( ) : if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : dictPairs . append ( ( original [ k ] , v ) ) else : original [ k ] = v",Recursively updates the values in original with the values from updates .
"def dictDiffAndReport ( da , db ) : differences = dictDiff ( da , db ) if not differences : return differences if differences [ 'inAButNotInB' ] : print "">>> inAButNotInB: %s"" % differences [ 'inAButNotInB' ] if differences [ 'inBButNotInA' ] : print "">>> inBButNotInA: %s"" % differences [ 'inBButNotInA' ] for key in differences [ 'differentValues' ] : print "">>> da[%s] != db[%s]"" % ( key , key ) print ""da[%s] = %r"" % ( key , da [ key ] ) print ""db[%s] = %r"" % ( key , db [ key ] ) return differences",Compares two python dictionaries at the top level and report differences if any to stdout
"def dictDiff ( da , db ) : different = False resultDict = dict ( ) resultDict [ 'inAButNotInB' ] = set ( da ) - set ( db ) if resultDict [ 'inAButNotInB' ] : different = True resultDict [ 'inBButNotInA' ] = set ( db ) - set ( da ) if resultDict [ 'inBButNotInA' ] : different = True resultDict [ 'differentValues' ] = [ ] for key in ( set ( da ) - resultDict [ 'inAButNotInB' ] ) : comparisonResult = da [ key ] == db [ key ] if isinstance ( comparisonResult , bool ) : isEqual = comparisonResult else : isEqual = comparisonResult . all ( ) if not isEqual : resultDict [ 'differentValues' ] . append ( key ) different = True assert ( ( ( resultDict [ 'inAButNotInB' ] or resultDict [ 'inBButNotInA' ] or resultDict [ 'differentValues' ] ) and different ) or not different ) return resultDict if different else None",Compares two python dictionaries at the top level and return differences
"def _seed ( self , seed = - 1 ) : if seed != - 1 : self . random = NupicRandom ( seed ) else : self . random = NupicRandom ( )",Initialize the random seed
"def _newRep ( self ) : maxAttempts = 1000 for _ in xrange ( maxAttempts ) : foundUnique = True population = numpy . arange ( self . n , dtype = numpy . uint32 ) choices = numpy . arange ( self . w , dtype = numpy . uint32 ) oneBits = sorted ( self . random . sample ( population , choices ) ) sdr = numpy . zeros ( self . n , dtype = 'uint8' ) sdr [ oneBits ] = 1 for i in xrange ( self . ncategories ) : if ( sdr == self . sdrs [ i ] ) . all ( ) : foundUnique = False break if foundUnique : break if not foundUnique : raise RuntimeError ( ""Error, could not find unique pattern %d after "" ""%d attempts"" % ( self . ncategories , maxAttempts ) ) return sdr",Generate a new and unique representation . Returns a numpy array of shape ( n ) .
"def getScalars ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ 0 ] ) index = self . categoryToIndex . get ( input , None ) if index is None : if self . _learningEnabled : self . _addCategory ( input ) index = self . ncategories - 1 else : index = 0 return numpy . array ( [ index ] )",See method description in base . py
"def decode ( self , encoded , parentFieldName = '' ) : assert ( encoded [ 0 : self . n ] <= 1.0 ) . all ( ) resultString = """" resultRanges = [ ] overlaps = ( self . sdrs * encoded [ 0 : self . n ] ) . sum ( axis = 1 ) if self . verbosity >= 2 : print ""Overlaps for decoding:"" for i in xrange ( 0 , self . ncategories ) : print ""%d %s"" % ( overlaps [ i ] , self . categories [ i ] ) matchingCategories = ( overlaps > self . thresholdOverlap ) . nonzero ( ) [ 0 ] for index in matchingCategories : if resultString != """" : resultString += "" "" resultString += str ( self . categories [ index ] ) resultRanges . append ( [ int ( index ) , int ( index ) ] ) if parentFieldName != '' : fieldName = ""%s.%s"" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( resultRanges , resultString ) } , [ fieldName ] )",See the function description in base . py
"def _getTopDownMapping ( self ) : if self . _topDownMappingM is None : self . _topDownMappingM = SM32 ( self . ncategories , self . n ) outputSpace = numpy . zeros ( self . n , dtype = GetNTAReal ( ) ) for i in xrange ( self . ncategories ) : self . encodeIntoArray ( self . categories [ i ] , outputSpace ) self . _topDownMappingM . setRowFromDense ( i , outputSpace ) return self . _topDownMappingM",Return the interal _topDownMappingM matrix used for handling the bucketInfo () and topDownCompute () methods . This is a matrix one row per category ( bucket ) where each row contains the encoded output for that category .
"def getBucketInfo ( self , buckets ) : if self . ncategories == 0 : return 0 topDownMappingM = self . _getTopDownMapping ( ) categoryIndex = buckets [ 0 ] category = self . categories [ categoryIndex ] encoding = topDownMappingM . getRow ( categoryIndex ) return [ EncoderResult ( value = category , scalar = categoryIndex , encoding = encoding ) ]",See the function description in base . py
"def topDownCompute ( self , encoded ) : if self . ncategories == 0 : return 0 topDownMappingM = self . _getTopDownMapping ( ) categoryIndex = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) category = self . categories [ categoryIndex ] encoding = topDownMappingM . getRow ( categoryIndex ) return EncoderResult ( value = category , scalar = categoryIndex , encoding = encoding )",See the function description in base . py
"def getScalarNames ( self , parentFieldName = '' ) : names = [ ] def _formFieldName ( encoder ) : if parentFieldName == '' : return encoder . name else : return '%s.%s' % ( parentFieldName , encoder . name ) if self . seasonEncoder is not None : names . append ( _formFieldName ( self . seasonEncoder ) ) if self . dayOfWeekEncoder is not None : names . append ( _formFieldName ( self . dayOfWeekEncoder ) ) if self . customDaysEncoder is not None : names . append ( _formFieldName ( self . customDaysEncoder ) ) if self . weekendEncoder is not None : names . append ( _formFieldName ( self . weekendEncoder ) ) if self . holidayEncoder is not None : names . append ( _formFieldName ( self . holidayEncoder ) ) if self . timeOfDayEncoder is not None : names . append ( _formFieldName ( self . timeOfDayEncoder ) ) return names",See method description in base . py
"def getEncodedValues ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ None ] ) assert isinstance ( input , datetime . datetime ) values = [ ] timetuple = input . timetuple ( ) timeOfDay = timetuple . tm_hour + float ( timetuple . tm_min ) / 60.0 if self . seasonEncoder is not None : dayOfYear = timetuple . tm_yday values . append ( dayOfYear - 1 ) if self . dayOfWeekEncoder is not None : dayOfWeek = timetuple . tm_wday + timeOfDay / 24.0 values . append ( dayOfWeek ) if self . weekendEncoder is not None : if timetuple . tm_wday == 6 or timetuple . tm_wday == 5 or ( timetuple . tm_wday == 4 and timeOfDay > 18 ) : weekend = 1 else : weekend = 0 values . append ( weekend ) if self . customDaysEncoder is not None : if timetuple . tm_wday in self . customDays : customDay = 1 else : customDay = 0 values . append ( customDay ) if self . holidayEncoder is not None : if len ( self . holidays ) == 0 : holidays = [ ( 12 , 25 ) ] else : holidays = self . holidays val = 0 for h in holidays : if len ( h ) == 3 : hdate = datetime . datetime ( h [ 0 ] , h [ 1 ] , h [ 2 ] , 0 , 0 , 0 ) else : hdate = datetime . datetime ( timetuple . tm_year , h [ 0 ] , h [ 1 ] , 0 , 0 , 0 ) if input > hdate : diff = input - hdate if diff . days == 0 : val = 1 break elif diff . days == 1 : val = 1.0 - ( float ( diff . seconds ) / 86400 ) break else : diff = hdate - input if diff . days == 0 : val = 1.0 - ( float ( diff . seconds ) / 86400 ) values . append ( val ) if self . timeOfDayEncoder is not None : values . append ( timeOfDay ) return values",See method description in base . py
"def getBucketIndices ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] * len ( self . encoders ) else : assert isinstance ( input , datetime . datetime ) scalars = self . getScalars ( input ) result = [ ] for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] result . extend ( encoder . getBucketIndices ( scalars [ i ] ) ) return result",See method description in base . py
"def encodeIntoArray ( self , input , output ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : output [ 0 : ] = 0 else : if not isinstance ( input , datetime . datetime ) : raise ValueError ( ""Input is type %s, expected datetime. Value: %s"" % ( type ( input ) , str ( input ) ) ) scalars = self . getScalars ( input ) for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] encoder . encodeIntoArray ( scalars [ i ] , output [ offset : ] )",See method description in base . py
"def getSpec ( cls ) : spec = { ""description"" : IdentityRegion . __doc__ , ""singleNodeOnly"" : True , ""inputs"" : { ""in"" : { ""description"" : ""The input vector."" , ""dataType"" : ""Real32"" , ""count"" : 0 , ""required"" : True , ""regionLevel"" : False , ""isDefaultInput"" : True , ""requireSplitterMap"" : False } , } , ""outputs"" : { ""out"" : { ""description"" : ""A copy of the input vector."" , ""dataType"" : ""Real32"" , ""count"" : 0 , ""regionLevel"" : True , ""isDefaultOutput"" : True } , } , ""parameters"" : { ""dataWidth"" : { ""description"" : ""Size of inputs"" , ""accessMode"" : ""Read"" , ""dataType"" : ""UInt32"" , ""count"" : 1 , ""constraints"" : """" } , } , } return spec",Return the Spec for IdentityRegion .
"def _setRandomEncoderResolution ( minResolution = 0.001 ) : encoder = ( model_params . MODEL_PARAMS [ ""modelParams"" ] [ ""sensorParams"" ] [ ""encoders"" ] [ ""value"" ] ) if encoder [ ""type"" ] == ""RandomDistributedScalarEncoder"" : rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 minValue = _INPUT_MIN - rangePadding maxValue = _INPUT_MAX + rangePadding resolution = max ( minResolution , ( maxValue - minValue ) / encoder . pop ( ""numBuckets"" ) ) encoder [ ""resolution"" ] = resolution",Given model params figure out the correct resolution for the RandomDistributed encoder . Modifies params in place .
"def addLabel ( self , start , end , labelName ) : if len ( self . saved_states ) == 0 : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for 'addLabel'. "" ""Model has no saved records."" ) startID = self . saved_states [ 0 ] . ROWID clippedStart = max ( 0 , start - startID ) clippedEnd = max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for 'addLabel'."" , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . saved_states ) } ) for state in self . saved_states [ clippedStart : clippedEnd ] : if labelName not in state . anomalyLabel : state . anomalyLabel . append ( labelName ) state . setByUser = True self . _addRecordToKNN ( state ) assert len ( self . saved_categories ) > 0 for state in self . saved_states [ clippedEnd : ] : self . _updateState ( state )",Add the label labelName to each record with record ROWID in range from start to end noninclusive of end .
"def removeLabels ( self , start = None , end = None , labelFilter = None ) : if len ( self . saved_states ) == 0 : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for "" ""'removeLabels'. Model has no saved records."" ) startID = self . saved_states [ 0 ] . ROWID clippedStart = 0 if start is None else max ( 0 , start - startID ) clippedEnd = len ( self . saved_states ) if end is None else max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) if clippedEnd <= clippedStart : raise HTMPredictionModelInvalidRangeError ( ""Invalid supplied range for "" ""'removeLabels'."" , debugInfo = { 'requestRange' : { 'startRecordID' : start , 'endRecordID' : end } , 'clippedRequestRange' : { 'startRecordID' : clippedStart , 'endRecordID' : clippedEnd } , 'validRange' : { 'startRecordID' : startID , 'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID } , 'numRecordsStored' : len ( self . saved_states ) } ) recordsToDelete = [ ] for state in self . saved_states [ clippedStart : clippedEnd ] : if labelFilter is not None : if labelFilter in state . anomalyLabel : state . anomalyLabel . remove ( labelFilter ) else : state . anomalyLabel = [ ] state . setByUser = False recordsToDelete . append ( state ) self . _deleteRecordsFromKNN ( recordsToDelete ) self . _deleteRangeFromKNN ( start , end ) for state in self . saved_states [ clippedEnd : ] : self . _updateState ( state ) return { 'status' : 'success' }",Remove labels from each record with record ROWID in range from start to end noninclusive of end . Removes all records if labelFilter is None otherwise only removes the labels eqaul to labelFilter .
"def _addRecordToKNN ( self , record ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) category = self . _labelListToCategoryNumber ( record . anomalyLabel ) if record . ROWID in prototype_idx : knn . prototypeSetCategory ( record . ROWID , category ) return pattern = self . _getStateAnomalyVector ( record ) rowID = record . ROWID knn . learn ( pattern , category , rowID = rowID )",This method will add the record to the KNN classifier .
"def _deleteRecordsFromKNN ( self , recordsToDelete ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) idsToDelete = [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete ) assert knn . _numPatterns == nProtos - len ( idsToDelete )",This method will remove the given records from the classifier .
"def _deleteRangeFromKNN ( self , start = 0 , end = None ) : classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn prototype_idx = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) if end is None : end = prototype_idx . max ( ) + 1 idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , prototype_idx < end ) idsToDelete = prototype_idx [ idsIdxToDelete ] nProtos = knn . _numPatterns knn . removeIds ( idsToDelete . tolist ( ) ) assert knn . _numPatterns == nProtos - len ( idsToDelete )",This method will remove any stored records within the range from start to end . Noninclusive of end .
"def _recomputeRecordFromKNN ( self , record ) : inputs = { ""categoryIn"" : [ None ] , ""bottomUpIn"" : self . _getStateAnomalyVector ( record ) , } outputs = { ""categoriesOut"" : numpy . zeros ( ( 1 , ) ) , ""bestPrototypeIndices"" : numpy . zeros ( ( 1 , ) ) , ""categoryProbabilitiesOut"" : numpy . zeros ( ( 1 , ) ) } classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) knn = classifier . getSelf ( ) . _knn classifier_indexes = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) valid_idx = numpy . where ( ( classifier_indexes >= self . _autoDetectWaitRecords ) & ( classifier_indexes < record . ROWID ) ) [ 0 ] . tolist ( ) if len ( valid_idx ) == 0 : return None classifier . setParameter ( 'inferenceMode' , True ) classifier . setParameter ( 'learningMode' , False ) classifier . getSelf ( ) . compute ( inputs , outputs ) classifier . setParameter ( 'learningMode' , True ) classifier_distances = classifier . getSelf ( ) . getLatestDistances ( ) valid_distances = classifier_distances [ valid_idx ] if valid_distances . min ( ) <= self . _classificationMaxDist : classifier_indexes_prev = classifier_indexes [ valid_idx ] rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] category = classifier . getSelf ( ) . getCategoryList ( ) [ indexID ] return category return None",return the classified labeling of record
"def _constructClassificationRecord ( self ) : model = self . htm_prediction_model sp = model . _getSPRegion ( ) tm = model . _getTPRegion ( ) tpImp = tm . getSelf ( ) . _tfdr activeColumns = sp . getOutputData ( ""bottomUpOut"" ) . nonzero ( ) [ 0 ] score = numpy . in1d ( activeColumns , self . _prevPredictedColumns ) . sum ( ) score = ( self . _activeColumnCount - score ) / float ( self . _activeColumnCount ) spSize = sp . getParameter ( 'activeOutputCount' ) tpSize = tm . getParameter ( 'cellsPerColumn' ) * tm . getParameter ( 'columnCount' ) classificationVector = numpy . array ( [ ] ) if self . _vectorType == 'tpc' : classificationVector = numpy . zeros ( tpSize ) activeCellMatrix = tpImp . getLearnActiveStateT ( ) . reshape ( tpSize , 1 ) activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] if activeCellIdx . shape [ 0 ] > 0 : classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 elif self . _vectorType == 'sp_tpe' : classificationVector = numpy . zeros ( spSize + spSize ) if activeColumns . shape [ 0 ] > 0 : classificationVector [ activeColumns ] = 1.0 errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeColumns ) if errorColumns . shape [ 0 ] > 0 : errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + spSize ) classificationVector [ errorColumnIndexes ] = 1.0 else : raise TypeError ( ""Classification vector type must be either 'tpc' or"" "" 'sp_tpe', current value is %s"" % ( self . _vectorType ) ) numPredictedCols = len ( self . _prevPredictedColumns ) predictedColumns = tm . getOutputData ( ""topDownOut"" ) . nonzero ( ) [ 0 ] self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) if self . _anomalyVectorLength is None : self . _anomalyVectorLength = len ( classificationVector ) result = _CLAClassificationRecord ( ROWID = int ( model . getParameter ( '__numRunCalls' ) - 1 ) , anomalyScore = score , anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , anomalyLabel = [ ] ) return result",Construct a _HTMClassificationRecord based on the current state of the htm_prediction_model of this classifier .
def compute ( self ) : result = self . _constructClassificationRecord ( ) if result . ROWID >= self . _autoDetectWaitRecords : self . _updateState ( result ) self . saved_states . append ( result ) if len ( self . saved_states ) > self . _history_length : self . saved_states . pop ( 0 ) return result,Run an iteration of this anomaly classifier
"def setAutoDetectWaitRecords ( self , waitRecords ) : if not isinstance ( waitRecords , int ) : raise HTMPredictionModelInvalidArgument ( ""Invalid argument type \'%s\'. WaitRecord "" ""must be a number."" % ( type ( waitRecords ) ) ) if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : raise HTMPredictionModelInvalidArgument ( ""Invalid value. autoDetectWaitRecord value "" ""must be valid record within output stream. Current minimum ROWID in "" ""output stream is %d."" % ( self . saved_states [ 0 ] . ROWID ) ) self . _autoDetectWaitRecords = waitRecords for state in self . saved_states : self . _updateState ( state )",Sets the autoDetectWaitRecords .
"def setAutoDetectThreshold ( self , threshold ) : if not ( isinstance ( threshold , float ) or isinstance ( threshold , int ) ) : raise HTMPredictionModelInvalidArgument ( ""Invalid argument type \'%s\'. threshold "" ""must be a number."" % ( type ( threshold ) ) ) self . _autoDetectThreshold = threshold for state in self . saved_states : self . _updateState ( state )",Sets the autoDetectThreshold . TODO : Ensure previously classified points outside of classifier are valid .
"def _getAdditionalSpecs ( spatialImp , kwargs = { } ) : typeNames = { int : 'UInt32' , float : 'Real32' , str : 'Byte' , bool : 'bool' , tuple : 'tuple' } def getArgType ( arg ) : t = typeNames . get ( type ( arg ) , 'Byte' ) count = 0 if t == 'Byte' else 1 if t == 'tuple' : t = typeNames . get ( type ( arg [ 0 ] ) , 'Byte' ) count = len ( arg ) if t == 'bool' : t = 'UInt32' return ( t , count ) def getConstraints ( arg ) : t = typeNames . get ( type ( arg ) , 'Byte' ) if t == 'Byte' : return 'multiple' elif t == 'bool' : return 'bool' else : return '' SpatialClass = getSPClass ( spatialImp ) sArgTuples = _buildArgs ( SpatialClass . __init__ ) spatialSpec = { } for argTuple in sArgTuples : d = dict ( description = argTuple [ 1 ] , accessMode = 'ReadWrite' , dataType = getArgType ( argTuple [ 2 ] ) [ 0 ] , count = getArgType ( argTuple [ 2 ] ) [ 1 ] , constraints = getConstraints ( argTuple [ 2 ] ) ) spatialSpec [ argTuple [ 0 ] ] = d spatialSpec . update ( dict ( columnCount = dict ( description = 'Total number of columns (coincidences).' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , inputWidth = dict ( description = 'Size of inputs to the SP.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , spInputNonZeros = dict ( description = 'The indices of the non-zero inputs to the spatial pooler' , accessMode = 'Read' , dataType = 'UInt32' , count = 0 , constraints = '' ) , spOutputNonZeros = dict ( description = 'The indices of the non-zero outputs from the spatial pooler' , accessMode = 'Read' , dataType = 'UInt32' , count = 0 , constraints = '' ) , spOverlapDistribution = dict ( description = """"""The overlaps between the active output coincidences
      and the input. The overlap amounts for each coincidence are sorted
      from highest to lowest. """""" , accessMode = 'Read' , dataType = 'Real32' , count = 0 , constraints = '' ) , sparseCoincidenceMatrix = dict ( description = 'The coincidences, as a SparseMatrix' , accessMode = 'Read' , dataType = 'Byte' , count = 0 , constraints = '' ) , denseOutput = dict ( description = 'Score for each coincidence.' , accessMode = 'Read' , dataType = 'Real32' , count = 0 , constraints = '' ) , spLearningStatsStr = dict ( description = """"""String representation of dictionary containing a number
                     of statistics related to learning."""""" , accessMode = 'Read' , dataType = 'Byte' , count = 0 , constraints = 'handle' ) , spatialImp = dict ( description = """"""Which spatial pooler implementation to use. Set to either
                      'py', or 'cpp'. The 'cpp' implementation is optimized for
                      speed in C++."""""" , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = 'enum: py, cpp' ) , ) ) otherSpec = dict ( learningMode = dict ( description = '1 if the node is learning (default 1).' , accessMode = 'ReadWrite' , dataType = 'UInt32' , count = 1 , constraints = 'bool' ) , inferenceMode = dict ( description = '1 if the node is inferring (default 0).' , accessMode = 'ReadWrite' , dataType = 'UInt32' , count = 1 , constraints = 'bool' ) , anomalyMode = dict ( description = '1 if an anomaly score is being computed' , accessMode = 'ReadWrite' , dataType = 'UInt32' , count = 1 , constraints = 'bool' ) , topDownMode = dict ( description = '1 if the node should do top down compute on the next call ' 'to compute into topDownOut (default 0).' , accessMode = 'ReadWrite' , dataType = 'UInt32' , count = 1 , constraints = 'bool' ) , activeOutputCount = dict ( description = 'Number of active elements in bottomUpOut output.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , logPathInput = dict ( description = 'Optional name of input log file. If set, every input vector' ' will be logged to this file.' , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = '' ) , logPathOutput = dict ( description = 'Optional name of output log file. If set, every output vector' ' will be logged to this file.' , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = '' ) , logPathOutputDense = dict ( description = 'Optional name of output log file. If set, every output vector' ' will be logged to this file as a dense vector.' , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = '' ) , ) return spatialSpec , otherSpec",Build the additional specs in three groups ( for the inspector )
"def _initializeEphemeralMembers ( self ) : for attrName in self . _getEphemeralMembersBase ( ) : if attrName != ""_loaded"" : if hasattr ( self , attrName ) : if self . _loaded : pass else : print self . __class__ . __name__ , ""contains base class member '%s'"" % attrName if not self . _loaded : for attrName in self . _getEphemeralMembersBase ( ) : if attrName != ""_loaded"" : assert not hasattr ( self , attrName ) else : assert hasattr ( self , attrName ) self . _profileObj = None self . _iterations = 0 self . _initEphemerals ( ) self . _checkEphemeralMembers ( )",Initialize all ephemeral data members and give the derived class the opportunity to do the same by invoking the virtual member _initEphemerals () which is intended to be overridden .
"def initialize ( self ) : self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , dtype = GetNTAReal ( ) ) self . _spatialPoolerInput = numpy . zeros ( ( 1 , self . inputWidth ) , dtype = GetNTAReal ( ) ) self . _allocateSpatialFDR ( None )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . initialize .
"def _allocateSpatialFDR ( self , rfInput ) : if self . _sfdr : return autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _spatialArgNames ) if ( ( self . SpatialClass == CPPSpatialPooler ) or ( self . SpatialClass == PYSpatialPooler ) ) : autoArgs [ 'columnDimensions' ] = [ self . columnCount ] autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] autoArgs [ 'potentialRadius' ] = self . inputWidth self . _sfdr = self . SpatialClass ( * * autoArgs )",Allocate the spatial pooler instance .
"def compute ( self , inputs , outputs ) : if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : import hotshot if self . _iterations == 10 : print ""\n  Collecting and sorting internal node profiling stats generated by hotshot..."" stats = hotshot . stats . load ( ""hotshot.stats"" ) stats . strip_dirs ( ) stats . sort_stats ( 'time' , 'calls' ) stats . print_stats ( ) if self . _profileObj is None : print ""\n  Preparing to capture profile using hotshot..."" if os . path . exists ( 'hotshot.stats' ) : os . remove ( 'hotshot.stats' ) self . _profileObj = hotshot . Profile ( ""hotshot.stats"" , 1 , 1 ) self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) else : self . _compute ( inputs , outputs )",Run one iteration profiling it if requested .
"def _compute ( self , inputs , outputs ) : if self . _sfdr is None : raise RuntimeError ( ""Spatial pooler has not been initialized"" ) if not self . topDownMode : self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 rfOutput = self . _doBottomUpCompute ( rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , resetSignal = resetSignal ) outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat else : topDownIn = inputs . get ( 'topDownIn' , None ) spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut if temporalTopDownOut is not None : outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut outputs [ 'anomalyScore' ] [ : ] = 0",Run one iteration of SPRegion s compute
"def _doBottomUpCompute ( self , rfInput , resetSignal ) : self . _conditionalBreak ( ) self . _spatialPoolerInput = rfInput . reshape ( - 1 ) assert ( rfInput . shape [ 0 ] == 1 ) inputVector = numpy . array ( rfInput [ 0 ] ) . astype ( 'uint32' ) outputVector = numpy . zeros ( self . _sfdr . getNumColumns ( ) ) . astype ( 'uint32' ) self . _sfdr . compute ( inputVector , self . learningMode , outputVector ) self . _spatialPoolerOutput [ : ] = outputVector [ : ] if self . _fpLogSP : output = self . _spatialPoolerOutput . reshape ( - 1 ) outputNZ = output . nonzero ( ) [ 0 ] outStr = "" "" . join ( [ ""%d"" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogSP , output . size , outStr if self . _fpLogSPInput : output = rfInput . reshape ( - 1 ) outputNZ = output . nonzero ( ) [ 0 ] outStr = "" "" . join ( [ ""%d"" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogSPInput , output . size , outStr return self . _spatialPoolerOutput",Do one iteration of inference and / or learning and return the result
"def getBaseSpec ( cls ) : spec = dict ( description = SPRegion . __doc__ , singleNodeOnly = True , inputs = dict ( bottomUpIn = dict ( description = """"""The input vector."""""" , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , resetIn = dict ( description = """"""A boolean flag that indicates whether
                         or not the input vector received in this compute cycle
                         represents the start of a new temporal sequence."""""" , dataType = 'Real32' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , topDownIn = dict ( description = """"""The top-down input signal, generated from
                        feedback from upper levels"""""" , dataType = 'Real32' , count = 0 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , sequenceIdIn = dict ( description = ""Sequence ID"" , dataType = 'UInt64' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , ) , outputs = dict ( bottomUpOut = dict ( description = """"""The output signal generated from the bottom-up inputs
                          from lower levels."""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = True ) , topDownOut = dict ( description = """"""The top-down output signal, generated from
                        feedback from upper levels"""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , spatialTopDownOut = dict ( description = """"""The top-down output, generated only from the current
                         SP output. This can be used to evaluate how well the
                         SP is representing the inputs independent of the TM."""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , temporalTopDownOut = dict ( description = """"""The top-down output, generated only from the current
                         TM output feedback down through the SP."""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , anomalyScore = dict ( description = """"""The score for how 'anomalous' (i.e. rare) this spatial
                        input pattern is. Higher values are increasingly rare"""""" , dataType = 'Real32' , count = 1 , regionLevel = True , isDefaultOutput = False ) , ) , parameters = dict ( breakPdb = dict ( description = 'Set to 1 to stop in the pdb debugger on the next compute' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , breakKomodo = dict ( description = 'Set to 1 to stop in the Komodo debugger on the next compute' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , ) , ) return spec",Doesn t include the spatial temporal and other parameters
"def getSpec ( cls ) : spec = cls . getBaseSpec ( ) s , o = _getAdditionalSpecs ( spatialImp = getDefaultSPImp ( ) ) spec [ 'parameters' ] . update ( s ) spec [ 'parameters' ] . update ( o ) return spec",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getSpec .
"def getParameter ( self , parameterName , index = - 1 ) : if parameterName == 'activeOutputCount' : return self . columnCount elif parameterName == 'spatialPoolerInput' : return list ( self . _spatialPoolerInput . reshape ( - 1 ) ) elif parameterName == 'spatialPoolerOutput' : return list ( self . _spatialPoolerOutput ) elif parameterName == 'spNumActiveOutputs' : return len ( self . _spatialPoolerOutput . nonzero ( ) [ 0 ] ) elif parameterName == 'spOutputNonZeros' : return [ len ( self . _spatialPoolerOutput ) ] + list ( self . _spatialPoolerOutput . nonzero ( ) [ 0 ] ) elif parameterName == 'spInputNonZeros' : import pdb pdb . set_trace ( ) return [ len ( self . _spatialPoolerInput ) ] + list ( self . _spatialPoolerInput . nonzero ( ) [ 0 ] ) elif parameterName == 'spLearningStatsStr' : try : return str ( self . _sfdr . getLearningStats ( ) ) except : return str ( dict ( ) ) else : return PyRegion . getParameter ( self , parameterName , index )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getParameter . Most parameters are handled automatically by PyRegion s parameter get mechanism . The ones that need special treatment are explicitly handled here .
"def setParameter ( self , parameterName , index , parameterValue ) : if parameterName in self . _spatialArgNames : setattr ( self . _sfdr , parameterName , parameterValue ) elif parameterName == ""logPathInput"" : self . logPathInput = parameterValue if self . _fpLogSPInput : self . _fpLogSPInput . close ( ) self . _fpLogSPInput = None if parameterValue : self . _fpLogSPInput = open ( self . logPathInput , 'w' ) elif parameterName == ""logPathOutput"" : self . logPathOutput = parameterValue if self . _fpLogSP : self . _fpLogSP . close ( ) self . _fpLogSP = None if parameterValue : self . _fpLogSP = open ( self . logPathOutput , 'w' ) elif parameterName == ""logPathOutputDense"" : self . logPathOutputDense = parameterValue if self . _fpLogSPDense : self . _fpLogSPDense . close ( ) self . _fpLogSPDense = None if parameterValue : self . _fpLogSPDense = open ( self . logPathOutputDense , 'w' ) elif hasattr ( self , parameterName ) : setattr ( self , parameterName , parameterValue ) else : raise Exception ( 'Unknown parameter: ' + parameterName )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . setParameter .
"def writeToProto ( self , proto ) : proto . spatialImp = self . spatialImp proto . columnCount = self . columnCount proto . inputWidth = self . inputWidth proto . learningMode = 1 if self . learningMode else 0 proto . inferenceMode = 1 if self . inferenceMode else 0 proto . anomalyMode = 1 if self . anomalyMode else 0 proto . topDownMode = 1 if self . topDownMode else 0 self . _sfdr . write ( proto . spatialPooler )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . writeToProto .
"def readFromProto ( cls , proto ) : instance = cls ( proto . columnCount , proto . inputWidth ) instance . spatialImp = proto . spatialImp instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . anomalyMode = proto . anomalyMode instance . topDownMode = proto . topDownMode spatialImp = proto . spatialImp instance . _sfdr = getSPClass ( spatialImp ) . read ( proto . spatialPooler ) return instance",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . readFromProto .
"def _initEphemerals ( self ) : if hasattr ( self , '_sfdr' ) and self . _sfdr : self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , dtype = GetNTAReal ( ) ) else : self . _spatialPoolerOutput = None self . _fpLogSPInput = None self . _fpLogSP = None self . _fpLogSPDense = None self . logPathInput = """" self . logPathOutput = """" self . logPathOutputDense = """"",Initialize all ephemerals used by derived classes .
"def getParameterArrayCount ( self , name , index ) : p = self . getParameter ( name ) if ( not hasattr ( p , '__len__' ) ) : raise Exception ( ""Attempt to access parameter '%s' as an array but it is not an array"" % name ) return len ( p )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getParameterArrayCount .
"def getParameterArray ( self , name , index , a ) : p = self . getParameter ( name ) if ( not hasattr ( p , '__len__' ) ) : raise Exception ( ""Attempt to access parameter '%s' as an array but it is not an array"" % name ) if len ( p ) > 0 : a [ : ] = p [ : ]",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getParameterArray .
def _cacheSequenceInfoType ( self ) : hasReset = self . resetFieldName is not None hasSequenceId = self . sequenceIdFieldName is not None if hasReset and not hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY self . _prevSequenceId = 0 elif not hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY self . _prevSequenceId = None elif hasReset and hasSequenceId : self . _sequenceInfoType = self . SEQUENCEINFO_BOTH else : self . _sequenceInfoType = self . SEQUENCEINFO_NONE,Figure out whether reset sequenceId both or neither are present in the data . Compute once instead of every time .
"def _getTPClass ( temporalImp ) : if temporalImp == 'py' : return backtracking_tm . BacktrackingTM elif temporalImp == 'cpp' : return backtracking_tm_cpp . BacktrackingTMCPP elif temporalImp == 'tm_py' : return backtracking_tm_shim . TMShim elif temporalImp == 'tm_cpp' : return backtracking_tm_shim . TMCPPShim elif temporalImp == 'monitored_tm_py' : return backtracking_tm_shim . MonitoredTMShim else : raise RuntimeError ( ""Invalid temporalImp '%s'. Legal values are: 'py', "" ""'cpp', 'tm_py', 'monitored_tm_py'"" % ( temporalImp ) )",Return the class corresponding to the given temporalImp string
"def _buildArgs ( f , self = None , kwargs = { } ) : argTuples = getArgumentDescriptions ( f ) argTuples = argTuples [ 1 : ] init = TMRegion . __init__ ourArgNames = [ t [ 0 ] for t in getArgumentDescriptions ( init ) ] ourArgNames += [ 'numberOfCols' , ] for argTuple in argTuples [ : ] : if argTuple [ 0 ] in ourArgNames : argTuples . remove ( argTuple ) if self : for argTuple in argTuples : argName = argTuple [ 0 ] if argName in kwargs : argValue = kwargs . pop ( argName ) else : if len ( argTuple ) == 2 : raise TypeError ( ""Must provide '%s'"" % argName ) argValue = argTuple [ 2 ] setattr ( self , argName , argValue ) return argTuples",Get the default arguments from the function and assign as instance vars .
"def _getAdditionalSpecs ( temporalImp , kwargs = { } ) : typeNames = { int : 'UInt32' , float : 'Real32' , str : 'Byte' , bool : 'bool' , tuple : 'tuple' } def getArgType ( arg ) : t = typeNames . get ( type ( arg ) , 'Byte' ) count = 0 if t == 'Byte' else 1 if t == 'tuple' : t = typeNames . get ( type ( arg [ 0 ] ) , 'Byte' ) count = len ( arg ) if t == 'bool' : t = 'UInt32' return ( t , count ) def getConstraints ( arg ) : t = typeNames . get ( type ( arg ) , 'Byte' ) if t == 'Byte' : return 'multiple' elif t == 'bool' : return 'bool' else : return '' TemporalClass = _getTPClass ( temporalImp ) tArgTuples = _buildArgs ( TemporalClass . __init__ ) temporalSpec = { } for argTuple in tArgTuples : d = dict ( description = argTuple [ 1 ] , accessMode = 'ReadWrite' , dataType = getArgType ( argTuple [ 2 ] ) [ 0 ] , count = getArgType ( argTuple [ 2 ] ) [ 1 ] , constraints = getConstraints ( argTuple [ 2 ] ) ) temporalSpec [ argTuple [ 0 ] ] = d temporalSpec . update ( dict ( columnCount = dict ( description = 'Total number of columns.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , cellsPerColumn = dict ( description = 'Number of cells per column.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , inputWidth = dict ( description = 'Number of inputs to the TM.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , predictedSegmentDecrement = dict ( description = 'Predicted segment decrement' , accessMode = 'Read' , dataType = 'Real' , count = 1 , constraints = '' ) , orColumnOutputs = dict ( description = """"""OR together the cell outputs from each column to produce
      the temporal memory output. When this mode is enabled, the number of
      cells per column must also be specified and the output size of the region
      should be set the same as columnCount"""""" , accessMode = 'Read' , dataType = 'Bool' , count = 1 , constraints = 'bool' ) , cellsSavePath = dict ( description = """"""Optional path to file in which large temporal memory cells
                     data structure is to be saved."""""" , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = '' ) , temporalImp = dict ( description = """"""Which temporal memory implementation to use. Set to either
       'py' or 'cpp'. The 'cpp' implementation is optimized for speed in C++."""""" , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = 'enum: py, cpp' ) , ) ) otherSpec = dict ( learningMode = dict ( description = 'True if the node is learning (default True).' , accessMode = 'ReadWrite' , dataType = 'Bool' , count = 1 , defaultValue = True , constraints = 'bool' ) , inferenceMode = dict ( description = 'True if the node is inferring (default False).' , accessMode = 'ReadWrite' , dataType = 'Bool' , count = 1 , defaultValue = False , constraints = 'bool' ) , computePredictedActiveCellIndices = dict ( description = 'True if active and predicted active indices should be computed' , accessMode = 'Create' , dataType = 'Bool' , count = 1 , defaultValue = False , constraints = 'bool' ) , anomalyMode = dict ( description = 'True if an anomaly score is being computed' , accessMode = 'Create' , dataType = 'Bool' , count = 1 , defaultValue = False , constraints = 'bool' ) , topDownMode = dict ( description = 'True if the node should do top down compute on the next call ' 'to compute into topDownOut (default False).' , accessMode = 'ReadWrite' , dataType = 'Bool' , count = 1 , defaultValue = False , constraints = 'bool' ) , activeOutputCount = dict ( description = 'Number of active elements in bottomUpOut output.' , accessMode = 'Read' , dataType = 'UInt32' , count = 1 , constraints = '' ) , storeDenseOutput = dict ( description = """"""Whether to keep the dense column output (needed for
                     denseOutput parameter)."""""" , accessMode = 'ReadWrite' , dataType = 'UInt32' , count = 1 , constraints = 'bool' ) , logPathOutput = dict ( description = 'Optional name of output log file. If set, every output vector' ' will be logged to this file as a sparse vector.' , accessMode = 'ReadWrite' , dataType = 'Byte' , count = 0 , constraints = '' ) , ) return temporalSpec , otherSpec",Build the additional specs in three groups ( for the inspector )
"def initialize ( self ) : autoArgs = dict ( ( name , getattr ( self , name ) ) for name in self . _temporalArgNames ) if self . _tfdr is None : tpClass = _getTPClass ( self . temporalImp ) if self . temporalImp in [ 'py' , 'cpp' , 'r' , 'tm_py' , 'tm_cpp' , 'monitored_tm_py' , ] : self . _tfdr = tpClass ( numberOfCols = self . columnCount , cellsPerColumn = self . cellsPerColumn , * * autoArgs ) else : raise RuntimeError ( ""Invalid temporalImp"" )",Overrides : meth : ~nupic . bindings . regions . PyRegion . initialize .
"def _compute ( self , inputs , outputs ) : if self . _tfdr is None : raise RuntimeError ( ""TM has not been initialized"" ) self . _conditionalBreak ( ) self . _iterations += 1 buInputVector = inputs [ 'bottomUpIn' ] resetSignal = False if 'resetIn' in inputs : assert len ( inputs [ 'resetIn' ] ) == 1 if inputs [ 'resetIn' ] [ 0 ] != 0 : self . _tfdr . reset ( ) self . _sequencePos = 0 if self . computePredictedActiveCellIndices : prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) if self . anomalyMode : prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) self . _sequencePos += 1 if self . orColumnOutputs : tpOutput = tpOutput . reshape ( self . columnCount , self . cellsPerColumn ) . max ( axis = 1 ) if self . _fpLogTPOutput : output = tpOutput . reshape ( - 1 ) outputNZ = tpOutput . nonzero ( ) [ 0 ] outStr = "" "" . join ( [ ""%d"" % int ( token ) for token in outputNZ ] ) print >> self . _fpLogTPOutput , output . size , outStr outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat if self . topDownMode : outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) if self . anomalyMode : activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) activeColumns = buInputVector . nonzero ( ) [ 0 ] outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) if self . computePredictedActiveCellIndices : activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) activeIndices = numpy . where ( activeState != 0 ) [ 0 ] predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) outputs [ ""activeCells"" ] . fill ( 0 ) outputs [ ""activeCells"" ] [ activeIndices ] = 1 outputs [ ""predictedActiveCells"" ] . fill ( 0 ) outputs [ ""predictedActiveCells"" ] [ predictedActiveIndices ] = 1",Run one iteration of TMRegion s compute
"def getBaseSpec ( cls ) : spec = dict ( description = TMRegion . __doc__ , singleNodeOnly = True , inputs = dict ( bottomUpIn = dict ( description = """"""The input signal, conceptually organized as an
                         image pyramid data structure, but internally
                         organized as a flattened vector."""""" , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , resetIn = dict ( description = """"""Effectively a boolean flag that indicates whether
                         or not the input vector received in this compute cycle
                         represents the first training presentation in a
                         new temporal sequence."""""" , dataType = 'Real32' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , sequenceIdIn = dict ( description = ""Sequence ID"" , dataType = 'UInt64' , count = 1 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , ) , outputs = dict ( bottomUpOut = dict ( description = """"""The output signal generated from the bottom-up inputs
                          from lower levels."""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = True ) , topDownOut = dict ( description = """"""The top-down inputsignal, generated from
                        feedback from upper levels"""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , activeCells = dict ( description = ""The cells that are active"" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , predictedActiveCells = dict ( description = ""The cells that are active and predicted"" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , anomalyScore = dict ( description = """"""The score for how 'anomalous' (i.e. rare) the current
                        sequence is. Higher values are increasingly rare"""""" , dataType = 'Real32' , count = 1 , regionLevel = True , isDefaultOutput = False ) , lrnActiveStateT = dict ( description = """"""Active cells during learn phase at time t.  This is
                        used for anomaly classification."""""" , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , ) , parameters = dict ( breakPdb = dict ( description = 'Set to 1 to stop in the pdb debugger on the next compute' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , breakKomodo = dict ( description = 'Set to 1 to stop in the Komodo debugger on the next compute' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , ) , commands = { } ) return spec",Doesn t include the spatial temporal and other parameters
"def getSpec ( cls ) : spec = cls . getBaseSpec ( ) t , o = _getAdditionalSpecs ( temporalImp = gDefaultTemporalImp ) spec [ 'parameters' ] . update ( t ) spec [ 'parameters' ] . update ( o ) return spec",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getSpec .
"def getParameter ( self , parameterName , index = - 1 ) : if parameterName in self . _temporalArgNames : return getattr ( self . _tfdr , parameterName ) else : return PyRegion . getParameter ( self , parameterName , index )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getParameter .
"def setParameter ( self , parameterName , index , parameterValue ) : if parameterName in self . _temporalArgNames : setattr ( self . _tfdr , parameterName , parameterValue ) elif parameterName == ""logPathOutput"" : self . logPathOutput = parameterValue if self . _fpLogTPOutput is not None : self . _fpLogTPOutput . close ( ) self . _fpLogTPOutput = None if parameterValue : self . _fpLogTPOutput = open ( self . logPathOutput , 'w' ) elif hasattr ( self , parameterName ) : setattr ( self , parameterName , parameterValue ) else : raise Exception ( 'Unknown parameter: ' + parameterName )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . setParameter .
"def finishLearning ( self ) : if self . _tfdr is None : raise RuntimeError ( ""Temporal memory has not been initialized"" ) if hasattr ( self . _tfdr , 'finishLearning' ) : self . resetSequenceStates ( ) self . _tfdr . finishLearning ( )",Perform an internal optimization step that speeds up inference if we know learning will not be performed anymore . This call may for example remove all potential inputs to each column .
"def writeToProto ( self , proto ) : proto . temporalImp = self . temporalImp proto . columnCount = self . columnCount proto . inputWidth = self . inputWidth proto . cellsPerColumn = self . cellsPerColumn proto . learningMode = self . learningMode proto . inferenceMode = self . inferenceMode proto . anomalyMode = self . anomalyMode proto . topDownMode = self . topDownMode proto . computePredictedActiveCellIndices = ( self . computePredictedActiveCellIndices ) proto . orColumnOutputs = self . orColumnOutputs if self . temporalImp == ""py"" : tmProto = proto . init ( ""backtrackingTM"" ) elif self . temporalImp == ""cpp"" : tmProto = proto . init ( ""backtrackingTMCpp"" ) elif self . temporalImp == ""tm_py"" : tmProto = proto . init ( ""temporalMemory"" ) elif self . temporalImp == ""tm_cpp"" : tmProto = proto . init ( ""temporalMemory"" ) else : raise TypeError ( ""Unsupported temporalImp for capnp serialization: {}"" . format ( self . temporalImp ) ) self . _tfdr . write ( tmProto )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . writeToProto .
"def readFromProto ( cls , proto ) : instance = cls ( proto . columnCount , proto . inputWidth , proto . cellsPerColumn ) instance . temporalImp = proto . temporalImp instance . learningMode = proto . learningMode instance . inferenceMode = proto . inferenceMode instance . anomalyMode = proto . anomalyMode instance . topDownMode = proto . topDownMode instance . computePredictedActiveCellIndices = ( proto . computePredictedActiveCellIndices ) instance . orColumnOutputs = proto . orColumnOutputs if instance . temporalImp == ""py"" : tmProto = proto . backtrackingTM elif instance . temporalImp == ""cpp"" : tmProto = proto . backtrackingTMCpp elif instance . temporalImp == ""tm_py"" : tmProto = proto . temporalMemory elif instance . temporalImp == ""tm_cpp"" : tmProto = proto . temporalMemory else : raise TypeError ( ""Unsupported temporalImp for capnp serialization: {}"" . format ( instance . temporalImp ) ) instance . _tfdr = _getTPClass ( proto . temporalImp ) . read ( tmProto ) return instance",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . readFromProto .
"def getOutputElementCount ( self , name ) : if name == 'bottomUpOut' : return self . outputWidth elif name == 'topDownOut' : return self . columnCount elif name == 'lrnActiveStateT' : return self . outputWidth elif name == ""activeCells"" : return self . outputWidth elif name == ""predictedActiveCells"" : return self . outputWidth else : raise Exception ( ""Invalid output name specified"" )",Overrides : meth : ~nupic . bindings . regions . PyRegion . PyRegion . getOutputElementCount .
"def computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) : nActiveColumns = len ( activeColumns ) if nActiveColumns > 0 : score = numpy . in1d ( activeColumns , prevPredictedColumns ) . sum ( ) score = ( nActiveColumns - score ) / float ( nActiveColumns ) else : score = 0.0 return score",Computes the raw anomaly score .
"def compute ( self , activeColumns , predictedColumns , inputValue = None , timestamp = None ) : anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) if self . _mode == Anomaly . MODE_PURE : score = anomalyScore elif self . _mode == Anomaly . MODE_LIKELIHOOD : if inputValue is None : raise ValueError ( ""Selected anomaly mode 'Anomaly.MODE_LIKELIHOOD' "" ""requires 'inputValue' as parameter to compute() method. "" ) probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = 1 - probability elif self . _mode == Anomaly . MODE_WEIGHTED : probability = self . _likelihood . anomalyProbability ( inputValue , anomalyScore , timestamp ) score = anomalyScore * ( 1 - probability ) if self . _movingAverage is not None : score = self . _movingAverage . next ( score ) if self . _binaryThreshold is not None : if score >= self . _binaryThreshold : score = 1.0 else : score = 0.0 return score",Compute the anomaly score as the percent of active columns not predicted .
"def addGraph ( self , data , position = 111 , xlabel = None , ylabel = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . plot ( data ) plt . draw ( )",Adds a graph to the plot s figure .
"def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , bins = None ) : ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . hist ( data , bins = bins , color = ""green"" , alpha = 0.8 ) plt . draw ( )",Adds a histogram to the plot s figure .
"def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , aspect = ""auto"" , interpolation = ""nearest"" , name = None ) : if cmap is None : cmap = cm . Greys ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) if self . _show : plt . draw ( ) if name is not None : if not os . path . exists ( ""log"" ) : os . mkdir ( ""log"" ) plt . savefig ( ""log/{name}.png"" . format ( name = name ) , bbox_inches = ""tight"" , figsize = ( 8 , 6 ) , dpi = 400 )",Adds an image to the plot s figure .
"def _addBase ( self , position , xlabel = None , ylabel = None ) : ax = self . _fig . add_subplot ( position ) ax . set_xlabel ( xlabel ) ax . set_ylabel ( ylabel ) return ax",Adds a subplot to the plot s figure at specified position .
"def _generateOverlapping ( filename = ""overlap.csv"" , numSequences = 2 , elementsPerSeq = 3 , numRepeats = 10 , hub = [ 0 , 1 ] , hubOffset = 1 , resets = False ) : assert ( hubOffset + len ( hub ) <= elementsPerSeq ) scriptDir = os . path . dirname ( __file__ ) pathname = os . path . join ( scriptDir , 'datasets' , filename ) print ""Creating %s..."" % ( pathname ) fields = [ ( 'reset' , 'int' , 'R' ) , ( 'field1' , 'string' , '' ) , ( 'field2' , 'float' , '' ) ] outFile = FileRecordStream ( pathname , write = True , fields = fields ) sequences = [ ] nextElemIdx = max ( hub ) + 1 for _ in range ( numSequences ) : seq = [ ] for j in range ( hubOffset ) : seq . append ( nextElemIdx ) nextElemIdx += 1 for j in hub : seq . append ( j ) j = hubOffset + len ( hub ) while j < elementsPerSeq : seq . append ( nextElemIdx ) nextElemIdx += 1 j += 1 sequences . append ( seq ) seqIdxs = [ ] for _ in range ( numRepeats ) : seqIdxs += range ( numSequences ) random . shuffle ( seqIdxs ) for seqIdx in seqIdxs : reset = int ( resets ) seq = sequences [ seqIdx ] for ( x ) in seq : outFile . appendRecord ( [ reset , str ( x ) , x ] ) reset = 0 outFile . close ( )",Generate a temporal dataset containing sequences that overlap one or more elements with other sequences . Parameters : ---------------------------------------------------- filename : name of the file to produce including extension . It will be created in a datasets sub - directory within the directory containing this script . numSequences : how many sequences to generate elementsPerSeq : length of each sequence numRepeats : how many times to repeat each sequence in the output hub : sub - sequence to place within each other sequence hubOffset : where within each sequence to place the hub resets : if True turn on reset at start of each sequence
"def _generateFirstOrder0 ( ) : numCategories = 5 initProb = numpy . zeros ( numCategories ) initProb [ 0 ] = 1.0 firstOrder = dict ( ) firstOrder [ '0' ] = numpy . array ( [ 0 , 0.1 , 0 , 0 , 0.9 ] ) firstOrder [ '1' ] = numpy . array ( [ 0 , 0 , 0.75 , 0.25 , 0 ] ) firstOrder [ '2' ] = numpy . array ( [ 1.0 , 0 , 0 , 0 , 0 ] ) firstOrder [ '3' ] = numpy . array ( [ 1.0 , 0 , 0 , 0 , 0 ] ) firstOrder [ '4' ] = numpy . array ( [ 0 , 0 , 0.5 , 0.5 , 0 ] ) secondOrder = None categoryList = [ '%d' % x for x in range ( 5 ) ] return ( initProb , firstOrder , secondOrder , 3 , categoryList )",Generate the initial first order and second order transition probabilities for probability0 . For this model we generate the following set of sequences : . 1 . 75 0 ---- 1 ----- 2 \ \ \ \ . 25 \ \ ----- 3 \ \ . 9 . 5 \ --- 4 --------- 2 \ \ . 5 \ --------- 3 Parameters : ---------------------------------------------------------------------- retval : ( initProb firstOrder secondOrder seqLen ) initProb : Initial probability for each category . This is a vector of length len ( categoryList ) . firstOrder : A dictionary of the 1st order probabilities . The key is the 1st element of the sequence the value is the probability of each 2nd element given the first . secondOrder : A dictionary of the 2nd order probabilities . The key is the first 2 elements of the sequence the value is the probability of each possible 3rd element given the first two . seqLen : Desired length of each sequence . The 1st element will be generated using the initProb the 2nd element by the firstOrder table and the 3rd and all successive elements by the secondOrder table . categoryList : list of category names to use
"def _generateFileFromProb ( filename , numRecords , categoryList , initProb , firstOrderProb , secondOrderProb , seqLen , numNoise = 0 , resetsEvery = None ) : print ""Creating %s..."" % ( filename ) fields = [ ( 'reset' , 'int' , 'R' ) , ( 'field1' , 'string' , '' ) , ( 'field2' , 'float' , '' ) ] scriptDir = os . path . dirname ( __file__ ) pathname = os . path . join ( scriptDir , 'datasets' , filename ) outFile = FileRecordStream ( pathname , write = True , fields = fields ) initCumProb = initProb . cumsum ( ) firstOrderCumProb = dict ( ) for ( key , value ) in firstOrderProb . iteritems ( ) : firstOrderCumProb [ key ] = value . cumsum ( ) if secondOrderProb is not None : secondOrderCumProb = dict ( ) for ( key , value ) in secondOrderProb . iteritems ( ) : secondOrderCumProb [ key ] = value . cumsum ( ) else : secondOrderCumProb = None elementsInSeq = [ ] numElementsSinceReset = 0 maxCatIdx = len ( categoryList ) - 1 for _ in xrange ( numRecords ) : if numElementsSinceReset == 0 : reset = 1 else : reset = 0 rand = numpy . random . rand ( ) if secondOrderCumProb is None : if len ( elementsInSeq ) == 0 : catIdx = numpy . searchsorted ( initCumProb , rand ) elif len ( elementsInSeq ) >= 1 and ( seqLen is None or len ( elementsInSeq ) < seqLen - numNoise ) : catIdx = numpy . searchsorted ( firstOrderCumProb [ str ( elementsInSeq [ - 1 ] ) ] , rand ) else : catIdx = numpy . random . randint ( len ( categoryList ) ) else : if len ( elementsInSeq ) == 0 : catIdx = numpy . searchsorted ( initCumProb , rand ) elif len ( elementsInSeq ) == 1 : catIdx = numpy . searchsorted ( firstOrderCumProb [ str ( elementsInSeq ) ] , rand ) elif ( len ( elementsInSeq ) >= 2 ) and ( seqLen is None or len ( elementsInSeq ) < seqLen - numNoise ) : catIdx = numpy . searchsorted ( secondOrderCumProb [ str ( elementsInSeq [ - 2 : ] ) ] , rand ) else : catIdx = numpy . random . randint ( len ( categoryList ) ) catIdx = min ( maxCatIdx , catIdx ) outFile . appendRecord ( [ reset , categoryList [ catIdx ] , catIdx ] ) elementsInSeq . append ( catIdx ) numElementsSinceReset += 1 if resetsEvery is not None and numElementsSinceReset == resetsEvery : numElementsSinceReset = 0 elementsInSeq = [ ] if seqLen is not None and ( len ( elementsInSeq ) == seqLen + numNoise ) : elementsInSeq = [ ] outFile . close ( )",Generate a set of records reflecting a set of probabilities . Parameters : ---------------------------------------------------------------- filename : name of . csv file to generate numRecords : number of records to generate categoryList : list of category names initProb : Initial probability for each category . This is a vector of length len ( categoryList ) . firstOrderProb : A dictionary of the 1st order probabilities . The key is the 1st element of the sequence the value is the probability of each 2nd element given the first . secondOrderProb : A dictionary of the 2nd order probabilities . The key is the first 2 elements of the sequence the value is the probability of each possible 3rd element given the first two . If this is None then the sequences will be first order only . seqLen : Desired length of each sequence . The 1st element will be generated using the initProb the 2nd element by the firstOrder table and the 3rd and all successive elements by the secondOrder table . None means infinite length . numNoise : Number of noise elements to place between each sequence . The noise elements are evenly distributed from all categories . resetsEvery : If not None generate a reset every N records Here is an example of some parameters : categoryList : [ cat1 cat2 cat3 ] initProb : [ 0 . 7 0 . 2 0 . 1 ] firstOrderProb : { [ 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 ] : [ 0 . 3 0 . 3 0 . 4 ] } secondOrderProb : { [ 0 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 0 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 0 2 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 2 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 2 ] : [ 0 . 3 0 . 3 0 . 4 ] }
"def getVersion ( ) : with open ( os . path . join ( REPO_DIR , ""VERSION"" ) , ""r"" ) as versionFile : return versionFile . read ( ) . strip ( )",Get version from local file .
"def nupicBindingsPrereleaseInstalled ( ) : try : nupicDistribution = pkg_resources . get_distribution ( ""nupic.bindings"" ) if pkg_resources . parse_version ( nupicDistribution . version ) . is_prerelease : return True except pkg_resources . DistributionNotFound : pass return False",Make an attempt to determine if a pre - release version of nupic . bindings is installed already .
"def findRequirements ( ) : requirementsPath = os . path . join ( REPO_DIR , ""requirements.txt"" ) requirements = parse_file ( requirementsPath ) if nupicBindingsPrereleaseInstalled ( ) : requirements = [ req for req in requirements if ""nupic.bindings"" not in req ] return requirements",Read the requirements . txt file and parse into requirements for setup s install_requirements option .
"def _handleDescriptionOption ( cmdArgStr , outDir , usageStr , hsVersion , claDescriptionTemplateFile ) : try : args = json . loads ( cmdArgStr ) except Exception , e : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( ""JSON arg parsing failed for --description: %s\n"" + ""ARG=<%s>"" ) % ( str ( e ) , cmdArgStr ) , usageStr ) ) filesDescription = _generateExperiment ( args , outDir , hsVersion = hsVersion , claDescriptionTemplateFile = claDescriptionTemplateFile ) pprint . pprint ( filesDescription ) return",Parses and validates the -- description option args and executes the request
"def _handleDescriptionFromFileOption ( filename , outDir , usageStr , hsVersion , claDescriptionTemplateFile ) : try : fileHandle = open ( filename , 'r' ) JSONStringFromFile = fileHandle . read ( ) . splitlines ( ) JSONStringFromFile = '' . join ( JSONStringFromFile ) except Exception , e : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( ""File open failed for --descriptionFromFile: %s\n"" + ""ARG=<%s>"" ) % ( str ( e ) , filename ) , usageStr ) ) _handleDescriptionOption ( JSONStringFromFile , outDir , usageStr , hsVersion = hsVersion , claDescriptionTemplateFile = claDescriptionTemplateFile ) return",Parses and validates the -- descriptionFromFile option and executes the request
"def _isInt ( x , precision = 0.0001 ) : xInt = int ( round ( x ) ) return ( abs ( x - xInt ) < precision * x , xInt )",Return ( isInt intValue ) for a given floating point number .
"def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : indent = _ONE_INDENT * indentLevels lines = str . splitlines ( True ) result = '' if len ( lines ) > 0 and not indentFirstLine : first = 1 result += lines [ 0 ] else : first = 0 for line in lines [ first : ] : result += indent + line return result",Indent all lines in the given string
"def _generateMetricSpecString ( inferenceElement , metric , params = None , field = None , returnLabel = False ) : metricSpecArgs = dict ( metric = metric , field = field , params = params , inferenceElement = inferenceElement ) metricSpecAsString = ""MetricSpec(%s)"" % ', ' . join ( [ '%s=%r' % ( item [ 0 ] , item [ 1 ] ) for item in metricSpecArgs . iteritems ( ) ] ) if not returnLabel : return metricSpecAsString spec = MetricSpec ( * * metricSpecArgs ) metricLabel = spec . getLabel ( ) return metricSpecAsString , metricLabel",Generates the string representation of a MetricSpec object and returns the metric key associated with the metric .
"def _generateFileFromTemplates ( templateFileNames , outputFilePath , replacementDict ) : installPath = os . path . dirname ( __file__ ) outputFile = open ( outputFilePath , ""w"" ) outputLines = [ ] inputLines = [ ] firstFile = True for templateFileName in templateFileNames : if not firstFile : inputLines . extend ( [ os . linesep ] * 2 ) firstFile = False inputFilePath = os . path . join ( installPath , templateFileName ) inputFile = open ( inputFilePath ) inputLines . extend ( inputFile . readlines ( ) ) inputFile . close ( ) print ""Writing "" , len ( inputLines ) , ""lines..."" for line in inputLines : tempLine = line for k , v in replacementDict . iteritems ( ) : if v is None : v = ""None"" tempLine = re . sub ( k , v , tempLine ) outputFile . write ( tempLine ) outputFile . close ( )",Generates a file by applying token replacements to the given template file
"def _generateEncoderChoicesV1 ( fieldInfo ) : width = 7 fieldName = fieldInfo [ 'fieldName' ] fieldType = fieldInfo [ 'fieldType' ] encoderChoicesList = [ ] if fieldType in [ 'float' , 'int' ] : aggFunction = 'mean' encoders = [ None ] for n in ( 13 , 50 , 150 , 500 ) : encoder = dict ( type = 'ScalarSpaceEncoder' , name = fieldName , fieldname = fieldName , n = n , w = width , clipInput = True , space = ""absolute"" ) if 'minValue' in fieldInfo : encoder [ 'minval' ] = fieldInfo [ 'minValue' ] if 'maxValue' in fieldInfo : encoder [ 'maxval' ] = fieldInfo [ 'maxValue' ] encoders . append ( encoder ) encoderChoicesList . append ( encoders ) elif fieldType == 'string' : aggFunction = 'first' encoders = [ None ] encoder = dict ( type = 'SDRCategoryEncoder' , name = fieldName , fieldname = fieldName , n = 100 , w = width ) encoders . append ( encoder ) encoderChoicesList . append ( encoders ) elif fieldType == 'datetime' : aggFunction = 'first' encoders = [ None ] for radius in ( 1 , 8 ) : encoder = dict ( type = 'DateEncoder' , name = '%s_timeOfDay' % ( fieldName ) , fieldname = fieldName , timeOfDay = ( width , radius ) ) encoders . append ( encoder ) encoderChoicesList . append ( encoders ) encoders = [ None ] for radius in ( 1 , 3 ) : encoder = dict ( type = 'DateEncoder' , name = '%s_dayOfWeek' % ( fieldName ) , fieldname = fieldName , dayOfWeek = ( width , radius ) ) encoders . append ( encoder ) encoderChoicesList . append ( encoders ) else : raise RuntimeError ( ""Unsupported field type '%s'"" % ( fieldType ) ) return ( encoderChoicesList , aggFunction )",Return a list of possible encoder parameter combinations for the given field and the default aggregation function to use . Each parameter combination is a dict defining the parameters for the encoder . Here is an example return value for the encoderChoicesList :
"def _generateEncoderStringsV1 ( includedFields ) : encoderChoicesList = [ ] for fieldInfo in includedFields : fieldName = fieldInfo [ 'fieldName' ] ( choicesList , aggFunction ) = _generateEncoderChoicesV1 ( fieldInfo ) encoderChoicesList . extend ( choicesList ) encoderSpecsList = [ ] for encoderChoices in encoderChoicesList : encoder = encoderChoices [ - 1 ] for c in _ILLEGAL_FIELDNAME_CHARACTERS : if encoder [ 'name' ] . find ( c ) >= 0 : raise _ExpGeneratorException ( ""Illegal character in field: %r (%r)"" % ( c , encoder [ 'name' ] ) ) encoderSpecsList . append ( ""%s: \n%s%s"" % ( _quoteAndEscape ( encoder [ 'name' ] ) , 2 * _ONE_INDENT , pprint . pformat ( encoder , indent = 2 * _INDENT_STEP ) ) ) encoderSpecsStr = ',\n  ' . join ( encoderSpecsList ) permEncoderChoicesList = [ ] for encoderChoices in encoderChoicesList : permEncoderChoicesList . append ( ""%s: %s,"" % ( _quoteAndEscape ( encoderChoices [ - 1 ] [ 'name' ] ) , pprint . pformat ( encoderChoices , indent = 2 * _INDENT_STEP ) ) ) permEncoderChoicesStr = '\n' . join ( permEncoderChoicesList ) permEncoderChoicesStr = _indentLines ( permEncoderChoicesStr , 1 , indentFirstLine = False ) return ( encoderSpecsStr , permEncoderChoicesStr )",Generate and return the following encoder related substitution variables :
"def _generatePermEncoderStr ( options , encoderDict ) : permStr = """" if encoderDict . get ( 'classifierOnly' , False ) : permStr = ""dict("" for key , value in encoderDict . items ( ) : if key == ""name"" : continue if key == 'n' and encoderDict [ 'type' ] != 'SDRCategoryEncoder' : permStr += ""n=PermuteInt(%d, %d), "" % ( encoderDict [ ""w"" ] + 7 , encoderDict [ ""w"" ] + 500 ) else : if issubclass ( type ( value ) , basestring ) : permStr += ""%s='%s', "" % ( key , value ) else : permStr += ""%s=%s, "" % ( key , value ) permStr += "")"" else : if encoderDict [ ""type"" ] in [ ""ScalarSpaceEncoder"" , ""AdaptiveScalarEncoder"" , ""ScalarEncoder"" , ""LogEncoder"" ] : permStr = ""PermuteEncoder("" for key , value in encoderDict . items ( ) : if key == ""fieldname"" : key = ""fieldName"" elif key == ""type"" : key = ""encoderClass"" elif key == ""name"" : continue if key == ""n"" : permStr += ""n=PermuteInt(%d, %d), "" % ( encoderDict [ ""w"" ] + 1 , encoderDict [ ""w"" ] + 500 ) elif key == ""runDelta"" : if value and not ""space"" in encoderDict : permStr += ""space=PermuteChoices([%s,%s]), "" % ( _quoteAndEscape ( ""delta"" ) , _quoteAndEscape ( ""absolute"" ) ) encoderDict . pop ( ""runDelta"" ) else : if issubclass ( type ( value ) , basestring ) : permStr += ""%s='%s', "" % ( key , value ) else : permStr += ""%s=%s, "" % ( key , value ) permStr += "")"" elif encoderDict [ ""type"" ] in [ ""SDRCategoryEncoder"" ] : permStr = ""PermuteEncoder("" for key , value in encoderDict . items ( ) : if key == ""fieldname"" : key = ""fieldName"" elif key == ""type"" : key = ""encoderClass"" elif key == ""name"" : continue if issubclass ( type ( value ) , basestring ) : permStr += ""%s='%s', "" % ( key , value ) else : permStr += ""%s=%s, "" % ( key , value ) permStr += "")"" elif encoderDict [ ""type"" ] in [ ""DateEncoder"" ] : permStr = ""PermuteEncoder("" for key , value in encoderDict . items ( ) : if key == ""fieldname"" : key = ""fieldName"" elif key == ""type"" : continue elif key == ""name"" : continue if key == ""timeOfDay"" : permStr += ""encoderClass='%s.timeOfDay', "" % ( encoderDict [ ""type"" ] ) permStr += ""radius=PermuteFloat(0.5, 12), "" permStr += ""w=%d, "" % ( value [ 0 ] ) elif key == ""dayOfWeek"" : permStr += ""encoderClass='%s.dayOfWeek', "" % ( encoderDict [ ""type"" ] ) permStr += ""radius=PermuteFloat(1, 6), "" permStr += ""w=%d, "" % ( value [ 0 ] ) elif key == ""weekend"" : permStr += ""encoderClass='%s.weekend', "" % ( encoderDict [ ""type"" ] ) permStr += ""radius=PermuteChoices([1]),  "" permStr += ""w=%d, "" % ( value ) else : if issubclass ( type ( value ) , basestring ) : permStr += ""%s='%s', "" % ( key , value ) else : permStr += ""%s=%s, "" % ( key , value ) permStr += "")"" else : raise RuntimeError ( ""Unsupported encoder type '%s'"" % ( encoderDict [ ""type"" ] ) ) return permStr",Generate the string that defines the permutations to apply for a given encoder .
"def _generateEncoderStringsV2 ( includedFields , options ) : width = 21 encoderDictsList = [ ] if options [ 'inferenceType' ] in [ ""NontemporalClassification"" , ""NontemporalMultiStep"" , ""TemporalMultiStep"" , ""MultiStep"" ] : classifierOnlyField = options [ 'inferenceArgs' ] [ 'predictedField' ] else : classifierOnlyField = None for fieldInfo in includedFields : fieldName = fieldInfo [ 'fieldName' ] fieldType = fieldInfo [ 'fieldType' ] if fieldType in [ 'float' , 'int' ] : runDelta = fieldInfo . get ( ""runDelta"" , False ) if runDelta or ""space"" in fieldInfo : encoderDict = dict ( type = 'ScalarSpaceEncoder' , name = fieldName , fieldname = fieldName , n = 100 , w = width , clipInput = True ) if runDelta : encoderDict [ ""runDelta"" ] = True else : encoderDict = dict ( type = 'AdaptiveScalarEncoder' , name = fieldName , fieldname = fieldName , n = 100 , w = width , clipInput = True ) if 'minValue' in fieldInfo : encoderDict [ 'minval' ] = fieldInfo [ 'minValue' ] if 'maxValue' in fieldInfo : encoderDict [ 'maxval' ] = fieldInfo [ 'maxValue' ] if ( 'minValue' in fieldInfo and 'maxValue' in fieldInfo ) and ( encoderDict [ 'type' ] == 'AdaptiveScalarEncoder' ) : encoderDict [ 'type' ] = 'ScalarEncoder' if 'encoderType' in fieldInfo : encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] if 'space' in fieldInfo : encoderDict [ 'space' ] = fieldInfo [ 'space' ] encoderDictsList . append ( encoderDict ) elif fieldType == 'string' : encoderDict = dict ( type = 'SDRCategoryEncoder' , name = fieldName , fieldname = fieldName , n = 100 + width , w = width ) if 'encoderType' in fieldInfo : encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] encoderDictsList . append ( encoderDict ) elif fieldType == 'datetime' : encoderDict = dict ( type = 'DateEncoder' , name = '%s_timeOfDay' % ( fieldName ) , fieldname = fieldName , timeOfDay = ( width , 1 ) ) if 'encoderType' in fieldInfo : encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] encoderDictsList . append ( encoderDict ) encoderDict = dict ( type = 'DateEncoder' , name = '%s_dayOfWeek' % ( fieldName ) , fieldname = fieldName , dayOfWeek = ( width , 1 ) ) if 'encoderType' in fieldInfo : encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] encoderDictsList . append ( encoderDict ) encoderDict = dict ( type = 'DateEncoder' , name = '%s_weekend' % ( fieldName ) , fieldname = fieldName , weekend = ( width ) ) if 'encoderType' in fieldInfo : encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] encoderDictsList . append ( encoderDict ) else : raise RuntimeError ( ""Unsupported field type '%s'"" % ( fieldType ) ) if fieldName == classifierOnlyField : clEncoderDict = dict ( encoderDict ) clEncoderDict [ 'classifierOnly' ] = True clEncoderDict [ 'name' ] = '_classifierInput' encoderDictsList . append ( clEncoderDict ) if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] == ""no"" : encoderDictsList . remove ( encoderDict ) if options . get ( 'fixedFields' ) is not None : tempList = [ ] for encoderDict in encoderDictsList : if encoderDict [ 'name' ] in options [ 'fixedFields' ] : tempList . append ( encoderDict ) encoderDictsList = tempList encoderSpecsList = [ ] permEncoderChoicesList = [ ] for encoderDict in encoderDictsList : if encoderDict [ 'name' ] . find ( '\\' ) >= 0 : raise _ExpGeneratorException ( ""Illegal character in field: '\\'"" ) for c in _ILLEGAL_FIELDNAME_CHARACTERS : if encoderDict [ 'name' ] . find ( c ) >= 0 : raise _ExpGeneratorException ( ""Illegal character %s in field %r"" % ( c , encoderDict [ 'name' ] ) ) constructorStr = _generatePermEncoderStr ( options , encoderDict ) encoderKey = _quoteAndEscape ( encoderDict [ 'name' ] ) encoderSpecsList . append ( ""%s: %s%s"" % ( encoderKey , 2 * _ONE_INDENT , pprint . pformat ( encoderDict , indent = 2 * _INDENT_STEP ) ) ) permEncoderChoicesList . append ( ""%s: %s,"" % ( encoderKey , constructorStr ) ) encoderSpecsStr = ',\n  ' . join ( encoderSpecsList ) permEncoderChoicesStr = '\n' . join ( permEncoderChoicesList ) permEncoderChoicesStr = _indentLines ( permEncoderChoicesStr , 1 , indentFirstLine = True ) return ( encoderSpecsStr , permEncoderChoicesStr )",Generate and return the following encoder related substitution variables :
"def _handleJAVAParameters ( options ) : if 'inferenceType' not in options : prediction = options . get ( 'prediction' , { InferenceType . TemporalNextStep : { 'optimize' : True } } ) inferenceType = None for infType , value in prediction . iteritems ( ) : if value [ 'optimize' ] : inferenceType = infType break if inferenceType == 'temporal' : inferenceType = InferenceType . TemporalNextStep if inferenceType != InferenceType . TemporalNextStep : raise _ExpGeneratorException ( ""Unsupported inference type %s"" % ( inferenceType ) ) options [ 'inferenceType' ] = inferenceType if 'predictionField' in options : if 'inferenceArgs' not in options : options [ 'inferenceArgs' ] = { 'predictedField' : options [ 'predictionField' ] } elif 'predictedField' not in options [ 'inferenceArgs' ] : options [ 'inferenceArgs' ] [ 'predictedField' ] = options [ 'predictionField' ]",Handle legacy options ( TEMPORARY )
"def _getPropertyValue ( schema , propertyName , options ) : if propertyName not in options : paramsSchema = schema [ 'properties' ] [ propertyName ] if 'default' in paramsSchema : options [ propertyName ] = paramsSchema [ 'default' ] else : options [ propertyName ] = None",Checks to see if property is specified in options . If not reads the default value from the schema
"def _getExperimentDescriptionSchema ( ) : installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) schemaFilePath = os . path . join ( installPath , ""experimentDescriptionSchema.json"" ) return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) )",Returns the experiment description schema . This implementation loads it in from file experimentDescriptionSchema . json .
"def _generateExperiment ( options , outputDirPath , hsVersion , claDescriptionTemplateFile ) : _gExperimentDescriptionSchema = _getExperimentDescriptionSchema ( ) try : validictory . validate ( options , _gExperimentDescriptionSchema ) except Exception , e : raise _InvalidCommandArgException ( ( ""JSON arg validation failed for option --description: "" + ""%s\nOPTION ARG=%s"" ) % ( str ( e ) , pprint . pformat ( options ) ) ) streamSchema = json . load ( resource_stream ( jsonschema . __name__ , 'stream_def.json' ) ) try : validictory . validate ( options [ 'streamDef' ] , streamSchema ) except Exception , e : raise _InvalidCommandArgException ( ( ""JSON arg validation failed for streamDef "" + ""%s\nOPTION ARG=%s"" ) % ( str ( e ) , json . dumps ( options ) ) ) _handleJAVAParameters ( options ) for propertyName in _gExperimentDescriptionSchema [ 'properties' ] : _getPropertyValue ( _gExperimentDescriptionSchema , propertyName , options ) if options [ 'inferenceArgs' ] is not None : infArgs = _gExperimentDescriptionSchema [ 'properties' ] [ 'inferenceArgs' ] for schema in infArgs [ 'type' ] : if isinstance ( schema , dict ) : for propertyName in schema [ 'properties' ] : _getPropertyValue ( schema , propertyName , options [ 'inferenceArgs' ] ) if options [ 'anomalyParams' ] is not None : anomalyArgs = _gExperimentDescriptionSchema [ 'properties' ] [ 'anomalyParams' ] for schema in anomalyArgs [ 'type' ] : if isinstance ( schema , dict ) : for propertyName in schema [ 'properties' ] : _getPropertyValue ( schema , propertyName , options [ 'anomalyParams' ] ) predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , None ) if options [ 'inferenceType' ] == InferenceType . NontemporalClassification : if predictionSteps is not None and predictionSteps != [ 0 ] : raise RuntimeError ( ""When NontemporalClassification is used, prediction"" "" steps must be [0]"" ) if predictionSteps == [ 0 ] and options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 'TemporalMultiStep' , 'MultiStep' ] : options [ 'inferenceType' ] = InferenceType . NontemporalClassification if options [ ""inferenceType"" ] == InferenceType . NontemporalClassification : if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] == ""yes"" or options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] == ""auto"" : raise RuntimeError ( ""When the inference type is NontemporalClassification"" "" inputPredictedField must be set to 'no'"" ) options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] = ""no"" swarmSize = options [ 'swarmSize' ] if swarmSize is None : if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] is None : options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] = ""auto"" elif swarmSize == 'small' : if options [ 'minParticlesPerSwarm' ] is None : options [ 'minParticlesPerSwarm' ] = 3 if options [ 'iterationCount' ] is None : options [ 'iterationCount' ] = 100 if options [ 'maxModels' ] is None : options [ 'maxModels' ] = 1 if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] is None : options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] = ""yes"" elif swarmSize == 'medium' : if options [ 'minParticlesPerSwarm' ] is None : options [ 'minParticlesPerSwarm' ] = 5 if options [ 'iterationCount' ] is None : options [ 'iterationCount' ] = 4000 if options [ 'maxModels' ] is None : options [ 'maxModels' ] = 200 if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] is None : options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] = ""auto"" elif swarmSize == 'large' : if options [ 'minParticlesPerSwarm' ] is None : options [ 'minParticlesPerSwarm' ] = 15 options [ 'tryAll3FieldCombinationsWTimestamps' ] = True if options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] is None : options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] = ""auto"" else : raise RuntimeError ( ""Unsupported swarm size: %s"" % ( swarmSize ) ) tokenReplacements = dict ( ) includedFields = options [ 'includedFields' ] if hsVersion == 'v1' : ( encoderSpecsStr , permEncoderChoicesStr ) = _generateEncoderStringsV1 ( includedFields ) elif hsVersion in [ 'v2' , 'ensemble' ] : ( encoderSpecsStr , permEncoderChoicesStr ) = _generateEncoderStringsV2 ( includedFields , options ) else : raise RuntimeError ( ""Unsupported hsVersion of %s"" % ( hsVersion ) ) if options [ 'resetPeriod' ] is not None : sensorAutoResetStr = pprint . pformat ( options [ 'resetPeriod' ] , indent = 2 * _INDENT_STEP ) else : sensorAutoResetStr = 'None' aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } aggFunctionsDict = { } if 'aggregation' in options [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in options [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = options [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in options [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in options [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList aggregationInfoStr = ""%s"" % ( pprint . pformat ( aggregationInfo , indent = 2 * _INDENT_STEP ) ) datasetSpec = options [ 'streamDef' ] if 'aggregation' in datasetSpec : datasetSpec . pop ( 'aggregation' ) if hasAggregation : datasetSpec [ 'aggregation' ] = '$SUBSTITUTE' datasetSpecStr = pprint . pformat ( datasetSpec , indent = 2 * _INDENT_STEP ) datasetSpecStr = datasetSpecStr . replace ( ""'$SUBSTITUTE'"" , ""config['aggregationInfo']"" ) datasetSpecStr = _indentLines ( datasetSpecStr , 2 , indentFirstLine = False ) computeInterval = options [ 'computeInterval' ] if computeInterval is not None and options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 'TemporalMultiStep' , 'MultiStep' ] : predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , [ 1 ] ) if len ( predictionSteps ) > 1 : raise _InvalidCommandArgException ( ""Invalid predictionSteps: %s. "" ""When computeInterval is specified, there can only be one "" ""stepSize in predictionSteps."" % predictionSteps ) if max ( aggregationInfo . values ( ) ) == 0 : raise _InvalidCommandArgException ( ""Missing or nil stream aggregation: "" ""When computeInterval is specified, then the stream aggregation "" ""interval must be non-zero."" ) numSteps = predictionSteps [ 0 ] predictAheadTime = dict ( aggregationPeriod ) for key in predictAheadTime . iterkeys ( ) : predictAheadTime [ key ] *= numSteps predictAheadTimeStr = pprint . pformat ( predictAheadTime , indent = 2 * _INDENT_STEP ) options [ 'dynamicPredictionSteps' ] = True else : options [ 'dynamicPredictionSteps' ] = False predictAheadTimeStr = ""None"" tokenReplacements [ '\$EXP_GENERATOR_PROGRAM_PATH' ] = _quoteAndEscape ( os . path . abspath ( __file__ ) ) inferenceType = options [ 'inferenceType' ] if inferenceType == 'MultiStep' : inferenceType = InferenceType . TemporalMultiStep tokenReplacements [ '\$INFERENCE_TYPE' ] = ""'%s'"" % inferenceType if inferenceType == InferenceType . NontemporalClassification : tokenReplacements [ '\$SP_ENABLE' ] = ""False"" tokenReplacements [ '\$TP_ENABLE' ] = ""False"" else : tokenReplacements [ '\$SP_ENABLE' ] = ""True"" tokenReplacements [ '\$TP_ENABLE' ] = ""True"" tokenReplacements [ '\$CLA_CLASSIFIER_IMPL' ] = """" tokenReplacements [ '\$ANOMALY_PARAMS' ] = pprint . pformat ( options [ 'anomalyParams' ] , indent = 2 * _INDENT_STEP ) tokenReplacements [ '\$ENCODER_SPECS' ] = encoderSpecsStr tokenReplacements [ '\$SENSOR_AUTO_RESET' ] = sensorAutoResetStr tokenReplacements [ '\$AGGREGATION_INFO' ] = aggregationInfoStr tokenReplacements [ '\$DATASET_SPEC' ] = datasetSpecStr if options [ 'iterationCount' ] is None : options [ 'iterationCount' ] = - 1 tokenReplacements [ '\$ITERATION_COUNT' ] = str ( options [ 'iterationCount' ] ) tokenReplacements [ '\$SP_POOL_PCT' ] = str ( options [ 'spCoincInputPoolPct' ] ) tokenReplacements [ '\$HS_MIN_PARTICLES' ] = str ( options [ 'minParticlesPerSwarm' ] ) tokenReplacements [ '\$SP_PERM_CONNECTED' ] = str ( options [ 'spSynPermConnected' ] ) tokenReplacements [ '\$FIELD_PERMUTATION_LIMIT' ] = str ( options [ 'fieldPermutationLimit' ] ) tokenReplacements [ '\$PERM_ENCODER_CHOICES' ] = permEncoderChoicesStr predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , [ 1 ] ) predictionStepsStr = ',' . join ( [ str ( x ) for x in predictionSteps ] ) tokenReplacements [ '\$PREDICTION_STEPS' ] = ""'%s'"" % ( predictionStepsStr ) tokenReplacements [ '\$PREDICT_AHEAD_TIME' ] = predictAheadTimeStr tokenReplacements [ '\$PERM_SP_CHOICES' ] = """" if options [ 'spPermuteDecrement' ] and options [ 'inferenceType' ] != 'NontemporalClassification' : tokenReplacements [ '\$PERM_SP_CHOICES' ] = _ONE_INDENT + ""'synPermInactiveDec': PermuteFloat(0.0003, 0.1),\n"" if options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 'NontemporalClassification' ] : tokenReplacements [ '\$PERM_TP_CHOICES' ] = """" else : tokenReplacements [ '\$PERM_TP_CHOICES' ] = ""  'activationThreshold': PermuteInt(12, 16),\n"" + ""  'minThreshold': PermuteInt(9, 12),\n"" + ""  'pamLength': PermuteInt(1, 5),\n"" if options [ 'inferenceType' ] == 'MultiStep' : tokenReplacements [ '\$PERM_INFERENCE_TYPE_CHOICES' ] = ""  'inferenceType': PermuteChoices(['NontemporalMultiStep', "" + ""'TemporalMultiStep']),"" else : tokenReplacements [ '\$PERM_INFERENCE_TYPE_CHOICES' ] = """" if options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 'TemporalMultiStep' , 'MultiStep' , 'TemporalAnomaly' , 'NontemporalClassification' ] : tokenReplacements [ '\$PERM_CL_CHOICES' ] = ""  'alpha': PermuteFloat(0.0001, 0.1),\n"" else : tokenReplacements [ '\$PERM_CL_CHOICES' ] = """" tokenReplacements [ '\$PERM_ALWAYS_INCLUDE_PREDICTED_FIELD' ] = ""inputPredictedField = '%s'"" % ( options [ ""inferenceArgs"" ] [ ""inputPredictedField"" ] ) if options . get ( 'minFieldContribution' , None ) is not None : tokenReplacements [ '\$PERM_MIN_FIELD_CONTRIBUTION' ] = ""minFieldContribution = %d"" % ( options [ 'minFieldContribution' ] ) else : tokenReplacements [ '\$PERM_MIN_FIELD_CONTRIBUTION' ] = """" if options . get ( 'killUselessSwarms' , None ) is not None : tokenReplacements [ '\$PERM_KILL_USELESS_SWARMS' ] = ""killUselessSwarms = %r"" % ( options [ 'killUselessSwarms' ] ) else : tokenReplacements [ '\$PERM_KILL_USELESS_SWARMS' ] = """" if options . get ( 'maxFieldBranching' , None ) is not None : tokenReplacements [ '\$PERM_MAX_FIELD_BRANCHING' ] = ""maxFieldBranching = %r"" % ( options [ 'maxFieldBranching' ] ) else : tokenReplacements [ '\$PERM_MAX_FIELD_BRANCHING' ] = """" if options . get ( 'tryAll3FieldCombinations' , None ) is not None : tokenReplacements [ '\$PERM_TRY_ALL_3_FIELD_COMBINATIONS' ] = ""tryAll3FieldCombinations = %r"" % ( options [ 'tryAll3FieldCombinations' ] ) else : tokenReplacements [ '\$PERM_TRY_ALL_3_FIELD_COMBINATIONS' ] = """" if options . get ( 'tryAll3FieldCombinationsWTimestamps' , None ) is not None : tokenReplacements [ '\$PERM_TRY_ALL_3_FIELD_COMBINATIONS_W_TIMESTAMPS' ] = ""tryAll3FieldCombinationsWTimestamps = %r"" % ( options [ 'tryAll3FieldCombinationsWTimestamps' ] ) else : tokenReplacements [ '\$PERM_TRY_ALL_3_FIELD_COMBINATIONS_W_TIMESTAMPS' ] = """" if options . get ( 'fixedFields' , None ) is not None : tokenReplacements [ '\$PERM_FIXED_FIELDS' ] = ""fixedFields = %r"" % ( options [ 'fixedFields' ] ) else : tokenReplacements [ '\$PERM_FIXED_FIELDS' ] = """" if options . get ( 'fastSwarmModelParams' , None ) is not None : tokenReplacements [ '\$PERM_FAST_SWARM_MODEL_PARAMS' ] = ""fastSwarmModelParams = %r"" % ( options [ 'fastSwarmModelParams' ] ) else : tokenReplacements [ '\$PERM_FAST_SWARM_MODEL_PARAMS' ] = """" if options . get ( 'maxModels' , None ) is not None : tokenReplacements [ '\$PERM_MAX_MODELS' ] = ""maxModels = %r"" % ( options [ 'maxModels' ] ) else : tokenReplacements [ '\$PERM_MAX_MODELS' ] = """" if options [ 'dynamicPredictionSteps' ] : debugAgg = True quotient = aggregationDivide ( computeInterval , aggregationPeriod ) ( isInt , multiple ) = _isInt ( quotient ) if not isInt or multiple < 1 : raise _InvalidCommandArgException ( ""Invalid computeInterval: %s. "" ""computeInterval must be an integer multiple of the stream "" ""aggregation (%s)."" % ( computeInterval , aggregationPeriod ) ) mTimesN = float ( predictionSteps [ 0 ] ) possibleNs = [ ] for n in xrange ( 1 , int ( mTimesN ) + 1 ) : m = mTimesN / n mInt = int ( round ( m ) ) if mInt < 1 : break if abs ( m - mInt ) > 0.0001 * m : continue possibleNs . append ( n ) if debugAgg : print ""All integer factors of %d are: %s"" % ( mTimesN , possibleNs ) aggChoices = [ ] for n in possibleNs : agg = dict ( aggregationPeriod ) for key in agg . iterkeys ( ) : agg [ key ] *= n quotient = aggregationDivide ( computeInterval , agg ) ( isInt , multiple ) = _isInt ( quotient ) if not isInt or multiple < 1 : continue aggChoices . append ( agg ) aggChoices = aggChoices [ - 5 : ] if debugAgg : print ""Aggregation choices that will be evaluted during swarming:"" for agg in aggChoices : print ""  ==>"" , agg print tokenReplacements [ '\$PERM_AGGREGATION_CHOICES' ] = ( ""PermuteChoices(%s)"" % ( pprint . pformat ( aggChoices , indent = 2 * _INDENT_STEP ) ) ) else : tokenReplacements [ '\$PERM_AGGREGATION_CHOICES' ] = aggregationInfoStr _generateInferenceArgs ( options , tokenReplacements ) _generateMetricsSubstitutions ( options , tokenReplacements ) environment = options [ 'environment' ] if environment == OpfEnvironment . Nupic : tokenReplacements [ '\$ENVIRONMENT' ] = ""'%s'"" % OpfEnvironment . Nupic controlTemplate = ""nupicEnvironmentTemplate.tpl"" elif environment == OpfEnvironment . Experiment : tokenReplacements [ '\$ENVIRONMENT' ] = ""'%s'"" % OpfEnvironment . Experiment controlTemplate = ""opfExperimentTemplate.tpl"" else : raise _InvalidCommandArgException ( ""Invalid environment type %s"" % environment ) if outputDirPath is None : outputDirPath = tempfile . mkdtemp ( ) if not os . path . exists ( outputDirPath ) : os . makedirs ( outputDirPath ) print ""Generating experiment files in directory: %s..."" % ( outputDirPath ) descriptionPyPath = os . path . join ( outputDirPath , ""description.py"" ) _generateFileFromTemplates ( [ claDescriptionTemplateFile , controlTemplate ] , descriptionPyPath , tokenReplacements ) permutationsPyPath = os . path . join ( outputDirPath , ""permutations.py"" ) if hsVersion == 'v1' : _generateFileFromTemplates ( [ 'permutationsTemplateV1.tpl' ] , permutationsPyPath , tokenReplacements ) elif hsVersion == 'ensemble' : _generateFileFromTemplates ( [ 'permutationsTemplateEnsemble.tpl' ] , permutationsPyPath , tokenReplacements ) elif hsVersion == 'v2' : _generateFileFromTemplates ( [ 'permutationsTemplateV2.tpl' ] , permutationsPyPath , tokenReplacements ) else : raise ( ValueError ( ""This permutation version is not supported yet: %s"" % hsVersion ) ) print ""done.""",Executes the -- description option which includes :
"def _generateMetricsSubstitutions ( options , tokenReplacements ) : options [ 'loggedMetrics' ] = [ "".*"" ] metricList , optimizeMetricLabel = _generateMetricSpecs ( options ) metricListString = "",\n"" . join ( metricList ) metricListString = _indentLines ( metricListString , 2 , indentFirstLine = False ) permOptimizeSettingStr = 'minimize = ""%s""' % optimizeMetricLabel loggedMetricsListAsStr = ""[%s]"" % ( "", "" . join ( [ ""'%s'"" % ptrn for ptrn in options [ 'loggedMetrics' ] ] ) ) tokenReplacements [ '\$LOGGED_METRICS' ] = loggedMetricsListAsStr tokenReplacements [ '\$METRICS' ] = metricListString tokenReplacements [ '\$PERM_OPTIMIZE_SETTING' ] = permOptimizeSettingStr",Generate the token substitution for metrics related fields . This includes : \ $METRICS \ $LOGGED_METRICS \ $PERM_OPTIMIZE_SETTING
"def _generateMetricSpecs ( options ) : inferenceType = options [ 'inferenceType' ] inferenceArgs = options [ 'inferenceArgs' ] predictionSteps = inferenceArgs [ 'predictionSteps' ] metricWindow = options [ 'metricWindow' ] if metricWindow is None : metricWindow = int ( Configuration . get ( ""nupic.opf.metricWindow"" ) ) metricSpecStrings = [ ] optimizeMetricLabel = """" metricSpecStrings . extend ( _generateExtraMetricSpecs ( options ) ) optimizeMetricSpec = None if options [ 'dynamicPredictionSteps' ] : assert len ( predictionSteps ) == 1 predictionSteps = [ '$REPLACE_ME' ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly , InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep , InferenceType . NontemporalClassification , 'MultiStep' ) : predictedFieldName , predictedFieldType = _getPredictedField ( options ) isCategory = _isCategory ( predictedFieldType ) metricNames = ( 'avg_err' , ) if isCategory else ( 'aae' , 'altMAPE' ) trivialErrorMetric = 'avg_err' if isCategory else 'altMAPE' oneGramErrorMetric = 'avg_err' if isCategory else 'altMAPE' movingAverageBaselineName = 'moving_mode' if isCategory else 'moving_mean' for metricName in metricNames : metricSpec , metricLabel = _generateMetricSpecString ( field = predictedFieldName , inferenceElement = InferenceElement . multiStepBestPredictions , metric = 'multiStep' , params = { 'errorMetric' : metricName , 'window' : metricWindow , 'steps' : predictionSteps } , returnLabel = True ) metricSpecStrings . append ( metricSpec ) if options [ ""customErrorMetric"" ] is not None : metricParams = dict ( options [ ""customErrorMetric"" ] ) metricParams [ 'errorMetric' ] = 'custom_error_metric' metricParams [ 'steps' ] = predictionSteps if not ""errorWindow"" in metricParams : metricParams [ ""errorWindow"" ] = metricWindow metricSpec , metricLabel = _generateMetricSpecString ( field = predictedFieldName , inferenceElement = InferenceElement . multiStepPredictions , metric = ""multiStep"" , params = metricParams , returnLabel = True ) metricSpecStrings . append ( metricSpec ) optimizeMetricSpec = metricSpec metricLabel = metricLabel . replace ( '[' , '\\[' ) metricLabel = metricLabel . replace ( ']' , '\\]' ) optimizeMetricLabel = metricLabel if options [ ""customErrorMetric"" ] is not None : optimizeMetricLabel = "".*custom_error_metric.*"" if options [ ""runBaselines"" ] and inferenceType != InferenceType . NontemporalClassification : for steps in predictionSteps : metricSpecStrings . append ( _generateMetricSpecString ( field = predictedFieldName , inferenceElement = InferenceElement . prediction , metric = ""trivial"" , params = { 'window' : metricWindow , ""errorMetric"" : trivialErrorMetric , 'steps' : steps } ) ) if isCategory : metricSpecStrings . append ( _generateMetricSpecString ( field = predictedFieldName , inferenceElement = InferenceElement . prediction , metric = movingAverageBaselineName , params = { 'window' : metricWindow , ""errorMetric"" : ""avg_err"" , ""mode_window"" : 200 , ""steps"" : steps } ) ) else : metricSpecStrings . append ( _generateMetricSpecString ( field = predictedFieldName , inferenceElement = InferenceElement . prediction , metric = movingAverageBaselineName , params = { 'window' : metricWindow , ""errorMetric"" : ""altMAPE"" , ""mean_window"" : 200 , ""steps"" : steps } ) ) elif inferenceType in ( InferenceType . TemporalClassification ) : metricName = 'avg_err' trivialErrorMetric = 'avg_err' oneGramErrorMetric = 'avg_err' movingAverageBaselineName = 'moving_mode' optimizeMetricSpec , optimizeMetricLabel = _generateMetricSpecString ( inferenceElement = InferenceElement . classification , metric = metricName , params = { 'window' : metricWindow } , returnLabel = True ) metricSpecStrings . append ( optimizeMetricSpec ) if options [ ""runBaselines"" ] : if inferenceType == InferenceType . TemporalClassification : metricSpecStrings . append ( _generateMetricSpecString ( inferenceElement = InferenceElement . classification , metric = ""trivial"" , params = { 'window' : metricWindow , ""errorMetric"" : trivialErrorMetric } ) ) metricSpecStrings . append ( _generateMetricSpecString ( inferenceElement = InferenceElement . classification , metric = ""two_gram"" , params = { 'window' : metricWindow , ""errorMetric"" : oneGramErrorMetric } ) ) metricSpecStrings . append ( _generateMetricSpecString ( inferenceElement = InferenceElement . classification , metric = movingAverageBaselineName , params = { 'window' : metricWindow , ""errorMetric"" : ""avg_err"" , ""mode_window"" : 200 } ) ) if not options [ ""customErrorMetric"" ] == None : if not ""errorWindow"" in options [ ""customErrorMetric"" ] : options [ ""customErrorMetric"" ] [ ""errorWindow"" ] = metricWindow optimizeMetricSpec = _generateMetricSpecString ( inferenceElement = InferenceElement . classification , metric = ""custom"" , params = options [ ""customErrorMetric"" ] ) optimizeMetricLabel = "".*custom_error_metric.*"" metricSpecStrings . append ( optimizeMetricSpec ) if options [ 'dynamicPredictionSteps' ] : for i in range ( len ( metricSpecStrings ) ) : metricSpecStrings [ i ] = metricSpecStrings [ i ] . replace ( ""'$REPLACE_ME'"" , ""predictionSteps"" ) optimizeMetricLabel = optimizeMetricLabel . replace ( ""'$REPLACE_ME'"" , "".*"" ) return metricSpecStrings , optimizeMetricLabel",Generates the Metrics for a given InferenceType
"def _generateExtraMetricSpecs ( options ) : _metricSpecSchema = { 'properties' : { } } results = [ ] for metric in options [ 'metrics' ] : for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : _getPropertyValue ( _metricSpecSchema , propertyName , metric ) specString , label = _generateMetricSpecString ( field = metric [ 'field' ] , metric = metric [ 'metric' ] , params = metric [ 'params' ] , inferenceElement = metric [ 'inferenceElement' ] , returnLabel = True ) if metric [ 'logged' ] : options [ 'loggedMetrics' ] . append ( label ) results . append ( specString ) return results",Generates the non - default metrics specified by the expGenerator params
"def _getPredictedField ( options ) : if not options [ 'inferenceArgs' ] or not options [ 'inferenceArgs' ] [ 'predictedField' ] : return None , None predictedField = options [ 'inferenceArgs' ] [ 'predictedField' ] predictedFieldInfo = None includedFields = options [ 'includedFields' ] for info in includedFields : if info [ 'fieldName' ] == predictedField : predictedFieldInfo = info break if predictedFieldInfo is None : raise ValueError ( ""Predicted field '%s' does not exist in included fields."" % predictedField ) predictedFieldType = predictedFieldInfo [ 'fieldType' ] return predictedField , predictedFieldType",Gets the predicted field and it s datatype from the options dictionary
"def _generateInferenceArgs ( options , tokenReplacements ) : inferenceType = options [ 'inferenceType' ] optionInferenceArgs = options . get ( 'inferenceArgs' , None ) resultInferenceArgs = { } predictedField = _getPredictedField ( options ) [ 0 ] if inferenceType in ( InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ) : assert predictedField , ""Inference Type '%s' needs a predictedField "" ""specified in the inferenceArgs dictionary"" % inferenceType if optionInferenceArgs : if options [ 'dynamicPredictionSteps' ] : altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) resultInferenceArgs = resultInferenceArgs . replace ( ""'$REPLACE_ME'"" , '[predictionSteps]' ) else : resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) tokenReplacements [ '\$INFERENCE_ARGS' ] = resultInferenceArgs tokenReplacements [ '\$PREDICTION_FIELD' ] = predictedField",Generates the token substitutions related to the predicted field and the supplemental arguments for prediction
"def expGenerator ( args ) : parser = OptionParser ( ) parser . set_usage ( ""%prog [options] --description='{json object with args}'\n"" + ""%prog [options] --descriptionFromFile='{filename}'\n"" + ""%prog [options] --showSchema"" ) parser . add_option ( ""--description"" , dest = ""description"" , help = ""Tells ExpGenerator to generate an experiment description.py and "" ""permutations.py file using the given JSON formatted experiment "" ""description string."" ) parser . add_option ( ""--descriptionFromFile"" , dest = 'descriptionFromFile' , help = ""Tells ExpGenerator to open the given filename and use it's "" ""contents as the JSON formatted experiment description."" ) parser . add_option ( ""--claDescriptionTemplateFile"" , dest = 'claDescriptionTemplateFile' , default = 'claDescriptionTemplate.tpl' , help = ""The file containing the template description file for "" "" ExpGenerator [default: %default]"" ) parser . add_option ( ""--showSchema"" , action = ""store_true"" , dest = ""showSchema"" , help = ""Prints the JSON schemas for the --description arg."" ) parser . add_option ( ""--version"" , dest = 'version' , default = 'v2' , help = ""Generate the permutations file for this version of hypersearch."" "" Possible choices are 'v1' and 'v2' [default: %default]."" ) parser . add_option ( ""--outDir"" , dest = ""outDir"" , default = None , help = ""Where to generate experiment. If not specified, "" ""then a temp directory will be created"" ) ( options , remainingArgs ) = parser . parse_args ( args ) if len ( remainingArgs ) > 0 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ""Unexpected command-line args: <%s>"" % ( ' ' . join ( remainingArgs ) , ) , parser . get_usage ( ) ) ) activeOptions = filter ( lambda x : getattr ( options , x ) != None , ( 'description' , 'showSchema' ) ) if len ( activeOptions ) > 1 : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ( ""The specified command options are "" + ""mutually-exclusive: %s"" ) % ( activeOptions , ) , parser . get_usage ( ) ) ) if options . showSchema : _handleShowSchemaOption ( ) elif options . description : _handleDescriptionOption ( options . description , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) elif options . descriptionFromFile : _handleDescriptionFromFileOption ( options . descriptionFromFile , options . outDir , parser . get_usage ( ) , hsVersion = options . version , claDescriptionTemplateFile = options . claDescriptionTemplateFile ) else : raise _InvalidCommandArgException ( _makeUsageErrorStr ( ""Error in validating command options. No option "" ""provided:\n"" , parser . get_usage ( ) ) )",Parses validates and executes command - line options ;
"def parseTimestamp ( s ) : s = s . strip ( ) for pattern in DATETIME_FORMATS : try : return datetime . datetime . strptime ( s , pattern ) except ValueError : pass raise ValueError ( 'The provided timestamp %s is malformed. The supported ' 'formats are: [%s]' % ( s , ', ' . join ( DATETIME_FORMATS ) ) )",Parses a textual datetime format and return a Python datetime object .
"def parseBool ( s ) : l = s . lower ( ) if l in ( ""true"" , ""t"" , ""1"" ) : return True if l in ( ""false"" , ""f"" , ""0"" ) : return False raise Exception ( ""Unable to convert string '%s' to a boolean value"" % s )",String to boolean
"def escape ( s ) : if s is None : return '' assert isinstance ( s , basestring ) , ""expected %s but got %s; value=%s"" % ( basestring , type ( s ) , s ) s = s . replace ( '\\' , '\\\\' ) s = s . replace ( '\n' , '\\n' ) s = s . replace ( '\t' , '\\t' ) s = s . replace ( ',' , '\t' ) return s",Escape commas tabs newlines and dashes in a string
"def unescape ( s ) : assert isinstance ( s , basestring ) s = s . replace ( '\t' , ',' ) s = s . replace ( '\\,' , ',' ) s = s . replace ( '\\n' , '\n' ) s = s . replace ( '\\\\' , '\\' ) return s",Unescapes a string that may contain commas tabs newlines and dashes
"def parseSdr ( s ) : assert isinstance ( s , basestring ) sdr = [ int ( c ) for c in s if c in ( ""0"" , ""1"" ) ] if len ( sdr ) != len ( s ) : raise ValueError ( ""The provided string %s is malformed. The string should "" ""have only 0's and 1's."" ) return sdr",Parses a string containing only 0 s and 1 s and return a Python list object .
"def parseStringList ( s ) : assert isinstance ( s , basestring ) return [ int ( i ) for i in s . split ( ) ]",Parse a string of space - separated numbers returning a Python list .
"def coordinatesFromIndex ( index , dimensions ) : coordinates = [ 0 ] * len ( dimensions ) shifted = index for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : coordinates [ i ] = shifted % dimensions [ i ] shifted = shifted / dimensions [ i ] coordinates [ 0 ] = shifted return coordinates",Translate an index into coordinates using the given coordinate system .
"def indexFromCoordinates ( coordinates , dimensions ) : index = 0 for i , dimension in enumerate ( dimensions ) : index *= dimension index += coordinates [ i ] return index",Translate coordinates into an index using the given coordinate system .
"def neighborhood ( centerIndex , radius , dimensions ) : centerPosition = coordinatesFromIndex ( centerIndex , dimensions ) intervals = [ ] for i , dimension in enumerate ( dimensions ) : left = max ( 0 , centerPosition [ i ] - radius ) right = min ( dimension - 1 , centerPosition [ i ] + radius ) intervals . append ( xrange ( left , right + 1 ) ) coords = numpy . array ( list ( itertools . product ( * intervals ) ) ) return numpy . ravel_multi_index ( coords . T , dimensions )",Get the points in the neighborhood of a point .
"def encodeIntoArray ( self , inputData , output ) : ( coordinate , radius ) = inputData assert isinstance ( radius , int ) , ( ""Expected integer radius, got: {} ({})"" . format ( radius , type ( radius ) ) ) neighbors = self . _neighbors ( coordinate , radius ) winners = self . _topWCoordinates ( neighbors , self . w ) bitFn = lambda coordinate : self . _bitForCoordinate ( coordinate , self . n ) indices = numpy . array ( [ bitFn ( w ) for w in winners ] ) output [ : ] = 0 output [ indices ] = 1",See nupic . encoders . base . Encoder for more information .
"def _neighbors ( coordinate , radius ) : ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) return numpy . array ( list ( itertools . product ( * ranges ) ) )",Returns coordinates around given coordinate within given radius . Includes given coordinate .
"def _topWCoordinates ( cls , coordinates , w ) : orders = numpy . array ( [ cls . _orderForCoordinate ( c ) for c in coordinates . tolist ( ) ] ) indices = numpy . argsort ( orders ) [ - w : ] return coordinates [ indices ]",Returns the top W coordinates by order .
"def _hashCoordinate ( coordinate ) : coordinateStr = "","" . join ( str ( v ) for v in coordinate ) hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) return hash",Hash a coordinate to a 64 bit integer .
"def _orderForCoordinate ( cls , coordinate ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getReal64 ( )",Returns the order for a coordinate .
"def _bitForCoordinate ( cls , coordinate , n ) : seed = cls . _hashCoordinate ( coordinate ) rng = Random ( seed ) return rng . getUInt32 ( n )",Maps the coordinate to a bit in the SDR .
"def binSearch ( arr , val ) : i = bisect_left ( arr , val ) if i != len ( arr ) and arr [ i ] == val : return i return - 1",Function for running binary search on a sorted list .
"def createSegment ( self , cell ) : cellData = self . _cells [ cell ] if len ( self . _freeFlatIdxs ) > 0 : flatIdx = self . _freeFlatIdxs . pop ( ) else : flatIdx = self . _nextFlatIdx self . _segmentForFlatIdx . append ( None ) self . _nextFlatIdx += 1 ordinal = self . _nextSegmentOrdinal self . _nextSegmentOrdinal += 1 segment = Segment ( cell , flatIdx , ordinal ) cellData . _segments . append ( segment ) self . _segmentForFlatIdx [ flatIdx ] = segment return segment",Adds a new segment on a cell .
"def destroySegment ( self , segment ) : for synapse in segment . _synapses : self . _removeSynapseFromPresynapticMap ( synapse ) self . _numSynapses -= len ( segment . _synapses ) segments = self . _cells [ segment . cell ] . _segments i = segments . index ( segment ) del segments [ i ] self . _freeFlatIdxs . append ( segment . flatIdx ) self . _segmentForFlatIdx [ segment . flatIdx ] = None",Destroys a segment .
"def createSynapse ( self , segment , presynapticCell , permanence ) : idx = len ( segment . _synapses ) synapse = Synapse ( segment , presynapticCell , permanence , self . _nextSynapseOrdinal ) self . _nextSynapseOrdinal += 1 segment . _synapses . add ( synapse ) self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) self . _numSynapses += 1 return synapse",Creates a new synapse on a segment .
"def destroySynapse ( self , synapse ) : self . _numSynapses -= 1 self . _removeSynapseFromPresynapticMap ( synapse ) synapse . segment . _synapses . remove ( synapse )",Destroys a synapse .
"def computeActivity ( self , activePresynapticCells , connectedPermanence ) : numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx threshold = connectedPermanence - EPSILON for cell in activePresynapticCells : for synapse in self . _synapsesForPresynapticCell [ cell ] : flatIdx = synapse . segment . flatIdx numActivePotentialSynapsesForSegment [ flatIdx ] += 1 if synapse . permanence > threshold : numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 return ( numActiveConnectedSynapsesForSegment , numActivePotentialSynapsesForSegment )",Compute each segment s number of active synapses for a given input . In the returned lists a segment s active synapse count is stored at index segment . flatIdx .
"def numSegments ( self , cell = None ) : if cell is not None : return len ( self . _cells [ cell ] . _segments ) return self . _nextFlatIdx - len ( self . _freeFlatIdxs )",Returns the number of segments .
"def segmentPositionSortKey ( self , segment ) : return segment . cell + ( segment . _ordinal / float ( self . _nextSegmentOrdinal ) )",Return a numeric key for sorting this segment . This can be used with the python built - in sorted () function .
"def write ( self , proto ) : protoCells = proto . init ( 'cells' , self . numCells ) for i in xrange ( self . numCells ) : segments = self . _cells [ i ] . _segments protoSegments = protoCells [ i ] . init ( 'segments' , len ( segments ) ) for j , segment in enumerate ( segments ) : synapses = segment . _synapses protoSynapses = protoSegments [ j ] . init ( 'synapses' , len ( synapses ) ) for k , synapse in enumerate ( sorted ( synapses , key = lambda s : s . _ordinal ) ) : protoSynapses [ k ] . presynapticCell = synapse . presynapticCell protoSynapses [ k ] . permanence = synapse . permanence",Writes serialized data to proto object .
"def read ( cls , proto ) : protoCells = proto . cells connections = cls ( len ( protoCells ) ) for cellIdx , protoCell in enumerate ( protoCells ) : protoCell = protoCells [ cellIdx ] protoSegments = protoCell . segments connections . _cells [ cellIdx ] = CellData ( ) segments = connections . _cells [ cellIdx ] . _segments for segmentIdx , protoSegment in enumerate ( protoSegments ) : segment = Segment ( cellIdx , connections . _nextFlatIdx , connections . _nextSegmentOrdinal ) segments . append ( segment ) connections . _segmentForFlatIdx . append ( segment ) connections . _nextFlatIdx += 1 connections . _nextSegmentOrdinal += 1 synapses = segment . _synapses protoSynapses = protoSegment . synapses for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : presynapticCell = protoSynapse . presynapticCell synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , ordinal = connections . _nextSynapseOrdinal ) connections . _nextSynapseOrdinal += 1 synapses . add ( synapse ) connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) connections . _numSynapses += 1 return connections",Reads deserialized data from proto object
"def getString ( cls , prop ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) envValue = os . environ . get ( ""%s%s"" % ( cls . envPropPrefix , prop . replace ( '.' , '_' ) ) , None ) if envValue is not None : return envValue return cls . _properties [ prop ]",Retrieve the requested property as a string . If property does not exist then KeyError will be raised .
"def getBool ( cls , prop ) : value = cls . getInt ( prop ) if value not in ( 0 , 1 ) : raise ValueError ( ""Expected 0 or 1, but got %r in config property %s"" % ( value , prop ) ) return bool ( value )",Retrieve the requested property and return it as a bool . If property does not exist then KeyError will be raised . If the property value is neither 0 nor 1 then ValueError will be raised
"def set ( cls , prop , value ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) cls . _properties [ prop ] = str ( value )",Set the value of the given configuration property .
"def dict ( cls ) : if cls . _properties is None : cls . _readStdConfigFiles ( ) result = dict ( cls . _properties ) keys = os . environ . keys ( ) replaceKeys = filter ( lambda x : x . startswith ( cls . envPropPrefix ) , keys ) for envKey in replaceKeys : key = envKey [ len ( cls . envPropPrefix ) : ] key = key . replace ( '_' , '.' ) result [ key ] = os . environ [ envKey ] return result",Return a dict containing all of the configuration properties
"def readConfigFile ( cls , filename , path = None ) : properties = cls . _readConfigFile ( filename , path ) if cls . _properties is None : cls . _properties = dict ( ) for name in properties : if 'value' in properties [ name ] : cls . _properties [ name ] = properties [ name ] [ 'value' ]",Parse the given XML file and store all properties it describes .
"def findConfigFile ( cls , filename ) : paths = cls . getConfigPaths ( ) for p in paths : testPath = os . path . join ( p , filename ) if os . path . isfile ( testPath ) : return os . path . join ( p , filename )",Search the configuration path ( specified via the NTA_CONF_PATH environment variable ) for the given filename . If found return the complete path to the file .
def getConfigPaths ( cls ) : configPaths = [ ] if cls . _configPaths is not None : return cls . _configPaths else : if 'NTA_CONF_PATH' in os . environ : configVar = os . environ [ 'NTA_CONF_PATH' ] configPaths = configVar . split ( os . pathsep ) return configPaths,Return the list of paths to search for configuration files .
"def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : if doForeground and doBackground : return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) else : if doForeground : return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) if doBackground : return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) return input",Add noise to the given input .
"def generateCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : coincMatrix0 = SM32 ( int ( nCoinc ) , int ( length ) ) theOnes = numpy . array ( [ 1.0 ] * activity , dtype = numpy . float32 ) for rowIdx in xrange ( nCoinc ) : coinc = numpy . array ( random . sample ( xrange ( length ) , activity ) , dtype = numpy . uint32 ) coinc . sort ( ) coincMatrix0 . setRowFromSparse ( rowIdx , coinc , theOnes ) coincMatrix = SM32 ( int ( nCoinc ) , int ( length ) ) coincMatrix . initializeWithFixedNNZR ( activity ) return coincMatrix0",Generate a coincidence matrix . This is used to generate random inputs to the temporal learner and to compare the predicted output against .
"def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : vectors = [ ] coinc = numpy . zeros ( length , dtype = 'int32' ) indexList = range ( length ) for i in xrange ( numVectors ) : coinc [ : ] = 0 coinc [ random . sample ( indexList , activity ) ] = 1 vectors . append ( coinc . copy ( ) ) return vectors",Generate a list of random sparse distributed vectors . This is used to generate training vectors to the spatial or temporal learner and to compare the predicted output against .
"def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) seqList = [ ] for i in xrange ( nSeq ) : if max ( seqLength ) <= nCoinc : seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) else : len = random . choice ( seqLength ) seq = [ ] for x in xrange ( len ) : seq . append ( random . choice ( coincList ) ) seqList . append ( seq ) return seqList",Generate a set of simple sequences . The elements of the sequences will be integers from 0 to nCoinc - 1 . The length of each sequence will be randomly chosen from the seqLength list .
"def generateHubSequences ( nCoinc = 10 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : coincList = range ( nCoinc ) for hub in hubs : coincList . remove ( hub ) seqList = [ ] for i in xrange ( nSeq ) : length = random . choice ( seqLength ) - 1 seq = random . sample ( coincList , length ) seq . insert ( length // 2 , random . choice ( hubs ) ) seqList . append ( seq ) return seqList",Generate a set of hub sequences . These are sequences which contain a hub element in the middle . The elements of the sequences will be integers from 0 to nCoinc - 1 . The hub elements will only appear in the middle of each sequence . The length of each sequence will be randomly chosen from the seqLength list .
"def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : assert nCoinc * activity <= length , ""can't generate non-overlapping coincidences"" coincMatrix = SM32 ( 0 , length ) coinc = numpy . zeros ( length , dtype = 'int32' ) for i in xrange ( nCoinc ) : coinc [ : ] = 0 coinc [ i * activity : ( i + 1 ) * activity ] = 1 coincMatrix . addRow ( coinc ) return coincMatrix",Generate a non overlapping coincidence matrix . This is used to generate random inputs to the temporal learner and to compare the predicted output against .
"def generateSequences ( nPatterns = 10 , patternLen = 500 , patternActivity = 50 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSimpleSequences = 50 , nHubSequences = 50 ) : patterns = generateCoincMatrix ( nCoinc = nPatterns , length = patternLen , activity = patternActivity ) seqList = generateSimpleSequences ( nCoinc = nPatterns , seqLength = seqLength , nSeq = nSimpleSequences ) + generateHubSequences ( nCoinc = nPatterns , hubs = hubs , seqLength = seqLength , nSeq = nHubSequences ) return ( seqList , patterns )",Generate a set of simple and hub sequences . A simple sequence contains a randomly chosen set of elements from 0 to nCoinc - 1 . A hub sequence always contains a hub element in the middle of it .
"def generateL2Sequences ( nL1Patterns = 10 , l1Hubs = [ 2 , 6 ] , l1SeqLength = [ 5 , 6 , 7 ] , nL1SimpleSequences = 50 , nL1HubSequences = 50 , l1Pooling = 4 , perfectStability = False , spHysteresisFactor = 1.0 , patternLen = 500 , patternActivity = 50 ) : l1SeqList = generateSimpleSequences ( nCoinc = nL1Patterns , seqLength = l1SeqLength , nSeq = nL1SimpleSequences ) + generateHubSequences ( nCoinc = nL1Patterns , hubs = l1Hubs , seqLength = l1SeqLength , nSeq = nL1HubSequences ) spOutput = generateSlowSPOutput ( seqListBelow = l1SeqList , poolingTimeBelow = l1Pooling , outputWidth = patternLen , activity = patternActivity , perfectStability = perfectStability , spHysteresisFactor = spHysteresisFactor ) outSeq = None outSeqList = [ ] outPatterns = SM32 ( 0 , patternLen ) for pattern in spOutput : if pattern . sum ( ) == 0 : if outSeq is not None : outSeqList . append ( outSeq ) outSeq = [ ] continue patternIdx = None if outPatterns . nRows ( ) > 0 : matches = outPatterns . rightVecSumAtNZ ( pattern ) outCoinc = matches . argmax ( ) . astype ( 'uint32' ) numOnes = pattern . sum ( ) if matches [ outCoinc ] == numOnes and outPatterns . getRow ( int ( outCoinc ) ) . sum ( ) == numOnes : patternIdx = outCoinc if patternIdx is None : outPatterns . addRow ( pattern ) patternIdx = outPatterns . nRows ( ) - 1 outSeq . append ( patternIdx ) if outSeq is not None : outSeqList . append ( outSeq ) return ( outSeqList , outPatterns )",Generate the simulated output from a spatial pooler that s sitting on top of another spatial pooler / temporal memory pair . The average on - time of the outputs from the simulated TM is given by the l1Pooling argument .
"def vectorsFromSeqList ( seqList , patternMatrix ) : totalLen = 0 for seq in seqList : totalLen += len ( seq ) vectors = numpy . zeros ( ( totalLen , patternMatrix . shape [ 1 ] ) , dtype = 'bool' ) vecOffset = 0 for seq in seqList : seq = numpy . array ( seq , dtype = 'uint32' ) for idx , coinc in enumerate ( seq ) : vectors [ vecOffset ] = patternMatrix . getRow ( int ( coinc ) ) vecOffset += 1 return vectors",Convert a list of sequences of pattern indices and a pattern lookup table into a an array of patterns
"def sameTMParams ( tp1 , tp2 ) : result = True for param in [ ""numberOfCols"" , ""cellsPerColumn"" , ""initialPerm"" , ""connectedPerm"" , ""minThreshold"" , ""newSynapseCount"" , ""permanenceInc"" , ""permanenceDec"" , ""permanenceMax"" , ""globalDecay"" , ""activationThreshold"" , ""doPooling"" , ""segUpdateValidDuration"" , ""burnIn"" , ""pamLength"" , ""maxAge"" ] : if getattr ( tp1 , param ) != getattr ( tp2 , param ) : print param , ""is different"" print getattr ( tp1 , param ) , ""vs"" , getattr ( tp2 , param ) result = False return result",Given two TM instances see if any parameters are different .
"def sameSynapse ( syn , synapses ) : for s in synapses : if ( s [ 0 ] == syn [ 0 ] ) and ( s [ 1 ] == syn [ 1 ] ) and ( abs ( s [ 2 ] - syn [ 2 ] ) <= 0.001 ) : return True return False",Given a synapse and a list of synapses check whether this synapse exist in the list . A synapse is represented as [ col cell permanence ] . A synapse matches if col and cell are identical and the permanence value is within 0 . 001 .
"def sameSegment ( seg1 , seg2 ) : result = True for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : result = False if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : result = False for syn in seg2 [ 1 : ] : if syn [ 2 ] <= 0 : print ""A synapse with zero permanence encountered"" result = False if result == True : for syn in seg1 [ 1 : ] : if syn [ 2 ] <= 0 : print ""A synapse with zero permanence encountered"" result = False res = sameSynapse ( syn , seg2 [ 1 : ] ) if res == False : result = False return result",Return True if seg1 and seg2 are identical ignoring order of synapses
"def tmDiff ( tm1 , tm2 , verbosity = 0 , relaxSegmentTests = True ) : if sameTMParams ( tm1 , tm2 ) == False : print ""Two TM's have different parameters"" return False result = True if ( tm1 . activeState [ 't' ] != tm2 . activeState [ 't' ] ) . any ( ) : print 'Active states diverge' , numpy . where ( tm1 . activeState [ 't' ] != tm2 . activeState [ 't' ] ) result = False if ( tm1 . predictedState [ 't' ] - tm2 . predictedState [ 't' ] ) . any ( ) : print 'Predicted states diverge' , numpy . where ( tm1 . predictedState [ 't' ] != tm2 . predictedState [ 't' ] ) result = False if tm1 . getNumSegments ( ) != tm2 . getNumSegments ( ) : print ""Number of segments are different"" , tm1 . getNumSegments ( ) , tm2 . getNumSegments ( ) result = False if tm1 . getNumSynapses ( ) != tm2 . getNumSynapses ( ) : print ""Number of synapses are different"" , tm1 . getNumSynapses ( ) , tm2 . getNumSynapses ( ) tm1 . printCells ( ) tm2 . printCells ( ) result = False for c in xrange ( tm1 . numberOfCols ) : for i in xrange ( tm2 . cellsPerColumn ) : if tm1 . getNumSegmentsInCell ( c , i ) != tm2 . getNumSegmentsInCell ( c , i ) : print ""Num segments different in cell:"" , c , i , print tm1 . getNumSegmentsInCell ( c , i ) , tm2 . getNumSegmentsInCell ( c , i ) result = False if result == True and not relaxSegmentTests : for c in xrange ( tm1 . numberOfCols ) : for i in xrange ( tm2 . cellsPerColumn ) : nSegs = tm1 . getNumSegmentsInCell ( c , i ) for segIdx in xrange ( nSegs ) : tm1seg = tm1 . getSegmentOnCell ( c , i , segIdx ) res = False for tm2segIdx in xrange ( nSegs ) : tm2seg = tm2 . getSegmentOnCell ( c , i , tm2segIdx ) if sameSegment ( tm1seg , tm2seg ) == True : res = True break if res == False : print ""\nSegments are different for cell:"" , c , i if verbosity >= 1 : print ""C++"" tm1 . printCell ( c , i ) print ""Py"" tm2 . printCell ( c , i ) result = False if result == True and ( verbosity > 1 ) : print ""TM's match"" return result",Given two TM instances list the difference between them and returns False if there is a difference . This function checks the major parameters . If this passes ( and checkLearn is true ) it checks the number of segments on each cell . If this passes checks each synapse on each segment . When comparing C ++ and Py the segments are usually in different orders in the cells . tmDiff ignores segment order when comparing TM s .
"def tmDiff2 ( tm1 , tm2 , verbosity = 0 , relaxSegmentTests = True , checkLearn = True , checkStates = True ) : if sameTMParams ( tm1 , tm2 ) == False : print ""Two TM's have different parameters"" return False tm1Label = ""<tm_1 (%s)>"" % tm1 . __class__ . __name__ tm2Label = ""<tm_2 (%s)>"" % tm2 . __class__ . __name__ result = True if checkStates : if ( tm1 . infActiveState [ 't' ] != tm2 . infActiveState [ 't' ] ) . any ( ) : print 'Active states diverged' , numpy . where ( tm1 . infActiveState [ 't' ] != tm2 . infActiveState [ 't' ] ) result = False if ( tm1 . infPredictedState [ 't' ] - tm2 . infPredictedState [ 't' ] ) . any ( ) : print 'Predicted states diverged' , numpy . where ( tm1 . infPredictedState [ 't' ] != tm2 . infPredictedState [ 't' ] ) result = False if checkLearn and ( tm1 . lrnActiveState [ 't' ] - tm2 . lrnActiveState [ 't' ] ) . any ( ) : print 'lrnActiveState[t] diverged' , numpy . where ( tm1 . lrnActiveState [ 't' ] != tm2 . lrnActiveState [ 't' ] ) result = False if checkLearn and ( tm1 . lrnPredictedState [ 't' ] - tm2 . lrnPredictedState [ 't' ] ) . any ( ) : print 'lrnPredictedState[t] diverged' , numpy . where ( tm1 . lrnPredictedState [ 't' ] != tm2 . lrnPredictedState [ 't' ] ) result = False if checkLearn and abs ( tm1 . getAvgLearnedSeqLength ( ) - tm2 . getAvgLearnedSeqLength ( ) ) > 0.01 : print ""Average learned sequence lengths differ: "" , print tm1 . getAvgLearnedSeqLength ( ) , "" vs "" , tm2 . getAvgLearnedSeqLength ( ) result = False if tm1 . getNumSegments ( ) != tm2 . getNumSegments ( ) : print ""Number of segments are different"" , tm1 . getNumSegments ( ) , tm2 . getNumSegments ( ) result = False if tm1 . getNumSynapses ( ) != tm2 . getNumSynapses ( ) : print ""Number of synapses are different"" , tm1 . getNumSynapses ( ) , tm2 . getNumSynapses ( ) if verbosity >= 3 : print ""%s: "" % tm1Label , tm1 . printCells ( ) print ""\n%s  : "" % tm2Label , tm2 . printCells ( ) for c in xrange ( tm1 . numberOfCols ) : for i in xrange ( tm2 . cellsPerColumn ) : if tm1 . getNumSegmentsInCell ( c , i ) != tm2 . getNumSegmentsInCell ( c , i ) : print ""Num segments different in cell:"" , c , i , print tm1 . getNumSegmentsInCell ( c , i ) , tm2 . getNumSegmentsInCell ( c , i ) result = False if result == True and not relaxSegmentTests and checkLearn : for c in xrange ( tm1 . numberOfCols ) : for i in xrange ( tm2 . cellsPerColumn ) : nSegs = tm1 . getNumSegmentsInCell ( c , i ) for segIdx in xrange ( nSegs ) : tm1seg = tm1 . getSegmentOnCell ( c , i , segIdx ) res = False for tm2segIdx in xrange ( nSegs ) : tm2seg = tm2 . getSegmentOnCell ( c , i , tm2segIdx ) if sameSegment ( tm1seg , tm2seg ) == True : res = True break if res == False : print ""\nSegments are different for cell:"" , c , i result = False if verbosity >= 0 : print ""%s : "" % tm1Label , tm1 . printCell ( c , i ) print ""\n%s  : "" % tm2Label , tm2 . printCell ( c , i ) if result == True and ( verbosity > 1 ) : print ""TM's match"" return result",Given two TM instances list the difference between them and returns False if there is a difference . This function checks the major parameters . If this passes ( and checkLearn is true ) it checks the number of segments on each cell . If this passes checks each synapse on each segment . When comparing C ++ and Py the segments are usually in different orders in the cells . tmDiff ignores segment order when comparing TM s .
"def spDiff ( SP1 , SP2 ) : if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : print ""Connected synapse matrices are different sizes"" return False if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : print ""Potential synapse matrices are different sizes"" return False if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : print ""Permanence matrices are different sizes"" return False for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : connected1 = SP1 . _masterConnectedM [ i ] connected2 = SP2 . _masterConnectedM [ i ] if ( connected1 != connected2 ) : print ""Connected Matrices for cell %d different"" % ( i ) return False permanences1 = SP1 . _masterPermanenceM [ i ] permanences2 = SP2 . _masterPermanenceM [ i ] if ( permanences1 != permanences2 ) : print ""Permanence Matrices for cell %d different"" % ( i ) return False potential1 = SP1 . _masterPotentialM [ i ] potential2 = SP2 . _masterPotentialM [ i ] if ( potential1 != potential2 ) : print ""Potential Matrices for cell %d different"" % ( i ) return False if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : print ""Firing boost factors are different between spatial poolers"" return False if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : print ""Duty cycles after inhibition are different between spatial poolers"" return False if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : print ""Duty cycles before inhibition are different between spatial poolers"" return False print ( ""Spatial Poolers are equivalent"" ) return True",Function that compares two spatial pooler instances . Compares the static variables between the two poolers to make sure that they are equivalent .
"def removeSeqStarts ( vectors , resets , numSteps = 1 ) : if numSteps == 0 : return vectors resetIndices = resets . nonzero ( ) [ 0 ] removeRows = resetIndices for i in range ( numSteps - 1 ) : removeRows = numpy . hstack ( ( removeRows , resetIndices + i + 1 ) ) return numpy . delete ( vectors , removeRows , axis = 0 )",Convert a list of sequences of pattern indices and a pattern lookup table into a an array of patterns
"def _accumulateFrequencyCounts ( values , freqCounts = None ) : values = numpy . array ( values ) numEntries = values . max ( ) + 1 if freqCounts is not None : numEntries = max ( numEntries , freqCounts . size ) if freqCounts is not None : if freqCounts . size != numEntries : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) newCounts [ 0 : freqCounts . size ] = freqCounts else : newCounts = freqCounts else : newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) for v in values : newCounts [ v ] += 1 return newCounts",Accumulate a list of values values into the frequency counts freqCounts and return the updated frequency counts
"def _listOfOnTimesInVec ( vector ) : durations = [ ] numOnTimes = 0 totalOnTime = 0 nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] if len ( nonzeros ) == 0 : return ( 0 , 0 , [ ] ) if len ( nonzeros ) == 1 : return ( 1 , 1 , [ 1 ] ) prev = nonzeros [ 0 ] onTime = 1 endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : totalOnTime += onTime numOnTimes += 1 durations . append ( onTime ) onTime = 1 else : onTime += 1 prev = idx totalOnTime += onTime numOnTimes += 1 durations . append ( onTime ) return ( totalOnTime , numOnTimes , durations )",Returns 3 things for a vector : * the total on time * the number of runs * a list of the durations of each run .
"def _fillInOnTimes ( vector , durations ) : nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] if len ( nonzeros ) == 0 : return if len ( nonzeros ) == 1 : durations [ nonzeros [ 0 ] ] = 1 return prev = nonzeros [ 0 ] onTime = 1 onStartIdx = prev endIdx = nonzeros [ - 1 ] for idx in nonzeros [ 1 : ] : if idx != prev + 1 : durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) onTime = 1 onStartIdx = idx else : onTime += 1 prev = idx durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 )",Helper function used by averageOnTimePerTimestep . durations is a vector which must be the same len as vector . For each on in vector it fills in the corresponding element of duration with the duration of that on signal up until that time
"def averageOnTimePerTimestep ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is not None : import pdb pdb . set_trace ( ) countOn = numpy . random . randint ( 0 , numElements , numSamples ) vectors = vectors [ : , countOn ] durations = numpy . zeros ( vectors . shape , dtype = 'int32' ) for col in xrange ( vectors . shape [ 1 ] ) : _fillInOnTimes ( vectors [ : , col ] , durations [ : , col ] ) sums = vectors . sum ( axis = 1 ) sums . clip ( min = 1 , max = numpy . inf , out = sums ) avgDurations = durations . sum ( axis = 1 , dtype = 'float64' ) / sums avgOnTime = avgDurations . sum ( ) / ( avgDurations > 0 ) . sum ( ) freqCounts = _accumulateFrequencyCounts ( avgDurations ) return ( avgOnTime , freqCounts )",Computes the average on - time of the outputs that are on at each time step and then averages this over all time steps .
"def averageOnTime ( vectors , numSamples = None ) : if vectors . ndim == 1 : vectors . shape = ( - 1 , 1 ) numTimeSteps = len ( vectors ) numElements = len ( vectors [ 0 ] ) if numSamples is None : numSamples = numElements countOn = range ( numElements ) else : countOn = numpy . random . randint ( 0 , numElements , numSamples ) sumOfLengths = 0.0 onTimeFreqCounts = None n = 0 for i in countOn : ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) if onTime != 0.0 : sumOfLengths += onTime n += segments onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) if n > 0 : return ( sumOfLengths / n , onTimeFreqCounts ) else : return ( 0.0 , onTimeFreqCounts )",Returns the average on - time averaged over all on - time runs .
"def plotOutputsOverTime ( vectors , buVectors = None , title = 'On-times' ) : import pylab pylab . ion ( ) pylab . figure ( ) imData = vectors . transpose ( ) if buVectors is not None : assert ( buVectors . shape == vectors . shape ) imData = imData . copy ( ) imData [ buVectors . transpose ( ) . astype ( 'bool' ) ] = 2 pylab . imshow ( imData , aspect = 'auto' , cmap = pylab . cm . gray_r , interpolation = 'nearest' ) pylab . title ( title )",Generate a figure that shows each output over time . Time goes left to right and each output is plotted on a different line allowing you to see the overlap in the outputs when they turn on / off etc .
"def plotHistogram ( freqCounts , title = 'On-Times Histogram' , xLabel = 'On-Time' ) : import pylab pylab . ion ( ) pylab . figure ( ) pylab . bar ( numpy . arange ( len ( freqCounts ) ) - 0.5 , freqCounts ) pylab . title ( title ) pylab . xlabel ( xLabel )",This is usually used to display a histogram of the on - times encountered in a particular output .
"def populationStability ( vectors , numSamples = None ) : numVectors = len ( vectors ) if numSamples is None : numSamples = numVectors - 1 countOn = range ( numVectors - 1 ) else : countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) sigmap = 0.0 for i in countOn : match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) if match [ 1 ] != 0 : sigmap += float ( match [ 0 ] ) / match [ 1 ] return sigmap / numSamples",Returns the stability for the population averaged over multiple time steps
"def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : totalSamples = len ( vectors ) windowSize = numSamples numWindows = 0 pctStable = 0 for wStart in range ( 0 , totalSamples - windowSize + 1 ) : data = vectors [ wStart : wStart + windowSize ] outputSums = data . sum ( axis = 0 ) stableOutputs = ( outputSums == windowSize ) . sum ( ) samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) print samplePctStable pctStable += samplePctStable numWindows += 1 return float ( pctStable ) / numWindows",Returns the percent of the outputs that remain completely stable over N time steps .
"def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : if not sparseForm : outputs = outputs . reshape ( outputsShape ) spOut = SM32 ( outputs ) else : if len ( outputs ) > 0 : assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) regionSize = 15 rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) regionSums = spOut . nNonZerosPerBox ( rows , cols ) ( locations , values ) = regionSums . tolist ( ) values /= float ( regionSize * regionSize ) sat = list ( values ) innerSat = [ ] locationSet = set ( locations ) for ( location , value ) in itertools . izip ( locations , values ) : ( row , col ) = location if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : innerSat . append ( value ) return ( sat , innerSat )",Compute the saturation for a continuous level . This breaks the level into multiple regions and computes the saturation level for each region .
"def checkMatch ( input , prediction , sparse = True , verbosity = 0 ) : if sparse : activeElementsInInput = set ( input ) activeElementsInPrediction = set ( prediction ) else : activeElementsInInput = set ( input . nonzero ( ) [ 0 ] ) activeElementsInPrediction = set ( prediction . nonzero ( ) [ 0 ] ) totalActiveInPrediction = len ( activeElementsInPrediction ) totalActiveInInput = len ( activeElementsInInput ) foundInInput = len ( activeElementsInPrediction . intersection ( activeElementsInInput ) ) missingFromInput = len ( activeElementsInPrediction . difference ( activeElementsInInput ) ) missingFromPrediction = len ( activeElementsInInput . difference ( activeElementsInPrediction ) ) if verbosity >= 1 : print ""preds. found in input:"" , foundInInput , ""out of"" , totalActiveInPrediction , print ""; preds. missing from input:"" , missingFromInput , ""out of"" , totalActiveInPrediction , print ""; unexpected active in input:"" , missingFromPrediction , ""out of"" , totalActiveInInput return ( foundInInput , totalActiveInInput , missingFromInput , totalActiveInPrediction )",Compares the actual input with the predicted input and returns results
"def predictionExtent ( inputs , resets , outputs , minOverlapPct = 100.0 ) : predCounts = None predTotal = 0 nSamples = len ( outputs ) predTotalNotLimited = 0 nSamplesNotLimited = 0 nCols = len ( inputs [ 0 ] ) nCellsPerCol = len ( outputs [ 0 ] ) // nCols for idx in xrange ( nSamples ) : activeCols = outputs [ idx ] . reshape ( nCols , nCellsPerCol ) . max ( axis = 1 ) steps = 0 while ( idx + steps + 1 < nSamples ) and ( resets [ idx + steps + 1 ] == 0 ) : overlap = numpy . logical_and ( inputs [ idx + steps + 1 ] , activeCols ) overlapPct = 100.0 * float ( overlap . sum ( ) ) / inputs [ idx + steps + 1 ] . sum ( ) if overlapPct >= minOverlapPct : steps += 1 else : break predCounts = _accumulateFrequencyCounts ( [ steps ] , predCounts ) predTotal += steps if resets [ idx ] or ( ( idx + steps + 1 < nSamples ) and ( not resets [ idx + steps + 1 ] ) ) : predTotalNotLimited += steps nSamplesNotLimited += 1 return ( float ( predTotal ) / nSamples , float ( predTotalNotLimited ) / nSamplesNotLimited , predCounts )",Computes the predictive ability of a temporal memory ( TM ) . This routine returns a value which is the average number of time steps of prediction provided by the TM . It accepts as input the inputs outputs and resets provided to the TM as well as a minOverlapPct used to evalulate whether or not a prediction is a good enough match to the actual input .
"def getCentreAndSpreadOffsets ( spaceShape , spreadShape , stepSize = 1 ) : from nupic . math . cross import cross shape = spaceShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : centerOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) centerOffsets = list ( cross ( yPositions , xPositions ) ) numCenterOffsets = len ( centerOffsets ) print ""centerOffsets:"" , centerOffsets shape = spreadShape if shape [ 0 ] == 1 and shape [ 1 ] == 1 : spreadOffsets = [ ( 0 , 0 ) ] else : xMin = - 1 * ( shape [ 1 ] // 2 ) xMax = xMin + shape [ 1 ] - 1 xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) yMin = - 1 * ( shape [ 0 ] // 2 ) yMax = yMin + shape [ 0 ] - 1 yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) spreadOffsets = list ( cross ( yPositions , xPositions ) ) spreadOffsets . remove ( ( 0 , 0 ) ) spreadOffsets . insert ( 0 , ( 0 , 0 ) ) numSpreadOffsets = len ( spreadOffsets ) print ""spreadOffsets:"" , spreadOffsets return centerOffsets , spreadOffsets",Generates centre offsets and spread offsets for block - mode based training regimes - star cross block .
"def makeCloneMap ( columnsShape , outputCloningWidth , outputCloningHeight = - 1 ) : if outputCloningHeight < 0 : outputCloningHeight = outputCloningWidth columnsHeight , columnsWidth = columnsShape numDistinctMasters = outputCloningWidth * outputCloningHeight a = numpy . empty ( ( columnsHeight , columnsWidth ) , 'uint32' ) for row in xrange ( columnsHeight ) : for col in xrange ( columnsWidth ) : a [ row , col ] = ( col % outputCloningWidth ) + ( row % outputCloningHeight ) * outputCloningWidth return a , numDistinctMasters",Make a two - dimensional clone map mapping columns to clone master .
"def numpyStr ( array , format = '%f' , includeIndices = False , includeZeros = True ) : shape = array . shape assert ( len ( shape ) <= 2 ) items = [ '[' ] if len ( shape ) == 1 : if includeIndices : format = '%d:' + format if includeZeros : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) ] else : rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) if x != 0 ] else : rowItems = [ format % ( x ) for x in array ] items . extend ( rowItems ) else : ( rows , cols ) = shape if includeIndices : format = '%d,%d:' + format for r in xrange ( rows ) : if includeIndices : rowItems = [ format % ( r , c , x ) for c , x in enumerate ( array [ r ] ) ] else : rowItems = [ format % ( x ) for x in array [ r ] ] if r > 0 : items . append ( '' ) items . append ( '[' ) items . extend ( rowItems ) if r < rows - 1 : items . append ( ']\n' ) else : items . append ( ']' ) items . append ( ']' ) return ' ' . join ( items )",Pretty print a numpy matrix using the given format string for each value . Return the string representation
"def sample ( self , rgen ) : rf = rgen . uniform ( 0 , self . sum ) index = bisect . bisect ( self . cdf , rf ) return self . keys [ index ] , numpy . log ( self . pmf [ index ] )",Generates a random sample from the discrete probability distribution and returns its value and the log of the probability of sampling that value .
"def logProbability ( self , distn ) : x = numpy . asarray ( distn ) n = x . sum ( ) return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + numpy . sum ( x * numpy . log ( self . dist . pmf ) ) )",Form of distribution must be an array of counts in order of self . keys .
"def sample ( self , rgen ) : x = rgen . poisson ( self . lambdaParameter ) return x , self . logDensity ( x )",Generates a random sample from the Poisson probability distribution and returns its value and the log of the probability of sampling that value .
"def createDataOutLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , ""UniformLink"" , """" , srcOutput = ""dataOut"" , destInput = ""bottomUpIn"" )",Link sensor region to other region so that it can pass it data .
"def createFeedForwardLink ( network , regionName1 , regionName2 ) : network . link ( regionName1 , regionName2 , ""UniformLink"" , """" , srcOutput = ""bottomUpOut"" , destInput = ""bottomUpIn"" )",Create a feed - forward link between 2 regions : regionName1 - > regionName2
"def createResetLink ( network , sensorRegionName , regionName ) : network . link ( sensorRegionName , regionName , ""UniformLink"" , """" , srcOutput = ""resetOut"" , destInput = ""resetIn"" )",Create a reset link from a sensor region : sensorRegionName - > regionName
"def createSensorToClassifierLinks ( network , sensorRegionName , classifierRegionName ) : network . link ( sensorRegionName , classifierRegionName , ""UniformLink"" , """" , srcOutput = ""bucketIdxOut"" , destInput = ""bucketIdxIn"" ) network . link ( sensorRegionName , classifierRegionName , ""UniformLink"" , """" , srcOutput = ""actValueOut"" , destInput = ""actValueIn"" ) network . link ( sensorRegionName , classifierRegionName , ""UniformLink"" , """" , srcOutput = ""categoryOut"" , destInput = ""categoryIn"" )",Create required links from a sensor region to a classifier region .
"def createNetwork ( dataSource ) : with open ( _PARAMS_PATH , ""r"" ) as f : modelParams = yaml . safe_load ( f ) [ ""modelParams"" ] network = Network ( ) network . addRegion ( ""sensor"" , ""py.RecordSensor"" , '{}' ) sensorRegion = network . regions [ ""sensor"" ] . getSelf ( ) sensorRegion . encoder = createEncoder ( modelParams [ ""sensorParams"" ] [ ""encoders"" ] ) sensorRegion . dataSource = dataSource modelParams [ ""spParams"" ] [ ""inputWidth"" ] = sensorRegion . encoder . getWidth ( ) network . addRegion ( ""SP"" , ""py.SPRegion"" , json . dumps ( modelParams [ ""spParams"" ] ) ) network . addRegion ( ""TM"" , ""py.TMRegion"" , json . dumps ( modelParams [ ""tmParams"" ] ) ) clName = ""py.%s"" % modelParams [ ""clParams"" ] . pop ( ""regionName"" ) network . addRegion ( ""classifier"" , clName , json . dumps ( modelParams [ ""clParams"" ] ) ) createSensorToClassifierLinks ( network , ""sensor"" , ""classifier"" ) createDataOutLink ( network , ""sensor"" , ""SP"" ) createFeedForwardLink ( network , ""SP"" , ""TM"" ) createFeedForwardLink ( network , ""TM"" , ""classifier"" ) createResetLink ( network , ""sensor"" , ""SP"" ) createResetLink ( network , ""sensor"" , ""TM"" ) network . initialize ( ) return network",Create and initialize a network .
"def getPredictionResults ( network , clRegionName ) : classifierRegion = network . regions [ clRegionName ] actualValues = classifierRegion . getOutputData ( ""actualValues"" ) probabilities = classifierRegion . getOutputData ( ""probabilities"" ) steps = classifierRegion . getSelf ( ) . stepsList N = classifierRegion . getSelf ( ) . maxCategoryCount results = { step : { } for step in steps } for i in range ( len ( steps ) ) : stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] mostLikelyCategoryIdx = stepProbabilities . argmax ( ) predictedValue = actualValues [ mostLikelyCategoryIdx ] predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] results [ steps [ i ] ] [ ""predictedValue"" ] = predictedValue results [ steps [ i ] ] [ ""predictionConfidence"" ] = predictionConfidence return results",Get prediction results for all prediction steps .
"def runHotgym ( numRecords ) : dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) network = createNetwork ( dataSource ) network . regions [ ""sensor"" ] . setParameter ( ""predictedField"" , ""consumption"" ) network . regions [ ""SP"" ] . setParameter ( ""learningMode"" , 1 ) network . regions [ ""TM"" ] . setParameter ( ""learningMode"" , 1 ) network . regions [ ""classifier"" ] . setParameter ( ""learningMode"" , 1 ) network . regions [ ""SP"" ] . setParameter ( ""inferenceMode"" , 1 ) network . regions [ ""TM"" ] . setParameter ( ""inferenceMode"" , 1 ) network . regions [ ""classifier"" ] . setParameter ( ""inferenceMode"" , 1 ) results = [ ] N = 1 for iteration in range ( 0 , numRecords , N ) : network . run ( N ) predictionResults = getPredictionResults ( network , ""classifier"" ) oneStep = predictionResults [ 1 ] [ ""predictedValue"" ] oneStepConfidence = predictionResults [ 1 ] [ ""predictionConfidence"" ] fiveStep = predictionResults [ 5 ] [ ""predictedValue"" ] fiveStepConfidence = predictionResults [ 5 ] [ ""predictionConfidence"" ] result = ( oneStep , oneStepConfidence * 100 , fiveStep , fiveStepConfidence * 100 ) print ""1-step: {:16} ({:4.4}%)\t 5-step: {:16} ({:4.4}%)"" . format ( * result ) results . append ( result ) return results",Run the Hot Gym example .
"def _loadDummyModelParameters ( self , params ) : for key , value in params . iteritems ( ) : if type ( value ) == list : index = self . modelIndex % len ( params [ key ] ) self . _params [ key ] = params [ key ] [ index ] else : self . _params [ key ] = params [ key ]",Loads all the parameters for this dummy model . For any paramters specified as lists read the appropriate value for this model using the model index
"def _computModelDelay ( self ) : if self . _params [ 'delay' ] is not None and self . _params [ 'sleepModelRange' ] is not None : raise RuntimeError ( ""Only one of 'delay' or "" ""'sleepModelRange' may be specified"" ) if self . _sleepModelRange is not None : range , delay = self . _sleepModelRange . split ( ':' ) delay = float ( delay ) range = map ( int , range . split ( ',' ) ) modelIDs = self . _jobsDAO . jobGetModelIDs ( self . _jobID ) modelIDs . sort ( ) range [ 1 ] = min ( range [ 1 ] , len ( modelIDs ) ) if self . _modelID in modelIDs [ range [ 0 ] : range [ 1 ] ] : self . _delay = delay else : self . _delay = self . _params [ 'delay' ]",Computes the amount of time ( if any ) to delay the run of this model . This can be determined by two mutually exclusive parameters : delay and sleepModelRange .
def _getMetrics ( self ) : metric = None if self . metrics is not None : metric = self . metrics ( self . _currentRecordIndex + 1 ) elif self . metricValue is not None : metric = self . metricValue else : raise RuntimeError ( 'No metrics or metric value specified for dummy model' ) return { self . _optimizeKeyPattern : metric },Protected function that can be overridden by subclasses . Its main purpose is to allow the the OPFDummyModelRunner to override this with deterministic values
"def run ( self ) : self . _logger . debug ( ""Starting Dummy Model: modelID=%s;"" % ( self . _modelID ) ) periodic = self . _initPeriodicActivities ( ) self . _optimizedMetricLabel = self . _optimizeKeyPattern self . _reportMetricLabels = [ self . _optimizeKeyPattern ] if self . _iterations >= 0 : iterTracker = iter ( xrange ( self . _iterations ) ) else : iterTracker = iter ( itertools . count ( ) ) doSysExit = False if self . _sysExitModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _sysExitModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : doSysExit = True if self . _delayModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _delayModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : time . sleep ( 10 ) if self . _errModelRange is not None : modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) modelIDs = [ x [ 0 ] for x in modelAndCounters ] modelIDs . sort ( ) ( beg , end ) = self . _errModelRange if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : raise RuntimeError ( ""Exiting with error due to errModelRange parameter"" ) if self . _delay is not None : time . sleep ( self . _delay ) self . _currentRecordIndex = 0 while True : if self . _isKilled : break if self . _isCanceled : break if self . _isMature : if not self . _isBestModel : self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED break else : self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF try : self . _currentRecordIndex = next ( iterTracker ) except StopIteration : break self . _writePrediction ( ModelResult ( None , None , None , None ) ) periodic . tick ( ) if self . __shouldSysExit ( self . _currentRecordIndex ) : sys . exit ( 1 ) if self . _busyWaitTime is not None : time . sleep ( self . _busyWaitTime ) self . __computeWaitTime ( ) if doSysExit : sys . exit ( 1 ) if self . _jobFailErr : raise utils . JobFailException ( ""E10000"" , ""dummyModel's jobFailErr was True."" ) if self . _doFinalize : if not self . _makeCheckpoint : self . _model = None if self . _finalDelay is not None : time . sleep ( self . _finalDelay ) self . _finalize ( ) self . _logger . info ( ""Finished: modelID=%r "" % ( self . _modelID ) ) return ( self . _cmpReason , None )",Runs the given OPF task against the given Model instance
"def _createPredictionLogger ( self ) : class DummyLogger : def writeRecord ( self , record ) : pass def writeRecords ( self , records , progressCB ) : pass def close ( self ) : pass self . _predictionLogger = DummyLogger ( )",Creates the model s PredictionLogger object which is an interface to write model results to a permanent storage location
"def __shouldSysExit ( self , iteration ) : if self . _exitAfter is None or iteration < self . _exitAfter : return False results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) modelIDs = [ e [ 0 ] for e in results ] modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , zip ( modelIDs , modelNums ) ) firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) return firstModelID == self . _modelID",Checks to see if the model should exit based on the exitAfter dummy parameter
"def getDescription ( self ) : description = { 'name' : self . name , 'fields' : [ f . name for f in self . fields ] , 'numRecords by field' : [ f . numRecords for f in self . fields ] } return description",Returns a description of the dataset
"def setSeed ( self , seed ) : rand . seed ( seed ) np . random . seed ( seed )",Set the random seed and the numpy seed Parameters : -------------------------------------------------------------------- seed : random seed
"def addField ( self , name , fieldParams , encoderParams ) : assert fieldParams is not None and 'type' in fieldParams dataClassName = fieldParams . pop ( 'type' ) try : dataClass = eval ( dataClassName ) ( fieldParams ) except TypeError , e : print ( ""#### Error in constructing %s class object. Possibly missing "" ""some required constructor parameters. Parameters "" ""that were provided are: %s"" % ( dataClass , fieldParams ) ) raise encoderParams [ 'dataClass' ] = dataClass encoderParams [ 'dataClassName' ] = dataClassName fieldIndex = self . defineField ( name , encoderParams )",Add a single field to the dataset . Parameters : ------------------------------------------------------------------- name : The user - specified name of the field fieldSpec : A list of one or more dictionaries specifying parameters to be used for dataClass initialization . Each dict must contain the key type that specifies a distribution for the values in this field encoderParams : Parameters for the field encoder
"def addMultipleFields ( self , fieldsInfo ) : assert all ( x in field for x in [ 'name' , 'fieldSpec' , 'encoderParams' ] for field in fieldsInfo ) for spec in fieldsInfo : self . addField ( spec . pop ( 'name' ) , spec . pop ( 'fieldSpec' ) , spec . pop ( 'encoderParams' ) )",Add multiple fields to the dataset . Parameters : ------------------------------------------------------------------- fieldsInfo : A list of dictionaries containing a field name specs for the data classes and encoder params for the corresponding field .
"def defineField ( self , name , encoderParams = None ) : self . fields . append ( _field ( name , encoderParams ) ) return len ( self . fields ) - 1",Initialize field using relevant encoder parameters . Parameters : ------------------------------------------------------------------- name : Field name encoderParams : Parameters for the encoder .
"def setFlag ( self , index , flag ) : assert len ( self . fields ) > index self . fields [ index ] . flag = flag",Set flag for field at index . Flags are special characters such as S for sequence or T for timestamp . Parameters : -------------------------------------------------------------------- index : index of field whose flag is being set flag : special character
"def generateRecord ( self , record ) : assert ( len ( record ) == len ( self . fields ) ) if record is not None : for x in range ( len ( self . fields ) ) : self . fields [ x ] . addValue ( record [ x ] ) else : for field in self . fields : field . addValue ( field . dataClass . getNext ( ) )",Generate a record . Each value is stored in its respective field . Parameters : -------------------------------------------------------------------- record : A 1 - D array containing as many values as the number of fields fields : An object of the class field that specifies the characteristics of each value in the record Assertion : -------------------------------------------------------------------- len ( record ) == len ( fields ) : A value for each field must be specified . Replace missing values of any type by SENTINEL_VALUE_FOR_MISSING_DATA
"def generateRecords ( self , records ) : if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' for record in records : self . generateRecord ( record )",Generate multiple records . Refer to definition for generateRecord
"def getRecord ( self , n = None ) : if n is None : assert len ( self . fields ) > 0 n = self . fields [ 0 ] . numRecords - 1 assert ( all ( field . numRecords > n for field in self . fields ) ) record = [ field . values [ n ] for field in self . fields ] return record",Returns the nth record
def getAllRecords ( self ) : values = [ ] numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) for x in range ( numRecords ) : values . append ( self . getRecord ( x ) ) return values,Returns all the records
"def encodeRecord ( self , record , toBeAdded = True ) : encoding = [ self . fields [ i ] . encodeValue ( record [ i ] , toBeAdded ) for i in xrange ( len ( self . fields ) ) ] return encoding",Encode a record as a sparse distributed representation Parameters : -------------------------------------------------------------------- record : Record to be encoded toBeAdded : Whether the encodings corresponding to the record are added to the corresponding fields
"def encodeAllRecords ( self , records = None , toBeAdded = True ) : if records is None : records = self . getAllRecords ( ) if self . verbosity > 0 : print 'Encoding' , len ( records ) , 'records.' encodings = [ self . encodeRecord ( record , toBeAdded ) for record in records ] return encodings",Encodes a list of records . Parameters : -------------------------------------------------------------------- records : One or more records . ( i j ) th element of this 2D array specifies the value at field j of record i . If unspecified records previously generated and stored are used . toBeAdded : Whether the encodings corresponding to the record are added to the corresponding fields
"def addValueToField ( self , i , value = None ) : assert ( len ( self . fields ) > i ) if value is None : value = self . fields [ i ] . dataClass . getNext ( ) self . fields [ i ] . addValue ( value ) return value else : self . fields [ i ] . addValue ( value )",Add value to the field i . Parameters : -------------------------------------------------------------------- value : value to be added i : value is added to field i
"def addValuesToField ( self , i , numValues ) : assert ( len ( self . fields ) > i ) values = [ self . addValueToField ( i ) for n in range ( numValues ) ] return values",Add values to the field i .
"def getSDRforValue ( self , i , j ) : assert len ( self . fields ) > i assert self . fields [ i ] . numRecords > j encoding = self . fields [ i ] . encodings [ j ] return encoding",Returns the sdr for jth value at column i
"def getZeroedOutEncoding ( self , n ) : assert all ( field . numRecords > n for field in self . fields ) encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) return encoding",Returns the nth encoding with the predictedField zeroed out
def getTotaln ( self ) : n = sum ( [ field . n for field in self . fields ] ) return n,Returns the cumulative n for all the fields in the dataset
def getTotalw ( self ) : w = sum ( [ field . w for field in self . fields ] ) return w,Returns the cumulative w for all the fields in the dataset
"def getEncoding ( self , n ) : assert ( all ( field . numEncodings > n for field in self . fields ) ) encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) return encoding",Returns the nth encoding
def getAllEncodings ( self ) : numEncodings = self . fields [ 0 ] . numEncodings assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] return encodings,Returns encodings for all the records
"def saveRecords ( self , path = 'myOutput' ) : numRecords = self . fields [ 0 ] . numRecords assert ( all ( field . numRecords == numRecords for field in self . fields ) ) import csv with open ( path + '.csv' , 'wb' ) as f : writer = csv . writer ( f ) writer . writerow ( self . getAllFieldNames ( ) ) writer . writerow ( self . getAllDataTypes ( ) ) writer . writerow ( self . getAllFlags ( ) ) writer . writerows ( self . getAllRecords ( ) ) if self . verbosity > 0 : print '******' , numRecords , 'records exported in numenta format to file:' , path , '******\n'",Export all the records into a csv file in numenta format .
"def removeAllRecords ( self ) : for field in self . fields : field . encodings , field . values = [ ] , [ ] field . numRecords , field . numEncodings = ( 0 , 0 )",Deletes all the values in the dataset
"def encodeValue ( self , value , toBeAdded = True ) : encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) if toBeAdded : self . encodings . append ( encodedValue ) self . numEncodings += 1 return encodedValue",Value is encoded as a sdr using the encoding parameters of the Field
"def _setTypes ( self , encoderSpec ) : if self . encoderType is None : if self . dataType in [ 'int' , 'float' ] : self . encoderType = 'adaptiveScalar' elif self . dataType == 'string' : self . encoderType = 'category' elif self . dataType in [ 'date' , 'datetime' ] : self . encoderType = 'date' if self . dataType is None : if self . encoderType in [ 'scalar' , 'adaptiveScalar' ] : self . dataType = 'float' elif self . encoderType in [ 'category' , 'enumeration' ] : self . dataType = 'string' elif self . encoderType in [ 'date' , 'datetime' ] : self . dataType = 'datetime'",Set up the dataTypes and initialize encoders
"def _initializeEncoders ( self , encoderSpec ) : if self . encoderType in [ 'adaptiveScalar' , 'scalar' ] : if 'minval' in encoderSpec : self . minval = encoderSpec . pop ( 'minval' ) else : self . minval = None if 'maxval' in encoderSpec : self . maxval = encoderSpec . pop ( 'maxval' ) else : self . maxval = None self . encoder = adaptive_scalar . AdaptiveScalarEncoder ( name = 'AdaptiveScalarEncoder' , w = self . w , n = self . n , minval = self . minval , maxval = self . maxval , periodic = False , forced = True ) elif self . encoderType == 'category' : self . encoder = sdr_category . SDRCategoryEncoder ( name = 'categoryEncoder' , w = self . w , n = self . n ) elif self . encoderType in [ 'date' , 'datetime' ] : self . encoder = date . DateEncoder ( name = 'dateEncoder' ) else : raise RuntimeError ( 'Error in constructing class object. Either encoder type' 'or dataType must be specified' )",Initialize the encoders
"def getScalars ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return numpy . array ( [ None ] ) else : return numpy . array ( [ self . categoryToIndex . get ( input , 0 ) ] )",See method description in base . py
"def getBucketIndices ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : return self . encoder . getBucketIndices ( self . categoryToIndex . get ( input , 0 ) )",See method description in base . py
"def decode ( self , encoded , parentFieldName = '' ) : ( fieldsDict , fieldNames ) = self . encoder . decode ( encoded ) if len ( fieldsDict ) == 0 : return ( fieldsDict , fieldNames ) assert ( len ( fieldsDict ) == 1 ) ( inRanges , inDesc ) = fieldsDict . values ( ) [ 0 ] outRanges = [ ] desc = """" for ( minV , maxV ) in inRanges : minV = int ( round ( minV ) ) maxV = int ( round ( maxV ) ) outRanges . append ( ( minV , maxV ) ) while minV <= maxV : if len ( desc ) > 0 : desc += "", "" desc += self . indexToCategory [ minV ] minV += 1 if parentFieldName != '' : fieldName = ""%s.%s"" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( outRanges , desc ) } , [ fieldName ] )",See the function description in base . py
"def closenessScores ( self , expValues , actValues , fractional = True , ) : expValue = expValues [ 0 ] actValue = actValues [ 0 ] if expValue == actValue : closeness = 1.0 else : closeness = 0.0 if not fractional : closeness = 1.0 - closeness return numpy . array ( [ closeness ] )",See the function description in base . py
def getBucketValues ( self ) : if self . _bucketValues is None : numBuckets = len ( self . encoder . getBucketValues ( ) ) self . _bucketValues = [ ] for bucketIndex in range ( numBuckets ) : self . _bucketValues . append ( self . getBucketInfo ( [ bucketIndex ] ) [ 0 ] . value ) return self . _bucketValues,See the function description in base . py
"def getBucketInfo ( self , buckets ) : bucketInfo = self . encoder . getBucketInfo ( buckets ) [ 0 ] categoryIndex = int ( round ( bucketInfo . value ) ) category = self . indexToCategory [ categoryIndex ] return [ EncoderResult ( value = category , scalar = categoryIndex , encoding = bucketInfo . encoding ) ]",See the function description in base . py
"def topDownCompute ( self , encoded ) : encoderResult = self . encoder . topDownCompute ( encoded ) [ 0 ] value = encoderResult . value categoryIndex = int ( round ( value ) ) category = self . indexToCategory [ categoryIndex ] return EncoderResult ( value = category , scalar = categoryIndex , encoding = encoderResult . encoding )",See the function description in base . py
"def loadExperiment ( path ) : if not os . path . isdir ( path ) : path = os . path . dirname ( path ) descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) return expIface . getModelDescription ( ) , expIface . getModelControl ( )",Loads the experiment description file from the path .
"def loadExperimentDescriptionScriptFromDir ( experimentDir ) : descriptionScriptPath = os . path . join ( experimentDir , ""description.py"" ) module = _loadDescriptionFile ( descriptionScriptPath ) return module",Loads the experiment description python script from the given experiment directory .
"def getExperimentDescriptionInterfaceFromModule ( module ) : result = module . descriptionInterface assert isinstance ( result , exp_description_api . DescriptionIface ) , ""expected DescriptionIface-based instance, but got %s"" % type ( result ) return result",: param module : imported description . py module
"def _loadDescriptionFile ( descriptionPyPath ) : global g_descriptionImportCount if not os . path . isfile ( descriptionPyPath ) : raise RuntimeError ( ( ""Experiment description file %s does not exist or "" + ""is not a file"" ) % ( descriptionPyPath , ) ) mod = imp . load_source ( ""pf_description%d"" % g_descriptionImportCount , descriptionPyPath ) g_descriptionImportCount += 1 if not hasattr ( mod , ""descriptionInterface"" ) : raise RuntimeError ( ""Experiment description file %s does not define %s"" % ( descriptionPyPath , ""descriptionInterface"" ) ) if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : raise RuntimeError ( ( ""Experiment description file %s defines %s but it "" + ""is not DescriptionIface-based"" ) % ( descriptionPyPath , name ) ) return mod",Loads a description file and returns it as a module .
"def update ( self , modelID , modelParams , modelParamsHash , metricResult , completed , completionReason , matured , numRecords ) : assert ( modelParamsHash is not None ) if completed : matured = True if metricResult is not None and matured and completionReason in [ ClientJobsDAO . CMPL_REASON_EOF , ClientJobsDAO . CMPL_REASON_STOPPED ] : if self . _hsObj . _maximize : errScore = - 1 * metricResult else : errScore = metricResult if errScore < self . _bestResult : self . _bestResult = errScore self . _bestModelID = modelID self . _hsObj . logger . info ( ""New best model after %d evaluations: errScore "" ""%g on model %s"" % ( len ( self . _allResults ) , self . _bestResult , self . _bestModelID ) ) else : errScore = numpy . inf if completed and completionReason in [ ClientJobsDAO . CMPL_REASON_ORPHAN ] : errScore = numpy . inf hidden = True else : hidden = False if completed : self . _completedModels . add ( modelID ) self . _numCompletedModels = len ( self . _completedModels ) if completionReason == ClientJobsDAO . CMPL_REASON_ERROR : self . _errModels . add ( modelID ) self . _numErrModels = len ( self . _errModels ) wasHidden = False if modelID not in self . _modelIDToIdx : assert ( modelParams is not None ) entry = dict ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , errScore = errScore , completed = completed , matured = matured , numRecords = numRecords , hidden = hidden ) self . _allResults . append ( entry ) entryIdx = len ( self . _allResults ) - 1 self . _modelIDToIdx [ modelID ] = entryIdx self . _paramsHashToIndexes [ modelParamsHash ] = entryIdx swarmId = modelParams [ 'particleState' ] [ 'swarmId' ] if not hidden : if swarmId in self . _swarmIdToIndexes : self . _swarmIdToIndexes [ swarmId ] . append ( entryIdx ) else : self . _swarmIdToIndexes [ swarmId ] = [ entryIdx ] genIdx = modelParams [ 'particleState' ] [ 'genIdx' ] numPsEntry = self . _swarmNumParticlesPerGeneration . get ( swarmId , [ 0 ] ) while genIdx >= len ( numPsEntry ) : numPsEntry . append ( 0 ) numPsEntry [ genIdx ] += 1 self . _swarmNumParticlesPerGeneration [ swarmId ] = numPsEntry else : entryIdx = self . _modelIDToIdx . get ( modelID , None ) assert ( entryIdx is not None ) entry = self . _allResults [ entryIdx ] wasHidden = entry [ 'hidden' ] if entry [ 'modelParamsHash' ] != modelParamsHash : self . _paramsHashToIndexes . pop ( entry [ 'modelParamsHash' ] ) self . _paramsHashToIndexes [ modelParamsHash ] = entryIdx entry [ 'modelParamsHash' ] = modelParamsHash modelParams = entry [ 'modelParams' ] swarmId = modelParams [ 'particleState' ] [ 'swarmId' ] genIdx = modelParams [ 'particleState' ] [ 'genIdx' ] if hidden and not wasHidden : assert ( entryIdx in self . _swarmIdToIndexes [ swarmId ] ) self . _swarmIdToIndexes [ swarmId ] . remove ( entryIdx ) self . _swarmNumParticlesPerGeneration [ swarmId ] [ genIdx ] -= 1 entry [ 'errScore' ] = errScore entry [ 'completed' ] = completed entry [ 'matured' ] = matured entry [ 'numRecords' ] = numRecords entry [ 'hidden' ] = hidden particleId = modelParams [ 'particleState' ] [ 'id' ] genIdx = modelParams [ 'particleState' ] [ 'genIdx' ] if matured and not hidden : ( oldResult , pos ) = self . _particleBest . get ( particleId , ( numpy . inf , None ) ) if errScore < oldResult : pos = Particle . getPositionFromState ( modelParams [ 'particleState' ] ) self . _particleBest [ particleId ] = ( errScore , pos ) prevGenIdx = self . _particleLatestGenIdx . get ( particleId , - 1 ) if not hidden and genIdx > prevGenIdx : self . _particleLatestGenIdx [ particleId ] = genIdx elif hidden and not wasHidden and genIdx == prevGenIdx : self . _particleLatestGenIdx [ particleId ] = genIdx - 1 if not hidden : swarmId = modelParams [ 'particleState' ] [ 'swarmId' ] if not swarmId in self . _swarmBestOverall : self . _swarmBestOverall [ swarmId ] = [ ] bestScores = self . _swarmBestOverall [ swarmId ] while genIdx >= len ( bestScores ) : bestScores . append ( ( None , numpy . inf ) ) if errScore < bestScores [ genIdx ] [ 1 ] : bestScores [ genIdx ] = ( modelID , errScore ) if not hidden : key = ( swarmId , genIdx ) if not key in self . _maturedSwarmGens : self . _modifiedSwarmGens . add ( key ) return errScore",Insert a new entry or update an existing one . If this is an update of an existing entry then modelParams will be None
"def getModelIDFromParamsHash ( self , paramsHash ) : entryIdx = self . _paramsHashToIndexes . get ( paramsHash , None ) if entryIdx is not None : return self . _allResults [ entryIdx ] [ 'modelID' ] else : return None",Return the modelID of the model with the given paramsHash or None if not found .
"def numModels ( self , swarmId = None , includeHidden = False ) : if includeHidden : if swarmId is None : return len ( self . _allResults ) else : return len ( self . _swarmIdToIndexes . get ( swarmId , [ ] ) ) else : if swarmId is None : entries = self . _allResults else : entries = [ self . _allResults [ entryIdx ] for entryIdx in self . _swarmIdToIndexes . get ( swarmId , [ ] ) ] return len ( [ entry for entry in entries if not entry [ 'hidden' ] ] )",Return the total # of models we have in our database ( if swarmId is None ) or in a specific swarm .
"def bestModelIdAndErrScore ( self , swarmId = None , genIdx = None ) : if swarmId is None : return ( self . _bestModelID , self . _bestResult ) else : if swarmId not in self . _swarmBestOverall : return ( None , numpy . inf ) genScores = self . _swarmBestOverall [ swarmId ] bestModelId = None bestScore = numpy . inf for ( i , ( modelId , errScore ) ) in enumerate ( genScores ) : if genIdx is not None and i > genIdx : break if errScore < bestScore : bestScore = errScore bestModelId = modelId return ( bestModelId , bestScore )",Return the model ID of the model with the best result so far and it s score on the optimize metric . If swarm is None then it returns the global best otherwise it returns the best for the given swarm for all generatons up to and including genIdx .
"def getParticleInfo ( self , modelId ) : entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , entry [ 'completed' ] , entry [ 'matured' ] )",Return particle info for a specific modelId .
"def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , matured = None , lastDescendent = False ) : if swarmId is not None : entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) else : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if swarmId is not None : assert ( not entry [ 'hidden' ] ) modelParams = entry [ 'modelParams' ] isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue if completed is not None and ( completed != isCompleted ) : continue if matured is not None and ( matured != isMatured ) : continue if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )",Return a list of particleStates for all particles we know about in the given swarm their model Ids and metric results .
"def getOrphanParticleInfos ( self , swarmId , genIdx ) : entryIdxs = range ( len ( self . _allResults ) ) if len ( entryIdxs ) == 0 : return ( [ ] , [ ] , [ ] , [ ] , [ ] ) particleStates = [ ] modelIds = [ ] errScores = [ ] completedFlags = [ ] maturedFlags = [ ] for idx in entryIdxs : entry = self . _allResults [ idx ] if not entry [ 'hidden' ] : continue modelParams = entry [ 'modelParams' ] if modelParams [ 'particleState' ] [ 'swarmId' ] != swarmId : continue isCompleted = entry [ 'completed' ] isMatured = entry [ 'matured' ] particleState = modelParams [ 'particleState' ] particleGenIdx = particleState [ 'genIdx' ] particleId = particleState [ 'id' ] if genIdx is not None and particleGenIdx != genIdx : continue particleStates . append ( particleState ) modelIds . append ( entry [ 'modelID' ] ) errScores . append ( entry [ 'errScore' ] ) completedFlags . append ( isCompleted ) maturedFlags . append ( isMatured ) return ( particleStates , modelIds , errScores , completedFlags , maturedFlags )",Return a list of particleStates for all particles in the given swarm generation that have been orphaned .
"def getMaturedSwarmGenerations ( self ) : result = [ ] modifiedSwarmGens = sorted ( self . _modifiedSwarmGens ) for key in modifiedSwarmGens : ( swarmId , genIdx ) = key if key in self . _maturedSwarmGens : self . _modifiedSwarmGens . remove ( key ) continue if ( genIdx >= 1 ) and not ( swarmId , genIdx - 1 ) in self . _maturedSwarmGens : continue ( _ , _ , errScores , completedFlags , maturedFlags ) = self . getParticleInfos ( swarmId , genIdx ) maturedFlags = numpy . array ( maturedFlags ) numMatured = maturedFlags . sum ( ) if numMatured >= self . _hsObj . _minParticlesPerSwarm and numMatured == len ( maturedFlags ) : errScores = numpy . array ( errScores ) bestScore = errScores . min ( ) self . _maturedSwarmGens . add ( key ) self . _modifiedSwarmGens . remove ( key ) result . append ( ( swarmId , genIdx , bestScore ) ) return result",Return a list of swarm generations that have completed and the best ( minimal ) errScore seen for each of them .
"def firstNonFullGeneration ( self , swarmId , minNumParticles ) : if not swarmId in self . _swarmNumParticlesPerGeneration : return None numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] numPsPerGen = numpy . array ( numPsPerGen ) firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] if len ( firstNonFull ) == 0 : return len ( numPsPerGen ) else : return firstNonFull [ 0 ]",Return the generation index of the first generation in the given swarm that does not have numParticles particles in it either still in the running state or completed . This does not include orphaned particles .
"def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : results = dict ( ) ( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , genIdx = None , matured = True ) for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : if maxGenIdx is not None : if particleState [ 'genIdx' ] > maxGenIdx : continue if resultErr == numpy . inf : continue position = Particle . getPositionFromState ( particleState ) varPosition = position [ varName ] varPositionStr = str ( varPosition ) if varPositionStr in results : results [ varPositionStr ] [ 1 ] . append ( resultErr ) else : results [ varPositionStr ] = ( varPosition , [ resultErr ] ) return results",Return a dict of the errors obtained on models that were run with each value from a PermuteChoice variable .
"def _getStreamDef ( self , modelDescription ) : aggregationPeriod = { 'days' : 0 , 'hours' : 0 , 'microseconds' : 0 , 'milliseconds' : 0 , 'minutes' : 0 , 'months' : 0 , 'seconds' : 0 , 'weeks' : 0 , 'years' : 0 , } aggFunctionsDict = { } if 'aggregation' in modelDescription [ 'streamDef' ] : for key in aggregationPeriod . keys ( ) : if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : aggFunctionsDict [ fieldName ] = str ( func ) hasAggregation = False for v in aggregationPeriod . values ( ) : if v != 0 : hasAggregation = True break aggFunctionList = aggFunctionsDict . items ( ) aggregationInfo = dict ( aggregationPeriod ) aggregationInfo [ 'fields' ] = aggFunctionList streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) return streamDef",Generate stream definition based on
"def close ( self ) : if self . _tempDir is not None and os . path . isdir ( self . _tempDir ) : self . logger . debug ( ""Removing temporary directory %r"" , self . _tempDir ) shutil . rmtree ( self . _tempDir ) self . _tempDir = None return",Deletes temporary system objects / files .
"def _readPermutationsFile ( self , filename , modelDescription ) : vars = { } permFile = execfile ( filename , globals ( ) , vars ) self . _reportKeys = vars . get ( 'report' , [ ] ) self . _filterFunc = vars . get ( 'permutationFilter' , None ) self . _dummyModelParamsFunc = vars . get ( 'dummyModelParams' , None ) self . _predictedField = None self . _predictedFieldEncoder = None self . _fixedFields = None self . _fastSwarmModelParams = vars . get ( 'fastSwarmModelParams' , None ) if self . _fastSwarmModelParams is not None : encoders = self . _fastSwarmModelParams [ 'structuredParams' ] [ 'modelParams' ] [ 'sensorParams' ] [ 'encoders' ] self . _fixedFields = [ ] for fieldName in encoders : if encoders [ fieldName ] is not None : self . _fixedFields . append ( fieldName ) if 'fixedFields' in vars : self . _fixedFields = vars [ 'fixedFields' ] self . _minParticlesPerSwarm = vars . get ( 'minParticlesPerSwarm' ) if self . _minParticlesPerSwarm == None : self . _minParticlesPerSwarm = Configuration . get ( 'nupic.hypersearch.minParticlesPerSwarm' ) self . _minParticlesPerSwarm = int ( self . _minParticlesPerSwarm ) self . _killUselessSwarms = vars . get ( 'killUselessSwarms' , True ) self . _inputPredictedField = vars . get ( ""inputPredictedField"" , ""yes"" ) self . _tryAll3FieldCombinations = vars . get ( 'tryAll3FieldCombinations' , False ) self . _tryAll3FieldCombinationsWTimestamps = vars . get ( 'tryAll3FieldCombinationsWTimestamps' , False ) minFieldContribution = vars . get ( 'minFieldContribution' , None ) if minFieldContribution is not None : self . _minFieldContribution = minFieldContribution maxBranching = vars . get ( 'maxFieldBranching' , None ) if maxBranching is not None : self . _maxBranching = maxBranching if 'maximize' in vars : self . _optimizeKey = vars [ 'maximize' ] self . _maximize = True elif 'minimize' in vars : self . _optimizeKey = vars [ 'minimize' ] self . _maximize = False else : raise RuntimeError ( ""Permutations file '%s' does not include a maximize"" "" or minimize metric."" ) maxModels = vars . get ( 'maxModels' ) if maxModels is not None : if self . _maxModels is None : self . _maxModels = maxModels else : raise RuntimeError ( 'It is an error to specify maxModels both in the job' ' params AND in the permutations file.' ) inferenceType = modelDescription [ 'modelParams' ] [ 'inferenceType' ] if not InferenceType . validate ( inferenceType ) : raise ValueError ( ""Invalid inference type %s"" % inferenceType ) if inferenceType in [ InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep ] : classifierOnlyEncoder = None for encoder in modelDescription [ ""modelParams"" ] [ ""sensorParams"" ] [ ""encoders"" ] . values ( ) : if encoder . get ( ""classifierOnly"" , False ) and encoder [ ""fieldname"" ] == vars . get ( 'predictedField' , None ) : classifierOnlyEncoder = encoder break if classifierOnlyEncoder is None or self . _inputPredictedField == ""yes"" : self . _searchType = HsSearchType . legacyTemporal else : self . _searchType = HsSearchType . temporal elif inferenceType in [ InferenceType . TemporalNextStep , InferenceType . TemporalAnomaly ] : self . _searchType = HsSearchType . legacyTemporal elif inferenceType in ( InferenceType . TemporalClassification , InferenceType . NontemporalClassification ) : self . _searchType = HsSearchType . classification else : raise RuntimeError ( ""Unsupported inference type: %s"" % inferenceType ) self . _predictedField = vars . get ( 'predictedField' , None ) if self . _predictedField is None : raise RuntimeError ( ""Permutations file '%s' does not have the required"" "" 'predictedField' variable"" % filename ) if 'permutations' not in vars : raise RuntimeError ( ""Permutations file '%s' does not define permutations"" % filename ) if not isinstance ( vars [ 'permutations' ] , dict ) : raise RuntimeError ( ""Permutations file '%s' defines a permutations variable "" ""but it is not a dict"" ) self . _encoderNames = [ ] self . _permutations = vars [ 'permutations' ] self . _flattenedPermutations = dict ( ) def _flattenPermutations ( value , keys ) : if ':' in keys [ - 1 ] : raise RuntimeError ( ""The permutation variable '%s' contains a ':' "" ""character, which is not allowed."" ) flatKey = _flattenKeys ( keys ) if isinstance ( value , PermuteEncoder ) : self . _encoderNames . append ( flatKey ) if value . fieldName == self . _predictedField : self . _predictedFieldEncoder = flatKey for encKey , encValue in value . kwArgs . iteritems ( ) : if isinstance ( encValue , PermuteVariable ) : self . _flattenedPermutations [ '%s:%s' % ( flatKey , encKey ) ] = encValue elif isinstance ( value , PermuteVariable ) : self . _flattenedPermutations [ flatKey ] = value else : if isinstance ( value , PermuteVariable ) : self . _flattenedPermutations [ key ] = value rApply ( self . _permutations , _flattenPermutations )",Read the permutations file and initialize the following member variables : _predictedField : field name of the field we are trying to predict _permutations : Dict containing the full permutations dictionary . _flattenedPermutations : Dict containing the flattened version of _permutations . The keys leading to the value in the dict are joined with a period to create the new key and permute variables within encoders are pulled out of the encoder . _encoderNames : keys from self . _permutations of only the encoder variables . _reportKeys : The report list from the permutations file . This is a list of the items from each experiment s pickled results file that should be included in the final report . The format of each item is a string of key names separated by colons each key being one level deeper into the experiment results dict . For example key1 : key2 . _filterFunc : a user - supplied function that can be used to filter out specific permutation combinations . _optimizeKey : which report key to optimize for _maximize : True if we should try and maximize the optimizeKey metric . False if we should minimize it . _dummyModelParamsFunc : a user - supplied function that can be used to artificially generate HTMPredictionModel results . When supplied the model is not actually run through the OPF but instead is run through a Dummy Model ( nupic . swarming . ModelRunner . OPFDummyModelRunner ) . This function returns the params dict used to control various options in the dummy model ( the returned metric the execution time etc . ) . This is used for hypersearch algorithm development .
"def _checkForOrphanedModels ( self ) : self . logger . debug ( ""Checking for orphaned models older than %s"" % ( self . _modelOrphanIntervalSecs ) ) while True : orphanedModelId = self . _cjDAO . modelAdoptNextOrphan ( self . _jobID , self . _modelOrphanIntervalSecs ) if orphanedModelId is None : return self . logger . info ( ""Removing orphaned model: %d"" % ( orphanedModelId ) ) for attempt in range ( 100 ) : paramsHash = hashlib . md5 ( ""OrphanParams.%d.%d"" % ( orphanedModelId , attempt ) ) . digest ( ) particleHash = hashlib . md5 ( ""OrphanParticle.%d.%d"" % ( orphanedModelId , attempt ) ) . digest ( ) try : self . _cjDAO . modelSetFields ( orphanedModelId , dict ( engParamsHash = paramsHash , engParticleHash = particleHash ) ) success = True except : success = False if success : break if not success : raise RuntimeError ( ""Unexpected failure to change paramsHash and "" ""particleHash of orphaned model"" ) self . _cjDAO . modelSetCompleted ( modelID = orphanedModelId , completionReason = ClientJobsDAO . CMPL_REASON_ORPHAN , completionMsg = ""Orphaned"" ) self . _resultsDB . update ( modelID = orphanedModelId , modelParams = None , modelParamsHash = paramsHash , metricResult = None , completed = True , completionReason = ClientJobsDAO . CMPL_REASON_ORPHAN , matured = True , numRecords = 0 )",If there are any models that haven t been updated in a while consider them dead and mark them as hidden in our resultsDB . We also change the paramsHash and particleHash of orphaned models so that we can re - generate that particle and / or model again if we desire .
"def _hsStatePeriodicUpdate ( self , exhaustedSwarmId = None ) : if self . _hsState is None : self . _hsState = HsState ( self ) self . _hsState . readStateFromDB ( ) completedSwarms = set ( ) if exhaustedSwarmId is not None : self . logger . info ( ""Removing swarm %s from the active set "" ""because we can't find any new unique particle "" ""positions"" % ( exhaustedSwarmId ) ) ( particles , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( swarmId = exhaustedSwarmId , matured = False ) if len ( particles ) > 0 : exhaustedSwarmStatus = 'completing' else : exhaustedSwarmStatus = 'completed' if self . _killUselessSwarms : self . _hsState . killUselessSwarms ( ) completingSwarms = self . _hsState . getCompletingSwarms ( ) for swarmId in completingSwarms : ( particles , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( swarmId = swarmId , matured = False ) if len ( particles ) == 0 : completedSwarms . add ( swarmId ) completedSwarmGens = self . _resultsDB . getMaturedSwarmGenerations ( ) priorCompletedSwarms = self . _hsState . getCompletedSwarms ( ) for ( swarmId , genIdx , errScore ) in completedSwarmGens : if swarmId in priorCompletedSwarms : continue completedList = self . _swarmTerminator . recordDataPoint ( swarmId = swarmId , generation = genIdx , errScore = errScore ) statusMsg = ""Completed generation #%d of swarm '%s' with a best"" "" errScore of %g"" % ( genIdx , swarmId , errScore ) if len ( completedList ) > 0 : statusMsg = ""%s. Matured swarm(s): %s"" % ( statusMsg , completedList ) self . logger . info ( statusMsg ) self . _cjDAO . jobSetFields ( jobID = self . _jobID , fields = dict ( engStatus = statusMsg ) , useConnectionID = False , ignoreUnchanged = True ) if 'NTA_TEST_recordSwarmTerminations' in os . environ : while True : resultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if resultsStr is None : results = { } else : results = json . loads ( resultsStr ) if not 'terminatedSwarms' in results : results [ 'terminatedSwarms' ] = { } for swarm in completedList : if swarm not in results [ 'terminatedSwarms' ] : results [ 'terminatedSwarms' ] [ swarm ] = ( genIdx , self . _swarmTerminator . swarmScores [ swarm ] ) newResultsStr = json . dumps ( results ) if newResultsStr == resultsStr : break updated = self . _cjDAO . jobSetFieldIfEqual ( jobID = self . _jobID , fieldName = 'results' , curValue = resultsStr , newValue = json . dumps ( results ) ) if updated : break if len ( completedList ) > 0 : for name in completedList : self . logger . info ( ""Swarm matured: %s. Score at generation %d: "" ""%s"" % ( name , genIdx , errScore ) ) completedSwarms = completedSwarms . union ( completedList ) if len ( completedSwarms ) == 0 and ( exhaustedSwarmId is None ) : return while True : if exhaustedSwarmId is not None : self . _hsState . setSwarmState ( exhaustedSwarmId , exhaustedSwarmStatus ) for swarmId in completedSwarms : self . _hsState . setSwarmState ( swarmId , 'completed' ) if not self . _hsState . isDirty ( ) : return success = self . _hsState . writeStateToDB ( ) if success : jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) bestModelId = jobResults . get ( 'bestModel' , None ) else : bestModelId = None for swarmId in list ( completedSwarms ) : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( swarmId = swarmId , completed = False ) if bestModelId in modelIds : modelIds . remove ( bestModelId ) if len ( modelIds ) == 0 : continue self . logger . info ( ""Killing the following models in swarm '%s' because"" ""the swarm is being terminated: %s"" % ( swarmId , str ( modelIds ) ) ) for modelId in modelIds : self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_KILLED ) , ignoreUnchanged = True ) return self . _hsState . readStateFromDB ( ) self . logger . debug ( ""New hsState has been set by some other worker to: "" "" \n%s"" % ( pprint . pformat ( self . _hsState . _state , indent = 4 ) ) )",Periodically check to see if we should remove a certain field combination from evaluation ( because it is doing so poorly ) or move on to the next sprint ( add in more fields ) .
"def _getCandidateParticleAndSwarm ( self , exhaustedSwarmId = None ) : jobCancel = self . _cjDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] if jobCancel : self . _jobCancelled = True ( workerCmpReason , workerCmpMsg ) = self . _cjDAO . jobGetFields ( self . _jobID , [ 'workerCompletionReason' , 'workerCompletionMsg' ] ) if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : self . logger . info ( ""Exiting due to job being cancelled"" ) self . _cjDAO . jobSetFields ( self . _jobID , dict ( workerCompletionMsg = ""Job was cancelled"" ) , useConnectionID = False , ignoreUnchanged = True ) else : self . logger . error ( ""Exiting because some worker set the "" ""workerCompletionReason to %s. WorkerCompletionMsg: %s"" % ( workerCmpReason , workerCmpMsg ) ) return ( True , None , None ) if self . _hsState is not None : priorActiveSwarms = self . _hsState . getActiveSwarms ( ) else : priorActiveSwarms = None self . _hsStatePeriodicUpdate ( exhaustedSwarmId = exhaustedSwarmId ) activeSwarms = self . _hsState . getActiveSwarms ( ) if activeSwarms != priorActiveSwarms : self . logger . info ( ""Active swarms changed to %s (from %s)"" % ( activeSwarms , priorActiveSwarms ) ) self . logger . debug ( ""Active swarms: %s"" % ( activeSwarms ) ) totalCmpModels = self . _resultsDB . getNumCompletedModels ( ) if totalCmpModels > 5 : numErrs = self . _resultsDB . getNumErrModels ( ) if ( float ( numErrs ) / totalCmpModels ) > self . _maxPctErrModels : errModelIds = self . _resultsDB . getErrModelIds ( ) resInfo = self . _cjDAO . modelsGetResultAndStatus ( [ errModelIds [ 0 ] ] ) [ 0 ] modelErrMsg = resInfo . completionMsg cmpMsg = ""%s: Exiting due to receiving too many models failing"" "" from exceptions (%d out of %d). \nModel Exception: %s"" % ( ErrorCodes . tooManyModelErrs , numErrs , totalCmpModels , modelErrMsg ) self . logger . error ( cmpMsg ) workerCmpReason = self . _cjDAO . jobGetFields ( self . _jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : self . _cjDAO . jobSetFields ( self . _jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = cmpMsg ) , useConnectionID = False , ignoreUnchanged = True ) return ( True , None , None ) if self . _hsState . isSearchOver ( ) : cmpMsg = ""Exiting because results did not improve in most recently"" "" completed sprint."" self . logger . info ( cmpMsg ) self . _cjDAO . jobSetFields ( self . _jobID , dict ( workerCompletionMsg = cmpMsg ) , useConnectionID = False , ignoreUnchanged = True ) return ( True , None , None ) sprintIdx = - 1 while True : sprintIdx += 1 ( active , eos ) = self . _hsState . isSprintActive ( sprintIdx ) if eos : if self . _hsState . anyGoodSprintsActive ( ) : self . logger . info ( ""No more sprints to explore, waiting for prior"" "" sprints to complete"" ) return ( False , None , None ) else : cmpMsg = ""Exiting because we've evaluated all possible field "" ""combinations"" self . _cjDAO . jobSetFields ( self . _jobID , dict ( workerCompletionMsg = cmpMsg ) , useConnectionID = False , ignoreUnchanged = True ) self . logger . info ( cmpMsg ) return ( True , None , None ) if not active : if not self . _speculativeParticles : if not self . _hsState . isSprintCompleted ( sprintIdx ) : self . logger . info ( ""Waiting for all particles in sprint %d to complete"" ""before evolving any more particles"" % ( sprintIdx ) ) return ( False , None , None ) continue swarmIds = self . _hsState . getActiveSwarms ( sprintIdx ) for swarmId in swarmIds : firstNonFullGenIdx = self . _resultsDB . firstNonFullGeneration ( swarmId = swarmId , minNumParticles = self . _minParticlesPerSwarm ) if firstNonFullGenIdx is None : continue if firstNonFullGenIdx < self . _resultsDB . highestGeneration ( swarmId ) : self . logger . info ( ""Cloning an earlier model in generation %d of swarm "" ""%s (sprintIdx=%s) to replace an orphaned model"" % ( firstNonFullGenIdx , swarmId , sprintIdx ) ) ( allParticles , allModelIds , errScores , completed , matured ) = self . _resultsDB . getOrphanParticleInfos ( swarmId , firstNonFullGenIdx ) if len ( allModelIds ) > 0 : newParticleId = True self . logger . info ( ""Cloning an orphaned model"" ) else : newParticleId = True self . logger . info ( ""No orphans found, so cloning a non-orphan"" ) ( allParticles , allModelIds , errScores , completed , matured ) = self . _resultsDB . getParticleInfos ( swarmId = swarmId , genIdx = firstNonFullGenIdx ) modelId = random . choice ( allModelIds ) self . logger . info ( ""Cloning model %r"" % ( modelId ) ) ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( modelId ) particle = Particle ( hsObj = self , resultsDB = self . _resultsDB , flattenedPermuteVars = self . _flattenedPermutations , newFromClone = particleState , newParticleId = newParticleId ) return ( False , particle , swarmId ) swarmSizes = numpy . array ( [ self . _resultsDB . numModels ( x ) for x in swarmIds ] ) swarmSizeAndIdList = zip ( swarmSizes , swarmIds ) swarmSizeAndIdList . sort ( ) for ( _ , swarmId ) in swarmSizeAndIdList : ( allParticles , allModelIds , errScores , completed , matured ) = ( self . _resultsDB . getParticleInfos ( swarmId ) ) if len ( allParticles ) < self . _minParticlesPerSwarm : particle = Particle ( hsObj = self , resultsDB = self . _resultsDB , flattenedPermuteVars = self . _flattenedPermutations , swarmId = swarmId , newFarFrom = allParticles ) bestPriorModel = None if sprintIdx >= 1 : ( bestPriorModel , errScore ) = self . _hsState . bestModelInSprint ( 0 ) if bestPriorModel is not None : self . logger . info ( ""Best model and errScore from previous sprint(%d):"" "" %s, %g"" % ( 0 , str ( bestPriorModel ) , errScore ) ) ( baseState , modelId , errScore , completed , matured ) = self . _resultsDB . getParticleInfo ( bestPriorModel ) particle . copyEncoderStatesFrom ( baseState ) particle . copyVarStatesFrom ( baseState , [ 'modelParams|inferenceType' ] ) whichVars = [ ] for varName in baseState [ 'varStates' ] : if ':' in varName : whichVars . append ( varName ) particle . newPosition ( whichVars ) self . logger . debug ( ""Particle after incorporating encoder vars from best "" ""model in previous sprint: \n%s"" % ( str ( particle ) ) ) return ( False , particle , swarmId ) ( readyParticles , readyModelIds , readyErrScores , _ , _ ) = ( self . _resultsDB . getParticleInfos ( swarmId , genIdx = None , matured = True , lastDescendent = True ) ) if len ( readyParticles ) > 0 : readyGenIdxs = [ x [ 'genIdx' ] for x in readyParticles ] sortedGenIdxs = sorted ( set ( readyGenIdxs ) ) genIdx = sortedGenIdxs [ 0 ] useParticle = None for particle in readyParticles : if particle [ 'genIdx' ] == genIdx : useParticle = particle break if not self . _speculativeParticles : ( particles , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( swarmId , genIdx = genIdx , matured = False ) if len ( particles ) > 0 : continue particle = Particle ( hsObj = self , resultsDB = self . _resultsDB , flattenedPermuteVars = self . _flattenedPermutations , evolveFromState = useParticle ) return ( False , particle , swarmId ) if not self . _speculativeParticles : self . logger . info ( ""Waiting for one or more of the %s swarms "" ""to complete a generation before evolving any more particles"" % ( str ( swarmIds ) ) ) return ( False , None , None )",Find or create a candidate particle to produce a new model .
"def _okToExit ( self ) : print >> sys . stderr , ""reporter:status:In hypersearchV2: _okToExit"" if not self . _jobCancelled : ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) if len ( modelIds ) > 0 : self . logger . info ( ""Ready to end hyperseach, but not all models have "" ""matured yet. Sleeping a bit to wait for all models "" ""to mature."" ) time . sleep ( 5.0 * random . random ( ) ) return False ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) for modelId in modelIds : self . logger . info ( ""Stopping model %d because the search has ended"" % ( modelId ) ) self . _cjDAO . modelSetFields ( modelId , dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , ignoreUnchanged = True ) self . _hsStatePeriodicUpdate ( ) pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] if jobResultsStr is not None : jobResults = json . loads ( jobResultsStr ) else : jobResults = { } if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : jobResults [ 'fieldContributions' ] = pctFieldContributions jobResults [ 'absoluteFieldContributions' ] = absFieldContributions isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , fieldName = 'results' , curValue = jobResultsStr , newValue = json . dumps ( jobResults ) ) if isUpdated : self . logger . info ( 'Successfully updated the field contributions:%s' , pctFieldContributions ) else : self . logger . info ( 'Failed updating the field contributions, ' 'another hypersearch worker must have updated it' ) return True",Test if it s OK to exit this worker . This is only called when we run out of prospective new models to evaluate . This method sees if all models have matured yet . If not it will sleep for a bit and return False . This will indicate to the hypersearch worker that we should keep running and check again later . This gives this worker a chance to pick up and adopt any model which may become orphaned by another worker before it matures .
"def createModels ( self , numModels = 1 ) : self . _checkForOrphanedModels ( ) modelResults = [ ] for _ in xrange ( numModels ) : candidateParticle = None if ( self . _maxModels is not None and ( self . _resultsDB . numModels ( ) - self . _resultsDB . getNumErrModels ( ) ) >= self . _maxModels ) : return ( self . _okToExit ( ) , [ ] ) if candidateParticle is None : ( exitNow , candidateParticle , candidateSwarm ) = ( self . _getCandidateParticleAndSwarm ( ) ) if candidateParticle is None : if exitNow : return ( self . _okToExit ( ) , [ ] ) else : print >> sys . stderr , ""reporter:status:In hypersearchV2: speculativeWait"" time . sleep ( self . _speculativeWaitSecondsMax * random . random ( ) ) return ( False , [ ] ) useEncoders = candidateSwarm . split ( '.' ) numAttempts = 0 while True : if numAttempts >= 1 : self . logger . debug ( ""Agitating particle to get unique position after %d "" ""failed attempts in a row"" % ( numAttempts ) ) candidateParticle . agitate ( ) position = candidateParticle . getPosition ( ) structuredParams = dict ( ) def _buildStructuredParams ( value , keys ) : flatKey = _flattenKeys ( keys ) if flatKey in self . _encoderNames : if flatKey in useEncoders : return value . getDict ( flatKey , position ) else : return None elif flatKey in position : return position [ flatKey ] else : return value structuredParams = rCopy ( self . _permutations , _buildStructuredParams , discardNoneKeys = False ) modelParams = dict ( structuredParams = structuredParams , particleState = candidateParticle . getState ( ) ) m = hashlib . md5 ( ) m . update ( sortedJSONDumpS ( structuredParams ) ) m . update ( self . _baseDescriptionHash ) paramsHash = m . digest ( ) particleInst = ""%s.%s"" % ( modelParams [ 'particleState' ] [ 'id' ] , modelParams [ 'particleState' ] [ 'genIdx' ] ) particleHash = hashlib . md5 ( particleInst ) . digest ( ) numAttempts += 1 if self . _filterFunc and not self . _filterFunc ( structuredParams ) : valid = False else : valid = True if valid and self . _resultsDB . getModelIDFromParamsHash ( paramsHash ) is None : break if numAttempts >= self . _maxUniqueModelAttempts : ( exitNow , candidateParticle , candidateSwarm ) = self . _getCandidateParticleAndSwarm ( exhaustedSwarmId = candidateSwarm ) if candidateParticle is None : if exitNow : return ( self . _okToExit ( ) , [ ] ) else : time . sleep ( self . _speculativeWaitSecondsMax * random . random ( ) ) return ( False , [ ] ) numAttempts = 0 useEncoders = candidateSwarm . split ( '.' ) if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( ""Submitting new potential model to HypersearchWorker: \n%s"" % ( pprint . pformat ( modelParams , indent = 4 ) ) ) modelResults . append ( ( modelParams , paramsHash , particleHash ) ) return ( False , modelResults )",Create one or more new models for evaluation . These should NOT be models that we already know are in progress ( i . e . those that have been sent to us via recordModelProgress ) . We return a list of models to the caller ( HypersearchWorker ) and if one can be successfully inserted into the models table ( i . e . it is not a duplicate ) then HypersearchWorker will turn around and call our runModel () method passing in this model . If it is a duplicate HypersearchWorker will call this method again . A model is a duplicate if either the modelParamsHash or particleHash is identical to another entry in the model table .
"def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , completed , completionReason , matured , numRecords ) : if results is None : metricResult = None else : metricResult = results [ 1 ] . values ( ) [ 0 ] errScore = self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = metricResult , completed = completed , completionReason = completionReason , matured = matured , numRecords = numRecords ) self . logger . debug ( 'Received progress on model %d: completed: %s, ' 'cmpReason: %s, numRecords: %d, errScore: %s' , modelID , completed , completionReason , numRecords , errScore ) ( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) self . logger . debug ( 'Best err score seen so far: %s on model %s' % ( bestResult , bestModelID ) )",Record or update the results for a model . This is called by the HSW whenever it gets results info for another model or updated results on a model that is still running .
"def runModel ( self , modelID , jobID , modelParams , modelParamsHash , jobsDAO , modelCheckpointGUID ) : if not self . _createCheckpoints : modelCheckpointGUID = None self . _resultsDB . update ( modelID = modelID , modelParams = modelParams , modelParamsHash = modelParamsHash , metricResult = None , completed = False , completionReason = None , matured = False , numRecords = 0 ) structuredParams = modelParams [ 'structuredParams' ] if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : self . logger . debug ( ""Running Model. \nmodelParams: %s, \nmodelID=%s, "" % ( pprint . pformat ( modelParams , indent = 4 ) , modelID ) ) cpuTimeStart = time . clock ( ) logLevel = self . logger . getEffectiveLevel ( ) try : if self . _dummyModel is None or self . _dummyModel is False : ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( modelID = modelID , jobID = jobID , baseDescription = self . _baseDescription , params = structuredParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) else : dummyParams = dict ( self . _dummyModel ) dummyParams [ 'permutationParams' ] = structuredParams if self . _dummyModelParamsFunc is not None : permInfo = dict ( structuredParams ) permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) ( cmpReason , cmpMsg ) = runDummyModel ( modelID = modelID , jobID = jobID , params = dummyParams , predictedField = self . _predictedField , reportKeys = self . _reportKeys , optimizeKey = self . _optimizeKey , jobsDAO = jobsDAO , modelCheckpointGUID = modelCheckpointGUID , logLevel = logLevel , predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) jobsDAO . modelSetCompleted ( modelID , completionReason = cmpReason , completionMsg = cmpMsg , cpuTime = time . clock ( ) - cpuTimeStart ) except InvalidConnectionException , e : self . logger . warn ( ""%s"" , e )",Run the given model .
"def _escape ( s ) : assert isinstance ( s , str ) , ""expected %s but got %s; value=%s"" % ( type ( str ) , type ( s ) , s ) s = s . replace ( ""\\"" , ""\\\\"" ) s = s . replace ( ""\n"" , ""\\n"" ) s = s . replace ( ""\t"" , ""\\t"" ) s = s . replace ( "","" , ""\t"" ) return s",Escape commas tabs newlines and dashes in a string
"def _engineServicesRunning ( ) : process = subprocess . Popen ( [ ""ps"" , ""aux"" ] , stdout = subprocess . PIPE ) stdout = process . communicate ( ) [ 0 ] result = process . returncode if result != 0 : raise RuntimeError ( ""Unable to check for running client job manager"" ) running = False for line in stdout . split ( ""\n"" ) : if ""python"" in line and ""clientjobmanager.client_job_manager"" in line : running = True break return running",Return true if the engine services are running
"def runWithConfig ( swarmConfig , options , outDir = None , outputLabel = ""default"" , permWorkDir = None , verbosity = 1 ) : global g_currentVerbosityLevel g_currentVerbosityLevel = verbosity if outDir is None : outDir = os . getcwd ( ) if permWorkDir is None : permWorkDir = os . getcwd ( ) _checkOverwrite ( options , outDir ) _generateExpFilesFromSwarmDescription ( swarmConfig , outDir ) options [ ""expDescConfig"" ] = swarmConfig options [ ""outputLabel"" ] = outputLabel options [ ""outDir"" ] = outDir options [ ""permWorkDir"" ] = permWorkDir runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )",Starts a swarm given an dictionary configuration . @param swarmConfig { dict } A complete [ swarm description ] ( http : // nupic . docs . numenta . org / 0 . 7 . 0 . dev0 / guides / swarming / running . html#the - swarm - description ) object . @param outDir { string } Optional path to write swarm details ( defaults to current working directory ) . @param outputLabel { string } Optional label for output ( defaults to default ) . @param permWorkDir { string } Optional location of working directory ( defaults to current working directory ) . @param verbosity { int } Optional ( 1 2 3 ) increasing verbosity of output .
"def runWithJsonFile ( expJsonFilePath , options , outputLabel , permWorkDir ) : if ""verbosityCount"" in options : verbosity = options [ ""verbosityCount"" ] del options [ ""verbosityCount"" ] else : verbosity = 1 _setupInterruptHandling ( ) with open ( expJsonFilePath , ""r"" ) as jsonFile : expJsonConfig = json . loads ( jsonFile . read ( ) ) outDir = os . path . dirname ( expJsonFilePath ) return runWithConfig ( expJsonConfig , options , outDir = outDir , outputLabel = outputLabel , permWorkDir = permWorkDir , verbosity = verbosity )",Starts a swarm given a path to a JSON file containing configuration .
"def runWithPermutationsScript ( permutationsFilePath , options , outputLabel , permWorkDir ) : global g_currentVerbosityLevel if ""verbosityCount"" in options : g_currentVerbosityLevel = options [ ""verbosityCount"" ] del options [ ""verbosityCount"" ] else : g_currentVerbosityLevel = 1 _setupInterruptHandling ( ) options [ ""permutationsScriptPath"" ] = permutationsFilePath options [ ""outputLabel"" ] = outputLabel options [ ""outDir"" ] = permWorkDir options [ ""permWorkDir"" ] = permWorkDir runOptions = _injectDefaultOptions ( options ) _validateOptions ( runOptions ) return _runAction ( runOptions )",Starts a swarm given a path to a permutations . py script .
"def _backupFile ( filePath ) : assert os . path . exists ( filePath ) stampNum = 0 ( prefix , suffix ) = os . path . splitext ( filePath ) while True : backupPath = ""%s.%d%s"" % ( prefix , stampNum , suffix ) stampNum += 1 if not os . path . exists ( backupPath ) : break shutil . copyfile ( filePath , backupPath ) return backupPath",Back up a file
"def _iterModels ( modelIDs ) : class ModelInfoIterator ( object ) : """"""ModelInfo iterator implementation class
    """""" __CACHE_LIMIT = 1000 debug = False def __init__ ( self , modelIDs ) : """"""
      Parameters:
      ----------------------------------------------------------------------
      modelIDs:     a sequence of Nupic model identifiers for which this
                    iterator will return _NupicModelInfo instances.
                    NOTE: The returned instances are NOT guaranteed to be in
                    the same order as the IDs in modelIDs sequence.
      retval:       nothing
      """""" self . __modelIDs = tuple ( modelIDs ) if self . debug : _emit ( Verbosity . DEBUG , ""MODELITERATOR: __init__; numModelIDs=%s"" % len ( self . __modelIDs ) ) self . __nextIndex = 0 self . __modelCache = collections . deque ( ) return def __iter__ ( self ) : """"""Iterator Protocol function

      Parameters:
      ----------------------------------------------------------------------
      retval:         self
      """""" return self def next ( self ) : """"""Iterator Protocol function

      Parameters:
      ----------------------------------------------------------------------
      retval:       A _NupicModelInfo instance or raises StopIteration to
                    signal end of iteration.
      """""" return self . __getNext ( ) def __getNext ( self ) : """"""Implementation of the next() Iterator Protocol function.

      When the modelInfo cache becomes empty, queries Nupic and fills the cache
      with the next set of NupicModelInfo instances.

      Parameters:
      ----------------------------------------------------------------------
      retval:       A _NupicModelInfo instance or raises StopIteration to
                    signal end of iteration.
      """""" if self . debug : _emit ( Verbosity . DEBUG , ""MODELITERATOR: __getNext(); modelCacheLen=%s"" % ( len ( self . __modelCache ) ) ) if not self . __modelCache : self . __fillCache ( ) if not self . __modelCache : raise StopIteration ( ) return self . __modelCache . popleft ( ) def __fillCache ( self ) : """"""Queries Nupic and fills an empty modelInfo cache with the next set of
      _NupicModelInfo instances

      Parameters:
      ----------------------------------------------------------------------
      retval:       nothing
      """""" assert ( not self . __modelCache ) numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 if self . __nextIndex >= numModelIDs : return idRange = self . __nextIndex + self . __CACHE_LIMIT if idRange > numModelIDs : idRange = numModelIDs lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] self . __nextIndex += ( idRange - self . __nextIndex ) infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) assert len ( infoList ) == len ( lookupIDs ) , ""modelsInfo returned %s elements; expected %s."" % ( len ( infoList ) , len ( lookupIDs ) ) for rawInfo in infoList : modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) self . __modelCache . append ( modelInfo ) assert len ( self . __modelCache ) == len ( lookupIDs ) , ""Added %s elements to modelCache; expected %s."" % ( len ( self . __modelCache ) , len ( lookupIDs ) ) if self . debug : _emit ( Verbosity . DEBUG , ""MODELITERATOR: Leaving __fillCache(); modelCacheLen=%s"" % ( len ( self . __modelCache ) , ) ) return ModelInfoIterator ( modelIDs )",Creates an iterator that returns ModelInfo elements for the given modelIDs
"def pickupSearch ( self ) : self . __searchJob = self . loadSavedHyperSearchJob ( permWorkDir = self . _options [ ""permWorkDir"" ] , outputLabel = self . _options [ ""outputLabel"" ] ) self . monitorSearchJob ( )",Pick up the latest search from a saved jobID and monitor it to completion Parameters : ---------------------------------------------------------------------- retval : nothing
"def monitorSearchJob ( self ) : assert self . __searchJob is not None jobID = self . __searchJob . getJobID ( ) startTime = time . time ( ) lastUpdateTime = datetime . now ( ) expectedNumModels = self . __searchJob . getExpectedNumModels ( searchMethod = self . _options [ ""searchMethod"" ] ) lastNumFinished = 0 finishedModelIDs = set ( ) finishedModelStats = _ModelStats ( ) lastWorkerState = None lastJobResults = None lastModelMilestones = None lastEngStatus = None hyperSearchFinished = False while not hyperSearchFinished : jobInfo = self . __searchJob . getJobStatus ( self . _workers ) hyperSearchFinished = jobInfo . isFinished ( ) modelIDs = self . __searchJob . queryModelIDs ( ) _emit ( Verbosity . DEBUG , ""Current number of models is %d (%d of them completed)"" % ( len ( modelIDs ) , len ( finishedModelIDs ) ) ) if len ( modelIDs ) > 0 : checkModelIDs = [ ] for modelID in modelIDs : if modelID not in finishedModelIDs : checkModelIDs . append ( modelID ) del modelIDs if checkModelIDs : _emit ( Verbosity . DEBUG , ""Checking %d models..."" % ( len ( checkModelIDs ) ) ) errorCompletionMsg = None for ( i , modelInfo ) in enumerate ( _iterModels ( checkModelIDs ) ) : _emit ( Verbosity . DEBUG , ""[%s] Checking completion: %s"" % ( i , modelInfo ) ) if modelInfo . isFinished ( ) : finishedModelIDs . add ( modelInfo . getModelID ( ) ) finishedModelStats . update ( modelInfo ) if ( modelInfo . getCompletionReason ( ) . isError ( ) and not errorCompletionMsg ) : errorCompletionMsg = modelInfo . getCompletionMsg ( ) metrics = modelInfo . getReportMetrics ( ) self . __foundMetrcsKeySet . update ( metrics . keys ( ) ) numFinished = len ( finishedModelIDs ) if numFinished != lastNumFinished : lastNumFinished = numFinished if expectedNumModels is None : expModelsStr = """" else : expModelsStr = ""of %s"" % ( expectedNumModels ) stats = finishedModelStats print ( ""<jobID: %s> %s %s models finished [success: %s; %s: %s; %s: "" ""%s; %s: %s; %s: %s; %s: %s; %s: %s]"" % ( jobID , numFinished , expModelsStr , ( stats . numCompletedEOF + stats . numCompletedStopped ) , ""EOF"" if stats . numCompletedEOF else ""eof"" , stats . numCompletedEOF , ""STOPPED"" if stats . numCompletedStopped else ""stopped"" , stats . numCompletedStopped , ""KILLED"" if stats . numCompletedKilled else ""killed"" , stats . numCompletedKilled , ""ERROR"" if stats . numCompletedError else ""error"" , stats . numCompletedError , ""ORPHANED"" if stats . numCompletedError else ""orphaned"" , stats . numCompletedOrphaned , ""UNKNOWN"" if stats . numCompletedOther else ""unknown"" , stats . numCompletedOther ) ) if errorCompletionMsg : print ""ERROR MESSAGE: %s"" % errorCompletionMsg workerState = jobInfo . getWorkerState ( ) if workerState != lastWorkerState : print ""##>> UPDATED WORKER STATE: \n%s"" % ( pprint . pformat ( workerState , indent = 4 ) ) lastWorkerState = workerState jobResults = jobInfo . getResults ( ) if jobResults != lastJobResults : print ""####>> UPDATED JOB RESULTS: \n%s (elapsed time: %g secs)"" % ( pprint . pformat ( jobResults , indent = 4 ) , time . time ( ) - startTime ) lastJobResults = jobResults modelMilestones = jobInfo . getModelMilestones ( ) if modelMilestones != lastModelMilestones : print ""##>> UPDATED MODEL MILESTONES: \n%s"" % ( pprint . pformat ( modelMilestones , indent = 4 ) ) lastModelMilestones = modelMilestones engStatus = jobInfo . getEngStatus ( ) if engStatus != lastEngStatus : print ""##>> UPDATED STATUS: \n%s"" % ( engStatus ) lastEngStatus = engStatus if not hyperSearchFinished : if self . _options [ ""timeout"" ] != None : if ( ( datetime . now ( ) - lastUpdateTime ) > timedelta ( minutes = self . _options [ ""timeout"" ] ) ) : print ""Timeout reached, exiting"" self . __cjDAO . jobCancel ( jobID ) sys . exit ( 1 ) time . sleep ( 1 ) modelIDs = self . __searchJob . queryModelIDs ( ) print ""Evaluated %s models"" % len ( modelIDs ) print ""HyperSearch finished!"" jobInfo = self . __searchJob . getJobStatus ( self . _workers ) print ""Worker completion message: %s"" % ( jobInfo . getWorkerCompletionMsg ( ) )",Parameters : ---------------------------------------------------------------------- retval : nothing
"def _launchWorkers ( self , cmdLine , numWorkers ) : self . _workers = [ ] for i in range ( numWorkers ) : stdout = tempfile . NamedTemporaryFile ( delete = False ) stderr = tempfile . NamedTemporaryFile ( delete = False ) p = subprocess . Popen ( cmdLine , bufsize = 1 , env = os . environ , shell = True , stdin = None , stdout = stdout , stderr = stderr ) p . _stderr_file = stderr p . _stdout_file = stdout self . _workers . append ( p )",Launch worker processes to execute the given command line
"def __startSearch ( self ) : params = _ClientJobUtils . makeSearchJobParamsDict ( options = self . _options , forRunning = True ) if self . _options [ ""action"" ] == ""dryRun"" : args = [ sys . argv [ 0 ] , ""--params=%s"" % ( json . dumps ( params ) ) ] print print ""=================================================================="" print ""RUNNING PERMUTATIONS INLINE as \""DRY RUN\""..."" print ""=================================================================="" jobID = hypersearch_worker . main ( args ) else : cmdLine = _setUpExports ( self . _options [ ""exports"" ] ) cmdLine += ""$HYPERSEARCH"" maxWorkers = self . _options [ ""maxWorkers"" ] jobID = self . __cjDAO . jobInsert ( client = ""GRP"" , cmdLine = cmdLine , params = json . dumps ( params ) , minimumWorkers = 1 , maximumWorkers = maxWorkers , jobType = self . __cjDAO . JOB_TYPE_HS ) cmdLine = ""python -m nupic.swarming.hypersearch_worker"" "" --jobID=%d"" % ( jobID ) self . _launchWorkers ( cmdLine , maxWorkers ) searchJob = _HyperSearchJob ( jobID ) self . __saveHyperSearchJobID ( permWorkDir = self . _options [ ""permWorkDir"" ] , outputLabel = self . _options [ ""outputLabel"" ] , hyperSearchJob = searchJob ) if self . _options [ ""action"" ] == ""dryRun"" : print ""Successfully executed \""dry-run\"" hypersearch, jobID=%d"" % ( jobID ) else : print ""Successfully submitted new HyperSearch job, jobID=%d"" % ( jobID ) _emit ( Verbosity . DEBUG , ""Each worker executing the command line: %s"" % ( cmdLine , ) ) return searchJob",Starts HyperSearch as a worker or runs it inline for the dryRun action
"def generateReport ( cls , options , replaceReport , hyperSearchJob , metricsKeys ) : if hyperSearchJob is None : hyperSearchJob = cls . loadSavedHyperSearchJob ( permWorkDir = options [ ""permWorkDir"" ] , outputLabel = options [ ""outputLabel"" ] ) modelIDs = hyperSearchJob . queryModelIDs ( ) bestModel = None metricstmp = set ( ) searchVar = set ( ) for modelInfo in _iterModels ( modelIDs ) : if modelInfo . isFinished ( ) : vars = modelInfo . getParamLabels ( ) . keys ( ) searchVar . update ( vars ) metrics = modelInfo . getReportMetrics ( ) metricstmp . update ( metrics . keys ( ) ) if metricsKeys is None : metricsKeys = metricstmp reportWriter = _ReportCSVWriter ( hyperSearchJob = hyperSearchJob , metricsKeys = metricsKeys , searchVar = searchVar , outputDirAbsPath = options [ ""permWorkDir"" ] , outputLabel = options [ ""outputLabel"" ] , replaceReport = replaceReport ) modelStats = _ModelStats ( ) print ""\nResults from all experiments:"" print ""----------------------------------------------------------------"" searchParams = hyperSearchJob . getParams ( ) ( optimizationMetricKey , maximizeMetric ) = ( _PermutationUtils . getOptimizationMetricInfo ( searchParams ) ) formatStr = None foundMetricsKeySet = set ( metricsKeys ) sortedMetricsKeys = [ ] jobInfo = _clientJobsDB ( ) . jobInfo ( hyperSearchJob . getJobID ( ) ) if jobInfo . cancel == 1 : raise Exception ( jobInfo . workerCompletionMsg ) try : results = json . loads ( jobInfo . results ) except Exception , e : print ""json.loads(jobInfo.results) raised an exception.  "" ""Here is some info to help with debugging:"" print ""jobInfo: "" , jobInfo print ""jobInfo.results: "" , jobInfo . results print ""EXCEPTION: "" , e raise bestModelNum = results [ ""bestModel"" ] bestModelIterIndex = None totalWallTime = 0 totalRecords = 0 scoreModelIDDescList = [ ] for ( i , modelInfo ) in enumerate ( _iterModels ( modelIDs ) ) : reportWriter . emit ( modelInfo ) totalRecords += modelInfo . getNumRecords ( ) format = ""%Y-%m-%d %H:%M:%S"" startTime = modelInfo . getStartTime ( ) if modelInfo . isFinished ( ) : endTime = modelInfo . getEndTime ( ) st = datetime . strptime ( startTime , format ) et = datetime . strptime ( endTime , format ) totalWallTime += ( et - st ) . seconds modelStats . update ( modelInfo ) expDesc = modelInfo . getModelDescription ( ) reportMetrics = modelInfo . getReportMetrics ( ) optimizationMetrics = modelInfo . getOptimizationMetrics ( ) if modelInfo . getModelID ( ) == bestModelNum : bestModel = modelInfo bestModelIterIndex = i bestMetric = optimizationMetrics . values ( ) [ 0 ] if optimizationMetrics : assert len ( optimizationMetrics ) == 1 , ( ""expected 1 opt key, but got %d (%s) in %s"" % ( len ( optimizationMetrics ) , optimizationMetrics , modelInfo ) ) if modelInfo . getCompletionReason ( ) . isEOF ( ) : scoreModelIDDescList . append ( ( optimizationMetrics . values ( ) [ 0 ] , modelInfo . getModelID ( ) , modelInfo . getGeneratedDescriptionFile ( ) , modelInfo . getParamLabels ( ) ) ) print ""[%d] Experiment %s\n(%s):"" % ( i , modelInfo , expDesc ) if ( modelInfo . isFinished ( ) and not ( modelInfo . getCompletionReason ( ) . isStopped or modelInfo . getCompletionReason ( ) . isEOF ( ) ) ) : print "">> COMPLETION MESSAGE: %s"" % modelInfo . getCompletionMsg ( ) if reportMetrics : foundMetricsKeySet . update ( reportMetrics . iterkeys ( ) ) if len ( sortedMetricsKeys ) != len ( foundMetricsKeySet ) : sortedMetricsKeys = sorted ( foundMetricsKeySet ) maxKeyLen = max ( [ len ( k ) for k in sortedMetricsKeys ] ) formatStr = ""  %%-%ds"" % ( maxKeyLen + 2 ) for key in sortedMetricsKeys : if key in reportMetrics : if key == optimizationMetricKey : m = ""%r (*)"" % reportMetrics [ key ] else : m = ""%r"" % reportMetrics [ key ] print formatStr % ( key + "":"" ) , m print print ""--------------------------------------------------------------"" if len ( modelIDs ) > 0 : print ""%d experiments total (%s).\n"" % ( len ( modelIDs ) , ( ""all completed successfully"" if ( modelStats . numCompletedKilled + modelStats . numCompletedEOF ) == len ( modelIDs ) else ""WARNING: %d models have not completed or there were errors"" % ( len ( modelIDs ) - ( modelStats . numCompletedKilled + modelStats . numCompletedEOF + modelStats . numCompletedStopped ) ) ) ) if modelStats . numStatusOther > 0 : print ""ERROR: models with unexpected status: %d"" % ( modelStats . numStatusOther ) print ""WaitingToStart: %d"" % modelStats . numStatusWaitingToStart print ""Running: %d"" % modelStats . numStatusRunning print ""Completed: %d"" % modelStats . numStatusCompleted if modelStats . numCompletedOther > 0 : print ""    ERROR: models with unexpected completion reason: %d"" % ( modelStats . numCompletedOther ) print ""    ran to EOF: %d"" % modelStats . numCompletedEOF print ""    ran to stop signal: %d"" % modelStats . numCompletedStopped print ""    were orphaned: %d"" % modelStats . numCompletedOrphaned print ""    killed off: %d"" % modelStats . numCompletedKilled print ""    failed: %d"" % modelStats . numCompletedError assert modelStats . numStatusOther == 0 , ""numStatusOther=%s"" % ( modelStats . numStatusOther ) assert modelStats . numCompletedOther == 0 , ""numCompletedOther=%s"" % ( modelStats . numCompletedOther ) else : print ""0 experiments total."" print global gCurrentSearch jobStatus = hyperSearchJob . getJobStatus ( gCurrentSearch . _workers ) jobResults = jobStatus . getResults ( ) if ""fieldContributions"" in jobResults : print ""Field Contributions:"" pprint . pprint ( jobResults [ ""fieldContributions"" ] , indent = 4 ) else : print ""Field contributions info not available"" if bestModel is not None : maxKeyLen = max ( [ len ( k ) for k in sortedMetricsKeys ] ) maxKeyLen = max ( maxKeyLen , len ( optimizationMetricKey ) ) formatStr = ""  %%-%ds"" % ( maxKeyLen + 2 ) bestMetricValue = bestModel . getOptimizationMetrics ( ) . values ( ) [ 0 ] optimizationMetricName = bestModel . getOptimizationMetrics ( ) . keys ( ) [ 0 ] print print ""Best results on the optimization metric %s (maximize=%s):"" % ( optimizationMetricName , maximizeMetric ) print ""[%d] Experiment %s (%s):"" % ( bestModelIterIndex , bestModel , bestModel . getModelDescription ( ) ) print formatStr % ( optimizationMetricName + "":"" ) , bestMetricValue print print ""Total number of Records processed: %d"" % totalRecords print print ""Total wall time for all models: %d"" % totalWallTime hsJobParams = hyperSearchJob . getParams ( ) if options [ ""genTopNDescriptions"" ] > 0 : print ""\nGenerating description files for top %d models..."" % ( options [ ""genTopNDescriptions"" ] ) scoreModelIDDescList . sort ( ) scoreModelIDDescList = scoreModelIDDescList [ 0 : options [ ""genTopNDescriptions"" ] ] i = - 1 for ( score , modelID , description , paramLabels ) in scoreModelIDDescList : i += 1 outDir = os . path . join ( options [ ""permWorkDir"" ] , ""model_%d"" % ( i ) ) print ""Generating description file for model %s at %s"" % ( modelID , outDir ) if not os . path . exists ( outDir ) : os . makedirs ( outDir ) base_description_path = os . path . join ( options [ ""outDir"" ] , ""description.py"" ) base_description_relpath = os . path . relpath ( base_description_path , start = outDir ) description = description . replace ( ""importBaseDescription('base.py', config)"" , ""importBaseDescription('%s', config)"" % base_description_relpath ) fd = open ( os . path . join ( outDir , ""description.py"" ) , ""wb"" ) fd . write ( description ) fd . close ( ) fd = open ( os . path . join ( outDir , ""params.csv"" ) , ""wb"" ) writer = csv . writer ( fd ) colNames = paramLabels . keys ( ) colNames . sort ( ) writer . writerow ( colNames ) row = [ paramLabels [ x ] for x in colNames ] writer . writerow ( row ) fd . close ( ) print ""Generating model params file..."" mod = imp . load_source ( ""description"" , os . path . join ( outDir , ""description.py"" ) ) model_description = mod . descriptionInterface . getModelDescription ( ) fd = open ( os . path . join ( outDir , ""model_params.py"" ) , ""wb"" ) fd . write ( ""%s\nMODEL_PARAMS = %s"" % ( getCopyrightHead ( ) , pprint . pformat ( model_description ) ) ) fd . close ( ) print reportWriter . finalize ( ) return model_description",Prints all available results in the given HyperSearch job and emits model information to the permutations report csv .
"def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , outputLabel = outputLabel ) searchJob = _HyperSearchJob ( nupicJobID = jobID ) return searchJob",Instantiates a _HyperSearchJob instance from info saved in file
"def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : jobID = hyperSearchJob . getJobID ( ) filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) if os . path . exists ( filePath ) : _backupFile ( filePath ) d = dict ( hyperSearchJobID = jobID ) with open ( filePath , ""wb"" ) as jobIdPickleFile : pickle . dump ( d , jobIdPickleFile )",Saves the given _HyperSearchJob instance s jobID to file
"def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , outputLabel = outputLabel ) jobID = None with open ( filePath , ""r"" ) as jobIdPickleFile : jobInfo = pickle . load ( jobIdPickleFile ) jobID = jobInfo [ ""hyperSearchJobID"" ] return jobID",Loads a saved jobID from file
"def __getHyperSearchJobIDFilePath ( cls , permWorkDir , outputLabel ) : basePath = permWorkDir filename = ""%s_HyperSearchJobID.pkl"" % ( outputLabel , ) filepath = os . path . join ( basePath , filename ) return filepath",Returns filepath where to store HyperSearch JobID
"def emit ( self , modelInfo ) : if self . __csvFileObj is None : self . __openAndInitCSVFile ( modelInfo ) csv = self . __csvFileObj print >> csv , ""%s, "" % ( self . __searchJobID ) , print >> csv , ""%s, "" % ( modelInfo . getModelID ( ) ) , print >> csv , ""%s, "" % ( modelInfo . statusAsString ( ) ) , if modelInfo . isFinished ( ) : print >> csv , ""%s, "" % ( modelInfo . getCompletionReason ( ) ) , else : print >> csv , ""NA, "" , if not modelInfo . isWaitingToStart ( ) : print >> csv , ""%s, "" % ( modelInfo . getStartTime ( ) ) , else : print >> csv , ""NA, "" , if modelInfo . isFinished ( ) : dateFormat = ""%Y-%m-%d %H:%M:%S"" startTime = modelInfo . getStartTime ( ) endTime = modelInfo . getEndTime ( ) print >> csv , ""%s, "" % endTime , st = datetime . strptime ( startTime , dateFormat ) et = datetime . strptime ( endTime , dateFormat ) print >> csv , ""%s, "" % ( str ( ( et - st ) . seconds ) ) , else : print >> csv , ""NA, "" , print >> csv , ""NA, "" , print >> csv , ""%s, "" % str ( modelInfo . getModelDescription ( ) ) , print >> csv , ""%s, "" % str ( modelInfo . getNumRecords ( ) ) , paramLabelsDict = modelInfo . getParamLabels ( ) for key in self . __sortedVariableNames : if key in paramLabelsDict : print >> csv , ""%s, "" % ( paramLabelsDict [ key ] ) , else : print >> csv , ""None, "" , metrics = modelInfo . getReportMetrics ( ) for key in self . __sortedMetricsKeys : value = metrics . get ( key , ""NA"" ) value = str ( value ) value = value . replace ( ""\n"" , "" "" ) print >> csv , ""%s, "" % ( value ) , print >> csv",Emit model info to csv file
"def finalize ( self ) : if self . __csvFileObj is not None : self . __csvFileObj . close ( ) self . __csvFileObj = None print ""Report csv saved in %s"" % ( self . __reportCSVPath , ) if self . __backupCSVPath : print ""Previous report csv file was backed up to %s"" % ( self . __backupCSVPath , ) else : print ""Nothing was written to report csv file.""",Close file and print report / backup csv file paths
"def __openAndInitCSVFile ( self , modelInfo ) : basePath = self . __outputDirAbsPath reportCSVName = ""%s_Report.csv"" % ( self . __outputLabel , ) reportCSVPath = self . __reportCSVPath = os . path . join ( basePath , reportCSVName ) backupCSVPath = None if os . path . exists ( reportCSVPath ) : backupCSVPath = self . __backupCSVPath = _backupFile ( reportCSVPath ) if self . __replaceReport : mode = ""w"" else : mode = ""a"" csv = self . __csvFileObj = open ( reportCSVPath , mode ) if not self . __replaceReport and backupCSVPath : print >> csv print >> csv print >> csv , ""jobID, "" , print >> csv , ""modelID, "" , print >> csv , ""status, "" , print >> csv , ""completionReason, "" , print >> csv , ""startTime, "" , print >> csv , ""endTime, "" , print >> csv , ""runtime(s), "" , print >> csv , ""expDesc, "" , print >> csv , ""numRecords, "" , for key in self . __sortedVariableNames : print >> csv , ""%s, "" % key , for key in self . __sortedMetricsKeys : print >> csv , ""%s, "" % key , print >> csv",- Backs up old report csv file ; - opens the report csv file in append or overwrite mode ( per self . __replaceReport ) ; - emits column fields ; - sets up self . __sortedVariableNames self . __csvFileObj self . __backupCSVPath and self . __reportCSVPath
"def getJobStatus ( self , workers ) : jobInfo = self . JobStatus ( self . __nupicJobID , workers ) return jobInfo",Parameters : ---------------------------------------------------------------------- workers : If this job was launched outside of the nupic job engine then this is an array of subprocess Popen instances one for each worker retval : _NupicJob . JobStatus instance
def queryModelIDs ( self ) : jobID = self . getJobID ( ) modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) return modelIDs,Queuries DB for model IDs of all currently instantiated models associated with this HyperSearch job .
"def makeSearchJobParamsDict ( cls , options , forRunning = False ) : if options [ ""searchMethod"" ] == ""v2"" : hsVersion = ""v2"" else : raise Exception ( ""Unsupported search method: %r"" % options [ ""searchMethod"" ] ) maxModels = options [ ""maxPermutations"" ] if options [ ""action"" ] == ""dryRun"" and maxModels is None : maxModels = 1 useTerminators = options [ ""useTerminators"" ] if useTerminators is None : params = { ""hsVersion"" : hsVersion , ""maxModels"" : maxModels , } else : params = { ""hsVersion"" : hsVersion , ""useTerminators"" : useTerminators , ""maxModels"" : maxModels , } if forRunning : params [ ""persistentJobGUID"" ] = str ( uuid . uuid1 ( ) ) if options [ ""permutationsScriptPath"" ] : params [ ""permutationsPyFilename"" ] = options [ ""permutationsScriptPath"" ] elif options [ ""expDescConfig"" ] : params [ ""description"" ] = options [ ""expDescConfig"" ] else : with open ( options [ ""expDescJsonPath"" ] , mode = ""r"" ) as fp : params [ ""description"" ] = json . load ( fp ) return params",Constructs a dictionary of HyperSearch parameters suitable for converting to json and passing as the params argument to ClientJobsDAO . jobInsert () Parameters : ---------------------------------------------------------------------- options : NupicRunPermutations options dict forRunning : True if the params are for running a Hypersearch job ; False if params are for introspection only .
"def getOptimizationMetricInfo ( cls , searchJobParams ) : if searchJobParams [ ""hsVersion"" ] == ""v2"" : search = HypersearchV2 ( searchParams = searchJobParams ) else : raise RuntimeError ( ""Unsupported hypersearch version \""%s\"""" % ( searchJobParams [ ""hsVersion"" ] ) ) info = search . getOptimizationMetricInfo ( ) return info",Retrives the optimization key name and optimization function .
"def getModelDescription ( self ) : params = self . __unwrapParams ( ) if ""experimentName"" in params : return params [ ""experimentName"" ] else : paramSettings = self . getParamLabels ( ) items = [ ] for key , value in paramSettings . items ( ) : items . append ( ""%s_%s"" % ( key , value ) ) return ""."" . join ( items )",Parameters : ---------------------------------------------------------------------- retval : Printable description of the model .
"def getParamLabels ( self ) : params = self . __unwrapParams ( ) if ""particleState"" in params : retval = dict ( ) queue = [ ( pair , retval ) for pair in params [ ""particleState"" ] [ ""varStates"" ] . iteritems ( ) ] while len ( queue ) > 0 : pair , output = queue . pop ( ) k , v = pair if ( ""position"" in v and ""bestPosition"" in v and ""velocity"" in v ) : output [ k ] = v [ ""position"" ] else : if k not in output : output [ k ] = dict ( ) queue . extend ( ( pair , output [ k ] ) for pair in v . iteritems ( ) ) return retval",Parameters : ---------------------------------------------------------------------- retval : a dictionary of model parameter labels . For each entry the key is the name of the parameter and the value is the value chosen for it .
"def __unwrapParams ( self ) : if self . __cachedParams is None : self . __cachedParams = json . loads ( self . __rawInfo . params ) assert self . __cachedParams is not None , ""%s resulted in None"" % self . __rawInfo . params return self . __cachedParams",Unwraps self . __rawInfo . params into the equivalent python dictionary and caches it in self . __cachedParams . Returns the unwrapped params
def getAllMetrics ( self ) : result = self . getReportMetrics ( ) result . update ( self . getOptimizationMetrics ( ) ) return result,Retrives a dictionary of metrics that combines all report and optimization metrics
"def __unwrapResults ( self ) : if self . __cachedResults is None : if self . __rawInfo . results is not None : resultList = json . loads ( self . __rawInfo . results ) assert len ( resultList ) == 2 , ""Expected 2 elements, but got %s (%s)."" % ( len ( resultList ) , resultList ) self . __cachedResults = self . ModelResults ( reportMetrics = resultList [ 0 ] , optimizationMetrics = resultList [ 1 ] ) else : self . __cachedResults = self . ModelResults ( reportMetrics = { } , optimizationMetrics = { } ) return self . __cachedResults",Unwraps self . __rawInfo . results and caches it in self . __cachedResults ; Returns the unwrapped params
"def getData ( self , n ) : records = [ self . getNext ( ) for x in range ( n ) ] return records",Returns the next n values for the distribution as a list .
"def getTerminationCallbacks ( self , terminationFunc ) : activities = [ None ] * len ( ModelTerminator . _MILESTONES ) for index , ( iteration , _ ) in enumerate ( ModelTerminator . _MILESTONES ) : cb = functools . partial ( terminationFunc , index = index ) activities [ index ] = PeriodicActivityRequest ( repeating = False , period = iteration , cb = cb )",Returns the periodic checks to see if the model should continue running .
"def groupby2 ( * args ) : generatorList = [ ] if len ( args ) % 2 == 1 : raise ValueError ( ""Must have a key function for every list."" ) advanceList = [ ] for i in xrange ( 0 , len ( args ) , 2 ) : listn = args [ i ] fn = args [ i + 1 ] if listn is not None : generatorList . append ( groupby ( listn , fn ) ) advanceList . append ( True ) else : generatorList . append ( None ) advanceList . append ( False ) n = len ( generatorList ) nextList = [ None ] * n while True : for i in xrange ( n ) : if advanceList [ i ] : try : nextList [ i ] = generatorList [ i ] . next ( ) except StopIteration : nextList [ i ] = None if all ( entry is None for entry in nextList ) : break minKeyVal = min ( nextVal [ 0 ] for nextVal in nextList if nextVal is not None ) retGroups = [ minKeyVal ] for i in xrange ( n ) : if nextList [ i ] is not None and nextList [ i ] [ 0 ] == minKeyVal : retGroups . append ( nextList [ i ] [ 1 ] ) advanceList [ i ] = True else : advanceList [ i ] = False retGroups . append ( None ) yield tuple ( retGroups )",Like itertools . groupby with the following additions :
"def _openStream ( dataUrl , isBlocking , maxTimeout , bookmark , firstRecordIdx ) : filePath = dataUrl [ len ( FILE_PREF ) : ] if not os . path . isabs ( filePath ) : filePath = os . path . join ( os . getcwd ( ) , filePath ) return FileRecordStream ( streamID = filePath , write = False , bookmark = bookmark , firstRecord = firstRecordIdx )",Open the underlying file stream This only supports file : // prefixed paths .
"def getNextRecord ( self ) : while True : if self . _sourceLastRecordIdx is not None and self . _recordStore . getNextRecordIdx ( ) >= self . _sourceLastRecordIdx : preAggValues = None bookmark = self . _recordStore . getBookmark ( ) else : preAggValues = self . _recordStore . getNextRecord ( ) bookmark = self . _recordStore . getBookmark ( ) if preAggValues == ( ) : if self . _eofOnTimeout : preAggValues = None else : return preAggValues self . _logger . debug ( 'Read source record #%d: %r' , self . _recordStore . getNextRecordIdx ( ) - 1 , preAggValues ) ( fieldValues , aggBookmark ) = self . _aggregator . next ( preAggValues , bookmark ) if fieldValues is not None : self . _aggBookmark = aggBookmark if preAggValues is None and fieldValues is None : return None if fieldValues is not None : break if self . _needFieldsFiltering : values = [ ] srcDict = dict ( zip ( self . _recordStoreFieldNames , fieldValues ) ) for name in self . _streamFieldNames : values . append ( srcDict [ name ] ) fieldValues = values if self . _writer is not None : self . _writer . appendRecord ( fieldValues ) self . _recordCount += 1 self . _logger . debug ( 'Returning aggregated record #%d from getNextRecord(): ' '%r. Bookmark: %r' , self . _recordCount - 1 , fieldValues , self . _aggBookmark ) return fieldValues",Returns combined data from all sources ( values only ) .
def getDataRowCount ( self ) : inputRowCountAfterAggregation = 0 while True : record = self . getNextRecord ( ) if record is None : return inputRowCountAfterAggregation inputRowCountAfterAggregation += 1 if inputRowCountAfterAggregation > 10000 : raise RuntimeError ( 'No end of datastream found.' ),Iterates through stream to calculate total records after aggregation . This will alter the bookmark state .
"def getStats ( self ) : recordStoreStats = self . _recordStore . getStats ( ) streamStats = dict ( ) for ( key , values ) in recordStoreStats . items ( ) : fieldStats = dict ( zip ( self . _recordStoreFieldNames , values ) ) streamValues = [ ] for name in self . _streamFieldNames : streamValues . append ( fieldStats [ name ] ) streamStats [ key ] = streamValues return streamStats",TODO : This method needs to be enhanced to get the stats on the * aggregated * records .
"def get ( self , number ) : if not number in self . _patterns : raise IndexError ( ""Invalid number"" ) return self . _patterns [ number ]",Return a pattern for a number .
"def addNoise ( self , bits , amount ) : newBits = set ( ) for bit in bits : if self . _random . getReal64 ( ) < amount : newBits . add ( self . _random . getUInt32 ( self . _n ) ) else : newBits . add ( bit ) return newBits",Add noise to pattern .
"def numbersForBit ( self , bit ) : if bit >= self . _n : raise IndexError ( ""Invalid bit"" ) numbers = set ( ) for index , pattern in self . _patterns . iteritems ( ) : if bit in pattern : numbers . add ( index ) return numbers",Return the set of pattern numbers that match a bit .
"def numberMapForBits ( self , bits ) : numberMap = dict ( ) for bit in bits : numbers = self . numbersForBit ( bit ) for number in numbers : if not number in numberMap : numberMap [ number ] = set ( ) numberMap [ number ] . add ( bit ) return numberMap",Return a map from number to matching on bits for all numbers that match a set of bits .
"def prettyPrintPattern ( self , bits , verbosity = 1 ) : numberMap = self . numberMapForBits ( bits ) text = """" numberList = [ ] numberItems = sorted ( numberMap . iteritems ( ) , key = lambda ( number , bits ) : len ( bits ) , reverse = True ) for number , bits in numberItems : if verbosity > 2 : strBits = [ str ( n ) for n in bits ] numberText = ""{0} (bits: {1})"" . format ( number , "","" . join ( strBits ) ) elif verbosity > 1 : numberText = ""{0} ({1} bits)"" . format ( number , len ( bits ) ) else : numberText = str ( number ) numberList . append ( numberText ) text += ""[{0}]"" . format ( "", "" . join ( numberList ) ) return text",Pretty print a pattern .
"def _generate ( self ) : candidates = np . array ( range ( self . _n ) , np . uint32 ) for i in xrange ( self . _num ) : self . _random . shuffle ( candidates ) pattern = candidates [ 0 : self . _getW ( ) ] self . _patterns [ i ] = set ( pattern )",Generates set of random patterns .
def _getW ( self ) : w = self . _w if type ( w ) is list : return w [ self . _random . getUInt32 ( len ( w ) ) ] else : return w,Gets a value of w for use in generating a pattern .
"def _generate ( self ) : n = self . _n w = self . _w assert type ( w ) is int , ""List for w not supported"" for i in xrange ( n / w ) : pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) self . _patterns [ i ] = pattern",Generates set of consecutive patterns .
"def compute ( self , recordNum , patternNZ , classification , learn , infer ) : if self . verbosity >= 1 : print ""  learn:"" , learn print ""  recordNum:"" , recordNum print ""  patternNZ (%d):"" % len ( patternNZ ) , patternNZ print ""  classificationIn:"" , classification if len ( self . _patternNZHistory ) > 0 : if recordNum < self . _patternNZHistory [ - 1 ] [ 0 ] : raise ValueError ( ""the record number has to increase monotonically"" ) if len ( self . _patternNZHistory ) == 0 or recordNum > self . _patternNZHistory [ - 1 ] [ 0 ] : self . _patternNZHistory . append ( ( recordNum , patternNZ ) ) retval = { } if max ( patternNZ ) > self . _maxInputIdx : newMaxInputIdx = max ( patternNZ ) for nSteps in self . steps : self . _weightMatrix [ nSteps ] = numpy . concatenate ( ( self . _weightMatrix [ nSteps ] , numpy . zeros ( shape = ( newMaxInputIdx - self . _maxInputIdx , self . _maxBucketIdx + 1 ) ) ) , axis = 0 ) self . _maxInputIdx = int ( newMaxInputIdx ) if classification is not None : if type ( classification [ ""bucketIdx"" ] ) is not list : bucketIdxList = [ classification [ ""bucketIdx"" ] ] actValueList = [ classification [ ""actValue"" ] ] numCategory = 1 else : bucketIdxList = classification [ ""bucketIdx"" ] actValueList = classification [ ""actValue"" ] numCategory = len ( classification [ ""bucketIdx"" ] ) else : if learn : raise ValueError ( ""classification cannot be None when learn=True"" ) actValueList = None bucketIdxList = None if infer : retval = self . infer ( patternNZ , actValueList ) if learn and classification [ ""bucketIdx"" ] is not None : for categoryI in range ( numCategory ) : bucketIdx = bucketIdxList [ categoryI ] actValue = actValueList [ categoryI ] if bucketIdx > self . _maxBucketIdx : for nSteps in self . steps : self . _weightMatrix [ nSteps ] = numpy . concatenate ( ( self . _weightMatrix [ nSteps ] , numpy . zeros ( shape = ( self . _maxInputIdx + 1 , bucketIdx - self . _maxBucketIdx ) ) ) , axis = 1 ) self . _maxBucketIdx = int ( bucketIdx ) while self . _maxBucketIdx > len ( self . _actualValues ) - 1 : self . _actualValues . append ( None ) if self . _actualValues [ bucketIdx ] is None : self . _actualValues [ bucketIdx ] = actValue else : if ( isinstance ( actValue , int ) or isinstance ( actValue , float ) or isinstance ( actValue , long ) ) : self . _actualValues [ bucketIdx ] = ( ( 1.0 - self . actValueAlpha ) * self . _actualValues [ bucketIdx ] + self . actValueAlpha * actValue ) else : self . _actualValues [ bucketIdx ] = actValue for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : error = self . _calculateError ( recordNum , bucketIdxList ) nSteps = recordNum - learnRecordNum if nSteps in self . steps : for bit in learnPatternNZ : self . _weightMatrix [ nSteps ] [ bit , : ] += self . alpha * error [ nSteps ] if infer and self . verbosity >= 1 : print ""  inference: combined bucket likelihoods:"" print ""    actual bucket values:"" , retval [ ""actualValues"" ] for ( nSteps , votes ) in retval . items ( ) : if nSteps == ""actualValues"" : continue print ""    %d steps: "" % ( nSteps ) , _pFormatArray ( votes ) bestBucketIdx = votes . argmax ( ) print ( ""      most likely bucket idx: "" ""%d, value: %s"" % ( bestBucketIdx , retval [ ""actualValues"" ] [ bestBucketIdx ] ) ) print return retval",Process one input sample .
"def infer ( self , patternNZ , actValueList ) : if self . steps [ 0 ] == 0 or actValueList is None : defaultValue = 0 else : defaultValue = actValueList [ 0 ] actValues = [ x if x is not None else defaultValue for x in self . _actualValues ] retval = { ""actualValues"" : actValues } for nSteps in self . steps : predictDist = self . inferSingleStep ( patternNZ , self . _weightMatrix [ nSteps ] ) retval [ nSteps ] = predictDist return retval",Return the inference value from one input sample . The actual learning happens in compute () .
"def inferSingleStep ( self , patternNZ , weightMatrix ) : outputActivation = weightMatrix [ patternNZ ] . sum ( axis = 0 ) outputActivation = outputActivation - numpy . max ( outputActivation ) expOutputActivation = numpy . exp ( outputActivation ) predictDist = expOutputActivation / numpy . sum ( expOutputActivation ) return predictDist",Perform inference for a single step . Given an SDR input and a weight matrix return a predicted distribution .
"def _calculateError ( self , recordNum , bucketIdxList ) : error = dict ( ) targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) numCategories = len ( bucketIdxList ) for bucketIdx in bucketIdxList : targetDist [ bucketIdx ] = 1.0 / numCategories for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : nSteps = recordNum - learnRecordNum if nSteps in self . steps : predictDist = self . inferSingleStep ( learnPatternNZ , self . _weightMatrix [ nSteps ] ) error [ nSteps ] = targetDist - predictDist return error",Calculate error signal
"def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : if fields is not None : assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) with FileRecordStream ( filename ) as f : if fields : fieldNames = [ ff [ 0 ] for ff in fields ] indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] assert len ( indices ) == len ( fields ) else : fileds = f . getFields ( ) fieldNames = f . getFieldNames ( ) indices = None key = [ fieldNames . index ( name ) for name in key ] chunk = 0 records = [ ] for i , r in enumerate ( f ) : if indices : temp = [ ] for i in indices : temp . append ( r [ i ] ) r = temp records . append ( r ) available_memory = psutil . avail_phymem ( ) if available_memory < watermark : _sortChunk ( records , key , chunk , fields ) records = [ ] chunk += 1 if len ( records ) > 0 : _sortChunk ( records , key , chunk , fields ) chunk += 1 _mergeFiles ( key , chunk , outputFile , fields )",Sort a potentially big file
"def _sortChunk ( records , key , chunkIndex , fields ) : title ( additional = '(key=%s, chunkIndex=%d)' % ( str ( key ) , chunkIndex ) ) assert len ( records ) > 0 records . sort ( key = itemgetter ( * key ) ) if chunkIndex is not None : filename = 'chunk_%d.csv' % chunkIndex with FileRecordStream ( filename , write = True , fields = fields ) as o : for r in records : o . appendRecord ( r ) assert os . path . getsize ( filename ) > 0 return records",Sort in memory chunk of records
"def _mergeFiles ( key , chunkCount , outputFile , fields ) : title ( ) files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] with FileRecordStream ( outputFile , write = True , fields = fields ) as o : files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] records = [ f . getNextRecord ( ) for f in files ] while not all ( r is None for r in records ) : indices = [ i for i , r in enumerate ( records ) if r is not None ] records = [ records [ i ] for i in indices ] files = [ files [ i ] for i in indices ] r = min ( records , key = itemgetter ( * key ) ) o . appendRecord ( r ) index = records . index ( r ) records [ index ] = files [ index ] . getNextRecord ( ) for i , f in enumerate ( files ) : f . close ( ) os . remove ( 'chunk_%d.csv' % i )",Merge sorted chunk files into a sorted output file
"def compute ( self , activeColumns , learn = True ) : bottomUpInput = numpy . zeros ( self . numberOfCols , dtype = dtype ) bottomUpInput [ list ( activeColumns ) ] = 1 super ( TemporalMemoryShim , self ) . compute ( bottomUpInput , enableLearn = learn , enableInference = True ) predictedState = self . getPredictedState ( ) self . predictiveCells = set ( numpy . flatnonzero ( predictedState ) )",Feeds input record through TM performing inference and learning . Updates member variables with new state .
"def read ( cls , proto ) : tm = super ( TemporalMemoryShim , cls ) . read ( proto . baseTM ) tm . predictiveCells = set ( proto . predictedState ) tm . connections = Connections . read ( proto . conncetions )",Deserialize from proto instance .
"def write ( self , proto ) : super ( TemporalMemoryShim , self ) . write ( proto . baseTM ) proto . connections . write ( self . connections ) proto . predictiveCells = self . predictiveCells",Populate serialization proto instance .
"def cPrint ( self , level , message , * args , * * kw ) : if level > self . consolePrinterVerbosity : return if len ( kw ) > 1 : raise KeyError ( ""Invalid keywords for cPrint: %s"" % str ( kw . keys ( ) ) ) newline = kw . get ( ""newline"" , True ) if len ( kw ) == 1 and 'newline' not in kw : raise KeyError ( ""Invalid keyword for cPrint: %s"" % kw . keys ( ) [ 0 ] ) if len ( args ) == 0 : if newline : print message else : print message , else : if newline : print message % args else : print message % args ,",Print a message to the console .
"def profileTM ( tmClass , tmDim , nRuns ) : tm = tmClass ( numberOfCols = tmDim ) data = numpy . random . randint ( 0 , 2 , [ tmDim , nRuns ] ) . astype ( 'float32' ) for i in xrange ( nRuns ) : d = data [ : , i ] tm . compute ( d , True )",profiling performance of TemporalMemory ( TM ) using the python cProfile module and ordered by cumulative time see how to run on command - line above .
"def runPermutations ( args ) : helpString = ( ""\n\n%prog [options] permutationsScript\n"" ""%prog [options] expDescription.json\n\n"" ""This script runs permutations of an experiment via Grok engine, as "" ""defined in a\npermutations.py script or an expGenerator experiment "" ""description json file.\nIn the expDescription.json form, the json file "" ""MUST have the file extension\n'.json' and MUST conform to "" ""expGenerator/experimentDescriptionSchema.json."" ) parser = optparse . OptionParser ( usage = helpString ) parser . add_option ( ""--replaceReport"" , dest = ""replaceReport"" , action = ""store_true"" , default = DEFAULT_OPTIONS [ ""replaceReport"" ] , help = ""Replace existing csv report file if it exists. Default is to "" ""append to the existing file. [default: %default]."" ) parser . add_option ( ""--action"" , dest = ""action"" , default = DEFAULT_OPTIONS [ ""action"" ] , choices = [ ""run"" , ""pickup"" , ""report"" , ""dryRun"" ] , help = ""Which action to perform. Possible actions are run, pickup, choices, "" ""report, list. "" ""run: run a new HyperSearch via Grok. "" ""pickup: pick up the latest run of a HyperSearch job. "" ""dryRun: run a single HypersearchWorker inline within the application "" ""process without the Grok infrastructure to flush out bugs in "" ""description and permutations scripts; defaults to "" ""maxPermutations=1: use --maxPermutations to change this; "" ""report: just print results from the last or current run. "" ""[default: %default]."" ) parser . add_option ( ""--maxPermutations"" , dest = ""maxPermutations"" , default = DEFAULT_OPTIONS [ ""maxPermutations"" ] , type = ""int"" , help = ""Maximum number of models to search. Applies only to the 'run' and "" ""'dryRun' actions. [default: %default]."" ) parser . add_option ( ""--exports"" , dest = ""exports"" , default = DEFAULT_OPTIONS [ ""exports"" ] , type = ""string"" , help = ""json dump of environment variable settings that should be applied"" ""for the job before running. [default: %default]."" ) parser . add_option ( ""--useTerminators"" , dest = ""useTerminators"" , action = ""store_true"" , default = DEFAULT_OPTIONS [ ""useTerminators"" ] , help = ""Use early model terminators in HyperSearch"" ""[default: %default]."" ) parser . add_option ( ""--maxWorkers"" , dest = ""maxWorkers"" , default = DEFAULT_OPTIONS [ ""maxWorkers"" ] , type = ""int"" , help = ""Maximum number of concurrent workers to launch. Applies only to "" ""the 'run' action. [default: %default]."" ) parser . add_option ( ""-v"" , dest = ""verbosityCount"" , action = ""count"" , default = 0 , help = ""Increase verbosity of the output.  Specify multiple times for "" ""increased verbosity. e.g., -vv is more verbose than -v."" ) parser . add_option ( ""--timeout"" , dest = ""timeout"" , default = DEFAULT_OPTIONS [ ""timeout"" ] , type = ""int"" , help = ""Time out for this search in minutes"" ""[default: %default]."" ) parser . add_option ( ""--overwrite"" , default = DEFAULT_OPTIONS [ ""overwrite"" ] , action = ""store_true"" , help = ""If 'yes', overwrite existing description.py and permutations.py"" "" (in the same directory as the <expDescription.json> file) if they"" "" already exist. [default: %default]."" ) parser . add_option ( ""--genTopNDescriptions"" , dest = ""genTopNDescriptions"" , default = DEFAULT_OPTIONS [ ""genTopNDescriptions"" ] , type = ""int"" , help = ""Generate description files for the top N models. Each one will be"" "" placed into it's own subdirectory under the base description file."" ""[default: %default]."" ) ( options , positionalArgs ) = parser . parse_args ( args ) if len ( positionalArgs ) != 1 : parser . error ( ""You must supply the name of exactly one permutations script "" ""or JSON description file."" ) fileArgPath = os . path . expanduser ( positionalArgs [ 0 ] ) fileArgPath = os . path . expandvars ( fileArgPath ) fileArgPath = os . path . abspath ( fileArgPath ) permWorkDir = os . path . dirname ( fileArgPath ) outputLabel = os . path . splitext ( os . path . basename ( fileArgPath ) ) [ 0 ] basename = os . path . basename ( fileArgPath ) fileExtension = os . path . splitext ( basename ) [ 1 ] optionsDict = vars ( options ) if fileExtension == "".json"" : returnValue = permutations_runner . runWithJsonFile ( fileArgPath , optionsDict , outputLabel , permWorkDir ) else : returnValue = permutations_runner . runWithPermutationsScript ( fileArgPath , optionsDict , outputLabel , permWorkDir ) return returnValue",The main function of the RunPermutations utility . This utility will automatically generate and run multiple prediction framework experiments that are permutations of a base experiment via the Grok engine . For example if you have an experiment that you want to test with 3 possible values of variable A and 2 possible values of variable B this utility will automatically generate the experiment directories and description files for each of the 6 different experiments .
"def _generateCategory ( filename = ""simple.csv"" , numSequences = 2 , elementsPerSeq = 1 , numRepeats = 10 , resets = False ) : scriptDir = os . path . dirname ( __file__ ) pathname = os . path . join ( scriptDir , 'datasets' , filename ) print ""Creating %s..."" % ( pathname ) fields = [ ( 'reset' , 'int' , 'R' ) , ( 'category' , 'int' , 'C' ) , ( 'field1' , 'string' , '' ) ] outFile = FileRecordStream ( pathname , write = True , fields = fields ) sequences = [ ] for i in range ( numSequences ) : seq = [ x for x in range ( i * elementsPerSeq , ( i + 1 ) * elementsPerSeq ) ] sequences . append ( seq ) seqIdxs = [ ] for i in range ( numRepeats ) : seqIdxs += range ( numSequences ) random . shuffle ( seqIdxs ) for seqIdx in seqIdxs : reset = int ( resets ) seq = sequences [ seqIdx ] for x in seq : outFile . appendRecord ( [ reset , str ( seqIdx ) , str ( x ) ] ) reset = 0 outFile . close ( )",Generate a simple dataset . This contains a bunch of non - overlapping sequences . Parameters : ---------------------------------------------------- filename : name of the file to produce including extension . It will be created in a datasets sub - directory within the directory containing this script . numSequences : how many sequences to generate elementsPerSeq : length of each sequence numRepeats : how many times to repeat each sequence in the output resets : if True turn on reset at start of each sequence
"def encodeIntoArray ( self , inputData , output ) : altitude = None if len ( inputData ) == 4 : ( speed , longitude , latitude , altitude ) = inputData else : ( speed , longitude , latitude ) = inputData coordinate = self . coordinateForPosition ( longitude , latitude , altitude ) radius = self . radiusForSpeed ( speed ) super ( GeospatialCoordinateEncoder , self ) . encodeIntoArray ( ( coordinate , radius ) , output )",See nupic . encoders . base . Encoder for more information .
"def coordinateForPosition ( self , longitude , latitude , altitude = None ) : coords = PROJ ( longitude , latitude ) if altitude is not None : coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) coordinate = numpy . array ( coords ) coordinate = coordinate / self . scale return coordinate . astype ( int )",Returns coordinate for given GPS position .
"def radiusForSpeed ( self , speed ) : overlap = 1.5 coordinatesPerTimestep = speed * self . timestep / self . scale radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) return max ( radius , minRadius )",Returns radius for given speed .
"def getSearch ( rootDir ) : dataPath = os . path . abspath ( os . path . join ( rootDir , 'datasets' , 'scalar_1.csv' ) ) streamDef = dict ( version = 1 , info = ""testSpatialClassification"" , streams = [ dict ( source = ""file://%s"" % ( dataPath ) , info = ""scalar_1.csv"" , columns = [ ""*"" ] , ) , ] , ) expDesc = { ""environment"" : 'nupic' , ""inferenceArgs"" : { ""predictedField"" : ""classification"" , ""predictionSteps"" : [ 0 ] , } , ""inferenceType"" : ""MultiStep"" , ""streamDef"" : streamDef , ""includedFields"" : [ { ""fieldName"" : ""field1"" , ""fieldType"" : ""float"" , } , { ""fieldName"" : ""classification"" , ""fieldType"" : ""string"" , } , { ""fieldName"" : ""randomData"" , ""fieldType"" : ""float"" , } , ] , ""iterationCount"" : - 1 , } return expDesc",This method returns search description . See the following file for the schema of the dictionary this method returns : py / nupic / swarming / exp_generator / experimentDescriptionSchema . json The streamDef element defines the stream for this model . The schema for this element can be found at : py / nupicengine / cluster / database / StreamDef . json
"def encodeIntoArray ( self , value , output ) : denseInput = numpy . zeros ( output . shape ) try : denseInput [ value ] = 1 except IndexError : if isinstance ( value , numpy . ndarray ) : raise ValueError ( ""Numpy array must have integer dtype but got {}"" . format ( value . dtype ) ) raise super ( SparsePassThroughEncoder , self ) . encodeIntoArray ( denseInput , output )",See method description in base . py
"def readFromFile ( cls , f , packed = True ) : schema = cls . getSchema ( ) if packed : proto = schema . read_packed ( f ) else : proto = schema . read ( f ) return cls . read ( proto )",Read serialized object from file .
"def writeToFile ( self , f , packed = True ) : schema = self . getSchema ( ) proto = schema . new_message ( ) self . write ( proto ) if packed : proto . write_packed ( f ) else : proto . write ( f )",Write serialized object to file .
"def read ( cls , proto ) : instance = object . __new__ ( cls ) super ( TwoGramModel , instance ) . __init__ ( proto = proto . modelBase ) instance . _logger = opf_utils . initLogger ( instance ) instance . _reset = proto . reset instance . _hashToValueDict = { x . hash : x . value for x in proto . hashToValueDict } instance . _learningEnabled = proto . learningEnabled instance . _encoder = encoders . MultiEncoder . read ( proto . encoder ) instance . _fieldNames = instance . _encoder . getScalarNames ( ) instance . _prevValues = list ( proto . prevValues ) instance . _twoGramDicts = [ dict ( ) for _ in xrange ( len ( proto . twoGramDicts ) ) ] for idx , field in enumerate ( proto . twoGramDicts ) : for entry in field : prev = None if entry . value == - 1 else entry . value instance . _twoGramDicts [ idx ] [ prev ] = collections . defaultdict ( int ) for bucket in entry . buckets : instance . _twoGramDicts [ idx ] [ prev ] [ bucket . index ] = bucket . count return instance",: param proto : capnp TwoGramModelProto message reader
"def write ( self , proto ) : super ( TwoGramModel , self ) . writeBaseToProto ( proto . modelBase ) proto . reset = self . _reset proto . learningEnabled = self . _learningEnabled proto . prevValues = self . _prevValues self . _encoder . write ( proto . encoder ) proto . hashToValueDict = [ { ""hash"" : h , ""value"" : v } for h , v in self . _hashToValueDict . items ( ) ] twoGramDicts = [ ] for items in self . _twoGramDicts : twoGramArr = [ ] for prev , values in items . iteritems ( ) : buckets = [ { ""index"" : index , ""count"" : count } for index , count in values . iteritems ( ) ] if prev is None : prev = - 1 twoGramArr . append ( { ""value"" : prev , ""buckets"" : buckets } ) twoGramDicts . append ( twoGramArr ) proto . twoGramDicts = twoGramDicts",: param proto : capnp TwoGramModelProto message builder
"def requireAnomalyModel ( func ) : @ wraps ( func ) def _decorator ( self , * args , * * kwargs ) : if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : raise RuntimeError ( ""Method required a TemporalAnomaly model."" ) if self . _getAnomalyClassifier ( ) is None : raise RuntimeError ( ""Model does not support this command. Model must"" ""be an active anomalyDetector model."" ) return func ( self , * args , * * kwargs ) return _decorator",Decorator for functions that require anomaly models .
"def anomalyRemoveLabels ( self , start , end , labelFilter ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter )",Remove labels from the anomaly classifier within this model . Removes all records if labelFilter == None otherwise only removes the labels equal to labelFilter .
"def anomalyAddLabel ( self , start , end , labelName ) : self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName )",Add labels from the anomaly classifier within this model .
"def anomalyGetLabels ( self , start , end ) : return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end )",Get labels from the anomaly classifier within this model .
"def _getSensorInputRecord ( self , inputRecord ) : sensor = self . _getSensorRegion ( ) dataRow = copy . deepcopy ( sensor . getSelf ( ) . getOutputValues ( 'sourceOut' ) ) dataDict = copy . deepcopy ( inputRecord ) inputRecordEncodings = sensor . getSelf ( ) . getOutputValues ( 'sourceEncodings' ) inputRecordCategory = int ( sensor . getOutputData ( 'categoryOut' ) [ 0 ] ) resetOut = sensor . getOutputData ( 'resetOut' ) [ 0 ] return SensorInput ( dataRow = dataRow , dataDict = dataDict , dataEncodings = inputRecordEncodings , sequenceReset = resetOut , category = inputRecordCategory )",inputRecord - dict containing the input to the sensor
"def _getClassifierInputRecord ( self , inputRecord ) : absoluteValue = None bucketIdx = None if self . _predictedFieldName is not None and self . _classifierInputEncoder is not None : absoluteValue = inputRecord [ self . _predictedFieldName ] bucketIdx = self . _classifierInputEncoder . getBucketIndices ( absoluteValue ) [ 0 ] return ClassifierInput ( dataRow = absoluteValue , bucketIndex = bucketIdx )",inputRecord - dict containing the input to the sensor
"def _anomalyCompute ( self ) : inferenceType = self . getInferenceType ( ) inferences = { } sp = self . _getSPRegion ( ) score = None if inferenceType == InferenceType . NontemporalAnomaly : score = sp . getOutputData ( ""anomalyScore"" ) [ 0 ] elif inferenceType == InferenceType . TemporalAnomaly : tm = self . _getTPRegion ( ) if sp is not None : activeColumns = sp . getOutputData ( ""bottomUpOut"" ) . nonzero ( ) [ 0 ] else : sensor = self . _getSensorRegion ( ) activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] if not self . _predictedFieldName in self . _input : raise ValueError ( ""Expected predicted field '%s' in input row, but was not found!"" % self . _predictedFieldName ) score = tm . getOutputData ( ""anomalyScore"" ) [ 0 ] if sp is not None : self . _getAnomalyClassifier ( ) . setParameter ( ""activeColumnCount"" , len ( activeColumns ) ) self . _getAnomalyClassifier ( ) . prepareInputs ( ) self . _getAnomalyClassifier ( ) . compute ( ) labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) inferences [ InferenceElement . anomalyLabel ] = ""%s"" % labels inferences [ InferenceElement . anomalyScore ] = score return inferences",Compute Anomaly score if required
"def _handleSDRClassifierMultiStep ( self , patternNZ , inputTSRecordIdx , rawInput ) : inferenceArgs = self . getInferenceArgs ( ) predictedFieldName = inferenceArgs . get ( 'predictedField' , None ) if predictedFieldName is None : raise ValueError ( ""No predicted field was enabled! Did you call enableInference()?"" ) self . _predictedFieldName = predictedFieldName classifier = self . _getClassifierRegion ( ) if not self . _hasCL or classifier is None : return { } sensor = self . _getSensorRegion ( ) minLikelihoodThreshold = self . _minLikelihoodThreshold maxPredictionsPerStep = self . _maxPredictionsPerStep needLearning = self . isLearningEnabled ( ) inferences = { } if self . _classifierInputEncoder is None : if predictedFieldName is None : raise RuntimeError ( ""This experiment description is missing "" ""the 'predictedField' in its config, which is required "" ""for multi-step prediction inference."" ) encoderList = sensor . getSelf ( ) . encoder . getEncoderList ( ) self . _numFields = len ( encoderList ) fieldNames = sensor . getSelf ( ) . encoder . getScalarNames ( ) if predictedFieldName in fieldNames : self . _predictedFieldIdx = fieldNames . index ( predictedFieldName ) else : self . _predictedFieldIdx = None if sensor . getSelf ( ) . disabledEncoder is not None : encoderList = sensor . getSelf ( ) . disabledEncoder . getEncoderList ( ) else : encoderList = [ ] if len ( encoderList ) >= 1 : fieldNames = sensor . getSelf ( ) . disabledEncoder . getScalarNames ( ) self . _classifierInputEncoder = encoderList [ fieldNames . index ( predictedFieldName ) ] else : encoderList = sensor . getSelf ( ) . encoder . getEncoderList ( ) self . _classifierInputEncoder = encoderList [ self . _predictedFieldIdx ] if not predictedFieldName in rawInput : raise ValueError ( ""Input row does not contain a value for the predicted "" ""field configured for this model. Missing value for '%s'"" % predictedFieldName ) absoluteValue = rawInput [ predictedFieldName ] bucketIdx = self . _classifierInputEncoder . getBucketIndices ( absoluteValue ) [ 0 ] if isinstance ( self . _classifierInputEncoder , DeltaEncoder ) : if not hasattr ( self , ""_ms_prevVal"" ) : self . _ms_prevVal = absoluteValue prevValue = self . _ms_prevVal self . _ms_prevVal = absoluteValue actualValue = absoluteValue - prevValue else : actualValue = absoluteValue if isinstance ( actualValue , float ) and math . isnan ( actualValue ) : actualValue = SENTINEL_VALUE_FOR_MISSING_DATA classifier . setParameter ( 'inferenceMode' , True ) classifier . setParameter ( 'learningMode' , needLearning ) classificationIn = { 'bucketIdx' : bucketIdx , 'actValue' : actualValue } if inputTSRecordIdx is not None : recordNum = inputTSRecordIdx else : recordNum = self . __numRunCalls clResults = classifier . getSelf ( ) . customCompute ( recordNum = recordNum , patternNZ = patternNZ , classification = classificationIn ) predictionSteps = classifier . getParameter ( 'steps' ) predictionSteps = [ int ( x ) for x in predictionSteps . split ( ',' ) ] inferences [ InferenceElement . multiStepPredictions ] = dict ( ) inferences [ InferenceElement . multiStepBestPredictions ] = dict ( ) inferences [ InferenceElement . multiStepBucketLikelihoods ] = dict ( ) for steps in predictionSteps : likelihoodsVec = clResults [ steps ] bucketValues = clResults [ 'actualValues' ] likelihoodsDict = dict ( ) bestActValue = None bestProb = None for ( actValue , prob ) in zip ( bucketValues , likelihoodsVec ) : if actValue in likelihoodsDict : likelihoodsDict [ actValue ] += prob else : likelihoodsDict [ actValue ] = prob if bestProb is None or likelihoodsDict [ actValue ] > bestProb : bestProb = likelihoodsDict [ actValue ] bestActValue = actValue likelihoodsDict = HTMPredictionModel . _removeUnlikelyPredictions ( likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) bucketLikelihood = { } for k in likelihoodsDict . keys ( ) : bucketLikelihood [ self . _classifierInputEncoder . getBucketIndices ( k ) [ 0 ] ] = ( likelihoodsDict [ k ] ) if isinstance ( self . _classifierInputEncoder , DeltaEncoder ) : if not hasattr ( self , '_ms_predHistories' ) : self . _ms_predHistories = dict ( ) predHistories = self . _ms_predHistories if not steps in predHistories : predHistories [ steps ] = deque ( ) predHistory = predHistories [ steps ] sumDelta = sum ( predHistory ) offsetDict = dict ( ) for ( k , v ) in likelihoodsDict . iteritems ( ) : if k is not None : offsetDict [ absoluteValue + float ( k ) + sumDelta ] = v bucketLikelihoodOffset = { } for k in offsetDict . keys ( ) : bucketLikelihoodOffset [ self . _classifierInputEncoder . getBucketIndices ( k ) [ 0 ] ] = ( offsetDict [ k ] ) if bestActValue is not None : predHistory . append ( bestActValue ) if len ( predHistory ) >= steps : predHistory . popleft ( ) if len ( offsetDict ) > 0 : inferences [ InferenceElement . multiStepPredictions ] [ steps ] = offsetDict inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = bucketLikelihoodOffset else : inferences [ InferenceElement . multiStepPredictions ] [ steps ] = likelihoodsDict inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = bucketLikelihood if bestActValue is None : inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = None else : inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = ( absoluteValue + sumDelta + bestActValue ) else : inferences [ InferenceElement . multiStepPredictions ] [ steps ] = ( likelihoodsDict ) inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = ( bestActValue ) inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = ( bucketLikelihood ) return inferences",Handle the CLA Classifier compute logic when implementing multi - step prediction . This is where the patternNZ is associated with one of the other fields from the dataset 0 to N steps in the future . This method is used by each type of network ( encoder only SP only SP + TM ) to handle the compute logic through the CLA Classifier . It fills in the inference dict with the results of the compute .
"def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) : maxVal = ( None , None ) for ( k , v ) in likelihoodsDict . items ( ) : if len ( likelihoodsDict ) <= 1 : break if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : del likelihoodsDict [ maxVal [ 0 ] ] maxVal = ( k , v ) elif v < minLikelihoodThreshold : del likelihoodsDict [ k ] likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : maxPredictionsPerStep ] ) return likelihoodsDict",Remove entries with 0 likelihood or likelihood less than minLikelihoodThreshold but don t leave an empty dict .
"def getRuntimeStats ( self ) : ret = { ""numRunCalls"" : self . __numRunCalls } temporalStats = dict ( ) if self . _hasTP : for stat in self . _netInfo . statsCollectors : sdict = stat . getStats ( ) temporalStats . update ( sdict ) ret [ InferenceType . getLabel ( InferenceType . TemporalNextStep ) ] = temporalStats return ret",Only returns data for a stat called numRunCalls . : return :
"def _getClassifierRegion ( self ) : if ( self . _netInfo . net is not None and ""Classifier"" in self . _netInfo . net . regions ) : return self . _netInfo . net . regions [ ""Classifier"" ] else : return None",Returns reference to the network s Classifier region
"def __createHTMNetwork ( self , sensorParams , spEnable , spParams , tmEnable , tmParams , clEnable , clParams , anomalyParams ) : n = Network ( ) n . addRegion ( ""sensor"" , ""py.RecordSensor"" , json . dumps ( dict ( verbosity = sensorParams [ 'verbosity' ] ) ) ) sensor = n . regions [ 'sensor' ] . getSelf ( ) enabledEncoders = copy . deepcopy ( sensorParams [ 'encoders' ] ) for name , params in enabledEncoders . items ( ) : if params is not None : classifierOnly = params . pop ( 'classifierOnly' , False ) if classifierOnly : enabledEncoders . pop ( name ) disabledEncoders = copy . deepcopy ( sensorParams [ 'encoders' ] ) for name , params in disabledEncoders . items ( ) : if params is None : disabledEncoders . pop ( name ) else : classifierOnly = params . pop ( 'classifierOnly' , False ) if not classifierOnly : disabledEncoders . pop ( name ) encoder = MultiEncoder ( enabledEncoders ) sensor . encoder = encoder sensor . disabledEncoder = MultiEncoder ( disabledEncoders ) sensor . dataSource = DataBuffer ( ) prevRegion = ""sensor"" prevRegionWidth = encoder . getWidth ( ) if spEnable : spParams = spParams . copy ( ) spParams [ 'inputWidth' ] = prevRegionWidth self . __logger . debug ( ""Adding SPRegion; spParams: %r"" % spParams ) n . addRegion ( ""SP"" , ""py.SPRegion"" , json . dumps ( spParams ) ) n . link ( ""sensor"" , ""SP"" , ""UniformLink"" , """" ) n . link ( ""sensor"" , ""SP"" , ""UniformLink"" , """" , srcOutput = ""resetOut"" , destInput = ""resetIn"" ) n . link ( ""SP"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""spatialTopDownOut"" , destInput = ""spatialTopDownIn"" ) n . link ( ""SP"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""temporalTopDownOut"" , destInput = ""temporalTopDownIn"" ) prevRegion = ""SP"" prevRegionWidth = spParams [ 'columnCount' ] if tmEnable : tmParams = tmParams . copy ( ) if prevRegion == 'sensor' : tmParams [ 'inputWidth' ] = tmParams [ 'columnCount' ] = prevRegionWidth else : assert tmParams [ 'columnCount' ] == prevRegionWidth tmParams [ 'inputWidth' ] = tmParams [ 'columnCount' ] self . __logger . debug ( ""Adding TMRegion; tmParams: %r"" % tmParams ) n . addRegion ( ""TM"" , ""py.TMRegion"" , json . dumps ( tmParams ) ) n . link ( prevRegion , ""TM"" , ""UniformLink"" , """" ) if prevRegion != ""sensor"" : n . link ( ""TM"" , prevRegion , ""UniformLink"" , """" , srcOutput = ""topDownOut"" , destInput = ""topDownIn"" ) else : n . link ( ""TM"" , prevRegion , ""UniformLink"" , """" , srcOutput = ""topDownOut"" , destInput = ""temporalTopDownIn"" ) n . link ( ""sensor"" , ""TM"" , ""UniformLink"" , """" , srcOutput = ""resetOut"" , destInput = ""resetIn"" ) prevRegion = ""TM"" prevRegionWidth = tmParams [ 'inputWidth' ] if clEnable and clParams is not None : clParams = clParams . copy ( ) clRegionName = clParams . pop ( 'regionName' ) self . __logger . debug ( ""Adding %s; clParams: %r"" % ( clRegionName , clParams ) ) n . addRegion ( ""Classifier"" , ""py.%s"" % str ( clRegionName ) , json . dumps ( clParams ) ) if str ( clRegionName ) == ""SDRClassifierRegion"" : n . link ( ""sensor"" , ""Classifier"" , ""UniformLink"" , """" , srcOutput = ""actValueOut"" , destInput = ""actValueIn"" ) n . link ( ""sensor"" , ""Classifier"" , ""UniformLink"" , """" , srcOutput = ""bucketIdxOut"" , destInput = ""bucketIdxIn"" ) n . link ( ""sensor"" , ""Classifier"" , ""UniformLink"" , """" , srcOutput = ""categoryOut"" , destInput = ""categoryIn"" ) n . link ( prevRegion , ""Classifier"" , ""UniformLink"" , """" ) if self . getInferenceType ( ) == InferenceType . TemporalAnomaly : anomalyClParams = dict ( trainRecords = anomalyParams . get ( 'autoDetectWaitRecords' , None ) , cacheSize = anomalyParams . get ( 'anomalyCacheRecords' , None ) ) self . _addAnomalyClassifierRegion ( n , anomalyClParams , spEnable , tmEnable ) n . initialize ( ) return NetworkInfo ( net = n , statsCollectors = [ ] )",Create a CLA network and return it .
"def write ( self , proto ) : super ( HTMPredictionModel , self ) . writeBaseToProto ( proto . modelBase ) proto . numRunCalls = self . __numRunCalls proto . minLikelihoodThreshold = self . _minLikelihoodThreshold proto . maxPredictionsPerStep = self . _maxPredictionsPerStep self . _netInfo . net . write ( proto . network ) proto . spLearningEnabled = self . __spLearningEnabled proto . tpLearningEnabled = self . __tpLearningEnabled if self . _predictedFieldIdx is None : proto . predictedFieldIdx . none = None else : proto . predictedFieldIdx . value = self . _predictedFieldIdx if self . _predictedFieldName is None : proto . predictedFieldName . none = None else : proto . predictedFieldName . value = self . _predictedFieldName if self . _numFields is None : proto . numFields . none = None else : proto . numFields . value = self . _numFields proto . trainSPNetOnlyIfRequested = self . __trainSPNetOnlyIfRequested proto . finishedLearning = self . __finishedLearning",: param proto : capnp HTMPredictionModelProto message builder
"def read ( cls , proto ) : obj = object . __new__ ( cls ) super ( HTMPredictionModel , obj ) . __init__ ( proto = proto . modelBase ) obj . _minLikelihoodThreshold = round ( proto . minLikelihoodThreshold , EPSILON_ROUND ) obj . _maxPredictionsPerStep = proto . maxPredictionsPerStep network = Network . read ( proto . network ) obj . _hasSP = ( ""SP"" in network . regions ) obj . _hasTP = ( ""TM"" in network . regions ) obj . _hasCL = ( ""Classifier"" in network . regions ) obj . _netInfo = NetworkInfo ( net = network , statsCollectors = [ ] ) obj . __spLearningEnabled = bool ( proto . spLearningEnabled ) obj . __tpLearningEnabled = bool ( proto . tpLearningEnabled ) obj . __numRunCalls = proto . numRunCalls obj . _classifierInputEncoder = None if proto . predictedFieldIdx . which ( ) == ""none"" : obj . _predictedFieldIdx = None else : obj . _predictedFieldIdx = proto . predictedFieldIdx . value if proto . predictedFieldName . which ( ) == ""none"" : obj . _predictedFieldName = None else : obj . _predictedFieldName = proto . predictedFieldName . value obj . _numFields = proto . numFields if proto . numFields . which ( ) == ""none"" : obj . _numFields = None else : obj . _numFields = proto . numFields . value obj . __trainSPNetOnlyIfRequested = proto . trainSPNetOnlyIfRequested obj . __finishedLearning = proto . finishedLearning obj . _input = None sensor = network . regions [ 'sensor' ] . getSelf ( ) sensor . dataSource = DataBuffer ( ) network . initialize ( ) obj . __logger = initLogger ( obj ) obj . __logger . debug ( ""Instantiating %s."" % obj . __myClassName ) obj . __restoringFromState = False obj . __restoringFromV1 = False return obj",: param proto : capnp HTMPredictionModelProto message reader
"def _serializeExtraData ( self , extraDataDir ) : makeDirectoryFromAbsolutePath ( extraDataDir ) outputDir = self . __getNetworkStateDirectory ( extraDataDir = extraDataDir ) self . __logger . debug ( ""Serializing network..."" ) self . _netInfo . net . save ( outputDir ) self . __logger . debug ( ""Finished serializing network"" ) return",[ virtual method override ] This method is called during serialization with an external directory path that can be used to bypass pickle for saving large binary states .
"def _deSerializeExtraData ( self , extraDataDir ) : assert self . __restoringFromState assert ( self . _netInfo . net is None ) , ""Network was already unpickled"" stateDir = self . __getNetworkStateDirectory ( extraDataDir = extraDataDir ) self . __logger . debug ( ""(%s) De-serializing network..."" , self ) self . _netInfo . net = Network ( stateDir ) self . __logger . debug ( ""(%s) Finished de-serializing network"" , self ) self . _netInfo . net . initialize ( ) if self . getInferenceType ( ) == InferenceType . TemporalAnomaly : classifierType = self . _getAnomalyClassifier ( ) . getSelf ( ) . __class__ . __name__ if classifierType is 'KNNClassifierRegion' : anomalyClParams = dict ( trainRecords = self . _classifier_helper . _autoDetectWaitRecords , cacheSize = self . _classifier_helper . _history_length , ) spEnable = ( self . _getSPRegion ( ) is not None ) tmEnable = True knnRegion = self . _getAnomalyClassifier ( ) . getSelf ( ) self . _addAnomalyClassifierRegion ( self . _netInfo . net , anomalyClParams , spEnable , tmEnable ) self . _getAnomalyClassifier ( ) . getSelf ( ) . _iteration = self . __numRunCalls self . _getAnomalyClassifier ( ) . getSelf ( ) . _recordsCache = ( self . _classifier_helper . saved_states ) self . _getAnomalyClassifier ( ) . getSelf ( ) . saved_categories = ( self . _classifier_helper . saved_categories ) self . _getAnomalyClassifier ( ) . getSelf ( ) . _knnclassifier = knnRegion self . _getTPRegion ( ) . setParameter ( 'anomalyMode' , True ) del self . _classifier_helper self . _netInfo . net . initialize ( ) self . __restoringFromState = False self . __logger . debug ( ""(%s) Finished restoring from state"" , self ) return",[ virtual method override ] This method is called during deserialization ( after __setstate__ ) with an external directory path that can be used to bypass pickle for loading large binary states .
"def _addAnomalyClassifierRegion ( self , network , params , spEnable , tmEnable ) : allParams = copy . deepcopy ( params ) knnParams = dict ( k = 1 , distanceMethod = 'rawOverlap' , distanceNorm = 1 , doBinarization = 1 , replaceDuplicates = 0 , maxStoredPatterns = 1000 ) allParams . update ( knnParams ) if allParams [ 'trainRecords' ] is None : allParams [ 'trainRecords' ] = DEFAULT_ANOMALY_TRAINRECORDS if allParams [ 'cacheSize' ] is None : allParams [ 'cacheSize' ] = DEFAULT_ANOMALY_CACHESIZE if self . _netInfo is not None and self . _netInfo . net is not None and self . _getAnomalyClassifier ( ) is not None : self . _netInfo . net . removeRegion ( 'AnomalyClassifier' ) network . addRegion ( ""AnomalyClassifier"" , ""py.KNNAnomalyClassifierRegion"" , json . dumps ( allParams ) ) if spEnable : network . link ( ""SP"" , ""AnomalyClassifier"" , ""UniformLink"" , """" , srcOutput = ""bottomUpOut"" , destInput = ""spBottomUpOut"" ) else : network . link ( ""sensor"" , ""AnomalyClassifier"" , ""UniformLink"" , """" , srcOutput = ""dataOut"" , destInput = ""spBottomUpOut"" ) if tmEnable : network . link ( ""TM"" , ""AnomalyClassifier"" , ""UniformLink"" , """" , srcOutput = ""topDownOut"" , destInput = ""tpTopDownOut"" ) network . link ( ""TM"" , ""AnomalyClassifier"" , ""UniformLink"" , """" , srcOutput = ""lrnActiveStateT"" , destInput = ""tpLrnActiveStateT"" ) else : raise RuntimeError ( ""TemporalAnomaly models require a TM region."" )",Attaches an AnomalyClassifier region to the network . Will remove current AnomalyClassifier region if it exists .
"def __getNetworkStateDirectory ( self , extraDataDir ) : if self . __restoringFromV1 : if self . getInferenceType ( ) == InferenceType . TemporalNextStep : leafName = 'temporal' + ""-network.nta"" else : leafName = 'nonTemporal' + ""-network.nta"" else : leafName = InferenceType . getLabel ( self . getInferenceType ( ) ) + ""-network.nta"" path = os . path . join ( extraDataDir , leafName ) path = os . path . abspath ( path ) return path",extraDataDir : Model s extra data directory path Returns : Absolute directory path for saving CLA Network
"def __manglePrivateMemberName ( self , privateMemberName , skipCheck = False ) : assert privateMemberName . startswith ( ""__"" ) , ""%r doesn't start with __"" % privateMemberName assert not privateMemberName . startswith ( ""___"" ) , ""%r starts with ___"" % privateMemberName assert not privateMemberName . endswith ( ""__"" ) , ""%r ends with more than one underscore"" % privateMemberName realName = ""_"" + ( self . __myClassName ) . lstrip ( ""_"" ) + privateMemberName if not skipCheck : getattr ( self , realName ) return realName",Mangles the given mangled ( private ) member name ; a mangled member name is one whose name begins with two or more underscores and ends with one or zero underscores .
def _setEncoderParams ( self ) : self . rangeInternal = float ( self . maxval - self . minval ) self . resolution = float ( self . rangeInternal ) / ( self . n - self . w ) self . radius = self . w * self . resolution self . range = self . rangeInternal + self . resolution self . nInternal = self . n - 2 * self . padding self . _bucketValues = None,Set the radius resolution and range . These values are updated when minval and / or maxval change .
"def setFieldStats ( self , fieldName , fieldStats ) : if fieldStats [ fieldName ] [ 'min' ] == None or fieldStats [ fieldName ] [ 'max' ] == None : return self . minval = fieldStats [ fieldName ] [ 'min' ] self . maxval = fieldStats [ fieldName ] [ 'max' ] if self . minval == self . maxval : self . maxval += 1 self . _setEncoderParams ( )",TODO : document
"def _setMinAndMax ( self , input , learn ) : self . slidingWindow . next ( input ) if self . minval is None and self . maxval is None : self . minval = input self . maxval = input + 1 self . _setEncoderParams ( ) elif learn : sorted = self . slidingWindow . getSlidingWindow ( ) sorted . sort ( ) minOverWindow = sorted [ 0 ] maxOverWindow = sorted [ len ( sorted ) - 1 ] if minOverWindow < self . minval : if self . verbosity >= 2 : print ""Input %s=%.2f smaller than minval %.2f. Adjusting minval to %.2f"" % ( self . name , input , self . minval , minOverWindow ) self . minval = minOverWindow self . _setEncoderParams ( ) if maxOverWindow > self . maxval : if self . verbosity >= 2 : print ""Input %s=%.2f greater than maxval %.2f. Adjusting maxval to %.2f"" % ( self . name , input , self . maxval , maxOverWindow ) self . maxval = maxOverWindow self . _setEncoderParams ( )",Potentially change the minval and maxval using input . ** The learn flag is currently not supported by cla regions . **
"def getBucketIndices ( self , input , learn = None ) : self . recordNum += 1 if learn is None : learn = self . _learningEnabled if type ( input ) is float and math . isnan ( input ) : input = SENTINEL_VALUE_FOR_MISSING_DATA if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : self . _setMinAndMax ( input , learn ) return super ( AdaptiveScalarEncoder , self ) . getBucketIndices ( input )",[ overrides nupic . encoders . scalar . ScalarEncoder . getBucketIndices ]
"def encodeIntoArray ( self , input , output , learn = None ) : self . recordNum += 1 if learn is None : learn = self . _learningEnabled if input == SENTINEL_VALUE_FOR_MISSING_DATA : output [ 0 : self . n ] = 0 elif not math . isnan ( input ) : self . _setMinAndMax ( input , learn ) super ( AdaptiveScalarEncoder , self ) . encodeIntoArray ( input , output )",[ overrides nupic . encoders . scalar . ScalarEncoder . encodeIntoArray ]
"def getBucketInfo ( self , buckets ) : if self . minval is None or self . maxval is None : return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ] return super ( AdaptiveScalarEncoder , self ) . getBucketInfo ( buckets )",[ overrides nupic . encoders . scalar . ScalarEncoder . getBucketInfo ]
"def topDownCompute ( self , encoded ) : if self . minval is None or self . maxval is None : return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ] return super ( AdaptiveScalarEncoder , self ) . topDownCompute ( encoded )",[ overrides nupic . encoders . scalar . ScalarEncoder . topDownCompute ]
"def recordDataPoint ( self , swarmId , generation , errScore ) : terminatedSwarms = [ ] if swarmId in self . swarmScores : entry = self . swarmScores [ swarmId ] assert ( len ( entry ) == generation ) entry . append ( errScore ) entry = self . swarmBests [ swarmId ] entry . append ( min ( errScore , entry [ - 1 ] ) ) assert ( len ( self . swarmBests [ swarmId ] ) == len ( self . swarmScores [ swarmId ] ) ) else : assert ( generation == 0 ) self . swarmScores [ swarmId ] = [ errScore ] self . swarmBests [ swarmId ] = [ errScore ] if generation + 1 < self . MATURITY_WINDOW : return terminatedSwarms if self . MAX_GENERATIONS is not None and generation > self . MAX_GENERATIONS : self . _logger . info ( 'Swarm %s has matured (more than %d generations). Stopping' % ( swarmId , self . MAX_GENERATIONS ) ) terminatedSwarms . append ( swarmId ) if self . _isTerminationEnabled : terminatedSwarms . extend ( self . _getTerminatedSwarms ( generation ) ) cumulativeBestScores = self . swarmBests [ swarmId ] if cumulativeBestScores [ - 1 ] == cumulativeBestScores [ - self . MATURITY_WINDOW ] : self . _logger . info ( 'Swarm %s has matured (no change in %d generations).' 'Stopping...' % ( swarmId , self . MATURITY_WINDOW ) ) terminatedSwarms . append ( swarmId ) self . terminatedSwarms = self . terminatedSwarms . union ( terminatedSwarms ) return terminatedSwarms",Record the best score for a swarm s generation index ( x ) Returns list of swarmIds to terminate .
"def getState ( self ) : return dict ( _position = self . _position , position = self . getPosition ( ) , velocity = self . _velocity , bestPosition = self . _bestPosition , bestResult = self . _bestResult )",See comments in base class .
"def setState ( self , state ) : self . _position = state [ '_position' ] self . _velocity = state [ 'velocity' ] self . _bestPosition = state [ 'bestPosition' ] self . _bestResult = state [ 'bestResult' ]",See comments in base class .
"def getPosition ( self ) : if self . stepSize is None : return self . _position numSteps = ( self . _position - self . min ) / self . stepSize numSteps = int ( round ( numSteps ) ) position = self . min + ( numSteps * self . stepSize ) position = max ( self . min , position ) position = min ( self . max , position ) return position",See comments in base class .
def agitate ( self ) : self . _velocity *= 1.5 / self . _inertia maxV = ( self . max - self . min ) / 2 if self . _velocity > maxV : self . _velocity = maxV elif self . _velocity < - maxV : self . _velocity = - maxV if self . _position == self . max and self . _velocity > 0 : self . _velocity *= - 1 if self . _position == self . min and self . _velocity < 0 : self . _velocity *= - 1,See comments in base class .
"def newPosition ( self , globalBestPosition , rng ) : lb = float ( Configuration . get ( ""nupic.hypersearch.randomLowerBound"" ) ) ub = float ( Configuration . get ( ""nupic.hypersearch.randomUpperBound"" ) ) self . _velocity = ( self . _velocity * self . _inertia + rng . uniform ( lb , ub ) * self . _cogRate * ( self . _bestPosition - self . getPosition ( ) ) ) if globalBestPosition is not None : self . _velocity += rng . uniform ( lb , ub ) * self . _socRate * ( globalBestPosition - self . getPosition ( ) ) self . _position += self . _velocity self . _position = max ( self . min , self . _position ) self . _position = min ( self . max , self . _position ) return self . getPosition ( )",See comments in base class .
"def pushAwayFrom ( self , otherPositions , rng ) : if self . max == self . min : return numPositions = len ( otherPositions ) * 4 if numPositions == 0 : return stepSize = float ( self . max - self . min ) / numPositions positions = numpy . arange ( self . min , self . max + stepSize , stepSize ) numPositions = len ( positions ) weights = numpy . zeros ( numPositions ) maxDistanceSq = - 1 * ( stepSize ** 2 ) for pos in otherPositions : distances = pos - positions varWeights = numpy . exp ( numpy . power ( distances , 2 ) / maxDistanceSq ) weights += varWeights positionIdx = weights . argmin ( ) self . _position = positions [ positionIdx ] self . _bestPosition = self . getPosition ( ) self . _velocity *= rng . choice ( [ 1 , - 1 ] )",See comments in base class .
"def resetVelocity ( self , rng ) : maxVelocity = ( self . max - self . min ) / 5.0 self . _velocity = maxVelocity self . _velocity *= rng . choice ( [ 1 , - 1 ] )",See comments in base class .
"def getPosition ( self ) : position = super ( PermuteInt , self ) . getPosition ( ) position = int ( round ( position ) ) return position",See comments in base class .
"def getState ( self ) : return dict ( _position = self . getPosition ( ) , position = self . getPosition ( ) , velocity = None , bestPosition = self . choices [ self . _bestPositionIdx ] , bestResult = self . _bestResult )",See comments in base class .
"def setState ( self , state ) : self . _positionIdx = self . choices . index ( state [ '_position' ] ) self . _bestPositionIdx = self . choices . index ( state [ 'bestPosition' ] ) self . _bestResult = state [ 'bestResult' ]",See comments in base class .
"def setResultsPerChoice ( self , resultsPerChoice ) : self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) for ( choiceValue , values ) in resultsPerChoice : choiceIndex = self . choices . index ( choiceValue ) self . _resultsPerChoice [ choiceIndex ] = list ( values )",Setup our resultsPerChoice history based on the passed in resultsPerChoice .
"def newPosition ( self , globalBestPosition , rng ) : numChoices = len ( self . choices ) meanScorePerChoice = [ ] overallSum = 0 numResults = 0 for i in range ( numChoices ) : if len ( self . _resultsPerChoice [ i ] ) > 0 : data = numpy . array ( self . _resultsPerChoice [ i ] ) meanScorePerChoice . append ( data . mean ( ) ) overallSum += data . sum ( ) numResults += data . size else : meanScorePerChoice . append ( None ) if numResults == 0 : overallSum = 1.0 numResults = 1 for i in range ( numChoices ) : if meanScorePerChoice [ i ] is None : meanScorePerChoice [ i ] = overallSum / numResults meanScorePerChoice = numpy . array ( meanScorePerChoice ) meanScorePerChoice = ( 1.1 * meanScorePerChoice . max ( ) ) - meanScorePerChoice if self . _fixEarly : meanScorePerChoice **= ( numResults * self . _fixEarlyFactor / numChoices ) total = meanScorePerChoice . sum ( ) if total == 0 : total = 1.0 meanScorePerChoice /= total distribution = meanScorePerChoice . cumsum ( ) r = rng . random ( ) * distribution [ - 1 ] choiceIdx = numpy . where ( r <= distribution ) [ 0 ] [ 0 ] self . _positionIdx = choiceIdx return self . getPosition ( )",See comments in base class .
"def pushAwayFrom ( self , otherPositions , rng ) : positions = [ self . choices . index ( x ) for x in otherPositions ] positionCounts = [ 0 ] * len ( self . choices ) for pos in positions : positionCounts [ pos ] += 1 self . _positionIdx = numpy . array ( positionCounts ) . argmin ( ) self . _bestPositionIdx = self . _positionIdx",See comments in base class .
"def getDict ( self , encoderName , flattenedChosenValues ) : encoder = dict ( fieldname = self . fieldName , name = self . name ) for encoderArg , value in self . kwArgs . iteritems ( ) : if isinstance ( value , PermuteVariable ) : value = flattenedChosenValues [ ""%s:%s"" % ( encoderName , encoderArg ) ] encoder [ encoderArg ] = value if '.' in self . encoderClass : ( encoder [ 'type' ] , argName ) = self . encoderClass . split ( '.' ) argValue = ( encoder [ 'w' ] , encoder [ 'radius' ] ) encoder [ argName ] = argValue encoder . pop ( 'w' ) encoder . pop ( 'radius' ) else : encoder [ 'type' ] = self . encoderClass return encoder",Return a dict that can be used to construct this encoder . This dict can be passed directly to the addMultipleEncoders () method of the multi encoder .
"def _translateMetricsToJSON ( self , metrics , label ) : metricsDict = metrics def _mapNumpyValues ( obj ) : """"""
      """""" import numpy if isinstance ( obj , numpy . float32 ) : return float ( obj ) elif isinstance ( obj , numpy . bool_ ) : return bool ( obj ) elif isinstance ( obj , numpy . ndarray ) : return obj . tolist ( ) else : raise TypeError ( ""UNEXPECTED OBJ: %s; class=%s"" % ( obj , obj . __class__ ) ) jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) return jsonString",Translates the given metrics value to JSON string
"def __openDatafile ( self , modelResult ) : resetFieldMeta = FieldMetaInfo ( name = ""reset"" , type = FieldMetaType . integer , special = FieldMetaSpecial . reset ) self . __outputFieldsMeta . append ( resetFieldMeta ) rawInput = modelResult . rawInput rawFields = rawInput . keys ( ) rawFields . sort ( ) for field in rawFields : if field . startswith ( '_' ) or field == 'reset' : continue value = rawInput [ field ] meta = FieldMetaInfo ( name = field , type = FieldMetaType . string , special = FieldMetaSpecial . none ) self . __outputFieldsMeta . append ( meta ) self . _rawInputNames . append ( field ) for inferenceElement , value in modelResult . inferences . iteritems ( ) : inferenceLabel = InferenceElement . getLabel ( inferenceElement ) if type ( value ) in ( list , tuple ) : self . __outputFieldsMeta . extend ( self . __getListMetaInfo ( inferenceElement ) ) elif isinstance ( value , dict ) : self . __outputFieldsMeta . extend ( self . __getDictMetaInfo ( inferenceElement , value ) ) else : if InferenceElement . getInputElement ( inferenceElement ) : self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel + "".actual"" , type = FieldMetaType . string , special = '' ) ) self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel , type = FieldMetaType . string , special = '' ) ) if self . __metricNames : for metricName in self . __metricNames : metricField = FieldMetaInfo ( name = metricName , type = FieldMetaType . float , special = FieldMetaSpecial . none ) self . __outputFieldsMeta . append ( metricField ) inferenceDir = _FileUtils . createExperimentInferenceDir ( self . __experimentDir ) filename = ( self . __label + ""."" + opf_utils . InferenceType . getLabel ( self . __inferenceType ) + "".predictionLog.csv"" ) self . __datasetPath = os . path . join ( inferenceDir , filename ) print ""OPENING OUTPUT FOR PREDICTION WRITER AT: %r"" % self . __datasetPath print ""Prediction field-meta: %r"" % ( [ tuple ( i ) for i in self . __outputFieldsMeta ] , ) self . __dataset = FileRecordStream ( streamID = self . __datasetPath , write = True , fields = self . __outputFieldsMeta ) if self . __checkpointCache is not None : self . __checkpointCache . seek ( 0 ) reader = csv . reader ( self . __checkpointCache , dialect = 'excel' ) try : header = reader . next ( ) except StopIteration : print ""Empty record checkpoint initializer for %r"" % ( self . __datasetPath , ) else : assert tuple ( self . __dataset . getFieldNames ( ) ) == tuple ( header ) , ""dataset.getFieldNames(): %r; predictionCheckpointFieldNames: %r"" % ( tuple ( self . __dataset . getFieldNames ( ) ) , tuple ( header ) ) numRowsCopied = 0 while True : try : row = reader . next ( ) except StopIteration : break self . __dataset . appendRecord ( row ) numRowsCopied += 1 self . __dataset . flush ( ) print ""Restored %d rows from checkpoint for %r"" % ( numRowsCopied , self . __datasetPath ) self . __checkpointCache . close ( ) self . __checkpointCache = None return",Open the data file and write the header row
"def setLoggedMetrics ( self , metricNames ) : if metricNames is None : self . __metricNames = set ( [ ] ) else : self . __metricNames = set ( metricNames )",Tell the writer which metrics should be written
"def __getListMetaInfo ( self , inferenceElement ) : fieldMetaInfo = [ ] inferenceLabel = InferenceElement . getLabel ( inferenceElement ) for inputFieldMeta in self . __inputFieldsMeta : if InferenceElement . getInputElement ( inferenceElement ) : outputFieldMeta = FieldMetaInfo ( name = inputFieldMeta . name + "".actual"" , type = inputFieldMeta . type , special = inputFieldMeta . special ) predictionField = FieldMetaInfo ( name = inputFieldMeta . name + ""."" + inferenceLabel , type = inputFieldMeta . type , special = inputFieldMeta . special ) fieldMetaInfo . append ( outputFieldMeta ) fieldMetaInfo . append ( predictionField ) return fieldMetaInfo",Get field metadata information for inferences that are of list type TODO : Right now we assume list inferences are associated with the input field metadata
"def __getDictMetaInfo ( self , inferenceElement , inferenceDict ) : fieldMetaInfo = [ ] inferenceLabel = InferenceElement . getLabel ( inferenceElement ) if InferenceElement . getInputElement ( inferenceElement ) : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + "".actual"" , type = FieldMetaType . string , special = '' ) ) keys = sorted ( inferenceDict . keys ( ) ) for key in keys : fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + ""."" + str ( key ) , type = FieldMetaType . string , special = '' ) ) return fieldMetaInfo",Get field metadate information for inferences that are of dict type
"def append ( self , modelResult ) : inferences = modelResult . inferences hasInferences = False if inferences is not None : for value in inferences . itervalues ( ) : hasInferences = hasInferences or ( value is not None ) if not hasInferences : return if self . __dataset is None : self . __openDatafile ( modelResult ) inputData = modelResult . sensorInput sequenceReset = int ( bool ( inputData . sequenceReset ) ) outputRow = [ sequenceReset ] rawInput = modelResult . rawInput for field in self . _rawInputNames : outputRow . append ( str ( rawInput [ field ] ) ) for inferenceElement , outputVal in inferences . iteritems ( ) : inputElement = InferenceElement . getInputElement ( inferenceElement ) if inputElement : inputVal = getattr ( inputData , inputElement ) else : inputVal = None if type ( outputVal ) in ( list , tuple ) : assert type ( inputVal ) in ( list , tuple , None ) for iv , ov in zip ( inputVal , outputVal ) : outputRow . append ( str ( iv ) ) outputRow . append ( str ( ov ) ) elif isinstance ( outputVal , dict ) : if inputVal is not None : if modelResult . predictedFieldName is not None : outputRow . append ( str ( inputVal [ modelResult . predictedFieldName ] ) ) else : outputRow . append ( str ( inputVal ) ) for key in sorted ( outputVal . keys ( ) ) : outputRow . append ( str ( outputVal [ key ] ) ) else : if inputVal is not None : outputRow . append ( str ( inputVal ) ) outputRow . append ( str ( outputVal ) ) metrics = modelResult . metrics for metricName in self . __metricNames : outputRow . append ( metrics . get ( metricName , 0.0 ) ) self . __dataset . appendRecord ( outputRow ) self . __dataset . flush ( ) return",[ virtual method override ] Emits a single prediction as input versus predicted .
"def checkpoint ( self , checkpointSink , maxRows ) : checkpointSink . truncate ( ) if self . __dataset is None : if self . __checkpointCache is not None : self . __checkpointCache . seek ( 0 ) shutil . copyfileobj ( self . __checkpointCache , checkpointSink ) checkpointSink . flush ( ) return else : return self . __dataset . flush ( ) totalDataRows = self . __dataset . getDataRowCount ( ) if totalDataRows == 0 : return reader = FileRecordStream ( self . __datasetPath , missingValues = [ ] ) writer = csv . writer ( checkpointSink ) writer . writerow ( reader . getFieldNames ( ) ) numToWrite = min ( maxRows , totalDataRows ) numRowsToSkip = totalDataRows - numToWrite for i in xrange ( numRowsToSkip ) : reader . next ( ) numWritten = 0 while True : row = reader . getNextRecord ( ) if row is None : break row = [ str ( element ) for element in row ] writer . writerow ( row ) numWritten += 1 assert numWritten == numToWrite , ""numWritten (%s) != numToWrite (%s)"" % ( numWritten , numToWrite ) checkpointSink . flush ( ) return",[ virtual method override ] Save a checkpoint of the prediction output stream . The checkpoint comprises up to maxRows of the most recent inference records .
"def update ( self , modelResult ) : self . __writer . append ( self . __inferenceShifter . shift ( modelResult ) )",Queue up the T ( i + 1 ) prediction value and emit a T ( i ) input / prediction pair if possible . E . g . if the previous T ( i - 1 ) iteration was learn - only then we would not have a T ( i ) prediction in our FIFO and would not be able to emit a meaningful input / prediction pair .
"def createExperimentInferenceDir ( cls , experimentDir ) : path = cls . getExperimentInferenceDirPath ( experimentDir ) cls . makeDirectory ( path ) return path",Creates the inference output directory for the given experiment
"def _generateModel0 ( numCategories ) : initProb = numpy . zeros ( numCategories ) initProb [ 0 ] = 0.5 initProb [ 4 ] = 0.5 firstOrder = dict ( ) for catIdx in range ( numCategories ) : key = str ( [ catIdx ] ) probs = numpy . ones ( numCategories ) / numCategories if catIdx == 0 or catIdx == 4 : probs . fill ( 0 ) probs [ 1 ] = 1.0 firstOrder [ key ] = probs secondOrder = dict ( ) for firstIdx in range ( numCategories ) : for secondIdx in range ( numCategories ) : key = str ( [ firstIdx , secondIdx ] ) probs = numpy . ones ( numCategories ) / numCategories if key == str ( [ 0 , 1 ] ) : probs . fill ( 0 ) probs [ 2 ] = 0.80 probs [ 3 ] = 0.20 elif key == str ( [ 4 , 1 ] ) : probs . fill ( 0 ) probs [ 2 ] = 0.20 probs [ 3 ] = 0.80 secondOrder [ key ] = probs return ( initProb , firstOrder , secondOrder , 3 )",Generate the initial first order and second order transition probabilities for model0 . For this model we generate the following set of sequences : 1 - 2 - 3 ( 4X ) 1 - 2 - 4 ( 1X ) 5 - 2 - 3 ( 1X ) 5 - 2 - 4 ( 4X ) Parameters : ---------------------------------------------------------------------- numCategories : Number of categories retval : ( initProb firstOrder secondOrder seqLen ) initProb : Initial probability for each category . This is a vector of length len ( categoryList ) . firstOrder : A dictionary of the 1st order probabilities . The key is the 1st element of the sequence the value is the probability of each 2nd element given the first . secondOrder : A dictionary of the 2nd order probabilities . The key is the first 2 elements of the sequence the value is the probability of each possible 3rd element given the first two . seqLen : Desired length of each sequence . The 1st element will be generated using the initProb the 2nd element by the firstOrder table and the 3rd and all successive elements by the secondOrder table .
"def _generateModel1 ( numCategories ) : initProb = numpy . zeros ( numCategories ) initProb [ 0 ] = 0.5 initProb [ 1 ] = 0.5 firstOrder = dict ( ) for catIdx in range ( numCategories ) : key = str ( [ catIdx ] ) probs = numpy . ones ( numCategories ) / numCategories if catIdx == 0 or catIdx == 1 : indices = numpy . array ( [ 10 , 11 , 12 , 13 , 14 ] ) probs . fill ( 0 ) probs [ indices ] = 1.0 probs /= probs . sum ( ) firstOrder [ key ] = probs secondOrder = dict ( ) for firstIdx in range ( numCategories ) : for secondIdx in range ( numCategories ) : key = str ( [ firstIdx , secondIdx ] ) probs = numpy . ones ( numCategories ) / numCategories if key == str ( [ 0 , 10 ] ) : probs . fill ( 0 ) probs [ 15 ] = 1 elif key == str ( [ 0 , 11 ] ) : probs . fill ( 0 ) probs [ 16 ] = 1 elif key == str ( [ 0 , 12 ] ) : probs . fill ( 0 ) probs [ 17 ] = 1 elif key == str ( [ 0 , 13 ] ) : probs . fill ( 0 ) probs [ 18 ] = 1 elif key == str ( [ 0 , 14 ] ) : probs . fill ( 0 ) probs [ 19 ] = 1 elif key == str ( [ 1 , 10 ] ) : probs . fill ( 0 ) probs [ 20 ] = 1 elif key == str ( [ 1 , 11 ] ) : probs . fill ( 0 ) probs [ 21 ] = 1 elif key == str ( [ 1 , 12 ] ) : probs . fill ( 0 ) probs [ 22 ] = 1 elif key == str ( [ 1 , 13 ] ) : probs . fill ( 0 ) probs [ 23 ] = 1 elif key == str ( [ 1 , 14 ] ) : probs . fill ( 0 ) probs [ 24 ] = 1 secondOrder [ key ] = probs return ( initProb , firstOrder , secondOrder , 3 )",Generate the initial first order and second order transition probabilities for model1 . For this model we generate the following set of sequences : 0 - 10 - 15 ( 1X ) 0 - 11 - 16 ( 1X ) 0 - 12 - 17 ( 1X ) 0 - 13 - 18 ( 1X ) 0 - 14 - 19 ( 1X )
"def _generateModel2 ( numCategories , alpha = 0.25 ) : initProb = numpy . ones ( numCategories ) / numCategories def generatePeakedProbabilities ( lastIdx , numCategories = numCategories , alpha = alpha ) : probs = numpy . random . dirichlet ( alpha = [ alpha ] * numCategories ) probs [ lastIdx ] = 0.0 probs /= probs . sum ( ) return probs firstOrder = dict ( ) for catIdx in range ( numCategories ) : key = str ( [ catIdx ] ) probs = generatePeakedProbabilities ( catIdx ) firstOrder [ key ] = probs secondOrder = dict ( ) for firstIdx in range ( numCategories ) : for secondIdx in range ( numCategories ) : key = str ( [ firstIdx , secondIdx ] ) probs = generatePeakedProbabilities ( secondIdx ) secondOrder [ key ] = probs return ( initProb , firstOrder , secondOrder , None )",Generate the initial first order and second order transition probabilities for model2 . For this model we generate peaked random transitions using dirichlet distributions . Parameters : ---------------------------------------------------------------------- numCategories : Number of categories alpha : Determines the peakedness of the transitions . Low alpha values ( alpha = 0 . 01 ) place the entire weight on a single transition . Large alpha values ( alpha = 10 ) distribute the evenly among all transitions . Intermediate values ( alpha = 0 . 5 ) give a moderately peaked transitions . retval : ( initProb firstOrder secondOrder seqLen ) initProb : Initial probability for each category . This is a vector of length len ( categoryList ) . firstOrder : A dictionary of the 1st order probabilities . The key is the 1st element of the sequence the value is the probability of each 2nd element given the first . secondOrder : A dictionary of the 2nd order probabilities . The key is the first 2 elements of the sequence the value is the probability of each possible 3rd element given the first two . seqLen : Desired length of each sequence . The 1st element will be generated using the initProb the 2nd element by the firstOrder table and the 3rd and all successive elements by the secondOrder table . None means infinite length .
"def _generateFile ( filename , numRecords , categoryList , initProb , firstOrderProb , secondOrderProb , seqLen , numNoise = 0 , resetsEvery = None ) : print ""Creating %s..."" % ( filename ) fields = [ ( 'reset' , 'int' , 'R' ) , ( 'name' , 'string' , '' ) ] outFile = FileRecordStream ( filename , write = True , fields = fields ) initCumProb = initProb . cumsum ( ) firstOrderCumProb = dict ( ) for ( key , value ) in firstOrderProb . iteritems ( ) : firstOrderCumProb [ key ] = value . cumsum ( ) secondOrderCumProb = dict ( ) for ( key , value ) in secondOrderProb . iteritems ( ) : secondOrderCumProb [ key ] = value . cumsum ( ) elementsInSeq = [ ] numElementsSinceReset = 0 maxCatIdx = len ( categoryList ) - 1 for i in xrange ( numRecords ) : if numElementsSinceReset == 0 : reset = 1 else : reset = 0 rand = numpy . random . rand ( ) if len ( elementsInSeq ) == 0 : catIdx = numpy . searchsorted ( initCumProb , rand ) elif len ( elementsInSeq ) == 1 : catIdx = numpy . searchsorted ( firstOrderCumProb [ str ( elementsInSeq ) ] , rand ) elif ( len ( elementsInSeq ) >= 2 ) and ( seqLen is None or len ( elementsInSeq ) < seqLen - numNoise ) : catIdx = numpy . searchsorted ( secondOrderCumProb [ str ( elementsInSeq [ - 2 : ] ) ] , rand ) else : catIdx = numpy . random . randint ( len ( categoryList ) ) catIdx = min ( maxCatIdx , catIdx ) outFile . appendRecord ( [ reset , categoryList [ catIdx ] ] ) elementsInSeq . append ( catIdx ) numElementsSinceReset += 1 if resetsEvery is not None and numElementsSinceReset == resetsEvery : numElementsSinceReset = 0 elementsInSeq = [ ] if seqLen is not None and ( len ( elementsInSeq ) == seqLen + numNoise ) : elementsInSeq = [ ] outFile . close ( )",Generate a set of records reflecting a set of probabilities . Parameters : ---------------------------------------------------------------- filename : name of . csv file to generate numRecords : number of records to generate categoryList : list of category names initProb : Initial probability for each category . This is a vector of length len ( categoryList ) . firstOrderProb : A dictionary of the 1st order probabilities . The key is the 1st element of the sequence the value is the probability of each 2nd element given the first . secondOrderProb : A dictionary of the 2nd order probabilities . The key is the first 2 elements of the sequence the value is the probability of each possible 3rd element given the first two . seqLen : Desired length of each sequence . The 1st element will be generated using the initProb the 2nd element by the firstOrder table and the 3rd and all successive elements by the secondOrder table . None means infinite length . numNoise : Number of noise elements to place between each sequence . The noise elements are evenly distributed from all categories . resetsEvery : If not None generate a reset every N records Here is an example of some parameters : categoryList : [ cat1 cat2 cat3 ] initProb : [ 0 . 7 0 . 2 0 . 1 ] firstOrderProb : { [ 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 ] : [ 0 . 3 0 . 3 0 . 4 ] } secondOrderProb : { [ 0 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 0 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 0 2 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 1 2 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 0 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 1 ] : [ 0 . 3 0 . 3 0 . 4 ] [ 2 2 ] : [ 0 . 3 0 . 3 0 . 4 ] }
"def _allow_new_attributes ( f ) : def decorated ( self , * args , * * kw ) : """"""The decorated function that replaces __init__() or __setstate__()

    """""" if not hasattr ( self , '_canAddAttributes' ) : self . __dict__ [ '_canAddAttributes' ] = 1 else : self . _canAddAttributes += 1 assert self . _canAddAttributes >= 1 count = self . _canAddAttributes f ( self , * args , * * kw ) if hasattr ( self , '_canAddAttributes' ) : self . _canAddAttributes -= 1 else : self . _canAddAttributes = count - 1 assert self . _canAddAttributes >= 0 if self . _canAddAttributes == 0 : del self . _canAddAttributes decorated . __doc__ = f . __doc__ decorated . __name__ = f . __name__ return decorated",A decorator that maintains the attribute lock state of an object
"def _simple_init ( self , * args , * * kw ) : type ( self ) . __base__ . __init__ ( self , * args , * * kw )",trivial init method that just calls base class s __init__ ()
"def generatePlot ( outputs , origData ) : PLOT_PRECISION = 100 distribMatrix = np . zeros ( ( PLOT_PRECISION + 1 , PLOT_PRECISION + 1 ) ) outputSize = len ( outputs ) for i in range ( 0 , outputSize ) : for j in range ( i + 1 , outputSize ) : in1 = outputs [ i ] in2 = outputs [ j ] dist = ( abs ( in1 - in2 ) > 0.1 ) intDist = int ( dist . sum ( ) / 2 + 0.1 ) orig1 = origData [ i ] orig2 = origData [ j ] origDist = ( abs ( orig1 - orig2 ) > 0.1 ) intOrigDist = int ( origDist . sum ( ) / 2 + 0.1 ) if intDist < 2 and intOrigDist > 10 : print 'Elements %d,%d has very small SP distance: %d' % ( i , j , intDist ) print 'Input elements distance is %d' % intOrigDist x = int ( PLOT_PRECISION * intDist / 40.0 ) y = int ( PLOT_PRECISION * intOrigDist / 42.0 ) if distribMatrix [ x , y ] < 0.1 : distribMatrix [ x , y ] = 3 else : if distribMatrix [ x , y ] < 10 : distribMatrix [ x , y ] += 1 distribMatrix [ 4 , 50 ] = 3 distribMatrix [ 4 , 52 ] = 4 distribMatrix [ 4 , 54 ] = 5 distribMatrix [ 4 , 56 ] = 6 distribMatrix [ 4 , 58 ] = 7 distribMatrix [ 4 , 60 ] = 8 distribMatrix [ 4 , 62 ] = 9 distribMatrix [ 4 , 64 ] = 10 return distribMatrix",Generates a table where each cell represent a frequency of pairs as described below . x coordinate is the % difference between input records ( origData list ) y coordinate is the % difference between corresponding output records .
"def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : inputs = [ ] for _ in xrange ( numRecords ) : input = np . zeros ( elemSize , dtype = realDType ) for _ in range ( 0 , numSet ) : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 while abs ( input . sum ( ) - numSet ) > 0.1 : ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] input [ ind ] = 1 inputs . append ( input ) return inputs",Generates a set of input record
def appendInputWithSimilarValues ( inputs ) : numInputs = len ( inputs ) for i in xrange ( numInputs ) : input = inputs [ i ] for j in xrange ( len ( input ) - 1 ) : if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput = copy . deepcopy ( input ) newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) break,Creates an one - off record for each record in the inputs . Appends new records to the same inputs list .
"def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : numInputs = len ( inputs ) skipOne = False for i in xrange ( numInputs ) : input = inputs [ i ] numChanged = 0 newInput = copy . deepcopy ( input ) for j in xrange ( len ( input ) - 1 ) : if skipOne : skipOne = False continue if input [ j ] == 1 and input [ j + 1 ] == 0 : newInput [ j ] = 0 newInput [ j + 1 ] = 1 inputs . append ( newInput ) newInput = copy . deepcopy ( newInput ) numChanged += 1 skipOne = True if numChanged == numNear : break",Creates a neighboring record for each record in the inputs and adds new records at the end of the inputs list
"def modifyBits ( inputVal , maxChanges ) : changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] if changes == 0 : return inputVal inputWidth = len ( inputVal ) whatToChange = np . random . random_integers ( 0 , 41 , changes ) runningIndex = - 1 numModsDone = 0 for i in xrange ( inputWidth ) : if numModsDone >= changes : break if inputVal [ i ] == 1 : runningIndex += 1 if runningIndex in whatToChange : if i != 0 and inputVal [ i - 1 ] == 0 : inputVal [ i - 1 ] = 1 inputVal [ i ] = 0 return inputVal",Modifies up to maxChanges number of bits in the inputVal
"def getRandomWithMods ( inputSpace , maxChanges ) : size = len ( inputSpace ) ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] value = copy . deepcopy ( inputSpace [ ind ] ) if maxChanges == 0 : return value return modifyBits ( value , maxChanges )",Returns a random selection from the inputSpace with randomly modified up to maxChanges number of bits .
"def createEncoder ( ) : encoder = MultiEncoder ( ) encoder . addMultipleEncoders ( { ""consumption"" : { ""fieldname"" : u""consumption"" , ""type"" : ""ScalarEncoder"" , ""name"" : u""consumption"" , ""minval"" : 0.0 , ""maxval"" : 100.0 , ""clipInput"" : True , ""w"" : 21 , ""n"" : 500 } , ""timestamp_timeOfDay"" : { ""fieldname"" : u""timestamp"" , ""type"" : ""DateEncoder"" , ""name"" : u""timestamp_timeOfDay"" , ""timeOfDay"" : ( 21 , 9.5 ) } } ) return encoder",Creates and returns a #MultiEncoder including a ScalarEncoder for energy consumption and a DateEncoder for the time of the day .
"def createRecordSensor ( network , name , dataSource ) : regionType = ""py.RecordSensor"" regionParams = json . dumps ( { ""verbosity"" : _VERBOSITY } ) network . addRegion ( name , regionType , regionParams ) sensorRegion = network . regions [ name ] . getSelf ( ) sensorRegion . encoder = createEncoder ( ) network . regions [ name ] . setParameter ( ""predictedField"" , ""consumption"" ) sensorRegion . dataSource = dataSource return sensorRegion",Creates a RecordSensor region that allows us to specify a file record stream as the input source .
"def createNetwork ( dataSource ) : network = Network ( ) sensor = createRecordSensor ( network , name = _RECORD_SENSOR , dataSource = dataSource ) createSpatialPooler ( network , name = _L1_SPATIAL_POOLER , inputWidth = sensor . encoder . getWidth ( ) ) linkType = ""UniformLink"" linkParams = """" network . link ( _RECORD_SENSOR , _L1_SPATIAL_POOLER , linkType , linkParams ) l1temporalMemory = createTemporalMemory ( network , _L1_TEMPORAL_MEMORY ) network . link ( _L1_SPATIAL_POOLER , _L1_TEMPORAL_MEMORY , linkType , linkParams ) classifierParams = { 'alpha' : 0.005 , 'steps' : '1' , 'implementation' : 'py' , 'verbosity' : 0 } l1Classifier = network . addRegion ( _L1_CLASSIFIER , ""py.SDRClassifierRegion"" , json . dumps ( classifierParams ) ) l1Classifier . setParameter ( 'inferenceMode' , True ) l1Classifier . setParameter ( 'learningMode' , True ) network . link ( _L1_TEMPORAL_MEMORY , _L1_CLASSIFIER , linkType , linkParams , srcOutput = ""bottomUpOut"" , destInput = ""bottomUpIn"" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = ""categoryOut"" , destInput = ""categoryIn"" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = ""bucketIdxOut"" , destInput = ""bucketIdxIn"" ) network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , srcOutput = ""actValueOut"" , destInput = ""actValueIn"" ) l2inputWidth = l1temporalMemory . getSelf ( ) . getOutputElementCount ( ""bottomUpOut"" ) createSpatialPooler ( network , name = _L2_SPATIAL_POOLER , inputWidth = l2inputWidth ) network . link ( _L1_TEMPORAL_MEMORY , _L2_SPATIAL_POOLER , linkType , linkParams ) createTemporalMemory ( network , _L2_TEMPORAL_MEMORY ) network . link ( _L2_SPATIAL_POOLER , _L2_TEMPORAL_MEMORY , linkType , linkParams ) l2Classifier = network . addRegion ( _L2_CLASSIFIER , ""py.SDRClassifierRegion"" , json . dumps ( classifierParams ) ) l2Classifier . setParameter ( 'inferenceMode' , True ) l2Classifier . setParameter ( 'learningMode' , True ) network . link ( _L2_TEMPORAL_MEMORY , _L2_CLASSIFIER , linkType , linkParams , srcOutput = ""bottomUpOut"" , destInput = ""bottomUpIn"" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = ""categoryOut"" , destInput = ""categoryIn"" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = ""bucketIdxOut"" , destInput = ""bucketIdxIn"" ) network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , srcOutput = ""actValueOut"" , destInput = ""actValueIn"" ) return network",Creates and returns a new Network with a sensor region reading data from dataSource . There are two hierarchical levels each with one SP and one TM .
"def runNetwork ( network , numRecords , writer ) : sensorRegion = network . regions [ _RECORD_SENSOR ] l1SpRegion = network . regions [ _L1_SPATIAL_POOLER ] l1TpRegion = network . regions [ _L1_TEMPORAL_MEMORY ] l1Classifier = network . regions [ _L1_CLASSIFIER ] l2SpRegion = network . regions [ _L2_SPATIAL_POOLER ] l2TpRegion = network . regions [ _L2_TEMPORAL_MEMORY ] l2Classifier = network . regions [ _L2_CLASSIFIER ] l1PreviousPredictedColumns = [ ] l2PreviousPredictedColumns = [ ] l1PreviousPrediction = None l2PreviousPrediction = None l1ErrorSum = 0.0 l2ErrorSum = 0.0 for record in xrange ( numRecords ) : network . run ( 1 ) actual = float ( sensorRegion . getOutputData ( ""actValueOut"" ) [ 0 ] ) l1Predictions = l1Classifier . getOutputData ( ""actualValues"" ) l1Probabilities = l1Classifier . getOutputData ( ""probabilities"" ) l1Prediction = l1Predictions [ l1Probabilities . argmax ( ) ] if l1PreviousPrediction is not None : l1ErrorSum += math . fabs ( l1PreviousPrediction - actual ) l1PreviousPrediction = l1Prediction l2Predictions = l2Classifier . getOutputData ( ""actualValues"" ) l2Probabilities = l2Classifier . getOutputData ( ""probabilities"" ) l2Prediction = l2Predictions [ l2Probabilities . argmax ( ) ] if l2PreviousPrediction is not None : l2ErrorSum += math . fabs ( l2PreviousPrediction - actual ) l2PreviousPrediction = l2Prediction l1AnomalyScore = l1TpRegion . getOutputData ( ""anomalyScore"" ) [ 0 ] l2AnomalyScore = l2TpRegion . getOutputData ( ""anomalyScore"" ) [ 0 ] writer . writerow ( ( record , actual , l1PreviousPrediction , l1AnomalyScore , l2PreviousPrediction , l2AnomalyScore ) ) l1PredictedColumns = l1TpRegion . getOutputData ( ""topDownOut"" ) . nonzero ( ) [ 0 ] l1PreviousPredictedColumns = copy . deepcopy ( l1PredictedColumns ) l2PredictedColumns = l2TpRegion . getOutputData ( ""topDownOut"" ) . nonzero ( ) [ 0 ] l2PreviousPredictedColumns = copy . deepcopy ( l2PredictedColumns ) if numRecords > 1 : print ""L1 ave abs class. error: %f"" % ( l1ErrorSum / ( numRecords - 1 ) ) print ""L2 ave abs class. error: %f"" % ( l2ErrorSum / ( numRecords - 1 ) )",Runs specified Network writing the ensuing anomaly scores to writer .
def clean ( s ) : lines = [ l . rstrip ( ) for l in s . split ( '\n' ) ] return '\n' . join ( lines ),Removes trailing whitespace on each line .
"def update ( self , results ) : self . _addResults ( results ) if not self . __metricSpecs or self . __currentInference is None : return { } metricResults = { } for metric , spec , label in zip ( self . __metrics , self . __metricSpecs , self . __metricLabels ) : inferenceElement = spec . inferenceElement field = spec . field groundTruth = self . _getGroundTruth ( inferenceElement ) inference = self . _getInference ( inferenceElement ) rawRecord = self . _getRawGroundTruth ( ) result = self . __currentResult if field : if type ( inference ) in ( list , tuple ) : if field in self . __fieldNameIndexMap : fieldIndex = self . __fieldNameIndexMap [ field ] inference = inference [ fieldIndex ] else : inference = None if groundTruth is not None : if type ( groundTruth ) in ( list , tuple ) : if field in self . __fieldNameIndexMap : fieldIndex = self . __fieldNameIndexMap [ field ] groundTruth = groundTruth [ fieldIndex ] else : groundTruth = None else : groundTruth = groundTruth [ field ] metric . addInstance ( groundTruth = groundTruth , prediction = inference , record = rawRecord , result = result ) metricResults [ label ] = metric . getMetric ( ) [ 'value' ] return metricResults",Compute the new metrics values given the next inference / ground - truth values
"def getMetrics ( self ) : result = { } for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : value = metricObj . getMetric ( ) result [ label ] = value [ 'value' ] return result",Gets the current metric values
"def getMetricDetails ( self , metricLabel ) : try : metricIndex = self . __metricLabels . index ( metricLabel ) except IndexError : return None return self . __metrics [ metricIndex ] . getMetric ( )",Gets detailed info about a given metric in addition to its value . This may including any statistics or auxilary data that are computed for a given metric .
"def _addResults ( self , results ) : if self . __isTemporal : shiftedInferences = self . __inferenceShifter . shift ( results ) . inferences self . __currentResult = copy . deepcopy ( results ) self . __currentResult . inferences = shiftedInferences self . __currentInference = shiftedInferences else : self . __currentResult = copy . deepcopy ( results ) self . __currentInference = copy . deepcopy ( results . inferences ) self . __currentGroundTruth = copy . deepcopy ( results )",Stores the current model results in the manager s internal store
"def _getGroundTruth ( self , inferenceElement ) : sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) if sensorInputElement is None : return None return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement )",Get the actual value for this field
"def __constructMetricsModules ( self , metricSpecs ) : if not metricSpecs : return self . __metricSpecs = metricSpecs for spec in metricSpecs : if not InferenceElement . validate ( spec . inferenceElement ) : raise ValueError ( ""Invalid inference element for metric spec: %r"" % spec ) self . __metrics . append ( metrics . getModule ( spec ) ) self . __metricLabels . append ( spec . getLabel ( ) )",Creates the required metrics modules
"def _generateSimple ( filename = ""simple.csv"" , numSequences = 1 , elementsPerSeq = 3 , numRepeats = 10 ) : scriptDir = os . path . dirname ( __file__ ) pathname = os . path . join ( scriptDir , 'datasets' , filename ) print ""Creating %s..."" % ( pathname ) fields = [ ( 'timestamp' , 'datetime' , 'T' ) , ( 'field1' , 'string' , '' ) , ( 'field2' , 'float' , '' ) ] outFile = FileRecordStream ( pathname , write = True , fields = fields ) sequences = [ ] for i in range ( numSequences ) : seq = [ x for x in range ( i * elementsPerSeq , ( i + 1 ) * elementsPerSeq ) ] sequences . append ( seq ) seqIdxs = [ ] for i in range ( numRepeats ) : seqIdxs += range ( numSequences ) random . shuffle ( seqIdxs ) timestamp = datetime . datetime ( year = 2012 , month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 ) timeDelta = datetime . timedelta ( hours = 1 ) for seqIdx in seqIdxs : seq = sequences [ seqIdx ] for x in seq : outFile . appendRecord ( [ timestamp , str ( x ) , x ] ) timestamp += timeDelta for seqIdx in seqIdxs : seq = sequences [ seqIdx ] for i , x in enumerate ( seq ) : if i != 1 : outFile . appendRecord ( [ timestamp , str ( x ) , x ] ) timestamp += timeDelta for seqIdx in seqIdxs : seq = sequences [ seqIdx ] for i , x in enumerate ( seq ) : if i != 1 : outFile . appendRecord ( [ timestamp , str ( x ) , x ] ) timestamp += timeDelta for seqIdx in seqIdxs : seq = sequences [ seqIdx ] for x in seq : outFile . appendRecord ( [ timestamp , str ( x ) , x ] ) timestamp += timeDelta outFile . close ( )",Generate a simple dataset . This contains a bunch of non - overlapping sequences . At the end of the dataset we introduce missing records so that test code can insure that the model didn t get confused by them . Parameters : ---------------------------------------------------- filename : name of the file to produce including extension . It will be created in a datasets sub - directory within the directory containing this script . numSequences : how many sequences to generate elementsPerSeq : length of each sequence numRepeats : how many times to repeat each sequence in the output
"def shift ( self , modelResult ) : inferencesToWrite = { } if self . _inferenceBuffer is None : maxDelay = InferenceElement . getMaxDelay ( modelResult . inferences ) self . _inferenceBuffer = collections . deque ( maxlen = maxDelay + 1 ) self . _inferenceBuffer . appendleft ( copy . deepcopy ( modelResult . inferences ) ) for inferenceElement , inference in modelResult . inferences . iteritems ( ) : if isinstance ( inference , dict ) : inferencesToWrite [ inferenceElement ] = { } for key , _ in inference . iteritems ( ) : delay = InferenceElement . getTemporalDelay ( inferenceElement , key ) if len ( self . _inferenceBuffer ) > delay : prevInference = self . _inferenceBuffer [ delay ] [ inferenceElement ] [ key ] inferencesToWrite [ inferenceElement ] [ key ] = prevInference else : inferencesToWrite [ inferenceElement ] [ key ] = None else : delay = InferenceElement . getTemporalDelay ( inferenceElement ) if len ( self . _inferenceBuffer ) > delay : inferencesToWrite [ inferenceElement ] = ( self . _inferenceBuffer [ delay ] [ inferenceElement ] ) else : if type ( inference ) in ( list , tuple ) : inferencesToWrite [ inferenceElement ] = [ None ] * len ( inference ) else : inferencesToWrite [ inferenceElement ] = None shiftedResult = ModelResult ( rawInput = modelResult . rawInput , sensorInput = modelResult . sensorInput , inferences = inferencesToWrite , metrics = modelResult . metrics , predictedFieldIdx = modelResult . predictedFieldIdx , predictedFieldName = modelResult . predictedFieldName ) return shiftedResult",Shift the model result and return the new instance .
"def generateStats ( filename , maxSamples = None , ) : statsCollectorMapping = { 'float' : FloatStatsCollector , 'int' : IntStatsCollector , 'string' : StringStatsCollector , 'datetime' : DateTimeStatsCollector , 'bool' : BoolStatsCollector , } filename = resource_filename ( ""nupic.datafiles"" , filename ) print ""*"" * 40 print ""Collecting statistics for file:'%s'"" % ( filename , ) dataFile = FileRecordStream ( filename ) statsCollectors = [ ] for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) statsCollectors . append ( statsCollector ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : record = dataFile . getNextRecord ( ) if record is None : break for i , value in enumerate ( record ) : statsCollectors [ i ] . addValue ( value ) stats = { } for statsCollector in statsCollectors : statsCollector . getStats ( stats ) if dataFile . getResetFieldIdx ( ) is not None : resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] stats . pop ( resetFieldName ) if VERBOSITY > 0 : pprint . pprint ( stats ) return stats",Collect statistics for each of the fields in the user input data file and return a stats dict object .
"def getStats ( self , stats ) : BaseStatsCollector . getStats ( self , stats ) sortedNumberList = sorted ( self . valueList ) listLength = len ( sortedNumberList ) min = sortedNumberList [ 0 ] max = sortedNumberList [ - 1 ] mean = numpy . mean ( self . valueList ) median = sortedNumberList [ int ( 0.5 * listLength ) ] percentile1st = sortedNumberList [ int ( 0.01 * listLength ) ] percentile99th = sortedNumberList [ int ( 0.99 * listLength ) ] differenceList = [ ( cur - prev ) for prev , cur in itertools . izip ( list ( self . valueSet ) [ : - 1 ] , list ( self . valueSet ) [ 1 : ] ) ] if min > max : print self . fieldname , min , max , '-----' meanResolution = numpy . mean ( differenceList ) stats [ self . fieldname ] [ 'min' ] = min stats [ self . fieldname ] [ 'max' ] = max stats [ self . fieldname ] [ 'mean' ] = mean stats [ self . fieldname ] [ 'median' ] = median stats [ self . fieldname ] [ 'percentile1st' ] = percentile1st stats [ self . fieldname ] [ 'percentile99th' ] = percentile99th stats [ self . fieldname ] [ 'meanResolution' ] = meanResolution passData = True if passData : stats [ self . fieldname ] [ 'data' ] = self . valueList if VERBOSITY > 2 : print '--' print ""Statistics:"" print ""min:"" , min print ""max:"" , max print ""mean:"" , mean print ""median:"" , median print ""1st percentile :"" , percentile1st print ""99th percentile:"" , percentile99th print '--' print ""Resolution:"" print ""Mean Resolution:"" , meanResolution if VERBOSITY > 3 : print '--' print ""Histogram:"" counts , bins = numpy . histogram ( self . valueList , new = True ) print ""Counts:"" , counts . tolist ( ) print ""Bins:"" , bins . tolist ( )",Override of getStats () in BaseStatsCollector
"def main ( ) : initLogging ( verbose = True ) initExperimentPrng ( ) @ staticmethod def _mockCreate ( * args , * * kwargs ) : kwargs . pop ( 'implementation' , None ) return SDRClassifierDiff ( * args , * * kwargs ) SDRClassifierFactory . create = _mockCreate runExperiment ( sys . argv [ 1 : ] )",Run according to options in sys . argv and diff classifiers .
"def _abbreviate ( text , threshold ) : if text is not None and len ( text ) > threshold : text = text [ : threshold ] + ""..."" return text",Abbreviate the given text to threshold chars and append an ellipsis if its length exceeds threshold ; used for logging ;
"def __getDBNameForVersion ( cls , dbVersion ) : prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) suffix = suffix . replace ( ""-"" , ""_"" ) suffix = suffix . replace ( ""."" , ""_"" ) dbName = '%s_%s' % ( prefix , suffix ) return dbName",Generates the ClientJobs database name for the given version of the database
def get ( ) : if ClientJobsDAO . _instance is None : cjDAO = ClientJobsDAO ( ) cjDAO . connect ( ) ClientJobsDAO . _instance = cjDAO return ClientJobsDAO . _instance,Get the instance of the ClientJobsDAO created for this process ( or perhaps at some point in the future for this thread ) .
"def _columnNameDBToPublic ( self , dbName ) : words = dbName . split ( '_' ) if dbName . startswith ( '_' ) : words = words [ 1 : ] pubWords = [ words [ 0 ] ] for word in words [ 1 : ] : pubWords . append ( word [ 0 ] . upper ( ) + word [ 1 : ] ) return '' . join ( pubWords )",Convert a database internal column name to a public name . This takes something of the form word1_word2_word3 and converts it to : word1Word2Word3 . If the db field name starts with _ it is stripped out so that the name is compatible with collections . namedtuple . for example : _word1_word2_word3 = > word1Word2Word3
"def connect ( self , deleteOldVersions = False , recreate = False ) : with ConnectionFactory . get ( ) as conn : self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , recreate = recreate ) conn . cursor . execute ( 'SELECT CONNECTION_ID()' ) self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] self . _logger . info ( ""clientJobsConnectionID=%r"" , self . _connectionID ) return",Locate the current version of the jobs DB or create a new one and optionally delete old versions laying around . If desired this method can be called at any time to re - create the tables from scratch delete old versions of the database etc .
"def _initTables ( self , cursor , deleteOldVersions , recreate ) : if deleteOldVersions : self . _logger . info ( ""Dropping old versions of client_jobs DB; called from: %r"" , traceback . format_stack ( ) ) for i in range ( self . _DB_VERSION ) : cursor . execute ( 'DROP DATABASE IF EXISTS %s' % ( self . __getDBNameForVersion ( i ) , ) ) if recreate : self . _logger . info ( ""Dropping client_jobs DB %r; called from: %r"" , self . dbName , traceback . format_stack ( ) ) cursor . execute ( 'DROP DATABASE IF EXISTS %s' % ( self . dbName ) ) cursor . execute ( 'CREATE DATABASE IF NOT EXISTS %s' % ( self . dbName ) ) cursor . execute ( 'SHOW TABLES IN %s' % ( self . dbName ) ) output = cursor . fetchall ( ) tableNames = [ x [ 0 ] for x in output ] if 'jobs' not in tableNames : self . _logger . info ( ""Creating table %r"" , self . jobsTableName ) fields = [ 'job_id                  INT UNSIGNED NOT NULL AUTO_INCREMENT' , 'client                  CHAR(%d)' % ( self . CLIENT_MAX_LEN ) , 'client_info             LONGTEXT' , 'client_key             varchar(255)' , 'cmd_line                LONGTEXT' , 'params                  LONGTEXT' , 'job_hash                BINARY(%d) DEFAULT NULL' % ( self . HASH_MAX_LEN ) , 'status                  VARCHAR(16) DEFAULT ""notStarted""' , 'completion_reason       VARCHAR(16)' , 'completion_msg          LONGTEXT' , 'worker_completion_reason   VARCHAR(16) DEFAULT ""%s""' % self . CMPL_REASON_SUCCESS , 'worker_completion_msg   LONGTEXT' , 'cancel                  BOOLEAN DEFAULT FALSE' , 'start_time              DATETIME DEFAULT NULL' , 'end_time                DATETIME DEFAULT NULL' , 'results                 LONGTEXT' , '_eng_job_type           VARCHAR(32)' , 'minimum_workers         INT UNSIGNED DEFAULT 0' , 'maximum_workers         INT UNSIGNED DEFAULT 0' , 'priority                 INT DEFAULT %d' % self . DEFAULT_JOB_PRIORITY , '_eng_allocate_new_workers    BOOLEAN DEFAULT TRUE' , '_eng_untended_dead_workers   BOOLEAN DEFAULT FALSE' , 'num_failed_workers           INT UNSIGNED DEFAULT 0' , 'last_failed_worker_error_msg  LONGTEXT' , '_eng_cleaning_status          VARCHAR(16) DEFAULT ""%s""' % self . CLEAN_NOT_DONE , 'gen_base_description    LONGTEXT' , 'gen_permutations        LONGTEXT' , '_eng_last_update_time   DATETIME DEFAULT NULL' , '_eng_cjm_conn_id        INT UNSIGNED' , '_eng_worker_state       LONGTEXT' , '_eng_status             LONGTEXT' , '_eng_model_milestones   LONGTEXT' , 'PRIMARY KEY (job_id)' , 'UNIQUE INDEX (client, job_hash)' , 'INDEX (status)' , 'INDEX (client_key)' ] options = [ 'AUTO_INCREMENT=1000' , ] query = 'CREATE TABLE IF NOT EXISTS %s (%s) %s' % ( self . jobsTableName , ',' . join ( fields ) , ',' . join ( options ) ) cursor . execute ( query ) if 'models' not in tableNames : self . _logger . info ( ""Creating table %r"" , self . modelsTableName ) fields = [ 'model_id                BIGINT UNSIGNED NOT NULL AUTO_INCREMENT' , 'job_id                  INT UNSIGNED NOT NULL' , 'params                  LONGTEXT NOT NULL' , 'status                  VARCHAR(16) DEFAULT ""notStarted""' , 'completion_reason       VARCHAR(16)' , 'completion_msg          LONGTEXT' , 'results                 LONGTEXT DEFAULT NULL' , 'optimized_metric        FLOAT ' , 'update_counter          INT UNSIGNED DEFAULT 0' , 'num_records             INT UNSIGNED DEFAULT 0' , 'start_time              DATETIME DEFAULT NULL' , 'end_time                DATETIME DEFAULT NULL' , 'cpu_time                FLOAT DEFAULT 0' , 'model_checkpoint_id     LONGTEXT' , 'gen_description         LONGTEXT' , '_eng_params_hash        BINARY(%d) DEFAULT NULL' % ( self . HASH_MAX_LEN ) , '_eng_particle_hash      BINARY(%d) DEFAULT NULL' % ( self . HASH_MAX_LEN ) , '_eng_last_update_time   DATETIME DEFAULT NULL' , '_eng_task_tracker_id    TINYBLOB' , '_eng_worker_id          TINYBLOB' , '_eng_attempt_id         TINYBLOB' , '_eng_worker_conn_id     INT DEFAULT 0' , '_eng_milestones         LONGTEXT' , '_eng_stop               VARCHAR(16) DEFAULT NULL' , '_eng_matured            BOOLEAN DEFAULT FALSE' , 'PRIMARY KEY (model_id)' , 'UNIQUE INDEX (job_id, _eng_params_hash)' , 'UNIQUE INDEX (job_id, _eng_particle_hash)' , ] options = [ 'AUTO_INCREMENT=1000' , ] query = 'CREATE TABLE IF NOT EXISTS %s (%s) %s' % ( self . modelsTableName , ',' . join ( fields ) , ',' . join ( options ) ) cursor . execute ( query ) cursor . execute ( 'DESCRIBE %s' % ( self . jobsTableName ) ) fields = cursor . fetchall ( ) self . _jobs . dbFieldNames = [ str ( field [ 0 ] ) for field in fields ] cursor . execute ( 'DESCRIBE %s' % ( self . modelsTableName ) ) fields = cursor . fetchall ( ) self . _models . dbFieldNames = [ str ( field [ 0 ] ) for field in fields ] self . _jobs . publicFieldNames = [ self . _columnNameDBToPublic ( x ) for x in self . _jobs . dbFieldNames ] self . _models . publicFieldNames = [ self . _columnNameDBToPublic ( x ) for x in self . _models . dbFieldNames ] self . _jobs . pubToDBNameDict = dict ( zip ( self . _jobs . publicFieldNames , self . _jobs . dbFieldNames ) ) self . _jobs . dbToPubNameDict = dict ( zip ( self . _jobs . dbFieldNames , self . _jobs . publicFieldNames ) ) self . _models . pubToDBNameDict = dict ( zip ( self . _models . publicFieldNames , self . _models . dbFieldNames ) ) self . _models . dbToPubNameDict = dict ( zip ( self . _models . dbFieldNames , self . _models . publicFieldNames ) ) self . _models . modelInfoNamedTuple = collections . namedtuple ( '_modelInfoNamedTuple' , self . _models . publicFieldNames ) self . _jobs . jobInfoNamedTuple = collections . namedtuple ( '_jobInfoNamedTuple' , self . _jobs . publicFieldNames ) return",Initialize tables if needed
"def _getMatchingRowsNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = None ) : assert fieldsToMatch , repr ( fieldsToMatch ) assert all ( k in tableInfo . dbFieldNames for k in fieldsToMatch . iterkeys ( ) ) , repr ( fieldsToMatch ) assert selectFieldNames , repr ( selectFieldNames ) assert all ( f in tableInfo . dbFieldNames for f in selectFieldNames ) , repr ( selectFieldNames ) matchPairs = fieldsToMatch . items ( ) matchExpressionGen = ( p [ 0 ] + ( ' IS ' + { True : 'TRUE' , False : 'FALSE' } [ p [ 1 ] ] if isinstance ( p [ 1 ] , bool ) else ' IS NULL' if p [ 1 ] is None else ' IN %s' if isinstance ( p [ 1 ] , self . _SEQUENCE_TYPES ) else '=%s' ) for p in matchPairs ) matchFieldValues = [ p [ 1 ] for p in matchPairs if ( not isinstance ( p [ 1 ] , ( bool ) ) and p [ 1 ] is not None ) ] query = 'SELECT %s FROM %s WHERE (%s)' % ( ',' . join ( selectFieldNames ) , tableInfo . tableName , ' AND ' . join ( matchExpressionGen ) ) sqlParams = matchFieldValues if maxRows is not None : query += ' LIMIT %s' sqlParams . append ( maxRows ) conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows : assert maxRows is None or len ( rows ) <= maxRows , ""%d !<= %d"" % ( len ( rows ) , maxRows ) assert len ( rows [ 0 ] ) == len ( selectFieldNames ) , ""%d != %d"" % ( len ( rows [ 0 ] ) , len ( selectFieldNames ) ) else : rows = tuple ( ) return rows",Return a sequence of matching rows with the requested field values from a table or empty sequence if nothing matched .
"def _getMatchingRowsWithRetries ( self , tableInfo , fieldsToMatch , selectFieldNames , maxRows = None ) : with ConnectionFactory . get ( ) as conn : return self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows )",Like _getMatchingRowsNoRetries () but with retries on transient MySQL failures
"def _getOneMatchingRowNoRetries ( self , tableInfo , conn , fieldsToMatch , selectFieldNames ) : rows = self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames , maxRows = 1 ) if rows : assert len ( rows ) == 1 , repr ( len ( rows ) ) result = rows [ 0 ] else : result = None return result",Return a single matching row with the requested field values from the the requested table or None if nothing matched .
"def _getOneMatchingRowWithRetries ( self , tableInfo , fieldsToMatch , selectFieldNames ) : with ConnectionFactory . get ( ) as conn : return self . _getOneMatchingRowNoRetries ( tableInfo , conn , fieldsToMatch , selectFieldNames )",Like _getOneMatchingRowNoRetries () but with retries on transient MySQL failures
"def _insertOrGetUniqueJobNoRetries ( self , conn , client , cmdLine , jobHash , clientInfo , clientKey , params , minimumWorkers , maximumWorkers , jobType , priority , alreadyRunning ) : assert len ( client ) <= self . CLIENT_MAX_LEN , ""client too long:"" + repr ( client ) assert cmdLine , ""Unexpected empty or None command-line: "" + repr ( cmdLine ) assert len ( jobHash ) == self . HASH_MAX_LEN , ""wrong hash len=%d"" % len ( jobHash ) if alreadyRunning : initStatus = self . STATUS_TESTMODE else : initStatus = self . STATUS_NOTSTARTED query = 'INSERT IGNORE INTO %s (status, client, client_info, client_key,' 'cmd_line, params, job_hash, _eng_last_update_time, ' 'minimum_workers, maximum_workers, priority, _eng_job_type) ' ' VALUES (%%s, %%s, %%s, %%s, %%s, %%s, %%s, ' '         UTC_TIMESTAMP(), %%s, %%s, %%s, %%s) ' % ( self . jobsTableName , ) sqlParams = ( initStatus , client , clientInfo , clientKey , cmdLine , params , jobHash , minimumWorkers , maximumWorkers , priority , jobType ) numRowsInserted = conn . cursor . execute ( query , sqlParams ) jobID = 0 if numRowsInserted == 1 : conn . cursor . execute ( 'SELECT LAST_INSERT_ID()' ) jobID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] if jobID == 0 : self . _logger . warn ( '_insertOrGetUniqueJobNoRetries: SELECT LAST_INSERT_ID() returned 0; ' 'likely due to reconnection in SteadyDB following INSERT. ' 'jobType=%r; client=%r; clientInfo=%r; clientKey=%s; jobHash=%r; ' 'cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 32 ) , clientKey , jobHash , cmdLine ) else : assert numRowsInserted == 0 , repr ( numRowsInserted ) if jobID == 0 : row = self . _getOneMatchingRowNoRetries ( self . _jobs , conn , dict ( client = client , job_hash = jobHash ) , [ 'job_id' ] ) assert row is not None assert len ( row ) == 1 , 'Unexpected num fields: ' + repr ( len ( row ) ) jobID = row [ 0 ] if alreadyRunning : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' '              start_time=UTC_TIMESTAMP(), ' '              _eng_last_update_time=UTC_TIMESTAMP() ' '          WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , ( self . _connectionID , jobID ) ) return jobID",Attempt to insert a row with the given parameters into the jobs table . Return jobID of the inserted row or of an existing row with matching client / jobHash key .
"def _resumeJobNoRetries ( self , conn , jobID , alreadyRunning ) : if alreadyRunning : initStatus = self . STATUS_TESTMODE else : initStatus = self . STATUS_NOTSTARTED assignments = [ 'status=%s' , 'completion_reason=DEFAULT' , 'completion_msg=DEFAULT' , 'worker_completion_reason=DEFAULT' , 'worker_completion_msg=DEFAULT' , 'end_time=DEFAULT' , 'cancel=DEFAULT' , '_eng_last_update_time=UTC_TIMESTAMP()' , '_eng_allocate_new_workers=DEFAULT' , '_eng_untended_dead_workers=DEFAULT' , 'num_failed_workers=DEFAULT' , 'last_failed_worker_error_msg=DEFAULT' , '_eng_cleaning_status=DEFAULT' , ] assignmentValues = [ initStatus ] if alreadyRunning : assignments += [ '_eng_cjm_conn_id=%s' , 'start_time=UTC_TIMESTAMP()' , '_eng_last_update_time=UTC_TIMESTAMP()' ] assignmentValues . append ( self . _connectionID ) else : assignments += [ '_eng_cjm_conn_id=DEFAULT' , 'start_time=DEFAULT' ] assignments = ', ' . join ( assignments ) query = 'UPDATE %s SET %s ' '          WHERE job_id=%%s AND status=%%s' % ( self . jobsTableName , assignments ) sqlParams = assignmentValues + [ jobID , self . STATUS_COMPLETED ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , repr ( numRowsAffected ) if numRowsAffected == 0 : self . _logger . info ( ""_resumeJobNoRetries: Redundant job-resume UPDATE: job was not "" ""suspended or was resumed by another process or operation was retried "" ""after connection failure; jobID=%s"" , jobID ) return",Resumes processing of an existing job that is presently in the STATUS_COMPLETED state .
"def jobResume ( self , jobID , alreadyRunning = False ) : row = self . jobGetFields ( jobID , [ 'status' ] ) ( jobStatus , ) = row if jobStatus != self . STATUS_COMPLETED : raise RuntimeError ( ( ""Failed to resume job: job was not suspended; "" ""jobID=%s; job status=%r"" ) % ( jobID , jobStatus ) ) @ g_retrySQL def resumeWithRetries ( ) : with ConnectionFactory . get ( ) as conn : self . _resumeJobNoRetries ( conn , jobID , alreadyRunning ) resumeWithRetries ( ) return",Resumes processing of an existing job that is presently in the STATUS_COMPLETED state .
"def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) @ g_retrySQL def insertWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHash , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) try : jobID = insertWithRetries ( ) except : self . _logger . exception ( 'jobInsert FAILED: jobType=%r; client=%r; clientInfo=%r; clientKey=%r;' 'jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsert: returning jobID=%s. jobType=%r; client=%r; clientInfo=%r; ' 'clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID",Add an entry to the jobs table for a new job request . This is called by clients that wish to startup a new job like a Hypersearch stream job or specific model evaluation from the engine .
"def jobInsertUnique ( self , client , cmdLine , jobHash , clientInfo = '' , clientKey = '' , params = '' , minimumWorkers = 0 , maximumWorkers = 0 , jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : assert cmdLine , ""Unexpected empty or None command-line: "" + repr ( cmdLine ) @ g_retrySQL def insertUniqueWithRetries ( ) : jobHashValue = self . _normalizeHash ( jobHash ) jobID = None with ConnectionFactory . get ( ) as conn : row = self . _getOneMatchingRowNoRetries ( self . _jobs , conn , dict ( client = client , job_hash = jobHashValue ) , [ 'job_id' , 'status' ] ) if row is not None : ( jobID , status ) = row if status == self . STATUS_COMPLETED : query = 'UPDATE %s SET client_info=%%s, ' '              client_key=%%s, ' '              cmd_line=%%s, ' '              params=%%s, ' '              minimum_workers=%%s, ' '              maximum_workers=%%s, ' '              priority=%%s, ' '              _eng_job_type=%%s ' '          WHERE (job_id=%%s AND status=%%s)' % ( self . jobsTableName , ) sqlParams = ( clientInfo , clientKey , cmdLine , params , minimumWorkers , maximumWorkers , priority , jobType , jobID , self . STATUS_COMPLETED ) numRowsUpdated = conn . cursor . execute ( query , sqlParams ) assert numRowsUpdated <= 1 , repr ( numRowsUpdated ) if numRowsUpdated == 0 : self . _logger . info ( ""jobInsertUnique: Redundant job-reuse UPDATE: job restarted by "" ""another process, values were unchanged, or operation was "" ""retried after connection failure; jobID=%s"" , jobID ) self . _resumeJobNoRetries ( conn , jobID , alreadyRunning = False ) else : jobID = self . _insertOrGetUniqueJobNoRetries ( conn , client = client , cmdLine = cmdLine , jobHash = jobHashValue , clientInfo = clientInfo , clientKey = clientKey , params = params , minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , jobType = jobType , priority = priority , alreadyRunning = False ) return jobID try : jobID = insertUniqueWithRetries ( ) except : self . _logger . exception ( 'jobInsertUnique FAILED: jobType=%r; client=%r; ' 'clientInfo=%r; clientKey=%r; jobHash=%r; cmdLine=%r' , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) raise else : self . _logger . info ( 'jobInsertUnique: returning jobID=%s. jobType=%r; client=%r; ' 'clientInfo=%r; clientKey=%r; jobHash=%r; cmdLine=%r' , jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , cmdLine ) return jobID",Add an entry to the jobs table for a new job request but only if the same job by the same client is not already running . If the job is already running or queued up to run this call does nothing . If the job does not exist in the jobs table or has completed it will be inserted and / or started up again .
"def _startJobWithRetries ( self , jobID ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' '            _eng_cjm_conn_id=%%s, ' '            start_time=UTC_TIMESTAMP(), ' '            _eng_last_update_time=UTC_TIMESTAMP() ' '          WHERE (job_id=%%s AND status=%%s)' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_RUNNING , self . _connectionID , jobID , self . STATUS_NOTSTARTED ] numRowsUpdated = conn . cursor . execute ( query , sqlParams ) if numRowsUpdated != 1 : self . _logger . warn ( 'jobStartNext: numRowsUpdated=%r instead of 1; ' 'likely side-effect of transient connection ' 'failure' , numRowsUpdated ) return",Place the given job in STATUS_RUNNING mode ; the job is expected to be STATUS_NOTSTARTED .
"def jobStartNext ( self ) : row = self . _getOneMatchingRowWithRetries ( self . _jobs , dict ( status = self . STATUS_NOTSTARTED ) , [ 'job_id' ] ) if row is None : return None ( jobID , ) = row self . _startJobWithRetries ( jobID ) return jobID",For use only by Nupic Scheduler ( also known as ClientJobManager ) Look through the jobs table and see if any new job requests have been queued up . If so pick one and mark it as starting up and create the model table to hold the results
"def jobReactivateRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_cjm_conn_id=%%s, ' '              _eng_allocate_new_workers=TRUE ' '    WHERE status=%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) return",Look through the jobs table and reactivate all that are already in the running state by setting their _eng_allocate_new_workers fields to True ; used by Nupic Scheduler as part of its failure - recovery procedure .
"def jobGetDemand ( self , ) : rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( status = self . STATUS_RUNNING ) , [ self . _jobs . pubToDBNameDict [ f ] for f in self . _jobs . jobDemandNamedTuple . _fields ] ) return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ]",Look through the jobs table and get the demand - minimum and maximum number of workers requested if new workers are to be allocated if there are any untended dead workers for all running jobs .
"def jobCancelAllRunningJobs ( self ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET cancel=TRUE WHERE status<>%%s ' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) return",Set cancel field of all currently - running jobs to true .
"def jobCountCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT COUNT(job_id) ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows [ 0 ] [ 0 ]",Look through the jobs table and count the running jobs whose cancel field is true .
"def jobGetCancellingJobs ( self , ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT job_id ' 'FROM %s ' 'WHERE (status<>%%s AND cancel is TRUE)' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return tuple ( r [ 0 ] for r in rows )",Look through the jobs table and get the list of running jobs whose cancel field is true .
"def partitionAtIntervals ( data , intervals ) : assert sum ( intervals ) <= len ( data ) start = 0 for interval in intervals : end = start + interval yield data [ start : end ] start = end raise StopIteration",Generator to allow iterating slices at dynamic intervals
"def _combineResults ( result , * namedTuples ) : results = ClientJobsDAO . partitionAtIntervals ( result , [ len ( nt . _fields ) for nt in namedTuples ] ) return [ nt . _make ( result ) for nt , result in zip ( namedTuples , results ) ]",Return a list of namedtuples from the result of a join query . A single database result is partitioned at intervals corresponding to the fields in namedTuples . The return value is the result of applying namedtuple . _make () to each of the partitions for each of the namedTuples .
"def jobInfoWithModels ( self , jobID ) : combinedResults = None with ConnectionFactory . get ( ) as conn : query = ' ' . join ( [ 'SELECT %s.*, %s.*' % ( self . jobsTableName , self . modelsTableName ) , 'FROM %s' % self . jobsTableName , 'LEFT JOIN %s USING(job_id)' % self . modelsTableName , 'WHERE job_id=%s' ] ) conn . cursor . execute ( query , ( jobID , ) ) if conn . cursor . rowcount > 0 : combinedResults = [ ClientJobsDAO . _combineResults ( result , self . _jobs . jobInfoNamedTuple , self . _models . modelInfoNamedTuple ) for result in conn . cursor . fetchall ( ) ] if combinedResults is not None : return combinedResults raise RuntimeError ( ""jobID=%s not found within the jobs table"" % ( jobID ) )",Get all info about a job with model details if available .
"def jobInfo ( self , jobID ) : row = self . _getOneMatchingRowWithRetries ( self . _jobs , dict ( job_id = jobID ) , [ self . _jobs . pubToDBNameDict [ n ] for n in self . _jobs . jobInfoNamedTuple . _fields ] ) if row is None : raise RuntimeError ( ""jobID=%s not found within the jobs table"" % ( jobID ) ) return self . _jobs . jobInfoNamedTuple . _make ( row )",Get all info about a job
"def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' '              _eng_last_update_time=UTC_TIMESTAMP() ' '          WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ status , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( ""Tried to change the status of job %d to %s, but "" ""this job belongs to some other CJM"" % ( jobID , status ) )",Change the status on the given job
"def jobSetCompleted ( self , jobID , completionReason , completionMsg , useConnectionID = True ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET status=%%s, ' '              completion_reason=%%s, ' '              completion_msg=%%s, ' '              end_time=UTC_TIMESTAMP(), ' '              _eng_last_update_time=UTC_TIMESTAMP() ' '          WHERE job_id=%%s' % ( self . jobsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( ""Tried to change the status of jobID=%s to "" ""completed, but this job could not be found or "" ""belongs to some other CJM"" % ( jobID ) )",Change the status on the given job to completed
"def jobCancel ( self , jobID ) : self . _logger . info ( 'Canceling jobID=%s' , jobID ) self . jobSetFields ( jobID , { ""cancel"" : True } , useConnectionID = False )",Cancel the given job . This will update the cancel field in the jobs table and will result in the job being cancelled .
"def jobGetModelIDs ( self , jobID ) : rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( job_id = jobID ) , [ 'model_id' ] ) return [ r [ 0 ] for r in rows ]",Fetch all the modelIDs that correspond to a given jobID ; empty sequence if none
"def getActiveJobCountForClientInfo ( self , clientInfo ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount",Return the number of jobs for the given clientInfo and a status that is not completed .
"def getActiveJobCountForClientKey ( self , clientKey ) : with ConnectionFactory . get ( ) as conn : query = 'SELECT count(job_id) ' 'FROM %s ' 'WHERE client_key = %%s ' ' AND status != %%s' % self . jobsTableName conn . cursor . execute ( query , [ clientKey , self . STATUS_COMPLETED ] ) activeJobCount = conn . cursor . fetchone ( ) [ 0 ] return activeJobCount",Return the number of jobs for the given clientKey and a status that is not completed .
"def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT %s FROM %s ' 'WHERE client_info = %%s ' ' AND status != %%s' % ( dbFieldsStr , self . jobsTableName ) conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) rows = conn . cursor . fetchall ( ) return rows",Fetch jobIDs for jobs in the table with optional fields given a specific clientInfo
"def getFieldsForActiveJobsOfType ( self , jobType , fields = [ ] ) : dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) with ConnectionFactory . get ( ) as conn : query = 'SELECT DISTINCT %s ' 'FROM %s j ' 'LEFT JOIN %s m USING(job_id) ' 'WHERE j.status != %%s ' 'AND _eng_job_type = %%s' % ( dbFieldsStr , self . jobsTableName , self . modelsTableName ) conn . cursor . execute ( query , [ self . STATUS_COMPLETED , jobType ] ) return conn . cursor . fetchall ( )",Helper function for querying the models table including relevant job info where the job type matches the specified jobType . Only records for which there is a matching jobId in both tables is returned and only the requested fields are returned in each result assuming that there is not a conflict . This function is useful for example in querying a cluster for a list of actively running production models ( according to the state of the client jobs database ) . jobType must be one of the JOB_TYPE_XXXX enumerations .
"def jobGetFields ( self , jobID , fields ) : return self . jobsGetFields ( [ jobID ] , fields , requireAll = True ) [ 0 ] [ 1 ]",Fetch the values of 1 or more fields from a job record . Here fields is a list with the names of the fields to fetch . The names are the public names of the fields ( camelBack not the lower_case_only form as stored in the DB ) .
"def jobsGetFields ( self , jobIDs , fields , requireAll = True ) : assert isinstance ( jobIDs , self . _SEQUENCE_TYPES ) assert len ( jobIDs ) >= 1 rows = self . _getMatchingRowsWithRetries ( self . _jobs , dict ( job_id = jobIDs ) , [ 'job_id' ] + [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] ) if requireAll and len ( rows ) < len ( jobIDs ) : raise RuntimeError ( ""jobIDs %s not found within the jobs table"" % ( ( set ( jobIDs ) - set ( r [ 0 ] for r in rows ) ) , ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]",Fetch the values of 1 or more fields from a sequence of job records . Here fields is a sequence ( list or tuple ) with the names of the fields to fetch . The names are the public names of the fields ( camelBack not the lower_case_only form as stored in the DB ) .
"def jobSetFields ( self , jobID , fields , useConnectionID = True , ignoreUnchanged = False ) : assignmentExpressions = ',' . join ( [ ""%s=%%s"" % ( self . _jobs . pubToDBNameDict [ f ] , ) for f in fields . iterkeys ( ) ] ) assignmentValues = fields . values ( ) query = 'UPDATE %s SET %s ' '          WHERE job_id=%%s' % ( self . jobsTableName , assignmentExpressions , ) sqlParams = assignmentValues + [ jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) with ConnectionFactory . get ( ) as conn : result = conn . cursor . execute ( query , sqlParams ) if result != 1 and not ignoreUnchanged : raise RuntimeError ( ""Tried to change fields (%r) of jobID=%s conn_id=%r), but an error "" ""occurred. result=%r; query=%r"" % ( assignmentExpressions , jobID , self . _connectionID , result , query ) )",Change the values of 1 or more fields in a job . Here fields is a dict with the name / value pairs to change . The names are the public names of the fields ( camelBack not the lower_case_only form as stored in the DB ) . This method is for private use by the ClientJobManager only .
"def jobSetFieldIfEqual ( self , jobID , fieldName , newValue , curValue ) : dbFieldName = self . _jobs . pubToDBNameDict [ fieldName ] conditionValue = [ ] if isinstance ( curValue , bool ) : conditionExpression = '%s IS %s' % ( dbFieldName , { True : 'TRUE' , False : 'FALSE' } [ curValue ] ) elif curValue is None : conditionExpression = '%s is NULL' % ( dbFieldName , ) else : conditionExpression = '%s=%%s' % ( dbFieldName , ) conditionValue . append ( curValue ) query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), %s=%%s ' '          WHERE job_id=%%s AND %s' % ( self . jobsTableName , dbFieldName , conditionExpression ) sqlParams = [ newValue , jobID ] + conditionValue with ConnectionFactory . get ( ) as conn : result = conn . cursor . execute ( query , sqlParams ) return ( result == 1 )",Change the value of 1 field in a job to newValue but only if the current value matches curValue . The fieldName is the public name of the field ( camelBack not the lower_case_only form as stored in the DB ) .
"def jobIncrementIntField ( self , jobID , fieldName , increment = 1 , useConnectionID = False ) : dbFieldName = self . _jobs . pubToDBNameDict [ fieldName ] with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET %s=%s+%%s ' '          WHERE job_id=%%s' % ( self . jobsTableName , dbFieldName , dbFieldName ) sqlParams = [ increment , jobID ] if useConnectionID : query += ' AND _eng_cjm_conn_id=%s' sqlParams . append ( self . _connectionID ) result = conn . cursor . execute ( query , sqlParams ) if result != 1 : raise RuntimeError ( ""Tried to increment the field (%r) of jobID=%s (conn_id=%r), but an "" ""error occurred. result=%r; query=%r"" % ( dbFieldName , jobID , self . _connectionID , result , query ) )",Incremet the value of 1 field in a job by increment . The fieldName is the public name of the field ( camelBack not the lower_case_only form as stored in the DB ) .
"def jobUpdateResults ( self , jobID , results ) : with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_last_update_time=UTC_TIMESTAMP(), ' '              results=%%s ' '          WHERE job_id=%%s' % ( self . jobsTableName , ) conn . cursor . execute ( query , [ results , jobID ] )",Update the results string and last - update - time fields of a model .
"def modelsClearAll ( self ) : self . _logger . info ( 'Deleting all rows from models table %r' , self . modelsTableName ) with ConnectionFactory . get ( ) as conn : query = 'DELETE FROM %s' % ( self . modelsTableName ) conn . cursor . execute ( query )",Delete all models from the models table
"def modelInsertAndStart ( self , jobID , params , paramsHash , particleHash = None ) : if particleHash is None : particleHash = paramsHash paramsHash = self . _normalizeHash ( paramsHash ) particleHash = self . _normalizeHash ( particleHash ) def findExactMatchNoRetries ( conn ) : return self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'job_id' : jobID , '_eng_params_hash' : paramsHash , '_eng_particle_hash' : particleHash } , [ 'model_id' , '_eng_worker_conn_id' ] ) @ g_retrySQL def findExactMatchWithRetries ( ) : with ConnectionFactory . get ( ) as conn : return findExactMatchNoRetries ( conn ) row = findExactMatchWithRetries ( ) if row is not None : return ( row [ 0 ] , False ) @ g_retrySQL def insertModelWithRetries ( ) : """""" NOTE: it's possible that another process on some machine is attempting
      to insert the same model at the same time as the caller """""" with ConnectionFactory . get ( ) as conn : query = 'INSERT INTO %s (job_id, params, status, _eng_params_hash, ' '  _eng_particle_hash, start_time, _eng_last_update_time, ' '  _eng_worker_conn_id) ' '  VALUES (%%s, %%s, %%s, %%s, %%s, UTC_TIMESTAMP(), ' '          UTC_TIMESTAMP(), %%s) ' % ( self . modelsTableName , ) sqlParams = ( jobID , params , self . STATUS_RUNNING , paramsHash , particleHash , self . _connectionID ) try : numRowsAffected = conn . cursor . execute ( query , sqlParams ) except Exception , e : if ""Duplicate entry"" not in str ( e ) : raise self . _logger . info ( 'Model insert attempt failed with DUP_ENTRY: ' 'jobID=%s; paramsHash=%s OR particleHash=%s; %r' , jobID , paramsHash . encode ( 'hex' ) , particleHash . encode ( 'hex' ) , e ) else : if numRowsAffected == 1 : conn . cursor . execute ( 'SELECT LAST_INSERT_ID()' ) modelID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] if modelID != 0 : return ( modelID , True ) else : self . _logger . warn ( 'SELECT LAST_INSERT_ID for model returned 0, implying loss of ' 'connection: jobID=%s; paramsHash=%r; particleHash=%r' , jobID , paramsHash , particleHash ) else : self . _logger . error ( 'Attempt to insert model resulted in unexpected numRowsAffected: ' 'expected 1, but got %r; jobID=%s; paramsHash=%r; ' 'particleHash=%r' , numRowsAffected , jobID , paramsHash , particleHash ) row = findExactMatchNoRetries ( conn ) if row is not None : ( modelID , connectionID ) = row return ( modelID , connectionID == self . _connectionID ) query = 'SELECT (model_id) FROM %s ' '                  WHERE job_id=%%s AND ' '                        (_eng_params_hash=%%s ' '                         OR _eng_particle_hash=%%s) ' '                  LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ jobID , paramsHash , particleHash ] numRowsFound = conn . cursor . execute ( query , sqlParams ) assert numRowsFound == 1 , ( 'Model not found: jobID=%s AND (paramsHash=%r OR particleHash=%r); ' 'numRowsFound=%r' ) % ( jobID , paramsHash , particleHash , numRowsFound ) ( modelID , ) = conn . cursor . fetchall ( ) [ 0 ] return ( modelID , False ) return insertModelWithRetries ( )",Insert a new unique model ( based on params ) into the model table in the running state . This will return two things : whether or not the model was actually inserted ( i . e . that set of params isn t already in the table ) and the modelID chosen for that set of params . Even if the model was not inserted by this call ( it was already there ) the modelID of the one already inserted is returned .
"def modelsInfo ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( ""wrong modelIDs type: %s"" ) % ( type ( modelIDs ) , ) assert modelIDs , ""modelIDs is empty"" rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( model_id = modelIDs ) , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . modelInfoNamedTuple . _fields ] ) results = [ self . _models . modelInfoNamedTuple . _make ( r ) for r in rows ] assert len ( results ) == len ( modelIDs ) , ""modelIDs not found: %s"" % ( set ( modelIDs ) - set ( r . modelId for r in results ) ) return results",Get ALL info for a set of models
"def modelsGetFields ( self , modelIDs , fields ) : assert len ( fields ) >= 1 , 'fields is empty' isSequence = isinstance ( modelIDs , self . _SEQUENCE_TYPES ) if isSequence : assert len ( modelIDs ) >= 1 , 'modelIDs is empty' else : modelIDs = [ modelIDs ] rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( model_id = modelIDs ) , [ 'model_id' ] + [ self . _models . pubToDBNameDict [ f ] for f in fields ] ) if len ( rows ) < len ( modelIDs ) : raise RuntimeError ( ""modelIDs not found within the models table: %s"" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) ) if not isSequence : return list ( rows [ 0 ] [ 1 : ] ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]",Fetch the values of 1 or more fields from a sequence of model records . Here fields is a list with the names of the fields to fetch . The names are the public names of the fields ( camelBack not the lower_case_only form as stored in the DB ) .
"def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : assert len ( fields ) >= 1 , 'fields is empty' dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] dbFieldsStr = ',' . join ( dbFields ) query = 'SELECT model_id, %s FROM %s ' '          WHERE job_id=%%s ' % ( dbFieldsStr , self . modelsTableName ) sqlParams = [ jobID ] if ignoreKilled : query += ' AND (completion_reason IS NULL OR completion_reason != %s)' sqlParams . append ( self . CMPL_REASON_KILLED ) with ConnectionFactory . get ( ) as conn : conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) if rows is None : self . _logger . error ( ""Unexpected None result from cursor.fetchall; "" ""query=%r; Traceback=%r"" , query , traceback . format_exc ( ) ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]",Gets the specified fields for all the models for a single job . This is similar to modelsGetFields
"def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : assert len ( fields ) >= 1 , ""fields is empty"" with ConnectionFactory . get ( ) as conn : dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] dbFieldStr = "", "" . join ( dbFields ) query = 'SELECT model_id, {fields} from {models}' '   WHERE job_id=%s AND model_checkpoint_id IS NOT NULL' . format ( fields = dbFieldStr , models = self . modelsTableName ) conn . cursor . execute ( query , [ jobID ] ) rows = conn . cursor . fetchall ( ) return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ]",Gets fields from all models in a job that have been checkpointed . This is used to figure out whether or not a new model should be checkpointed .
"def modelSetFields ( self , modelID , fields , ignoreUnchanged = False ) : assignmentExpressions = ',' . join ( '%s=%%s' % ( self . _models . pubToDBNameDict [ f ] , ) for f in fields . iterkeys ( ) ) assignmentValues = fields . values ( ) query = 'UPDATE %s SET %s, update_counter = update_counter+1 ' '          WHERE model_id=%%s' % ( self . modelsTableName , assignmentExpressions ) sqlParams = assignmentValues + [ modelID ] with ConnectionFactory . get ( ) as conn : numAffectedRows = conn . cursor . execute ( query , sqlParams ) self . _logger . debug ( ""Executed: numAffectedRows=%r, query=%r, sqlParams=%r"" , numAffectedRows , query , sqlParams ) if numAffectedRows != 1 and not ignoreUnchanged : raise RuntimeError ( ( ""Tried to change fields (%r) of model %r (conn_id=%r), but an error "" ""occurred. numAffectedRows=%r; query=%r; sqlParams=%r"" ) % ( fields , modelID , self . _connectionID , numAffectedRows , query , sqlParams , ) )",Change the values of 1 or more fields in a model . Here fields is a dict with the name / value pairs to change . The names are the public names of the fields ( camelBack not the lower_case_only form as stored in the DB ) .
"def modelsGetParams ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( ""Wrong modelIDs type: %r"" ) % ( type ( modelIDs ) , ) assert len ( modelIDs ) >= 1 , ""modelIDs is empty"" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getParamsNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , ""Didn't find modelIDs: %r"" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ]",Get the params and paramsHash for a set of models .
"def modelsGetResultAndStatus ( self , modelIDs ) : assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( ""Wrong modelIDs type: %r"" ) % type ( modelIDs ) assert len ( modelIDs ) >= 1 , ""modelIDs is empty"" rows = self . _getMatchingRowsWithRetries ( self . _models , { 'model_id' : modelIDs } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getResultAndStatusNamedTuple . _fields ] ) assert len ( rows ) == len ( modelIDs ) , ""Didn't find modelIDs: %r"" % ( ( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) return [ self . _models . getResultAndStatusNamedTuple . _make ( r ) for r in rows ]",Get the results string and other status fields for a set of models .
"def modelsGetUpdateCounters ( self , jobID ) : rows = self . _getMatchingRowsWithRetries ( self . _models , { 'job_id' : jobID } , [ self . _models . pubToDBNameDict [ f ] for f in self . _models . getUpdateCountersNamedTuple . _fields ] ) return [ self . _models . getUpdateCountersNamedTuple . _make ( r ) for r in rows ]",Return info on all of the models that are in already in the models table for a given job . For each model this returns a tuple containing : ( modelID updateCounter ) .
"def modelUpdateResults ( self , modelID , results = None , metricValue = None , numRecords = None ) : assignmentExpressions = [ '_eng_last_update_time=UTC_TIMESTAMP()' , 'update_counter=update_counter+1' ] assignmentValues = [ ] if results is not None : assignmentExpressions . append ( 'results=%s' ) assignmentValues . append ( results ) if numRecords is not None : assignmentExpressions . append ( 'num_records=%s' ) assignmentValues . append ( numRecords ) if metricValue is not None and ( metricValue == metricValue ) : assignmentExpressions . append ( 'optimized_metric=%s' ) assignmentValues . append ( float ( metricValue ) ) query = 'UPDATE %s SET %s ' '          WHERE model_id=%%s and _eng_worker_conn_id=%%s' % ( self . modelsTableName , ',' . join ( assignmentExpressions ) ) sqlParams = assignmentValues + [ modelID , self . _connectionID ] with ConnectionFactory . get ( ) as conn : numRowsAffected = conn . cursor . execute ( query , sqlParams ) if numRowsAffected != 1 : raise InvalidConnectionException ( ( ""Tried to update the info of modelID=%r using connectionID=%r, but "" ""this model belongs to some other worker or modelID not found; "" ""numRowsAffected=%r"" ) % ( modelID , self . _connectionID , numRowsAffected , ) )",Update the results string and / or num_records fields of a model . This will fail if the model does not currently belong to this client ( connection_id doesn t match ) .
"def modelSetCompleted ( self , modelID , completionReason , completionMsg , cpuTime = 0 , useConnectionID = True ) : if completionMsg is None : completionMsg = '' query = 'UPDATE %s SET status=%%s, ' '            completion_reason=%%s, ' '            completion_msg=%%s, ' '            end_time=UTC_TIMESTAMP(), ' '            cpu_time=%%s, ' '            _eng_last_update_time=UTC_TIMESTAMP(), ' '            update_counter=update_counter+1 ' '        WHERE model_id=%%s' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , cpuTime , modelID ] if useConnectionID : query += "" AND _eng_worker_conn_id=%s"" sqlParams . append ( self . _connectionID ) with ConnectionFactory . get ( ) as conn : numRowsAffected = conn . cursor . execute ( query , sqlParams ) if numRowsAffected != 1 : raise InvalidConnectionException ( ( ""Tried to set modelID=%r using connectionID=%r, but this model "" ""belongs to some other worker or modelID not found; "" ""numRowsAffected=%r"" ) % ( modelID , self . _connectionID , numRowsAffected ) )",Mark a model as completed with the given completionReason and completionMsg . This will fail if the model does not currently belong to this client ( connection_id doesn t match ) .
"def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : @ g_retrySQL def findCandidateModelWithRetries ( ) : modelID = None with ConnectionFactory . get ( ) as conn : query = 'SELECT model_id FROM %s ' '   WHERE  status=%%s ' '          AND job_id=%%s ' '          AND TIMESTAMPDIFF(SECOND, ' '                            _eng_last_update_time, ' '                            UTC_TIMESTAMP()) > %%s ' '   LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] numRows = conn . cursor . execute ( query , sqlParams ) rows = conn . cursor . fetchall ( ) assert numRows <= 1 , ""Unexpected numRows: %r"" % numRows if numRows == 1 : ( modelID , ) = rows [ 0 ] return modelID @ g_retrySQL def adoptModelWithRetries ( modelID ) : adopted = False with ConnectionFactory . get ( ) as conn : query = 'UPDATE %s SET _eng_worker_conn_id=%%s, ' '            _eng_last_update_time=UTC_TIMESTAMP() ' '        WHERE model_id=%%s ' '              AND status=%%s' '              AND TIMESTAMPDIFF(SECOND, ' '                                _eng_last_update_time, ' '                                UTC_TIMESTAMP()) > %%s ' '        LIMIT 1 ' % ( self . modelsTableName , ) sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , maxUpdateInterval ] numRowsAffected = conn . cursor . execute ( query , sqlParams ) assert numRowsAffected <= 1 , 'Unexpected numRowsAffected=%r' % ( numRowsAffected , ) if numRowsAffected == 1 : adopted = True else : ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( self . _models , conn , { 'model_id' : modelID } , [ 'status' , '_eng_worker_conn_id' ] ) adopted = ( status == self . STATUS_RUNNING and connectionID == self . _connectionID ) return adopted adoptedModelID = None while True : modelID = findCandidateModelWithRetries ( ) if modelID is None : break if adoptModelWithRetries ( modelID ) : adoptedModelID = modelID break return adoptedModelID",Look through the models table for an orphaned model which is a model that is not completed yet whose _eng_last_update_time is more than maxUpdateInterval seconds ago .
"def profileSP ( spClass , spDim , nRuns ) : inDim = [ 10000 , 1 , 1 ] colDim = [ spDim , 1 , 1 ] sp = spClass ( inputDimensions = inDim , columnDimensions = colDim , potentialRadius = 3 , potentialPct = 0.5 , globalInhibition = False , localAreaDensity = - 1.0 , numActiveColumnsPerInhArea = 3 , stimulusThreshold = 1 , synPermInactiveDec = 0.01 , synPermActiveInc = 0.1 , synPermConnected = 0.10 , minPctOverlapDutyCycle = 0.1 , dutyCyclePeriod = 10 , boostStrength = 10.0 , seed = 42 , spVerbosity = 0 ) dataDim = inDim dataDim . append ( nRuns ) data = numpy . random . randint ( 0 , 2 , dataDim ) . astype ( 'float32' ) for i in xrange ( nRuns ) : d = data [ : , : , : , i ] activeArray = numpy . zeros ( colDim ) sp . compute ( d , True , activeArray )",profiling performance of SpatialPooler ( SP ) using the python cProfile module and ordered by cumulative time see how to run on command - line above .
"def getSpec ( cls ) : ns = dict ( description = KNNClassifierRegion . __doc__ , singleNodeOnly = True , inputs = dict ( categoryIn = dict ( description = 'Vector of zero or more category indices for this input' 'sample. -1 implies no category.' , dataType = 'Real32' , count = 0 , required = True , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , bottomUpIn = dict ( description = 'Belief values over children\'s groups' , dataType = 'Real32' , count = 0 , required = True , regionLevel = False , isDefaultInput = True , requireSplitterMap = False ) , partitionIn = dict ( description = 'Partition ID of the input sample' , dataType = 'Real32' , count = 0 , required = True , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) , auxDataIn = dict ( description = 'Auxiliary data from the sensor' , dataType = 'Real32' , count = 0 , required = False , regionLevel = True , isDefaultInput = False , requireSplitterMap = False ) ) , outputs = dict ( categoriesOut = dict ( description = 'A vector representing, for each category ' 'index, the likelihood that the input to the node belongs ' 'to that category based on the number of neighbors of ' 'that category that are among the nearest K.' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = True ) , bestPrototypeIndices = dict ( description = 'A vector that lists, in descending order of ' 'the match, the positions of the prototypes ' 'that best match the input pattern.' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = False ) , categoryProbabilitiesOut = dict ( description = 'A vector representing, for each category ' 'index, the probability that the input to the node belongs ' 'to that category based on the distance to the nearest ' 'neighbor of each category.' , dataType = 'Real32' , count = 0 , regionLevel = True , isDefaultOutput = True ) , ) , parameters = dict ( learningMode = dict ( description = 'Boolean (0/1) indicating whether or not a region ' 'is in learning mode.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 1 , accessMode = 'ReadWrite' ) , inferenceMode = dict ( description = 'Boolean (0/1) indicating whether or not a region ' 'is in inference mode.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'ReadWrite' ) , acceptanceProbability = dict ( description = 'During learning, inputs are learned with ' 'probability equal to this parameter. ' 'If set to 1.0, the default, ' 'all inputs will be considered ' '(subject to other tests).' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 1.0 , accessMode = 'ReadWrite' ) , confusion = dict ( description = 'Confusion matrix accumulated during inference. ' 'Reset with reset(). This is available to Python ' 'client code only.' , dataType = 'Handle' , count = 2 , constraints = '' , defaultValue = None , accessMode = 'Read' ) , activeOutputCount = dict ( description = 'The number of active elements in the ' '""categoriesOut"" output.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Read' ) , categoryCount = dict ( description = 'An integer indicating the number of ' 'categories that have been learned' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = None , accessMode = 'Read' ) , patternCount = dict ( description = 'Number of patterns learned by the classifier.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = None , accessMode = 'Read' ) , patternMatrix = dict ( description = 'The actual patterns learned by the classifier, ' 'returned as a matrix.' , dataType = 'Handle' , count = 1 , constraints = '' , defaultValue = None , accessMode = 'Read' ) , k = dict ( description = 'The number of nearest neighbors to use ' 'during inference.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 1 , accessMode = 'Create' ) , maxCategoryCount = dict ( description = 'The maximal number of categories the ' 'classifier will distinguish between.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 2 , accessMode = 'Create' ) , distanceNorm = dict ( description = 'The norm to use for a distance metric (i.e., ' 'the ""p"" in Lp-norm)' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 2.0 , accessMode = 'ReadWrite' ) , distanceMethod = dict ( description = 'Method used to compute distances between inputs and' 'prototypes. Possible options are norm, rawOverlap, ' 'pctOverlapOfLarger, and pctOverlapOfProto' , dataType = ""Byte"" , count = 0 , constraints = 'enum: norm, rawOverlap, pctOverlapOfLarger, ' 'pctOverlapOfProto, pctOverlapOfInput' , defaultValue = 'norm' , accessMode = 'ReadWrite' ) , outputProbabilitiesByDist = dict ( description = 'If True, categoryProbabilitiesOut is the probability of ' 'each category based on the distance to the nearest neighbor of ' 'each category. If False, categoryProbabilitiesOut is the ' 'percentage of neighbors among the top K that are of each category.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , distThreshold = dict ( description = 'Distance Threshold.  If a pattern that ' 'is less than distThreshold apart from ' 'the input pattern already exists in the ' 'KNN memory, then the input pattern is ' 'not added to KNN memory.' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0.0 , accessMode = 'ReadWrite' ) , inputThresh = dict ( description = 'Input binarization threshold, used if ' '""doBinarization"" is True.' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0.5 , accessMode = 'Create' ) , doBinarization = dict ( description = 'Whether or not to binarize the input vectors.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , useSparseMemory = dict ( description = 'A boolean flag that determines whether or ' 'not the KNNClassifier will use sparse Memory' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 1 , accessMode = 'Create' ) , minSparsity = dict ( description = ""If useSparseMemory is set, only vectors with sparsity"" "" >= minSparsity will be stored during learning. A value"" "" of 0.0 implies all vectors will be stored. A value of"" "" 0.1 implies only vectors with at least 10% sparsity"" "" will be stored"" , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0.0 , accessMode = 'ReadWrite' ) , sparseThreshold = dict ( description = 'If sparse memory is used, input variables ' 'whose absolute value is less than this ' 'threshold  will be stored as zero' , dataType = 'Real32' , count = 1 , constraints = '' , defaultValue = 0.0 , accessMode = 'Create' ) , relativeThreshold = dict ( description = 'Whether to multiply sparseThreshold by max value ' ' in input' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , winnerCount = dict ( description = 'Only this many elements of the input are ' 'stored. All elements are stored if 0.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , doSphering = dict ( description = 'A boolean indicating whether or not data should' 'be ""sphered"" (i.e. each dimension should be normalized such' 'that its mean and variance are zero and one, respectively.) This' ' sphering normalization would be performed after all training ' 'samples had been received but before inference was performed. ' 'The dimension-specific normalization constants would then ' ' be applied to all future incoming vectors prior to performing ' ' conventional NN inference.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , SVDSampleCount = dict ( description = 'If not 0, carries out SVD transformation after ' 'that many samples have been seen.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , SVDDimCount = dict ( description = 'Number of dimensions to keep after SVD if greater ' 'than 0. If set to -1 it is considered unspecified. ' 'If set to 0 it is consider ""adaptive"" and the number ' 'is chosen automatically.' , dataType = 'Int32' , count = 1 , constraints = '' , defaultValue = - 1 , accessMode = 'Create' ) , fractionOfMax = dict ( description = 'The smallest singular value which is retained ' 'as a fraction of the largest singular value. This is ' 'used only if SVDDimCount==0 (""adaptive"").' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , useAuxiliary = dict ( description = 'Whether or not the classifier should use auxiliary ' 'input data.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , justUseAuxiliary = dict ( description = 'Whether or not the classifier should ONLUY use the ' 'auxiliary input data.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = 0 , accessMode = 'Create' ) , verbosity = dict ( description = 'An integer that controls the verbosity level, ' '0 means no verbose output, increasing integers ' 'provide more verbosity.' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'ReadWrite' ) , keepAllDistances = dict ( description = 'Whether to store all the protoScores in an array, ' 'rather than just the ones for the last inference. ' 'When this parameter is changed from True to False, ' 'all the scores are discarded except for the most ' 'recent one.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = None , accessMode = 'ReadWrite' ) , replaceDuplicates = dict ( description = 'A boolean flag that determines whether or' 'not the KNNClassifier should replace duplicates' 'during learning. This should be on when online' 'learning.' , dataType = 'UInt32' , count = 1 , constraints = 'bool' , defaultValue = None , accessMode = 'ReadWrite' ) , cellsPerCol = dict ( description = 'If >= 1, we assume the input is organized into columns, ' 'in the same manner as the temporal memory AND ' 'whenever we store a new prototype, we only store the ' 'start cell (first cell) in any column which is bursting.' 'colum ' , dataType = 'UInt32' , count = 1 , constraints = '' , defaultValue = 0 , accessMode = 'Create' ) , maxStoredPatterns = dict ( description = 'Limits the maximum number of the training patterns ' 'stored. When KNN learns in a fixed capacity mode, ' 'the unused patterns are deleted once the number ' 'of stored patterns is greater than maxStoredPatterns' 'columns. [-1 is no limit] ' , dataType = 'Int32' , count = 1 , constraints = '' , defaultValue = - 1 , accessMode = 'Create' ) , ) , commands = dict ( ) ) return ns",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getSpec .
"def _initEphemerals ( self ) : self . _firstComputeCall = True self . _accuracy = None self . _protoScores = None self . _categoryDistances = None self . _knn = knn_classifier . KNNClassifier ( * * self . knnParams ) for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , '_scanInfo' , '_protoScores' ) : if not hasattr ( self , x ) : setattr ( self , x , None )",Initialize attributes that are not saved with the checkpoint .
"def getParameter ( self , name , index = - 1 ) : if name == ""patternCount"" : return self . _knn . _numPatterns elif name == ""patternMatrix"" : return self . _getPatternMatrix ( ) elif name == ""k"" : return self . _knn . k elif name == ""distanceNorm"" : return self . _knn . distanceNorm elif name == ""distanceMethod"" : return self . _knn . distanceMethod elif name == ""distThreshold"" : return self . _knn . distThreshold elif name == ""inputThresh"" : return self . _knn . binarizationThreshold elif name == ""doBinarization"" : return self . _knn . doBinarization elif name == ""useSparseMemory"" : return self . _knn . useSparseMemory elif name == ""sparseThreshold"" : return self . _knn . sparseThreshold elif name == ""winnerCount"" : return self . _knn . numWinners elif name == ""relativeThreshold"" : return self . _knn . relativeThreshold elif name == ""SVDSampleCount"" : v = self . _knn . numSVDSamples return v if v is not None else 0 elif name == ""SVDDimCount"" : v = self . _knn . numSVDDims return v if v is not None else 0 elif name == ""fractionOfMax"" : v = self . _knn . fractionOfMax return v if v is not None else 0 elif name == ""useAuxiliary"" : return self . _useAuxiliary elif name == ""justUseAuxiliary"" : return self . _justUseAuxiliary elif name == ""doSphering"" : return self . _doSphering elif name == ""cellsPerCol"" : return self . _knn . cellsPerCol elif name == ""maxStoredPatterns"" : return self . maxStoredPatterns elif name == 'categoryRecencyList' : return self . _knn . _categoryRecencyList else : return PyRegion . getParameter ( self , name , index )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getParameter .
"def setParameter ( self , name , index , value ) : if name == ""learningMode"" : self . learningMode = bool ( int ( value ) ) self . _epoch = 0 elif name == ""inferenceMode"" : self . _epoch = 0 if int ( value ) and not self . inferenceMode : self . _finishLearning ( ) self . inferenceMode = bool ( int ( value ) ) elif name == ""distanceNorm"" : self . _knn . distanceNorm = value elif name == ""distanceMethod"" : self . _knn . distanceMethod = value elif name == ""keepAllDistances"" : self . keepAllDistances = bool ( value ) if not self . keepAllDistances : if self . _protoScores is not None and self . _protoScores . shape [ 0 ] > 1 : self . _protoScores = self . _protoScores [ - 1 , : ] if self . _protoScores is not None : self . _protoScoreCount = 1 else : self . _protoScoreCount = 0 elif name == ""verbosity"" : self . verbosity = value self . _knn . verbosity = value else : return PyRegion . setParameter ( self , name , index , value )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . setParameter .
"def enableTap ( self , tapPath ) : self . _tapFileIn = open ( tapPath + '.in' , 'w' ) self . _tapFileOut = open ( tapPath + '.out' , 'w' )",Begin writing output tap files .
def disableTap ( self ) : if self . _tapFileIn is not None : self . _tapFileIn . close ( ) self . _tapFileIn = None if self . _tapFileOut is not None : self . _tapFileOut . close ( ) self . _tapFileOut = None,Disable writing of output tap files .
"def handleLogInput ( self , inputs ) : if self . _tapFileIn is not None : for input in inputs : for k in range ( len ( input ) ) : print >> self . _tapFileIn , input [ k ] , print >> self . _tapFileIn",Write inputs to output tap file . : param inputs : ( iter ) some inputs .
"def handleLogOutput ( self , output ) : if self . _tapFileOut is not None : for k in range ( len ( output ) ) : print >> self . _tapFileOut , output [ k ] , print >> self . _tapFileOut",Write outputs to output tap file .
"def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : if self . _samples is None : self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) assert self . _labels is None self . _labels = [ ] self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) self . _labels += [ trueCatIndex ] if self . _partitions is None : self . _partitions = [ ] if partition is None : partition = 0 self . _partitions += [ partition ]",Store a training sample and associated category label
"def compute ( self , inputs , outputs ) : if self . _useAuxiliary is None : self . _useAuxiliary = False if self . _firstComputeCall : self . _firstComputeCall = False if self . _useAuxiliary : if self . _justUseAuxiliary == True : print ""  Warning: You have chosen to ignore the image data and instead just use the auxiliary data stream."" inputVector = inputs [ 'bottomUpIn' ] if self . _useAuxiliary == True : auxVector = inputs [ 'auxDataIn' ] if auxVector . dtype != numpy . float32 : raise RuntimeError , ""KNNClassifierRegion expects numpy.float32 for the auxiliary data vector"" if self . _justUseAuxiliary == True : inputVector = inputs [ 'auxDataIn' ] else : inputVector = numpy . concatenate ( [ inputVector , inputs [ 'auxDataIn' ] ] ) self . handleLogInput ( [ inputVector ] ) assert ""categoryIn"" in inputs , ""No linked category input."" categories = inputs [ 'categoryIn' ] if ""partitionIn"" in inputs : assert len ( inputs [ ""partitionIn"" ] ) == 1 , ""Must have exactly one link to partition input."" partInput = inputs [ 'partitionIn' ] assert len ( partInput ) == 1 , ""Partition input element count must be exactly 1."" partition = int ( partInput [ 0 ] ) else : partition = None if self . inferenceMode : categoriesOut = outputs [ 'categoriesOut' ] probabilitiesOut = outputs [ 'categoryProbabilitiesOut' ] if self . _doSphering : inputVector = ( inputVector + self . _normOffset ) * self . _normScale nPrototypes = 0 if ""bestPrototypeIndices"" in outputs : bestPrototypeIndicesOut = outputs [ ""bestPrototypeIndices"" ] nPrototypes = len ( bestPrototypeIndicesOut ) winner , inference , protoScores , categoryDistances = self . _knn . infer ( inputVector , partitionId = partition ) if not self . keepAllDistances : self . _protoScores = protoScores else : if self . _protoScores is None : self . _protoScores = numpy . zeros ( ( 1 , protoScores . shape [ 0 ] ) , protoScores . dtype ) self . _protoScores [ 0 , : ] = protoScores self . _protoScoreCount = 1 else : if self . _protoScoreCount == self . _protoScores . shape [ 0 ] : newProtoScores = numpy . zeros ( ( self . _protoScores . shape [ 0 ] * 2 , self . _protoScores . shape [ 1 ] ) , self . _protoScores . dtype ) newProtoScores [ : self . _protoScores . shape [ 0 ] , : ] = self . _protoScores self . _protoScores = newProtoScores self . _protoScores [ self . _protoScoreCount , : ] = protoScores self . _protoScoreCount += 1 self . _categoryDistances = categoryDistances if self . outputProbabilitiesByDist : scores = 1.0 - self . _categoryDistances else : scores = inference total = scores . sum ( ) if total == 0 : numScores = len ( scores ) probabilities = numpy . ones ( numScores ) / numScores else : probabilities = scores / total nout = min ( len ( categoriesOut ) , len ( inference ) ) categoriesOut . fill ( 0 ) categoriesOut [ 0 : nout ] = inference [ 0 : nout ] probabilitiesOut . fill ( 0 ) probabilitiesOut [ 0 : nout ] = probabilities [ 0 : nout ] if self . verbosity >= 1 : print ""KNNRegion: categoriesOut: "" , categoriesOut [ 0 : nout ] print ""KNNRegion: probabilitiesOut: "" , probabilitiesOut [ 0 : nout ] if self . _scanInfo is not None : self . _scanResults = [ tuple ( inference [ : nout ] ) ] for category in categories : if category >= 0 : dims = max ( int ( category ) + 1 , len ( inference ) ) oldDims = len ( self . confusion ) if oldDims < dims : confusion = numpy . zeros ( ( dims , dims ) ) confusion [ 0 : oldDims , 0 : oldDims ] = self . confusion self . confusion = confusion self . confusion [ inference . argmax ( ) , int ( category ) ] += 1 if nPrototypes > 1 : bestPrototypeIndicesOut . fill ( 0 ) if categoryDistances is not None : indices = categoryDistances . argsort ( ) nout = min ( len ( indices ) , nPrototypes ) bestPrototypeIndicesOut [ 0 : nout ] = indices [ 0 : nout ] elif nPrototypes == 1 : if ( categoryDistances is not None ) and len ( categoryDistances ) : bestPrototypeIndicesOut [ 0 ] = categoryDistances . argmin ( ) else : bestPrototypeIndicesOut [ 0 ] = 0 self . handleLogOutput ( inference ) if self . learningMode : if ( self . acceptanceProbability < 1.0 ) and ( self . _rgen . getReal64 ( ) > self . acceptanceProbability ) : pass else : for category in categories : if category >= 0 : if self . _doSphering : self . _storeSample ( inputVector , category , partition ) else : self . _knn . learn ( inputVector , category , partition ) self . _epoch += 1",Process one input sample . This method is called by the runtime engine .
def _finishLearning ( self ) : if self . _doSphering : self . _finishSphering ( ) self . _knn . finishLearning ( ) self . _accuracy = None,Does nothing . Kept here for API compatibility
"def _finishSphering ( self ) : self . _normOffset = self . _samples . mean ( axis = 0 ) * - 1.0 self . _samples += self . _normOffset variance = self . _samples . var ( axis = 0 ) variance [ numpy . where ( variance == 0.0 ) ] = 1.0 self . _normScale = 1.0 / numpy . sqrt ( variance ) self . _samples *= self . _normScale for sampleIndex in range ( len ( self . _labels ) ) : self . _knn . learn ( self . _samples [ sampleIndex ] , self . _labels [ sampleIndex ] , self . _partitions [ sampleIndex ] )",Compute normalization constants for each feature dimension based on the collected training samples . Then normalize our training samples using these constants ( so that each input dimension has mean and variance of zero and one respectively . ) Then feed these sphered training samples into the underlying SVM model .
"def getOutputElementCount ( self , name ) : if name == 'categoriesOut' : return self . maxCategoryCount elif name == 'categoryProbabilitiesOut' : return self . maxCategoryCount elif name == 'bestPrototypeIndices' : return self . _bestPrototypeIndexCount if self . _bestPrototypeIndexCount else 0 else : raise Exception ( 'Unknown output: ' + name )",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . getOutputElementCount .
"def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : if not isinstance ( statsInfo , dict ) : raise RuntimeError ( ""statsInfo must be a dict -- "" ""found '%s' instead"" % type ( statsInfo ) ) filename = resource_filename ( ""nupic.datafiles"" , filename ) if cache : statsFilename = getStatsFilename ( filename , statsInfo , filters ) if os . path . exists ( statsFilename ) : try : r = pickle . load ( open ( statsFilename , ""rb"" ) ) except : print ""Warning: unable to load stats for %s -- "" ""will regenerate"" % filename r = dict ( ) requestedKeys = set ( [ s for s in statsInfo ] ) availableKeys = set ( r . keys ( ) ) unavailableKeys = requestedKeys . difference ( availableKeys ) if len ( unavailableKeys ) == 0 : return r else : print ""generateStats: re-generating stats file %s because "" ""keys %s are not available"" % ( filename , str ( unavailableKeys ) ) os . remove ( filename ) print ""Generating statistics for file '%s' with filters '%s'"" % ( filename , filters ) sensor = RecordSensor ( ) sensor . dataSource = FileRecordStream ( filename ) sensor . preEncodingFilters = filters stats = [ ] for field in statsInfo : if statsInfo [ field ] == ""number"" : statsInfo [ field ] = NumberStatsCollector ( ) elif statsInfo [ field ] == ""category"" : statsInfo [ field ] = CategoryStatsCollector ( ) else : raise RuntimeError ( ""Unknown stats type '%s' for field '%s'"" % ( statsInfo [ field ] , field ) ) if maxSamples is None : maxSamples = 500000 for i in xrange ( maxSamples ) : try : record = sensor . getNextRecord ( ) except StopIteration : break for ( name , collector ) in statsInfo . items ( ) : collector . add ( record [ name ] ) del sensor r = dict ( ) for ( field , collector ) in statsInfo . items ( ) : stats = collector . getStats ( ) if field not in r : r [ field ] = stats else : r [ field ] . update ( stats ) if cache : f = open ( statsFilename , ""wb"" ) pickle . dump ( r , f ) f . close ( ) r [ ""_filename"" ] = statsFilename return r",Generate requested statistics for a dataset and cache to a file . If filename is None then don t cache to a file
"def getScalarMetricWithTimeOfDayAnomalyParams ( metricData , minVal = None , maxVal = None , minResolution = None , tmImplementation = ""cpp"" ) : if minResolution is None : minResolution = 0.001 if minVal is None or maxVal is None : compMinVal , compMaxVal = _rangeGen ( metricData ) if minVal is None : minVal = compMinVal if maxVal is None : maxVal = compMaxVal if minVal == maxVal : maxVal = minVal + 1 if ( tmImplementation is ""cpp"" ) : paramFileRelativePath = os . path . join ( ""anomaly_params_random_encoder"" , ""best_single_metric_anomaly_params_cpp.json"" ) elif ( tmImplementation is ""tm_cpp"" ) : paramFileRelativePath = os . path . join ( ""anomaly_params_random_encoder"" , ""best_single_metric_anomaly_params_tm_cpp.json"" ) else : raise ValueError ( ""Invalid string for tmImplementation. Try cpp or tm_cpp"" ) with resource_stream ( __name__ , paramFileRelativePath ) as infile : paramSet = json . load ( infile ) _fixupRandomEncoderParams ( paramSet , minVal , maxVal , minResolution ) return paramSet",Return a dict that can be used to create an anomaly model via : meth : nupic . frameworks . opf . model_factory . ModelFactory . create .
"def _rangeGen ( data , std = 1 ) : dataStd = np . std ( data ) if dataStd == 0 : dataStd = 1 minval = np . min ( data ) - std * dataStd maxval = np . max ( data ) + std * dataStd return minval , maxval",Return reasonable min / max values to use given the data .
"def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : encodersDict = ( params [ ""modelConfig"" ] [ ""modelParams"" ] [ ""sensorParams"" ] [ ""encoders"" ] ) for encoder in encodersDict . itervalues ( ) : if encoder is not None : if encoder [ ""type"" ] == ""RandomDistributedScalarEncoder"" : resolution = max ( minResolution , ( maxVal - minVal ) / encoder . pop ( ""numBuckets"" ) ) encodersDict [ ""c1"" ] [ ""resolution"" ] = resolution",Given model params figure out the correct parameters for the RandomDistributed encoder . Modifies params in place .
"def read ( cls , proto ) : tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) tm . mmName = None tm . _mmTraces = None tm . _mmData = None tm . mmClearHistory ( ) tm . _mmResetActive = True return tm",Intercepts TemporalMemory deserialization request in order to initialize TemporalMemoryMonitorMixin state
"def read ( cls , proto ) : tm = super ( TMShimMixin , cls ) . read ( proto ) tm . infActiveState = { ""t"" : None } return tm",Intercepts TemporalMemory deserialization request in order to initialize self . infActiveState
"def topDownCompute ( self , topDownIn = None ) : output = numpy . zeros ( self . numberOfColumns ( ) ) columns = [ self . columnForCell ( idx ) for idx in self . getPredictiveCells ( ) ] output [ columns ] = 1 return output",( From backtracking_tm . py ) Top - down compute - generate expected input given output of the TM
"def read ( cls , proto ) : tm = super ( MonitoredTMShim , cls ) . read ( proto ) tm . infActiveState = { ""t"" : None } return tm",Intercepts TemporalMemory deserialization request in order to initialize self . infActiveState
"def compute ( self , bottomUpInput , enableLearn , computeInfOutput = None ) : super ( MonitoredTMShim , self ) . compute ( set ( bottomUpInput . nonzero ( ) [ 0 ] ) , learn = enableLearn ) numberOfCells = self . numberOfCells ( ) activeState = numpy . zeros ( numberOfCells ) activeState [ self . getActiveCells ( ) ] = 1 self . infActiveState [ ""t"" ] = activeState output = numpy . zeros ( numberOfCells ) output [ self . getPredictiveCells ( ) + self . getActiveCells ( ) ] = 1 return output",( From backtracking_tm . py ) Handle one compute possibly learning .
"def pickByDistribution ( distribution , r = None ) : if r is None : r = random x = r . uniform ( 0 , sum ( distribution ) ) for i , d in enumerate ( distribution ) : if x <= d : return i x -= d",Pick a value according to the provided distribution .
"def Indicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) x [ pos ] = 1 return x",Returns an array of length size and type dtype that is everywhere 0 except in the index in pos .
"def MultiArgMax ( x ) : m = x . max ( ) return ( i for i , v in enumerate ( x ) if v == m )",Get tuple ( actually a generator ) of indices where the max value of array x occurs . Requires that x have a max () method as x . max () ( in the case of NumPy ) is much faster than max ( x ) . For a simpler faster argmax when there is only a single maximum entry or when knowing only the first index where the maximum occurs call argmax () on a NumPy array .
"def Any ( sequence ) : return bool ( reduce ( lambda x , y : x or y , sequence , False ) )",Tests much faster ( 30% ) than bool ( sum ( bool ( x ) for x in sequence )) .
"def All ( sequence ) : return bool ( reduce ( lambda x , y : x and y , sequence , True ) )",: param sequence : Any sequence whose elements can be evaluated as booleans . : returns : true if all elements of the sequence satisfy True and x .
"def MultiIndicator ( pos , size , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : for i in pos : x [ i ] = 1 else : x [ pos ] = 1 return x",Returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos .
"def Distribution ( pos , size , counts , dtype ) : x = numpy . zeros ( size , dtype = dtype ) if hasattr ( pos , '__iter__' ) : total = 0 for i in pos : total += counts [ i ] total = float ( total ) for i in pos : x [ i ] = counts [ i ] / total else : x [ pos ] = 1 return x",Returns an array of length size and type dtype that is everywhere 0 except in the indices listed in sequence pos . The non - zero indices contain a normalized distribution based on the counts .
"def grow ( self , rows , cols ) : if not self . hist_ : self . hist_ = SparseMatrix ( rows , cols ) self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) self . colSums_ = numpy . zeros ( cols , dtype = dtype ) self . hack_ = None else : oldRows = self . hist_ . nRows ( ) oldCols = self . hist_ . nCols ( ) nextRows = max ( oldRows , rows ) nextCols = max ( oldCols , cols ) if ( oldRows < nextRows ) or ( oldCols < nextCols ) : self . hist_ . resize ( nextRows , nextCols ) if oldRows < nextRows : oldSums = self . rowSums_ self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None if oldCols < nextCols : oldSums = self . colSums_ self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) self . colSums_ [ 0 : len ( oldSums ) ] = oldSums self . hack_ = None",Grows the histogram to have rows rows and cols columns . Must not have been initialized before or already have the same number of columns . If rows is smaller than the current number of rows does not shrink . Also updates the sizes of the row and column sums .
"def updateRow ( self , row , distribution ) : self . grow ( row + 1 , len ( distribution ) ) self . hist_ . axby ( row , 1 , 1 , distribution ) self . rowSums_ [ row ] += distribution . sum ( ) self . colSums_ += distribution self . hack_ = None",Add distribution to row row . Distribution should be an array of probabilities or counts .
"def inferRowCompat ( self , distribution ) : if self . hack_ is None : self . clean_outcpd ( ) return self . hack_ . vecMaxProd ( distribution )",Equivalent to the category inference of zeta1 . TopLevel . Computes the max_prod ( maximum component of a component - wise multiply ) between the rows of the histogram and the incoming distribution . May be slow if the result of clean_outcpd () is not valid .
"def clean_outcpd ( self ) : m = self . hist_ . toDense ( ) for j in xrange ( m . shape [ 1 ] ) : cmax = m [ : , j ] . max ( ) if cmax : m [ : , j ] = numpy . array ( m [ : , j ] == cmax , dtype = dtype ) self . hack_ = SparseMatrix ( 0 , self . hist_ . nCols ( ) ) for i in xrange ( m . shape [ 0 ] ) : self . hack_ . addRow ( m [ i , : ] )",Hack to act like clean_outcpd on zeta1 . TopLevelNode . Take the max element in each to column set it to 1 and set all the other elements to 0 . Only called by inferRowMaxProd () and only needed if an updateRow () has been called since the last clean_outcpd () .
"def importAndRunFunction ( path , moduleName , funcName , * * keywords ) : import sys originalPath = sys . path try : augmentedPath = [ path ] + sys . path sys . path = augmentedPath func = getattr ( __import__ ( moduleName , fromlist = [ funcName ] ) , funcName ) sys . path = originalPath except : sys . path = originalPath raise return func ( * * keywords )",Run a named function specified by a filesystem path module name and function name .
"def transferCoincidences ( network , fromElementName , toElementName ) : coincidenceHandle = getLockedHandle ( runtimeElement = network . getElement ( fromElementName ) , expression = ""self._cd._W"" ) network . getElement ( toElementName ) . setParameter ( ""coincidencesAbove"" , coincidenceHandle )",Gets the coincidence matrix from one element and sets it on another element ( using locked handles a la nupic . bindings . research . lockHandle ) .
"def compute ( slidingWindow , total , newVal , windowSize ) : if len ( slidingWindow ) == windowSize : total -= slidingWindow . pop ( 0 ) slidingWindow . append ( newVal ) total += newVal return float ( total ) / len ( slidingWindow ) , slidingWindow , total",Routine for computing a moving average .
"def next ( self , newValue ) : newAverage , self . slidingWindow , self . total = self . compute ( self . slidingWindow , self . total , newValue , self . windowSize ) return newAverage",Instance method wrapper around compute .
"def getModule ( metricSpec ) : metricName = metricSpec . metric if metricName == 'rmse' : return MetricRMSE ( metricSpec ) if metricName == 'nrmse' : return MetricNRMSE ( metricSpec ) elif metricName == 'aae' : return MetricAAE ( metricSpec ) elif metricName == 'acc' : return MetricAccuracy ( metricSpec ) elif metricName == 'avg_err' : return MetricAveError ( metricSpec ) elif metricName == 'trivial' : return MetricTrivial ( metricSpec ) elif metricName == 'two_gram' : return MetricTwoGram ( metricSpec ) elif metricName == 'moving_mean' : return MetricMovingMean ( metricSpec ) elif metricName == 'moving_mode' : return MetricMovingMode ( metricSpec ) elif metricName == 'neg_auc' : return MetricNegAUC ( metricSpec ) elif metricName == 'custom_error_metric' : return CustomErrorMetric ( metricSpec ) elif metricName == 'multiStep' : return MetricMultiStep ( metricSpec ) elif metricName == 'multiStepProbability' : return MetricMultiStepProbability ( metricSpec ) elif metricName == 'ms_aae' : return MetricMultiStepAAE ( metricSpec ) elif metricName == 'ms_avg_err' : return MetricMultiStepAveError ( metricSpec ) elif metricName == 'passThruPrediction' : return MetricPassThruPrediction ( metricSpec ) elif metricName == 'altMAPE' : return MetricAltMAPE ( metricSpec ) elif metricName == 'MAPE' : return MetricMAPE ( metricSpec ) elif metricName == 'multi' : return MetricMulti ( metricSpec ) elif metricName == 'negativeLogLikelihood' : return MetricNegativeLogLikelihood ( metricSpec ) else : raise Exception ( ""Unsupported metric type: %s"" % metricName )",Factory method to return an appropriate : class : MetricsIface module . - rmse : : class : MetricRMSE - nrmse : : class : MetricNRMSE - aae : : class : MetricAAE - acc : : class : MetricAccuracy - avg_err : : class : MetricAveError - trivial : : class : MetricTrivial - two_gram : : class : MetricTwoGram - moving_mean : : class : MetricMovingMean - moving_mode : : class : MetricMovingMode - neg_auc : : class : MetricNegAUC - custom_error_metric : : class : CustomErrorMetric - multiStep : : class : MetricMultiStep - ms_aae : : class : MetricMultiStepAAE - ms_avg_err : : class : MetricMultiStepAveError - passThruPrediction : : class : MetricPassThruPrediction - altMAPE : : class : MetricAltMAPE - MAPE : : class : MetricMAPE - multi : : class : MetricMulti - negativeLogLikelihood : : class : MetricNegativeLogLikelihood : param metricSpec : ( : class : MetricSpec ) metric to find module for . metricSpec . metric must be in the list above . : returns : ( : class : AggregateMetric ) an appropriate metric module
"def getLabel ( self , inferenceType = None ) : result = [ ] if inferenceType is not None : result . append ( InferenceType . getLabel ( inferenceType ) ) result . append ( self . inferenceElement ) result . append ( self . metric ) params = self . params if params is not None : sortedParams = params . keys ( ) sortedParams . sort ( ) for param in sortedParams : if param in ( 'customFuncSource' , 'customFuncDef' , 'customExpr' ) : continue value = params [ param ] if isinstance ( value , str ) : result . extend ( [ ""%s='%s'"" % ( param , value ) ] ) else : result . extend ( [ ""%s=%s"" % ( param , value ) ] ) if self . field : result . append ( ""field=%s"" % ( self . field ) ) return self . _LABEL_SEPARATOR . join ( result )",Helper method that generates a unique label for a : class : MetricSpec / : class : ~nupic . frameworks . opf . opf_utils . InferenceType pair . The label is formatted as follows :
"def getInferenceTypeFromLabel ( cls , label ) : infType , _ , _ = label . partition ( cls . _LABEL_SEPARATOR ) if not InferenceType . validate ( infType ) : return None return infType",Extracts the PredictionKind ( temporal vs . nontemporal ) from the given metric label .
"def _getShiftedGroundTruth ( self , groundTruth ) : self . _groundTruthHistory . append ( groundTruth ) assert ( len ( self . _predictionSteps ) == 1 ) if len ( self . _groundTruthHistory ) > self . _predictionSteps [ 0 ] : return self . _groundTruthHistory . popleft ( ) else : if hasattr ( groundTruth , '__iter__' ) : return [ None ] * len ( groundTruth ) else : return None",Utility function that saves the passed in groundTruth into a local history buffer and returns the groundTruth from self . _predictionSteps ago where self . _predictionSteps is defined by the steps parameter . This can be called from the beginning of a derived class s addInstance () before it passes groundTruth and prediction onto accumulate () .
"def addInstance ( self , groundTruth , prediction , record = None , result = None ) : self . value = self . avg ( prediction )",Compute and store metric value
"def mostLikely ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] mostLikelyOutcome = None maxProbability = 0 for prediction , probability in pred . items ( ) : if probability > maxProbability : mostLikelyOutcome = prediction maxProbability = probability return mostLikelyOutcome",Helper function to return a scalar value representing the most likely outcome given a probability distribution
"def expValue ( self , pred ) : if len ( pred ) == 1 : return pred . keys ( ) [ 0 ] return sum ( [ x * p for x , p in pred . items ( ) ] )",Helper function to return a scalar value representing the expected value of a probability distribution
"def encode ( self , inputData ) : output = numpy . zeros ( ( self . getWidth ( ) , ) , dtype = defaultDtype ) self . encodeIntoArray ( inputData , output ) return output",Convenience wrapper for : meth : . encodeIntoArray .
"def getScalarNames ( self , parentFieldName = '' ) : names = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subNames = encoder . getScalarNames ( parentFieldName = name ) if parentFieldName != '' : subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] names . extend ( subNames ) else : if parentFieldName != '' : names . append ( parentFieldName ) else : names . append ( self . name ) return names",Return the field names for each of the scalar values returned by getScalars .
"def getDecoderOutputFieldTypes ( self ) : if hasattr ( self , '_flattenedFieldTypeList' ) and self . _flattenedFieldTypeList is not None : return self . _flattenedFieldTypeList fieldTypes = [ ] for ( name , encoder , offset ) in self . encoders : subTypes = encoder . getDecoderOutputFieldTypes ( ) fieldTypes . extend ( subTypes ) self . _flattenedFieldTypeList = fieldTypes return fieldTypes",Returns a sequence of field types corresponding to the elements in the decoded output field array . The types are defined by : class : ~nupic . data . field_meta . FieldMetaType .
"def _getInputValue ( self , obj , fieldName ) : if isinstance ( obj , dict ) : if not fieldName in obj : knownFields = "", "" . join ( key for key in obj . keys ( ) if not key . startswith ( ""_"" ) ) raise ValueError ( ""Unknown field name '%s' in input record. Known fields are '%s'.\n"" ""This could be because input headers are mislabeled, or because "" ""input data rows do not contain a value for '%s'."" % ( fieldName , knownFields , fieldName ) ) return obj [ fieldName ] else : return getattr ( obj , fieldName )",Gets the value of a given field from the input record
"def getEncoderList ( self ) : if hasattr ( self , '_flattenedEncoderList' ) and self . _flattenedEncoderList is not None : return self . _flattenedEncoderList encoders = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : subEncoders = encoder . getEncoderList ( ) encoders . extend ( subEncoders ) else : encoders . append ( self ) self . _flattenedEncoderList = encoders return encoders",: return : a reference to each sub - encoder in this encoder . They are returned in the same order as they are for : meth : . getScalarNames and : meth : . getScalars .
"def getScalars ( self , inputData ) : retVals = numpy . array ( [ ] ) if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : values = encoder . getScalars ( self . _getInputValue ( inputData , name ) ) retVals = numpy . hstack ( ( retVals , values ) ) else : retVals = numpy . hstack ( ( retVals , inputData ) ) return retVals",Returns a numpy array containing the sub - field scalar value ( s ) for each sub - field of the inputData . To get the associated field names for each of the scalar values call : meth : . getScalarNames () .
"def getEncodedValues ( self , inputData ) : retVals = [ ] if self . encoders is not None : for name , encoders , offset in self . encoders : values = encoders . getEncodedValues ( self . _getInputValue ( inputData , name ) ) if _isSequence ( values ) : retVals . extend ( values ) else : retVals . append ( values ) else : if _isSequence ( inputData ) : retVals . extend ( inputData ) else : retVals . append ( inputData ) return tuple ( retVals )",Returns the input in the same format as is returned by : meth : . topDownCompute . For most encoder types this is the same as the input data . For instance for scalar and category types this corresponds to the numeric and string values respectively from the inputs . For datetime encoders this returns the list of scalars for each of the sub - fields ( timeOfDay dayOfWeek etc . )
"def getBucketIndices ( self , inputData ) : retVals = [ ] if self . encoders is not None : for ( name , encoder , offset ) in self . encoders : values = encoder . getBucketIndices ( self . _getInputValue ( inputData , name ) ) retVals . extend ( values ) else : assert False , ""Should be implemented in base classes that are not "" ""containers for other encoders"" return retVals",Returns an array containing the sub - field bucket indices for each sub - field of the inputData . To get the associated field names for each of the buckets call : meth : . getScalarNames .
"def scalarsToStr ( self , scalarValues , scalarNames = None ) : if scalarNames is None : scalarNames = self . getScalarNames ( ) desc = '' for ( name , value ) in zip ( scalarNames , scalarValues ) : if len ( desc ) > 0 : desc += "", %s:%.2f"" % ( name , value ) else : desc += ""%s:%.2f"" % ( name , value ) return desc",Return a pretty print string representing the return values from : meth : . getScalars and : meth : . getScalarNames .
"def getFieldDescription ( self , fieldName ) : description = self . getDescription ( ) + [ ( ""end"" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if ( name == fieldName ) : break if i >= len ( description ) - 1 : raise RuntimeError ( ""Field name %s not found in this encoder"" % fieldName ) return ( offset , description [ i + 1 ] [ 1 ] - offset )",Return the offset and length of a given field within the encoded output .
"def encodedBitDescription ( self , bitOffset , formatted = False ) : ( prevFieldName , prevFieldOffset ) = ( None , None ) description = self . getDescription ( ) for i in xrange ( len ( description ) ) : ( name , offset ) = description [ i ] if formatted : offset = offset + i if bitOffset == offset - 1 : prevFieldName = ""separator"" prevFieldOffset = bitOffset break if bitOffset < offset : break ( prevFieldName , prevFieldOffset ) = ( name , offset ) width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) if prevFieldOffset is None or bitOffset > self . getWidth ( ) : raise IndexError ( ""Bit is outside of allowable range: [0 - %d]"" % width ) return ( prevFieldName , bitOffset - prevFieldOffset )",Return a description of the given bit in the encoded output . This will include the field name and the offset within the field .
"def pprintHeader ( self , prefix = """" ) : print prefix , description = self . getDescription ( ) + [ ( ""end"" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) - 1 ) : name = description [ i ] [ 0 ] width = description [ i + 1 ] [ 1 ] - description [ i ] [ 1 ] formatStr = ""%%-%ds |"" % width if len ( name ) > width : pname = name [ 0 : width ] else : pname = name print formatStr % pname , print print prefix , ""-"" * ( self . getWidth ( ) + ( len ( description ) - 1 ) * 3 - 1 )",Pretty - print a header that labels the sub - fields of the encoded output . This can be used in conjuction with : meth : . pprint .
"def pprint ( self , output , prefix = """" ) : print prefix , description = self . getDescription ( ) + [ ( ""end"" , self . getWidth ( ) ) ] for i in xrange ( len ( description ) - 1 ) : offset = description [ i ] [ 1 ] nextoffset = description [ i + 1 ] [ 1 ] print ""%s |"" % bitsToString ( output [ offset : nextoffset ] ) , print",Pretty - print the encoded output using ascii art .
"def decode ( self , encoded , parentFieldName = '' ) : fieldsDict = dict ( ) fieldsOrder = [ ] if parentFieldName == '' : parentName = self . name else : parentName = ""%s.%s"" % ( parentFieldName , self . name ) if self . encoders is not None : for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] ( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , parentFieldName = parentName ) fieldsDict . update ( subFieldsDict ) fieldsOrder . extend ( subFieldsOrder ) return ( fieldsDict , fieldsOrder )",Takes an encoded output and does its best to work backwards and generate the input that would have generated it .
"def decodedToStr ( self , decodeResults ) : ( fieldsDict , fieldsOrder ) = decodeResults desc = '' for fieldName in fieldsOrder : ( ranges , rangesStr ) = fieldsDict [ fieldName ] if len ( desc ) > 0 : desc += "", %s:"" % ( fieldName ) else : desc += ""%s:"" % ( fieldName ) desc += ""[%s]"" % ( rangesStr ) return desc",Return a pretty print string representing the return value from : meth : . decode .
"def getBucketInfo ( self , buckets ) : if self . encoders is None : raise RuntimeError ( ""Must be implemented in sub-class"" ) retVals = [ ] bucketOffset = 0 for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if encoder . encoders is not None : nextBucketOffset = bucketOffset + len ( encoder . encoders ) else : nextBucketOffset = bucketOffset + 1 bucketIndices = buckets [ bucketOffset : nextBucketOffset ] values = encoder . getBucketInfo ( bucketIndices ) retVals . extend ( values ) bucketOffset = nextBucketOffset return retVals",Returns a list of : class : . EncoderResult namedtuples describing the inputs for each sub - field that correspond to the bucket indices passed in buckets . To get the associated field names for each of the values call : meth : . getScalarNames .
"def topDownCompute ( self , encoded ) : if self . encoders is None : raise RuntimeError ( ""Must be implemented in sub-class"" ) retVals = [ ] for i in xrange ( len ( self . encoders ) ) : ( name , encoder , offset ) = self . encoders [ i ] if i < len ( self . encoders ) - 1 : nextOffset = self . encoders [ i + 1 ] [ 2 ] else : nextOffset = self . width fieldOutput = encoded [ offset : nextOffset ] values = encoder . topDownCompute ( fieldOutput ) if _isSequence ( values ) : retVals . extend ( values ) else : retVals . append ( values ) return retVals",Returns a list of : class : . EncoderResult namedtuples describing the top - down best guess inputs for each sub - field given the encoded output . These are the values which are most likely to generate the given encoded output . To get the associated field names for each of the values call : meth : . getScalarNames .
"def closenessScores ( self , expValues , actValues , fractional = True ) : if self . encoders is None : err = abs ( expValues [ 0 ] - actValues [ 0 ] ) if fractional : denom = max ( expValues [ 0 ] , actValues [ 0 ] ) if denom == 0 : denom = 1.0 closeness = 1.0 - float ( err ) / denom if closeness < 0 : closeness = 0 else : closeness = err return numpy . array ( [ closeness ] ) scalarIdx = 0 retVals = numpy . array ( [ ] ) for ( name , encoder , offset ) in self . encoders : values = encoder . closenessScores ( expValues [ scalarIdx : ] , actValues [ scalarIdx : ] , fractional = fractional ) scalarIdx += len ( values ) retVals = numpy . hstack ( ( retVals , values ) ) return retVals",Compute closeness scores between the expected scalar value ( s ) and actual scalar value ( s ) . The expected scalar values are typically those obtained from the : meth : . getScalars method . The actual scalar values are typically those returned from : meth : . topDownCompute .
"def POST ( self , name ) : global g_models data = json . loads ( web . data ( ) ) modelParams = data [ ""modelParams"" ] predictedFieldName = data [ ""predictedFieldName"" ] if name in g_models . keys ( ) : raise web . badrequest ( ""Model with name <%s> already exists"" % name ) model = ModelFactory . create ( modelParams ) model . enableInference ( { 'predictedField' : predictedFieldName } ) g_models [ name ] = model return json . dumps ( { ""success"" : name } )",/ models / { name }
"def POST ( self , name ) : global g_models data = json . loads ( web . data ( ) ) data [ ""timestamp"" ] = datetime . datetime . strptime ( data [ ""timestamp"" ] , ""%m/%d/%y %H:%M"" ) if name not in g_models . keys ( ) : raise web . notfound ( ""Model with name <%s> does not exist."" % name ) modelResult = g_models [ name ] . run ( data ) predictionNumber = modelResult . predictionNumber anomalyScore = modelResult . inferences [ ""anomalyScore"" ] return json . dumps ( { ""predictionNumber"" : predictionNumber , ""anomalyScore"" : anomalyScore } )",/ models / { name } / run
"def analyzeOverlaps ( activeCoincsFile , encodingsFile , dataset ) : lines = activeCoincsFile . readlines ( ) inputs = encodingsFile . readlines ( ) w = len ( inputs [ 0 ] . split ( ' ' ) ) - 1 patterns = set ( [ ] ) encodings = set ( [ ] ) coincs = [ ] reUsedCoincs = [ ] firstLine = inputs [ 0 ] . split ( ' ' ) size = int ( firstLine . pop ( 0 ) ) spOutput = np . zeros ( ( len ( lines ) , 40 ) ) inputBits = np . zeros ( ( len ( lines ) , w ) ) print 'Total n:' , size print 'Total number of records in the file:' , len ( lines ) , '\n' print 'w:' , w count = 0 for x in xrange ( len ( lines ) ) : inputSpace = [ ] spBUout = [ int ( z ) for z in lines [ x ] . split ( ' ' ) ] spBUout . pop ( 0 ) temp = set ( spBUout ) spOutput [ x ] = spBUout input = [ int ( z ) for z in inputs [ x ] . split ( ' ' ) ] input . pop ( 0 ) tempInput = set ( input ) inputBits [ x ] = input for m in xrange ( size ) : if m in tempInput : inputSpace . append ( m ) else : inputSpace . append ( '|' ) repeatedBits = tempInput . intersection ( encodings ) reUsed = temp . intersection ( patterns ) if len ( reUsed ) == 0 : coincs . append ( ( count , temp , repeatedBits , inputSpace , tempInput ) ) else : reUsedCoincs . append ( ( count , temp , repeatedBits , inputSpace , tempInput ) ) patterns = patterns . union ( temp ) encodings = encodings . union ( tempInput ) count += 1 overlap = { } overlapVal = 0 seen = [ ] seen = ( printOverlaps ( coincs , coincs , seen ) ) print len ( seen ) , 'sets of 40 cells' seen = printOverlaps ( reUsedCoincs , coincs , seen ) Summ = [ ] for z in coincs : c = 0 for y in reUsedCoincs : c += len ( z [ 1 ] . intersection ( y [ 1 ] ) ) Summ . append ( c ) print 'Sum: ' , Summ for m in xrange ( 3 ) : displayLimit = min ( 51 , len ( spOutput [ m * 200 : ] ) ) if displayLimit > 0 : drawFile ( dataset , np . zeros ( [ len ( inputBits [ : ( m + 1 ) * displayLimit ] ) , len ( inputBits [ : ( m + 1 ) * displayLimit ] ) ] ) , inputBits [ : ( m + 1 ) * displayLimit ] , spOutput [ : ( m + 1 ) * displayLimit ] , w , m + 1 ) else : print 'No more records to display' pyl . show ( )",Mirror Image Visualization : Shows the encoding space juxtaposed against the coincidence space . The encoding space is the bottom - up sensory encoding and the coincidence space depicts the corresponding activation of coincidences in the SP . Hence the mirror image visualization is a visual depiction of the mapping of SP cells to the input representations . Note : * The files spBUOut and sensorBUOut are assumed to be in the output format used for LPF experiment outputs . * BU outputs for some sample datasets are provided . Specify the name of the dataset as an option while running this script .
"def drawFile ( dataset , matrix , patterns , cells , w , fnum ) : score = 0 count = 0 assert len ( patterns ) == len ( cells ) for p in xrange ( len ( patterns ) - 1 ) : matrix [ p + 1 : , p ] = [ len ( set ( patterns [ p ] ) . intersection ( set ( q ) ) ) * 100 / w for q in patterns [ p + 1 : ] ] matrix [ p , p + 1 : ] = [ len ( set ( cells [ p ] ) . intersection ( set ( r ) ) ) * 5 / 2 for r in cells [ p + 1 : ] ] score += sum ( abs ( np . array ( matrix [ p + 1 : , p ] ) - np . array ( matrix [ p , p + 1 : ] ) ) ) count += len ( matrix [ p + 1 : , p ] ) print 'Score' , score / count fig = pyl . figure ( figsize = ( 10 , 10 ) , num = fnum ) pyl . matshow ( matrix , fignum = fnum ) pyl . colorbar ( ) pyl . title ( 'Coincidence Space' , verticalalignment = 'top' , fontsize = 12 ) pyl . xlabel ( 'The Mirror Image Visualization for ' + dataset , fontsize = 17 ) pyl . ylabel ( 'Encoding space' , fontsize = 12 )",The similarity of two patterns in the bit - encoding space is displayed alongside their similarity in the sp - coinc space .
"def printOverlaps ( comparedTo , coincs , seen ) : inputOverlap = 0 cellOverlap = 0 for y in comparedTo : closestInputs = [ ] closestCells = [ ] if len ( seen ) > 0 : inputOverlap = max ( [ len ( seen [ m ] [ 1 ] . intersection ( y [ 4 ] ) ) for m in xrange ( len ( seen ) ) ] ) cellOverlap = max ( [ len ( seen [ m ] [ 0 ] . intersection ( y [ 1 ] ) ) for m in xrange ( len ( seen ) ) ] ) for m in xrange ( len ( seen ) ) : if len ( seen [ m ] [ 1 ] . intersection ( y [ 4 ] ) ) == inputOverlap : closestInputs . append ( seen [ m ] [ 2 ] ) if len ( seen [ m ] [ 0 ] . intersection ( y [ 1 ] ) ) == cellOverlap : closestCells . append ( seen [ m ] [ 2 ] ) seen . append ( ( y [ 1 ] , y [ 4 ] , y [ 0 ] ) ) print 'Pattern' , y [ 0 ] + 1 , ':' , ' ' . join ( str ( len ( z [ 1 ] . intersection ( y [ 1 ] ) ) ) . rjust ( 2 ) for z in coincs ) , 'input overlap:' , inputOverlap , ';' , len ( closestInputs ) , 'closest encodings:' , ',' . join ( str ( m + 1 ) for m in closestInputs ) . ljust ( 15 ) , 'cell overlap:' , cellOverlap , ';' , len ( closestCells ) , 'closest set(s):' , ',' . join ( str ( m + 1 ) for m in closestCells ) return seen",Compare the results and return True if success False if failure Parameters : -------------------------------------------------------------------- coincs : Which cells are we comparing? comparedTo : The set of 40 cells we being compared to ( they have no overlap with seen ) seen : Which of the cells we are comparing to have already been encountered . This helps glue together the unique and reused coincs
"def createInput ( self ) : print ""-"" * 70 + ""Creating a random input vector"" + ""-"" * 70 self . inputArray [ 0 : ] = 0 for i in range ( self . inputSize ) : self . inputArray [ i ] = random . randrange ( 2 )",create a random input vector
"def run ( self ) : print ""-"" * 80 + ""Computing the SDR"" + ""-"" * 80 self . sp . compute ( self . inputArray , True , self . activeArray ) print self . activeArray . nonzero ( )",Run the spatial pooler with the input vector
"def addNoise ( self , noiseLevel ) : for _ in range ( int ( noiseLevel * self . inputSize ) ) : randomPosition = int ( random . random ( ) * self . inputSize ) if self . inputArray [ randomPosition ] == 1 : self . inputArray [ randomPosition ] = 0 else : self . inputArray [ randomPosition ] = 1",Flip the value of 10% of input bits ( add noise )
"def _labeledInput ( activeInputs , cellsPerCol = 32 ) : if cellsPerCol == 0 : cellsPerCol = 1 cols = activeInputs . size / cellsPerCol activeInputs = activeInputs . reshape ( cols , cellsPerCol ) ( cols , cellIdxs ) = activeInputs . nonzero ( ) if len ( cols ) == 0 : return ""NONE"" items = [ ""(%d): "" % ( len ( cols ) ) ] prevCol = - 1 for ( col , cellIdx ) in zip ( cols , cellIdxs ) : if col != prevCol : if prevCol != - 1 : items . append ( ""] "" ) items . append ( ""Col %d: ["" % col ) prevCol = col items . append ( ""%d,"" % cellIdx ) items . append ( ""]"" ) return "" "" . join ( items )",Print the list of [ column cellIdx ] indices for each of the active cells in activeInputs .
"def clear ( self ) : self . _Memory = None self . _numPatterns = 0 self . _M = None self . _categoryList = [ ] self . _partitionIdList = [ ] self . _partitionIdMap = { } self . _finishedLearning = False self . _iterationIdx = - 1 if self . maxStoredPatterns > 0 : assert self . useSparseMemory , ( ""Fixed capacity KNN is implemented only "" ""in the sparse memory mode"" ) self . fixedCapacity = True self . _categoryRecencyList = [ ] else : self . fixedCapacity = False self . _protoSizes = None self . _s = None self . _vt = None self . _nc = None self . _mean = None self . _specificIndexTraining = False self . _nextTrainingIndices = None",Clears the state of the KNNClassifier .
"def prototypeSetCategory ( self , idToCategorize , newCategory ) : if idToCategorize not in self . _categoryRecencyList : return recordIndex = self . _categoryRecencyList . index ( idToCategorize ) self . _categoryList [ recordIndex ] = newCategory",Allows ids to be assigned a category and subsequently enables users to use :
"def removeIds ( self , idsToRemove ) : rowsToRemove = [ k for k , rowID in enumerate ( self . _categoryRecencyList ) if rowID in idsToRemove ] self . _removeRows ( rowsToRemove )",There are two caveats . First this is a potentially slow operation . Second pattern indices will shift if patterns before them are removed .
"def removeCategory ( self , categoryToRemove ) : removedRows = 0 if self . _Memory is None : return removedRows catToRemove = float ( categoryToRemove ) rowsToRemove = [ k for k , catID in enumerate ( self . _categoryList ) if catID == catToRemove ] self . _removeRows ( rowsToRemove ) assert catToRemove not in self . _categoryList",There are two caveats . First this is a potentially slow operation . Second pattern indices will shift if patterns before them are removed .
"def _removeRows ( self , rowsToRemove ) : removalArray = numpy . array ( rowsToRemove ) self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , removalArray ) . tolist ( ) if self . fixedCapacity : self . _categoryRecencyList = numpy . delete ( numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) for row in reversed ( rowsToRemove ) : self . _partitionIdList . pop ( row ) self . _rebuildPartitionIdMap ( self . _partitionIdList ) if self . useSparseMemory : for rowIndex in rowsToRemove [ : : - 1 ] : self . _Memory . deleteRow ( rowIndex ) else : self . _M = numpy . delete ( self . _M , removalArray , 0 ) numRemoved = len ( rowsToRemove ) numRowsExpected = self . _numPatterns - numRemoved if self . useSparseMemory : if self . _Memory is not None : assert self . _Memory . nRows ( ) == numRowsExpected else : assert self . _M . shape [ 0 ] == numRowsExpected assert len ( self . _categoryList ) == numRowsExpected self . _numPatterns -= numRemoved return numRemoved",A list of row indices to remove . There are two caveats . First this is a potentially slow operation . Second pattern indices will shift if patterns before them are removed .
"def learn ( self , inputPattern , inputCategory , partitionId = None , isSparse = 0 , rowID = None ) : if self . verbosity >= 1 : print ""%s learn:"" % g_debugPrefix print ""  category:"" , int ( inputCategory ) print ""  active inputs:"" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) if isSparse > 0 : assert all ( inputPattern [ i ] <= inputPattern [ i + 1 ] for i in xrange ( len ( inputPattern ) - 1 ) ) , ""Sparse inputPattern must be sorted."" assert all ( bit < isSparse for bit in inputPattern ) , ( ""Sparse inputPattern must not index outside the dense "" ""representation's bounds."" ) if rowID is None : rowID = self . _iterationIdx if not self . useSparseMemory : assert self . cellsPerCol == 0 , ""not implemented for dense vectors"" if isSparse > 0 : denseInput = numpy . zeros ( isSparse ) denseInput [ inputPattern ] = 1.0 inputPattern = denseInput if self . _specificIndexTraining and not self . _nextTrainingIndices : return self . _numPatterns if self . _Memory is None : inputWidth = len ( inputPattern ) self . _Memory = numpy . zeros ( ( 100 , inputWidth ) ) self . _numPatterns = 0 self . _M = self . _Memory [ : self . _numPatterns ] addRow = True if self . _vt is not None : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) if self . distThreshold > 0 : dist = self . _calcDistance ( inputPattern ) minDist = dist . min ( ) addRow = ( minDist >= self . distThreshold ) if addRow : self . _protoSizes = None if self . _numPatterns == self . _Memory . shape [ 0 ] : self . _doubleMemoryNumRows ( ) if not self . _specificIndexTraining : self . _Memory [ self . _numPatterns ] = inputPattern self . _numPatterns += 1 self . _categoryList . append ( int ( inputCategory ) ) else : vectorIndex = self . _nextTrainingIndices . pop ( 0 ) while vectorIndex >= self . _Memory . shape [ 0 ] : self . _doubleMemoryNumRows ( ) self . _Memory [ vectorIndex ] = inputPattern self . _numPatterns = max ( self . _numPatterns , vectorIndex + 1 ) if vectorIndex >= len ( self . _categoryList ) : self . _categoryList += [ - 1 ] * ( vectorIndex - len ( self . _categoryList ) + 1 ) self . _categoryList [ vectorIndex ] = int ( inputCategory ) self . _M = self . _Memory [ 0 : self . _numPatterns ] self . _addPartitionId ( self . _numPatterns - 1 , partitionId ) else : if isSparse > 0 and ( self . _vt is not None or self . distThreshold > 0 or self . numSVDDims is not None or self . numSVDSamples > 0 or self . numWinners > 0 ) : denseInput = numpy . zeros ( isSparse ) denseInput [ inputPattern ] = 1.0 inputPattern = denseInput isSparse = 0 if isSparse > 0 : inputWidth = isSparse else : inputWidth = len ( inputPattern ) if self . _Memory is None : self . _Memory = NearestNeighbor ( 0 , inputWidth ) if self . _vt is not None : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) if isSparse == 0 : thresholdedInput = self . _sparsifyVector ( inputPattern , True ) addRow = True if self . cellsPerCol >= 1 : burstingCols = thresholdedInput . reshape ( - 1 , self . cellsPerCol ) . min ( axis = 1 ) . nonzero ( ) [ 0 ] for col in burstingCols : thresholdedInput [ ( col * self . cellsPerCol ) + 1 : ( col * self . cellsPerCol ) + self . cellsPerCol ] = 0 if self . _Memory . nRows ( ) > 0 : dist = None if self . replaceDuplicates : dist = self . _calcDistance ( thresholdedInput , distanceNorm = 1 ) if dist . min ( ) == 0 : rowIdx = dist . argmin ( ) self . _categoryList [ rowIdx ] = int ( inputCategory ) if self . fixedCapacity : self . _categoryRecencyList [ rowIdx ] = rowID addRow = False if self . distThreshold > 0 : if dist is None or self . distanceNorm != 1 : dist = self . _calcDistance ( thresholdedInput ) minDist = dist . min ( ) addRow = ( minDist >= self . distThreshold ) if not addRow : if self . fixedCapacity : rowIdx = dist . argmin ( ) self . _categoryRecencyList [ rowIdx ] = rowID if addRow and self . minSparsity > 0.0 : if isSparse == 0 : sparsity = ( float ( len ( thresholdedInput . nonzero ( ) [ 0 ] ) ) / len ( thresholdedInput ) ) else : sparsity = float ( len ( inputPattern ) ) / isSparse if sparsity < self . minSparsity : addRow = False if addRow : self . _protoSizes = None if isSparse == 0 : self . _Memory . addRow ( thresholdedInput ) else : self . _Memory . addRowNZ ( inputPattern , [ 1 ] * len ( inputPattern ) ) self . _numPatterns += 1 self . _categoryList . append ( int ( inputCategory ) ) self . _addPartitionId ( self . _numPatterns - 1 , partitionId ) if self . fixedCapacity : self . _categoryRecencyList . append ( rowID ) if self . _numPatterns > self . maxStoredPatterns and self . maxStoredPatterns > 0 : leastRecentlyUsedPattern = numpy . argmin ( self . _categoryRecencyList ) self . _Memory . deleteRow ( leastRecentlyUsedPattern ) self . _categoryList . pop ( leastRecentlyUsedPattern ) self . _categoryRecencyList . pop ( leastRecentlyUsedPattern ) self . _numPatterns -= 1 if self . numSVDDims is not None and self . numSVDSamples > 0 and self . _numPatterns == self . numSVDSamples : self . computeSVD ( ) return self . _numPatterns",Train the classifier to associate specified input pattern with a particular category .
"def getOverlaps ( self , inputPattern ) : assert self . useSparseMemory , ""Not implemented yet for dense storage"" overlaps = self . _Memory . rightVecSumAtNZ ( inputPattern ) return ( overlaps , self . _categoryList )",Return the degree of overlap between an input pattern and each category stored in the classifier . The overlap is computed by computing :
"def getDistances ( self , inputPattern ) : dist = self . _getDistances ( inputPattern ) return ( dist , self . _categoryList )",Return the distances between the input pattern and all other stored patterns .
"def infer ( self , inputPattern , computeScores = True , overCategories = True , partitionId = None ) : sparsity = 0.0 if self . minSparsity > 0.0 : sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / len ( inputPattern ) ) if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : winner = None inferenceResult = numpy . zeros ( 1 ) dist = numpy . ones ( 1 ) categoryDist = numpy . ones ( 1 ) else : maxCategoryIdx = max ( self . _categoryList ) inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) dist = self . _getDistances ( inputPattern , partitionId = partitionId ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) if self . exact : exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] if len ( exactMatches ) > 0 : for i in exactMatches [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ i ] ] += 1.0 else : sorted = dist . argsort ( ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 if inferenceResult . any ( ) : winner = inferenceResult . argmax ( ) inferenceResult /= inferenceResult . sum ( ) else : winner = None categoryDist = min_score_per_category ( maxCategoryIdx , self . _categoryList , dist ) categoryDist . clip ( 0 , 1.0 , categoryDist ) if self . verbosity >= 1 : print ""%s infer:"" % ( g_debugPrefix ) print ""  active inputs:"" , _labeledInput ( inputPattern , cellsPerCol = self . cellsPerCol ) print ""  winner category:"" , winner print ""  pct neighbors of each category:"" , inferenceResult print ""  dist of each prototype:"" , dist print ""  dist of each category:"" , categoryDist result = ( winner , inferenceResult , dist , categoryDist ) return result",Finds the category that best matches the input pattern . Returns the winning category index as well as a distribution over all categories .
"def getClosest ( self , inputPattern , topKCategories = 3 ) : inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) for j in sorted [ : min ( self . k , validVectorCount ) ] : inferenceResult [ self . _categoryList [ j ] ] += 1.0 winner = inferenceResult . argmax ( ) topNCats = [ ] for i in range ( topKCategories ) : topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) return winner , dist , topNCats",Returns the index of the pattern that is closest to inputPattern the distances of all patterns to inputPattern and the indices of the k closest categories .
"def closestTrainingPattern ( self , inputPattern , cat ) : dist = self . _getDistances ( inputPattern ) sorted = dist . argsort ( ) for patIdx in sorted : patternCat = self . _categoryList [ patIdx ] if patternCat == cat : if self . useSparseMemory : closestPattern = self . _Memory . getRow ( int ( patIdx ) ) else : closestPattern = self . _M [ patIdx ] return closestPattern return None",Returns the closest training pattern to inputPattern that belongs to category cat .
"def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : if cat is not None : assert idx is None idx = self . _categoryList . index ( cat ) if not self . useSparseMemory : pattern = self . _Memory [ idx ] if sparseBinaryForm : pattern = pattern . nonzero ( ) [ 0 ] else : ( nz , values ) = self . _Memory . rowNonZeros ( idx ) if not sparseBinaryForm : pattern = numpy . zeros ( self . _Memory . nCols ( ) ) numpy . put ( pattern , nz , 1 ) else : pattern = nz return pattern",Gets a training pattern either by index or category number .
"def getPartitionId ( self , i ) : if ( i < 0 ) or ( i >= self . _numPatterns ) : raise RuntimeError ( ""index out of bounds"" ) partitionId = self . _partitionIdList [ i ] if partitionId == numpy . inf : return None else : return partitionId",Gets the partition id given an index .
"def _addPartitionId ( self , index , partitionId = None ) : if partitionId is None : self . _partitionIdList . append ( numpy . inf ) else : self . _partitionIdList . append ( partitionId ) indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( index ) self . _partitionIdMap [ partitionId ] = indices",Adds partition id for pattern index
"def _rebuildPartitionIdMap ( self , partitionIdList ) : self . _partitionIdMap = { } for row , partitionId in enumerate ( partitionIdList ) : indices = self . _partitionIdMap . get ( partitionId , [ ] ) indices . append ( row ) self . _partitionIdMap [ partitionId ] = indices",Rebuilds the partition Id map using the given partitionIdList
"def _calcDistance ( self , inputPattern , distanceNorm = None ) : if distanceNorm is None : distanceNorm = self . distanceNorm if self . useSparseMemory : if self . _protoSizes is None : self . _protoSizes = self . _Memory . rowSums ( ) overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) inputPatternSum = inputPattern . sum ( ) if self . distanceMethod == ""rawOverlap"" : dist = inputPattern . sum ( ) - overlapsWithProtos elif self . distanceMethod == ""pctOverlapOfInput"" : dist = inputPatternSum - overlapsWithProtos if inputPatternSum > 0 : dist /= inputPatternSum elif self . distanceMethod == ""pctOverlapOfProto"" : overlapsWithProtos /= self . _protoSizes dist = 1.0 - overlapsWithProtos elif self . distanceMethod == ""pctOverlapOfLarger"" : maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) if maxVal . all ( ) > 0 : overlapsWithProtos /= maxVal dist = 1.0 - overlapsWithProtos elif self . distanceMethod == ""norm"" : dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) distMax = dist . max ( ) if distMax > 0 : dist /= distMax else : raise RuntimeError ( ""Unimplemented distance method %s"" % self . distanceMethod ) else : if self . distanceMethod == ""norm"" : dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) dist = dist . sum ( 1 ) dist = numpy . power ( dist , 1.0 / self . distanceNorm ) dist /= dist . max ( ) else : raise RuntimeError ( ""Not implemented yet for dense storage...."" ) return dist",Calculate the distances from inputPattern to all stored patterns . All distances are between 0 . 0 and 1 . 0
"def _getDistances ( self , inputPattern , partitionId = None ) : if not self . _finishedLearning : self . finishLearning ( ) self . _finishedLearning = True if self . _vt is not None and len ( self . _vt ) > 0 : inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) sparseInput = self . _sparsifyVector ( inputPattern ) dist = self . _calcDistance ( sparseInput ) if self . _specificIndexTraining : dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf if partitionId is not None : dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf return dist",Return the distances from inputPattern to all stored patterns .
"def computeSVD ( self , numSVDSamples = 0 , finalize = True ) : if numSVDSamples == 0 : numSVDSamples = self . _numPatterns if not self . useSparseMemory : self . _a = self . _Memory [ : self . _numPatterns ] else : self . _a = self . _Memory . toDense ( ) [ : self . _numPatterns ] self . _mean = numpy . mean ( self . _a , axis = 0 ) self . _a -= self . _mean u , self . _s , self . _vt = numpy . linalg . svd ( self . _a [ : numSVDSamples ] ) if finalize : self . _finalizeSVD ( ) return self . _s",Compute the singular value decomposition ( SVD ) . The SVD is a factorization of a real or complex matrix . It factors the matrix a as u * np . diag ( s ) * v where u and v are unitary and s is a 1 - d array of a s singular values .
"def getAdaptiveSVDDims ( self , singularValues , fractionOfMax = 0.001 ) : v = singularValues / singularValues [ 0 ] idx = numpy . where ( v < fractionOfMax ) [ 0 ] if len ( idx ) : print ""Number of PCA dimensions chosen: "" , idx [ 0 ] , ""out of "" , len ( v ) return idx [ 0 ] else : print ""Number of PCA dimensions chosen: "" , len ( v ) - 1 , ""out of "" , len ( v ) return len ( v ) - 1",Compute the number of eigenvectors ( singularValues ) to keep .
"def _finalizeSVD ( self , numSVDDims = None ) : if numSVDDims is not None : self . numSVDDims = numSVDDims if self . numSVDDims == ""adaptive"" : if self . fractionOfMax is not None : self . numSVDDims = self . getAdaptiveSVDDims ( self . _s , self . fractionOfMax ) else : self . numSVDDims = self . getAdaptiveSVDDims ( self . _s ) if self . _vt . shape [ 0 ] < self . numSVDDims : print ""******************************************************************"" print ( ""Warning: The requested number of PCA dimensions is more than "" ""the number of pattern dimensions."" ) print ""Setting numSVDDims = "" , self . _vt . shape [ 0 ] print ""******************************************************************"" self . numSVDDims = self . _vt . shape [ 0 ] self . _vt = self . _vt [ : self . numSVDDims ] if len ( self . _vt ) == 0 : return self . _Memory = numpy . zeros ( ( self . _numPatterns , self . numSVDDims ) ) self . _M = self . _Memory self . useSparseMemory = False for i in range ( self . _numPatterns ) : self . _Memory [ i ] = numpy . dot ( self . _vt , self . _a [ i ] ) self . _a = None",Called by finalizeLearning () . This will project all the patterns onto the SVD eigenvectors . : param numSVDDims : ( int ) number of egeinvectors used for projection . : return :
"def remapCategories ( self , mapping ) : categoryArray = numpy . array ( self . _categoryList ) newCategoryArray = numpy . zeros ( categoryArray . shape [ 0 ] ) newCategoryArray . fill ( - 1 ) for i in xrange ( len ( mapping ) ) : newCategoryArray [ categoryArray == i ] = mapping [ i ] self . _categoryList = list ( newCategoryArray )",Change the category indices .
"def setCategoryOfVectors ( self , vectorIndices , categoryIndices ) : if not hasattr ( vectorIndices , ""__iter__"" ) : vectorIndices = [ vectorIndices ] categoryIndices = [ categoryIndices ] elif not hasattr ( categoryIndices , ""__iter__"" ) : categoryIndices = [ categoryIndices ] * len ( vectorIndices ) for i in xrange ( len ( vectorIndices ) ) : vectorIndex = vectorIndices [ i ] categoryIndex = categoryIndices [ i ] if vectorIndex < len ( self . _categoryList ) : self . _categoryList [ vectorIndex ] = categoryIndex",Change the category associated with this vector ( s ) .
"def getNextRecord ( self ) : allFiltersHaveEnoughData = False while not allFiltersHaveEnoughData : data = self . dataSource . getNextRecordDict ( ) if not data : raise StopIteration ( ""Datasource has no more data"" ) if ""_reset"" not in data : data [ ""_reset"" ] = 0 if ""_sequenceId"" not in data : data [ ""_sequenceId"" ] = 0 if ""_category"" not in data : data [ ""_category"" ] = [ None ] data , allFiltersHaveEnoughData = self . applyFilters ( data ) self . lastRecord = data return data",Get the next record to encode . Includes getting a record from the dataSource and applying filters . If the filters request more data from the dataSource continue to get data from the dataSource until all filters are satisfied . This method is separate from : meth : . RecordSensor . compute so that we can use a standalone : class : . RecordSensor to get filtered data .
"def applyFilters ( self , data ) : if self . verbosity > 0 : print ""RecordSensor got data: %s"" % data allFiltersHaveEnoughData = True if len ( self . preEncodingFilters ) > 0 : originalReset = data [ '_reset' ] actualReset = originalReset for f in self . preEncodingFilters : filterHasEnoughData = f . process ( data ) allFiltersHaveEnoughData = ( allFiltersHaveEnoughData and filterHasEnoughData ) actualReset = actualReset or data [ '_reset' ] data [ '_reset' ] = originalReset data [ '_reset' ] = actualReset return data , allFiltersHaveEnoughData",Apply pre - encoding filters . These filters may modify or add data . If a filter needs another record ( e . g . a delta filter ) it will request another record by returning False and the current record will be skipped ( but will still be given to all filters ) .
"def populateCategoriesOut ( self , categories , output ) : if categories [ 0 ] is None : output [ : ] = - 1 else : for i , cat in enumerate ( categories [ : len ( output ) ] ) : output [ i ] = cat output [ len ( categories ) : ] = - 1",Populate the output array with the category indices . .. note :: Non - categories are represented with - 1 .
"def compute ( self , inputs , outputs ) : if not self . topDownMode : data = self . getNextRecord ( ) reset = data [ ""_reset"" ] sequenceId = data [ ""_sequenceId"" ] categories = data [ ""_category"" ] self . encoder . encodeIntoArray ( data , outputs [ ""dataOut"" ] ) if self . predictedField is not None and self . predictedField != ""vector"" : allEncoders = list ( self . encoder . encoders ) if self . disabledEncoder is not None : allEncoders . extend ( self . disabledEncoder . encoders ) encoders = [ e for e in allEncoders if e [ 0 ] == self . predictedField ] if len ( encoders ) == 0 : raise ValueError ( ""There is no encoder for set for the predicted "" ""field: %s"" % self . predictedField ) else : encoder = encoders [ 0 ] [ 1 ] actualValue = data [ self . predictedField ] outputs [ ""bucketIdxOut"" ] [ : ] = encoder . getBucketIndices ( actualValue ) if isinstance ( actualValue , str ) : outputs [ ""actValueOut"" ] [ : ] = encoder . getBucketIndices ( actualValue ) else : outputs [ ""actValueOut"" ] [ : ] = actualValue outputs [ ""sourceOut"" ] [ : ] = self . encoder . getScalars ( data ) self . _outputValues [ ""sourceOut"" ] = self . encoder . getEncodedValues ( data ) encoders = self . encoder . getEncoderList ( ) prevOffset = 0 sourceEncodings = [ ] bitData = outputs [ ""dataOut"" ] for encoder in encoders : nextOffset = prevOffset + encoder . getWidth ( ) sourceEncodings . append ( bitData [ prevOffset : nextOffset ] ) prevOffset = nextOffset self . _outputValues [ 'sourceEncodings' ] = sourceEncodings for filter in self . postEncodingFilters : filter . process ( encoder = self . encoder , data = outputs [ 'dataOut' ] ) outputs [ 'resetOut' ] [ 0 ] = reset outputs [ 'sequenceIdOut' ] [ 0 ] = sequenceId self . populateCategoriesOut ( categories , outputs [ 'categoryOut' ] ) if self . verbosity >= 1 : if self . _iterNum == 0 : self . encoder . pprintHeader ( prefix = ""sensor:"" ) if reset : print ""RESET - sequenceID:%d"" % sequenceId if self . verbosity >= 2 : print if self . verbosity >= 1 : self . encoder . pprint ( outputs [ ""dataOut"" ] , prefix = ""%7d:"" % ( self . _iterNum ) ) scalarValues = self . encoder . getScalars ( data ) nz = outputs [ ""dataOut"" ] . nonzero ( ) [ 0 ] print ""     nz: (%d)"" % ( len ( nz ) ) , nz print ""  encIn:"" , self . encoder . scalarsToStr ( scalarValues ) if self . verbosity >= 2 : print ""   data:"" , str ( data ) if self . verbosity >= 3 : decoded = self . encoder . decode ( outputs [ ""dataOut"" ] ) print ""decoded:"" , self . encoder . decodedToStr ( decoded ) self . _iterNum += 1 else : spatialTopDownIn = inputs [ 'spatialTopDownIn' ] spatialTopDownOut = self . encoder . topDownCompute ( spatialTopDownIn ) values = [ elem . value for elem in spatialTopDownOut ] scalars = [ elem . scalar for elem in spatialTopDownOut ] encodings = [ elem . encoding for elem in spatialTopDownOut ] self . _outputValues [ 'spatialTopDownOut' ] = values outputs [ 'spatialTopDownOut' ] [ : ] = numpy . array ( scalars ) self . _outputValues [ 'spatialTopDownEncodings' ] = encodings temporalTopDownIn = inputs [ 'temporalTopDownIn' ] temporalTopDownOut = self . encoder . topDownCompute ( temporalTopDownIn ) values = [ elem . value for elem in temporalTopDownOut ] scalars = [ elem . scalar for elem in temporalTopDownOut ] encodings = [ elem . encoding for elem in temporalTopDownOut ] self . _outputValues [ 'temporalTopDownOut' ] = values outputs [ 'temporalTopDownOut' ] [ : ] = numpy . array ( scalars ) self . _outputValues [ 'temporalTopDownEncodings' ] = encodings assert len ( spatialTopDownOut ) == len ( temporalTopDownOut ) , ( ""Error: spatialTopDownOut and temporalTopDownOut should be the same "" ""size"" )",Get a record from the dataSource and encode it . Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . compute .
"def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : encoders = self . encoder . getEncoderList ( ) types = self . encoder . getDecoderOutputFieldTypes ( ) for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : spatialData = spatialOutput [ i ] temporalData = temporalOutput [ i ] if type != FieldMetaType . integer and type != FieldMetaType . float : spatialData = encoder . getScalars ( spatialData ) [ 0 ] temporalData = encoder . getScalars ( temporalData ) [ 0 ] assert isinstance ( spatialData , ( float , int ) ) assert isinstance ( temporalData , ( float , int ) ) output [ 'spatialTopDownOut' ] [ i ] = spatialData output [ 'temporalTopDownOut' ] [ i ] = temporalData",Converts all of the non - numeric fields from spatialOutput and temporalOutput into their scalar equivalents and records them in the output dictionary .
"def getOutputElementCount ( self , name ) : if name == ""resetOut"" : print ( ""WARNING: getOutputElementCount should not have been called with "" ""resetOut"" ) return 1 elif name == ""sequenceIdOut"" : print ( ""WARNING: getOutputElementCount should not have been called with "" ""sequenceIdOut"" ) return 1 elif name == ""dataOut"" : if self . encoder is None : raise Exception ( ""NuPIC requested output element count for 'dataOut' "" ""on a RecordSensor node, but the encoder has not "" ""been set"" ) return self . encoder . getWidth ( ) elif name == ""sourceOut"" : if self . encoder is None : raise Exception ( ""NuPIC requested output element count for 'sourceOut' "" ""on a RecordSensor node, "" ""but the encoder has not been set"" ) return len ( self . encoder . getDescription ( ) ) elif name == ""bucketIdxOut"" : return 1 elif name == ""actValueOut"" : return 1 elif name == ""categoryOut"" : return self . numCategories elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : if self . encoder is None : raise Exception ( ""NuPIC requested output element count for 'sourceOut' "" ""on a RecordSensor node, "" ""but the encoder has not been set"" ) return len ( self . encoder . getDescription ( ) ) else : raise Exception ( ""Unknown output %s"" % name )",Computes the width of dataOut .
"def setParameter ( self , parameterName , index , parameterValue ) : if parameterName == 'topDownMode' : self . topDownMode = parameterValue elif parameterName == 'predictedField' : self . predictedField = parameterValue else : raise Exception ( 'Unknown parameter: ' + parameterName )",Set the value of a Spec parameter . Most parameters are handled automatically by PyRegion s parameter set mechanism . The ones that need special treatment are explicitly handled here .
"def writeToProto ( self , proto ) : self . encoder . write ( proto . encoder ) if self . disabledEncoder is not None : self . disabledEncoder . write ( proto . disabledEncoder ) proto . topDownMode = int ( self . topDownMode ) proto . verbosity = self . verbosity proto . numCategories = self . numCategories",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . writeToProto .
"def readFromProto ( cls , proto ) : instance = cls ( ) instance . encoder = MultiEncoder . read ( proto . encoder ) if proto . disabledEncoder is not None : instance . disabledEncoder = MultiEncoder . read ( proto . disabledEncoder ) instance . topDownMode = bool ( proto . topDownMode ) instance . verbosity = proto . verbosity instance . numCategories = proto . numCategories return instance",Overrides : meth : nupic . bindings . regions . PyRegion . PyRegion . readFromProto .
"def computeAccuracy ( model , size , top ) : accuracy = [ ] filename = os . path . join ( os . path . dirname ( __file__ ) , ""msnbc990928.zip"" ) with zipfile . ZipFile ( filename ) as archive : with archive . open ( ""msnbc990928.seq"" ) as datafile : for _ in xrange ( 7 ) : next ( datafile ) for _ in xrange ( LEARNING_RECORDS ) : next ( datafile ) for _ in xrange ( size ) : pages = readUserSession ( datafile ) model . resetSequenceStates ( ) for i in xrange ( len ( pages ) - 1 ) : result = model . run ( { ""page"" : pages [ i ] } ) inferences = result . inferences [ ""multiStepPredictions"" ] [ 1 ] predicted = sorted ( inferences . items ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : top ] accuracy . append ( 1 if pages [ i + 1 ] in zip ( * predicted ) [ 0 ] else 0 ) return np . mean ( accuracy )",Compute prediction accuracy by checking if the next page in the sequence is within the top N predictions calculated by the model Args : model : HTM model size : Sample size top : top N predictions to use
def readUserSession ( datafile ) : for line in datafile : pages = line . split ( ) total = len ( pages ) if total < 2 : continue if total > 500 : continue return [ PAGE_CATEGORIES [ int ( i ) - 1 ] for i in pages ] return [ ],Reads the user session record from the file s cursor position Args : datafile : Data file whose cursor points at the beginning of the record
"def rewind ( self ) : super ( FileRecordStream , self ) . rewind ( ) self . close ( ) self . _file = open ( self . _filename , self . _mode ) self . _reader = csv . reader ( self . _file , dialect = ""excel"" ) self . _reader . next ( ) self . _reader . next ( ) self . _reader . next ( ) self . _recordCount = 0",Put us back at the beginning of the file again .
"def getNextRecord ( self , useCache = True ) : assert self . _file is not None assert self . _mode == self . _FILE_READ_MODE try : line = self . _reader . next ( ) except StopIteration : if self . rewindAtEOF : if self . _recordCount == 0 : raise Exception ( ""The source configured to reset at EOF but "" ""'%s' appears to be empty"" % self . _filename ) self . rewind ( ) line = self . _reader . next ( ) else : return None self . _recordCount += 1 record = [ ] for i , f in enumerate ( line ) : if f in self . _missingValues : record . append ( SENTINEL_VALUE_FOR_MISSING_DATA ) else : record . append ( self . _adapters [ i ] ( f ) ) return record",Returns next available data record from the file .
"def appendRecord ( self , record ) : assert self . _file is not None assert self . _mode == self . _FILE_WRITE_MODE assert isinstance ( record , ( list , tuple ) ) , ""unexpected record type: "" + repr ( type ( record ) ) assert len ( record ) == self . _fieldCount , ""len(record): %s, fieldCount: %s"" % ( len ( record ) , self . _fieldCount ) if self . _recordCount == 0 : names , types , specials = zip ( * self . getFields ( ) ) for line in names , types , specials : self . _writer . writerow ( line ) self . _updateSequenceInfo ( record ) line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] self . _writer . writerow ( line ) self . _recordCount += 1",Saves the record in the underlying csv file .
"def appendRecords ( self , records , progressCB = None ) : for record in records : self . appendRecord ( record ) if progressCB is not None : progressCB ( )",Saves multiple records in the underlying storage .
"def getBookmark ( self ) : if self . _write and self . _recordCount == 0 : return None rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , currentRow = self . _recordCount ) return json . dumps ( rowDict )",Gets a bookmark or anchor to the current position .
"def seekFromEnd ( self , numRecords ) : self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) return self . getBookmark ( )",Seeks to numRecords from the end and returns a bookmark to the new position .
"def getStats ( self ) : if self . _stats == None : assert self . _mode == self . _FILE_READ_MODE inFile = open ( self . _filename , self . _FILE_READ_MODE ) reader = csv . reader ( inFile , dialect = ""excel"" ) names = [ n . strip ( ) for n in reader . next ( ) ] types = [ t . strip ( ) for t in reader . next ( ) ] reader . next ( ) self . _stats = dict ( ) self . _stats [ 'min' ] = [ ] self . _stats [ 'max' ] = [ ] for i in xrange ( len ( names ) ) : self . _stats [ 'min' ] . append ( None ) self . _stats [ 'max' ] . append ( None ) while True : try : line = reader . next ( ) for i , f in enumerate ( line ) : if ( len ( types ) > i and types [ i ] in [ FieldMetaType . integer , FieldMetaType . float ] and f not in self . _missingValues ) : value = self . _adapters [ i ] ( f ) if self . _stats [ 'max' ] [ i ] == None or self . _stats [ 'max' ] [ i ] < value : self . _stats [ 'max' ] [ i ] = value if self . _stats [ 'min' ] [ i ] == None or self . _stats [ 'min' ] [ i ] > value : self . _stats [ 'min' ] [ i ] = value except StopIteration : break return self . _stats",Parse the file using dedicated reader and collect fields stats . Never called if user of : class : ~ . FileRecordStream does not invoke : meth : ~ . FileRecordStream . getStats method .
"def _updateSequenceInfo ( self , r ) : newSequence = False sequenceId = ( r [ self . _sequenceIdIdx ] if self . _sequenceIdIdx is not None else None ) if sequenceId != self . _currSequence : if sequenceId in self . _sequences : raise Exception ( 'Broken sequence: %s, record: %s' % ( sequenceId , r ) ) self . _sequences . add ( self . _currSequence ) self . _currSequence = sequenceId if self . _resetIdx : assert r [ self . _resetIdx ] == 1 newSequence = True else : reset = False if self . _resetIdx : reset = r [ self . _resetIdx ] if reset == 1 : newSequence = True if not newSequence : if self . _timeStampIdx and self . _currTime is not None : t = r [ self . _timeStampIdx ] if t < self . _currTime : raise Exception ( 'No time travel. Early timestamp for record: %s' % r ) if self . _timeStampIdx : self . _currTime = r [ self . _timeStampIdx ]",Keep track of sequence and make sure time goes forward
"def _getStartRow ( self , bookmark ) : bookMarkDict = json . loads ( bookmark ) realpath = os . path . realpath ( self . _filename ) bookMarkFile = bookMarkDict . get ( 'filepath' , None ) if bookMarkFile != realpath : print ( ""Ignoring bookmark due to mismatch between File's "" ""filename realpath vs. bookmark; realpath: %r; bookmark: %r"" ) % ( realpath , bookMarkDict ) return 0 else : return bookMarkDict [ 'currentRow' ]",Extracts start row from the bookmark information
"def _getTotalLineCount ( self ) : if self . _mode == self . _FILE_WRITE_MODE : self . _file . flush ( ) return sum ( 1 for line in open ( self . _filename , self . _FILE_READ_MODE ) )",Returns : count of ALL lines in dataset including header lines
def getDataRowCount ( self ) : numLines = self . _getTotalLineCount ( ) if numLines == 0 : assert self . _mode == self . _FILE_WRITE_MODE and self . _recordCount == 0 numDataRows = 0 else : numDataRows = numLines - self . _NUM_HEADER_ROWS assert numDataRows >= 0 return numDataRows,: returns : ( int ) count of data rows in dataset ( excluding header lines )
"def runIoThroughNupic ( inputData , model , gymName , plot ) : inputFile = open ( inputData , ""rb"" ) csvReader = csv . reader ( inputFile ) csvReader . next ( ) csvReader . next ( ) csvReader . next ( ) shifter = InferenceShifter ( ) if plot : output = nupic_anomaly_output . NuPICPlotOutput ( gymName ) else : output = nupic_anomaly_output . NuPICFileOutput ( gymName ) counter = 0 for row in csvReader : counter += 1 if ( counter % 100 == 0 ) : print ""Read %i lines..."" % counter timestamp = datetime . datetime . strptime ( row [ 0 ] , DATE_FORMAT ) consumption = float ( row [ 1 ] ) result = model . run ( { ""timestamp"" : timestamp , ""kw_energy_consumption"" : consumption } ) if plot : result = shifter . shift ( result ) prediction = result . inferences [ ""multiStepBestPredictions"" ] [ 1 ] anomalyScore = result . inferences [ ""anomalyScore"" ] output . write ( timestamp , consumption , prediction , anomalyScore ) inputFile . close ( ) output . close ( )",Handles looping over the input data and passing each row into the given model object as well as extracting the result object and passing it into an output handler . : param inputData : file path to input data CSV : param model : OPF Model object : param gymName : Gym name used for output handler naming : param plot : Whether to use matplotlib or not . If false uses file output .
"def topDownCompute ( self , encoded ) : if self . _prevAbsolute == None or self . _prevDelta == None : return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ] ret = self . _adaptiveScalarEnc . topDownCompute ( encoded ) if self . _prevAbsolute != None : ret = [ EncoderResult ( value = ret [ 0 ] . value + self . _prevAbsolute , scalar = ret [ 0 ] . scalar + self . _prevAbsolute , encoding = ret [ 0 ] . encoding ) ] return ret",[ ScalarEncoder class method override ]
def isTemporal ( inferenceElement ) : if InferenceElement . __temporalInferenceElements is None : InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) return inferenceElement in InferenceElement . __temporalInferenceElements,Returns True if the inference from this timestep is predicted the input for the NEXT timestep .
"def getTemporalDelay ( inferenceElement , key = None ) : if inferenceElement in ( InferenceElement . prediction , InferenceElement . encodings ) : return 1 if inferenceElement in ( InferenceElement . anomalyScore , InferenceElement . anomalyLabel , InferenceElement . classification , InferenceElement . classConfidences ) : return 0 if inferenceElement in ( InferenceElement . multiStepPredictions , InferenceElement . multiStepBestPredictions ) : return int ( key ) return 0",Returns the number of records that elapse between when an inference is made and when the corresponding input record will appear . For example a multistep prediction for 3 timesteps out will have a delay of 3
"def getMaxDelay ( inferences ) : maxDelay = 0 for inferenceElement , inference in inferences . iteritems ( ) : if isinstance ( inference , dict ) : for key in inference . iterkeys ( ) : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement , key ) , maxDelay ) else : maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement ) , maxDelay ) return maxDelay",Returns the maximum delay for the InferenceElements in the inference dictionary
"def isTemporal ( inferenceType ) : if InferenceType . __temporalInferenceTypes is None : InferenceType . __temporalInferenceTypes = set ( [ InferenceType . TemporalNextStep , InferenceType . TemporalClassification , InferenceType . TemporalAnomaly , InferenceType . TemporalMultiStep , InferenceType . NontemporalMultiStep ] ) return inferenceType in InferenceType . __temporalInferenceTypes",Returns True if the inference type is temporal i . e . requires a temporal memory in the network .
"def Enum ( * args , * * kwargs ) : def getLabel ( cls , val ) : """""" Get a string label for the current value of the enum """""" return cls . __labels [ val ] def validate ( cls , val ) : """""" Returns True if val is a valid value for the enumeration """""" return val in cls . __values def getValues ( cls ) : """""" Returns a list of all the possible values for this enum """""" return list ( cls . __values ) def getLabels ( cls ) : """""" Returns a list of all possible labels for this enum """""" return list ( cls . __labels . values ( ) ) def getValue ( cls , label ) : """""" Returns value given a label """""" return cls . __labels [ label ] for arg in list ( args ) + kwargs . keys ( ) : if type ( arg ) is not str : raise TypeError ( ""Enum arg {0} must be a string"" . format ( arg ) ) if not __isidentifier ( arg ) : raise ValueError ( ""Invalid enum value '{0}'. "" ""'{0}' is not a valid identifier"" . format ( arg ) ) kwargs . update ( zip ( args , args ) ) newType = type ( ""Enum"" , ( object , ) , kwargs ) newType . __labels = dict ( ( v , k ) for k , v in kwargs . iteritems ( ) ) newType . __values = set ( newType . __labels . keys ( ) ) newType . getLabel = functools . partial ( getLabel , newType ) newType . validate = functools . partial ( validate , newType ) newType . getValues = functools . partial ( getValues , newType ) newType . getLabels = functools . partial ( getLabels , newType ) newType . getValue = functools . partial ( getValue , newType ) return newType",Utility function for creating enumerations in python
"def makeDirectoryFromAbsolutePath ( absDirPath ) : assert os . path . isabs ( absDirPath ) try : os . makedirs ( absDirPath ) except OSError , e : if e . errno != os . errno . EEXIST : raise return absDirPath",Makes directory for the given directory path with default permissions . If the directory already exists it is treated as success .
"def _readConfigFile ( cls , filename , path = None ) : outputProperties = dict ( ) if path is None : filePath = cls . findConfigFile ( filename ) else : filePath = os . path . join ( path , filename ) try : if filePath is not None : try : _getLoggerBase ( ) . debug ( ""Loading config file: %s"" , filePath ) with open ( filePath , 'r' ) as inp : contents = inp . read ( ) except Exception : raise RuntimeError ( ""Expected configuration file at %s"" % filePath ) else : try : contents = resource_string ( ""nupic.support"" , filename ) except Exception as resourceException : if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : contents = '<configuration/>' else : raise resourceException elements = ElementTree . XML ( contents ) if elements . tag != 'configuration' : raise RuntimeError ( ""Expected top-level element to be 'configuration' "" ""but got '%s'"" % ( elements . tag ) ) propertyElements = elements . findall ( './property' ) for propertyItem in propertyElements : propInfo = dict ( ) propertyAttributes = list ( propertyItem ) for propertyAttribute in propertyAttributes : propInfo [ propertyAttribute . tag ] = propertyAttribute . text name = propInfo . get ( 'name' , None ) if 'value' in propInfo and propInfo [ 'value' ] is None : value = '' else : value = propInfo . get ( 'value' , None ) if value is None : if 'novalue' in propInfo : continue else : raise RuntimeError ( ""Missing 'value' element within the property "" ""element: => %s "" % ( str ( propInfo ) ) ) restOfValue = value value = '' while True : pos = restOfValue . find ( '${env.' ) if pos == - 1 : value += restOfValue break value += restOfValue [ 0 : pos ] varTailPos = restOfValue . find ( '}' , pos ) if varTailPos == - 1 : raise RuntimeError ( ""Trailing environment variable tag delimiter '}'"" "" not found in %r"" % ( restOfValue ) ) varname = restOfValue [ pos + 6 : varTailPos ] if varname not in os . environ : raise RuntimeError ( ""Attempting to use the value of the environment"" "" variable %r, which is not defined"" % ( varname ) ) envVarValue = os . environ [ varname ] value += envVarValue restOfValue = restOfValue [ varTailPos + 1 : ] if name is None : raise RuntimeError ( ""Missing 'name' element within following property "" ""element:\n => %s "" % ( str ( propInfo ) ) ) propInfo [ 'value' ] = value outputProperties [ name ] = propInfo return outputProperties except Exception : _getLoggerBase ( ) . exception ( ""Error while parsing configuration file: %s."" , filePath ) raise",Parse the given XML file and return a dict describing the file .
"def setCustomProperties ( cls , properties ) : _getLogger ( ) . info ( ""Setting custom configuration properties=%r; caller=%r"" , properties , traceback . format_stack ( ) ) _CustomConfigurationFileWrapper . edit ( properties ) for propertyName , value in properties . iteritems ( ) : cls . set ( propertyName , value )",Set multiple custom properties and persist them to the custom configuration store .
"def clear ( cls ) : super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = False )",Clear all configuration properties from in - memory cache but do NOT alter the custom configuration file . Used in unit - testing .
"def resetCustomConfig ( cls ) : _getLogger ( ) . info ( ""Resetting all custom configuration properties; "" ""caller=%r"" , traceback . format_stack ( ) ) super ( Configuration , cls ) . clear ( ) _CustomConfigurationFileWrapper . clear ( persistent = True )",Clear all custom configuration settings and delete the persistent custom configuration store .
"def clear ( cls , persistent = False ) : if persistent : try : os . unlink ( cls . getPath ( ) ) except OSError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( ""Error %s while trying to remove dynamic "" ""configuration file: %s"" , e . errno , cls . getPath ( ) ) raise cls . _path = None",If persistent is True delete the temporary file
"def getCustomDict ( cls ) : if not os . path . exists ( cls . getPath ( ) ) : return dict ( ) properties = Configuration . _readConfigFile ( os . path . basename ( cls . getPath ( ) ) , os . path . dirname ( cls . getPath ( ) ) ) values = dict ( ) for propName in properties : if 'value' in properties [ propName ] : values [ propName ] = properties [ propName ] [ 'value' ] return values",Returns a dict of all temporary values in custom configuration file
"def edit ( cls , properties ) : copyOfProperties = copy ( properties ) configFilePath = cls . getPath ( ) try : with open ( configFilePath , 'r' ) as fp : contents = fp . read ( ) except IOError , e : if e . errno != errno . ENOENT : _getLogger ( ) . exception ( ""Error %s reading custom configuration store "" ""from %s, while editing properties %s."" , e . errno , configFilePath , properties ) raise contents = '<configuration/>' try : elements = ElementTree . XML ( contents ) ElementTree . tostring ( elements ) except Exception , e : msg = ""File contents of custom configuration is corrupt.  File "" ""location: %s; Contents: '%s'. Original Error (%s): %s."" % ( configFilePath , contents , type ( e ) , e ) _getLogger ( ) . exception ( msg ) raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] if elements . tag != 'configuration' : e = ""Expected top-level element to be 'configuration' but got '%s'"" % ( elements . tag ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyItem in elements . findall ( './property' ) : propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) name = propInfo [ 'name' ] if name in copyOfProperties : foundValues = propertyItem . findall ( './value' ) if len ( foundValues ) > 0 : foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) if not copyOfProperties : break else : e = ""Property %s missing value tag."" % ( name , ) _getLogger ( ) . error ( e ) raise RuntimeError ( e ) for propertyName , value in copyOfProperties . iteritems ( ) : newProp = ElementTree . Element ( 'property' ) nameTag = ElementTree . Element ( 'name' ) nameTag . text = propertyName newProp . append ( nameTag ) valueTag = ElementTree . Element ( 'value' ) valueTag . text = str ( value ) newProp . append ( valueTag ) elements . append ( newProp ) try : makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) with open ( configFilePath , 'w' ) as fp : fp . write ( ElementTree . tostring ( elements ) ) except Exception , e : _getLogger ( ) . exception ( ""Error while saving custom configuration "" ""properties %s in %s."" , properties , configFilePath ) raise",Edits the XML configuration file with the parameters specified by properties
"def _setPath ( cls ) : cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , cls . customFileName )",Sets the path of the custom configuration file
"def getState ( self ) : varStates = dict ( ) for varName , var in self . permuteVars . iteritems ( ) : varStates [ varName ] = var . getState ( ) return dict ( id = self . particleId , genIdx = self . genIdx , swarmId = self . swarmId , varStates = varStates )",Get the particle state as a dict . This is enough information to instantiate this particle on another worker .
"def initStateFrom ( self , particleId , particleState , newBest ) : if newBest : ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) else : bestResult = bestPosition = None varStates = particleState [ 'varStates' ] for varName in varStates . keys ( ) : varState = copy . deepcopy ( varStates [ varName ] ) if newBest : varState [ 'bestResult' ] = bestResult if bestPosition is not None : varState [ 'bestPosition' ] = bestPosition [ varName ] self . permuteVars [ varName ] . setState ( varState )",Init all of our variable positions velocities and optionally the best result and best position from the given particle .
"def copyVarStatesFrom ( self , particleState , varNames ) : allowedToMove = True for varName in particleState [ 'varStates' ] : if varName in varNames : if varName not in self . permuteVars : continue state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) state [ '_position' ] = state [ 'position' ] state [ 'bestPosition' ] = state [ 'position' ] if not allowedToMove : state [ 'velocity' ] = 0 self . permuteVars [ varName ] . setState ( state ) if allowedToMove : self . permuteVars [ varName ] . resetVelocity ( self . _rng )",Copy specific variables from particleState into this particle .
"def getPosition ( self ) : result = dict ( ) for ( varName , value ) in self . permuteVars . iteritems ( ) : result [ varName ] = value . getPosition ( ) return result",Return the position of this particle . This returns a dict () of key value pairs where each key is the name of the flattened permutation variable and the value is its chosen value .
"def getPositionFromState ( pState ) : result = dict ( ) for ( varName , value ) in pState [ 'varStates' ] . iteritems ( ) : result [ varName ] = value [ 'position' ] return result",Return the position of a particle given its state dict .
"def agitate ( self ) : for ( varName , var ) in self . permuteVars . iteritems ( ) : var . agitate ( ) self . newPosition ( )",Agitate this particle so that it is likely to go to a new position . Every time agitate is called the particle is jiggled an even greater amount .
"def newPosition ( self , whichVars = None ) : globalBestPosition = None if self . _hsObj . _speculativeParticles : genIdx = self . genIdx else : genIdx = self . genIdx - 1 if genIdx >= 0 : ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , genIdx ) if bestModelId is not None : ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( bestModelId ) globalBestPosition = Particle . getPositionFromState ( particleState ) for ( varName , var ) in self . permuteVars . iteritems ( ) : if whichVars is not None and varName not in whichVars : continue if globalBestPosition is None : var . newPosition ( None , self . _rng ) else : var . newPosition ( globalBestPosition [ varName ] , self . _rng ) position = self . getPosition ( ) if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : msg = StringIO . StringIO ( ) print >> msg , ""New particle position: \n%s"" % ( pprint . pformat ( position , indent = 4 ) ) print >> msg , ""Particle variables:"" for ( varName , var ) in self . permuteVars . iteritems ( ) : print >> msg , ""  %s: %s"" % ( varName , str ( var ) ) self . logger . debug ( msg . getvalue ( ) ) msg . close ( ) return position",Choose a new position based on results obtained so far from all other particles .
"def propose ( self , current , r ) : stay = ( r . uniform ( 0 , 1 ) < self . kernel ) if stay : logKernel = numpy . log ( self . kernel ) return current , logKernel , logKernel else : curIndex = self . keyMap [ current ] ri = r . randint ( 0 , self . nKeys - 1 ) logKernel = numpy . log ( 1.0 - self . kernel ) lp = logKernel + self . logp if ri < curIndex : return self . keys [ ri ] , lp , lp else : return self . keys [ ri + 1 ] , lp , lp",Generates a random sample from the discrete probability distribution and returns its value the log of the probability of sampling that value and the log of the probability of sampling the current value ( passed in ) .
"def propose ( self , current , r ) : curLambda = current + self . offset x , logProb = PoissonDistribution ( curLambda ) . sample ( r ) logBackward = PoissonDistribution ( x + self . offset ) . logDensity ( current ) return x , logProb , logBackward",Generates a random sample from the Poisson probability distribution with with location and scale parameter equal to the current value ( passed in ) . Returns the value of the random sample the log of the probability of sampling that value and the log of the probability of sampling the current value if the roles of the new sample and the current sample were reversed ( the log of the backward proposal probability ) .
def __getLogger ( cls ) : if cls . __logger is None : cls . __logger = opf_utils . initLogger ( cls ) return cls . __logger,Get the logger for this object .
"def create ( modelConfig , logLevel = logging . ERROR ) : logger = ModelFactory . __getLogger ( ) logger . setLevel ( logLevel ) logger . debug ( ""ModelFactory returning Model from dict: %s"" , modelConfig ) modelClass = None if modelConfig [ 'model' ] == ""HTMPrediction"" : modelClass = HTMPredictionModel elif modelConfig [ 'model' ] == ""TwoGram"" : modelClass = TwoGramModel elif modelConfig [ 'model' ] == ""PreviousValue"" : modelClass = PreviousValueModel else : raise Exception ( ""ModelFactory received unsupported Model type: %s"" % modelConfig [ 'model' ] ) return modelClass ( * * modelConfig [ 'modelParams' ] )",Create a new model instance given a description dictionary .
"def loadFromCheckpoint ( savedModelDir , newSerialization = False ) : if newSerialization : return HTMPredictionModel . readFromCheckpoint ( savedModelDir ) else : return Model . load ( savedModelDir )",Load saved model .
"def compute ( self , activeColumns , learn = True ) : self . activateCells ( sorted ( activeColumns ) , learn ) self . activateDendrites ( learn )",Perform one time step of the Temporal Memory algorithm .
"def activateCells ( self , activeColumns , learn = True ) : prevActiveCells = self . activeCells prevWinnerCells = self . winnerCells self . activeCells = [ ] self . winnerCells = [ ] segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) identity = lambda x : x for columnData in groupby2 ( activeColumns , identity , self . activeSegments , segToCol , self . matchingSegments , segToCol ) : ( column , activeColumns , columnActiveSegments , columnMatchingSegments ) = columnData if activeColumns is not None : if columnActiveSegments is not None : cellsToAdd = self . activatePredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells += cellsToAdd else : ( cellsToAdd , winnerCell ) = self . burstColumn ( column , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) self . activeCells += cellsToAdd self . winnerCells . append ( winnerCell ) else : if learn : self . punishPredictedColumn ( column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells )",Calculate the active cells using the current active columns and dendrite segments . Grow and reinforce synapses .
"def activateDendrites ( self , learn = True ) : ( numActiveConnected , numActivePotential ) = self . connections . computeActivity ( self . activeCells , self . connectedPermanence ) activeSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActiveConnected ) ) if numActiveConnected [ i ] >= self . activationThreshold ) matchingSegments = ( self . connections . segmentForFlatIdx ( i ) for i in xrange ( len ( numActivePotential ) ) if numActivePotential [ i ] >= self . minThreshold ) self . activeSegments = sorted ( activeSegments , key = self . connections . segmentPositionSortKey ) self . matchingSegments = sorted ( matchingSegments , key = self . connections . segmentPositionSortKey ) self . numActiveConnectedSynapsesForSegment = numActiveConnected self . numActivePotentialSynapsesForSegment = numActivePotential if learn : for segment in self . activeSegments : self . lastUsedIterationForSegment [ segment . flatIdx ] = self . iteration self . iteration += 1",Calculate dendrite segment activity using the current active cells .
def reset ( self ) : self . activeCells = [ ] self . winnerCells = [ ] self . activeSegments = [ ] self . matchingSegments = [ ],Indicates the start of a new sequence . Clears any predictions and makes sure synapses don t grow to the currently active cells in the next time step .
"def activatePredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells , learn ) : return self . _activatePredictedColumn ( self . connections , self . _random , columnActiveSegments , prevActiveCells , prevWinnerCells , self . numActivePotentialSynapsesForSegment , self . maxNewSynapseCount , self . initialPermanence , self . permanenceIncrement , self . permanenceDecrement , self . maxSynapsesPerSegment , learn )",Determines which cells in a predicted column should be added to winner cells list and learns on the segments that correctly predicted this column .
"def punishPredictedColumn ( self , column , columnActiveSegments , columnMatchingSegments , prevActiveCells , prevWinnerCells ) : self . _punishPredictedColumn ( self . connections , columnMatchingSegments , prevActiveCells , self . predictedSegmentDecrement )",Punishes the Segments that incorrectly predicted a column to be active .
"def createSegment ( self , cell ) : return self . _createSegment ( self . connections , self . lastUsedIterationForSegment , cell , self . iteration , self . maxSegmentsPerCell )",Create a : class : ~nupic . algorithms . connections . Segment on the specified cell . This method calls : meth : ~nupic . algorithms . connections . Connections . createSegment on the underlying : class : ~nupic . algorithms . connections . Connections and it does some extra bookkeeping . Unit tests should call this method and not : meth : ~nupic . algorithms . connections . Connections . createSegment .
"def _activatePredictedColumn ( cls , connections , random , columnActiveSegments , prevActiveCells , prevWinnerCells , numActivePotentialSynapsesForSegment , maxNewSynapseCount , initialPermanence , permanenceIncrement , permanenceDecrement , maxSynapsesPerSegment , learn ) : cellsToAdd = [ ] previousCell = None for segment in columnActiveSegments : if segment . cell != previousCell : cellsToAdd . append ( segment . cell ) previousCell = segment . cell if learn : cls . _adaptSegment ( connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) active = numActivePotentialSynapsesForSegment [ segment . flatIdx ] nGrowDesired = maxNewSynapseCount - active if nGrowDesired > 0 : cls . _growSynapses ( connections , random , segment , nGrowDesired , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) return cellsToAdd",: param connections : ( Object ) Connections for the TM . Gets mutated .
"def _burstColumn ( cls , connections , random , lastUsedIterationForSegment , column , columnMatchingSegments , prevActiveCells , prevWinnerCells , cellsForColumn , numActivePotentialSynapsesForSegment , iteration , maxNewSynapseCount , initialPermanence , permanenceIncrement , permanenceDecrement , maxSegmentsPerCell , maxSynapsesPerSegment , learn ) : if columnMatchingSegments is not None : numActive = lambda s : numActivePotentialSynapsesForSegment [ s . flatIdx ] bestMatchingSegment = max ( columnMatchingSegments , key = numActive ) winnerCell = bestMatchingSegment . cell if learn : cls . _adaptSegment ( connections , bestMatchingSegment , prevActiveCells , permanenceIncrement , permanenceDecrement ) nGrowDesired = maxNewSynapseCount - numActive ( bestMatchingSegment ) if nGrowDesired > 0 : cls . _growSynapses ( connections , random , bestMatchingSegment , nGrowDesired , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) else : winnerCell = cls . _leastUsedCell ( random , cellsForColumn , connections ) if learn : nGrowExact = min ( maxNewSynapseCount , len ( prevWinnerCells ) ) if nGrowExact > 0 : segment = cls . _createSegment ( connections , lastUsedIterationForSegment , winnerCell , iteration , maxSegmentsPerCell ) cls . _growSynapses ( connections , random , segment , nGrowExact , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) return cellsForColumn , winnerCell",: param connections : ( Object ) Connections for the TM . Gets mutated .
"def _punishPredictedColumn ( cls , connections , columnMatchingSegments , prevActiveCells , predictedSegmentDecrement ) : if predictedSegmentDecrement > 0.0 and columnMatchingSegments is not None : for segment in columnMatchingSegments : cls . _adaptSegment ( connections , segment , prevActiveCells , - predictedSegmentDecrement , 0.0 )",: param connections : ( Object ) Connections for the TM . Gets mutated .
"def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , iteration , maxSegmentsPerCell ) : while connections . numSegments ( cell ) >= maxSegmentsPerCell : leastRecentlyUsedSegment = min ( connections . segmentsForCell ( cell ) , key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) connections . destroySegment ( leastRecentlyUsedSegment ) segment = connections . createSegment ( cell ) if segment . flatIdx == len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment . append ( iteration ) elif segment . flatIdx < len ( lastUsedIterationForSegment ) : lastUsedIterationForSegment [ segment . flatIdx ] = iteration else : raise AssertionError ( ""All segments should be created with the TM createSegment method."" ) return segment",Create a segment on the connections enforcing the maxSegmentsPerCell parameter .
"def _destroyMinPermanenceSynapses ( cls , connections , random , segment , nDestroy , excludeCells ) : destroyCandidates = sorted ( ( synapse for synapse in connections . synapsesForSegment ( segment ) if synapse . presynapticCell not in excludeCells ) , key = lambda s : s . _ordinal ) for _ in xrange ( nDestroy ) : if len ( destroyCandidates ) == 0 : break minSynapse = None minPermanence = float ( ""inf"" ) for synapse in destroyCandidates : if synapse . permanence < minPermanence - EPSILON : minSynapse = synapse minPermanence = synapse . permanence connections . destroySynapse ( minSynapse ) destroyCandidates . remove ( minSynapse )",Destroy nDestroy synapses on the specified segment but don t destroy synapses to the excludeCells .
"def _leastUsedCell ( cls , random , cells , connections ) : leastUsedCells = [ ] minNumSegments = float ( ""inf"" ) for cell in cells : numSegments = connections . numSegments ( cell ) if numSegments < minNumSegments : minNumSegments = numSegments leastUsedCells = [ ] if numSegments == minNumSegments : leastUsedCells . append ( cell ) i = random . getUInt32 ( len ( leastUsedCells ) ) return leastUsedCells [ i ]",Gets the cell with the smallest number of segments . Break ties randomly .
"def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : candidates = list ( prevWinnerCells ) for synapse in connections . synapsesForSegment ( segment ) : i = binSearch ( candidates , synapse . presynapticCell ) if i != - 1 : del candidates [ i ] nActual = min ( nDesiredNewSynapes , len ( candidates ) ) overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment if overrun > 0 : cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , prevWinnerCells ) nActual = min ( nActual , maxSynapsesPerSegment - connections . numSynapses ( segment ) ) for _ in range ( nActual ) : i = random . getUInt32 ( len ( candidates ) ) connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) del candidates [ i ]",Creates nDesiredNewSynapes synapses on the segment passed in if possible choosing random cells from the previous winner cells that are not already on the segment .
"def _adaptSegment ( cls , connections , segment , prevActiveCells , permanenceIncrement , permanenceDecrement ) : synapsesToDestroy = [ ] for synapse in connections . synapsesForSegment ( segment ) : permanence = synapse . permanence if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : permanence += permanenceIncrement else : permanence -= permanenceDecrement permanence = max ( 0.0 , min ( 1.0 , permanence ) ) if permanence < EPSILON : synapsesToDestroy . append ( synapse ) else : connections . updateSynapsePermanence ( synapse , permanence ) for synapse in synapsesToDestroy : connections . destroySynapse ( synapse ) if connections . numSynapses ( segment ) == 0 : connections . destroySegment ( segment )",Updates synapses on segment . Strengthens active synapses ; weakens inactive synapses .
"def columnForCell ( self , cell ) : self . _validateCell ( cell ) return int ( cell / self . cellsPerColumn )",Returns the index of the column that a cell belongs to .
"def cellsForColumn ( self , column ) : self . _validateColumn ( column ) start = self . cellsPerColumn * column end = start + self . cellsPerColumn return range ( start , end )",Returns the indices of cells that belong to a column .
"def mapCellsToColumns ( self , cells ) : cellsForColumns = defaultdict ( set ) for cell in cells : column = self . columnForCell ( cell ) cellsForColumns [ column ] . add ( cell ) return cellsForColumns",Maps cells to the columns they belong to .
def getPredictiveCells ( self ) : previousCell = None predictiveCells = [ ] for segment in self . activeSegments : if segment . cell != previousCell : predictiveCells . append ( segment . cell ) previousCell = segment . cell return predictiveCells,Returns the indices of the predictive cells .
"def write ( self , proto ) : proto . columnDimensions = list ( self . columnDimensions ) proto . cellsPerColumn = self . cellsPerColumn proto . activationThreshold = self . activationThreshold proto . initialPermanence = round ( self . initialPermanence , EPSILON_ROUND ) proto . connectedPermanence = round ( self . connectedPermanence , EPSILON_ROUND ) proto . minThreshold = self . minThreshold proto . maxNewSynapseCount = self . maxNewSynapseCount proto . permanenceIncrement = round ( self . permanenceIncrement , EPSILON_ROUND ) proto . permanenceDecrement = round ( self . permanenceDecrement , EPSILON_ROUND ) proto . predictedSegmentDecrement = self . predictedSegmentDecrement proto . maxSegmentsPerCell = self . maxSegmentsPerCell proto . maxSynapsesPerSegment = self . maxSynapsesPerSegment self . connections . write ( proto . connections ) self . _random . write ( proto . random ) proto . activeCells = list ( self . activeCells ) proto . winnerCells = list ( self . winnerCells ) protoActiveSegments = proto . init ( ""activeSegments"" , len ( self . activeSegments ) ) for i , segment in enumerate ( self . activeSegments ) : protoActiveSegments [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoActiveSegments [ i ] . idxOnCell = idx protoMatchingSegments = proto . init ( ""matchingSegments"" , len ( self . matchingSegments ) ) for i , segment in enumerate ( self . matchingSegments ) : protoMatchingSegments [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoMatchingSegments [ i ] . idxOnCell = idx protoNumActivePotential = proto . init ( ""numActivePotentialSynapsesForSegment"" , len ( self . numActivePotentialSynapsesForSegment ) ) for i , numActivePotentialSynapses in enumerate ( self . numActivePotentialSynapsesForSegment ) : segment = self . connections . segmentForFlatIdx ( i ) if segment is not None : protoNumActivePotential [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoNumActivePotential [ i ] . idxOnCell = idx protoNumActivePotential [ i ] . number = numActivePotentialSynapses proto . iteration = self . iteration protoLastUsedIteration = proto . init ( ""lastUsedIterationForSegment"" , len ( self . numActivePotentialSynapsesForSegment ) ) for i , lastUsed in enumerate ( self . lastUsedIterationForSegment ) : segment = self . connections . segmentForFlatIdx ( i ) if segment is not None : protoLastUsedIteration [ i ] . cell = segment . cell idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) protoLastUsedIteration [ i ] . idxOnCell = idx protoLastUsedIteration [ i ] . number = lastUsed",Writes serialized data to proto object .
"def read ( cls , proto ) : tm = object . __new__ ( cls ) tm . columnDimensions = tuple ( proto . columnDimensions ) tm . cellsPerColumn = int ( proto . cellsPerColumn ) tm . activationThreshold = int ( proto . activationThreshold ) tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) tm . minThreshold = int ( proto . minThreshold ) tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , EPSILON_ROUND ) tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) tm . connections = Connections . read ( proto . connections ) tm . _random = Random ( ) tm . _random . read ( proto . random ) tm . activeCells = [ int ( x ) for x in proto . activeCells ] tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] flatListLength = tm . connections . segmentFlatListLength ( ) tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength tm . lastUsedIterationForSegment = [ 0 ] * flatListLength tm . activeSegments = [ ] tm . matchingSegments = [ ] for protoSegment in proto . activeSegments : tm . activeSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . matchingSegments : tm . matchingSegments . append ( tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) ) for protoSegment in proto . numActivePotentialSynapsesForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( int ( protoSegment . number ) ) tm . iteration = long ( proto . iteration ) for protoSegment in proto . lastUsedIterationForSegment : segment = tm . connections . getSegment ( protoSegment . cell , protoSegment . idxOnCell ) tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( long ( protoSegment . number ) ) return tm",Reads deserialized data from proto object .
"def generateFromNumbers ( self , numbers ) : sequence = [ ] for number in numbers : if number == None : sequence . append ( number ) else : pattern = self . patternMachine . get ( number ) sequence . append ( pattern ) return sequence",Generate a sequence from a list of numbers .
"def addSpatialNoise ( self , sequence , amount ) : newSequence = [ ] for pattern in sequence : if pattern is not None : pattern = self . patternMachine . addNoise ( pattern , amount ) newSequence . append ( pattern ) return newSequence",Add spatial noise to each pattern in the sequence .
"def prettyPrintSequence ( self , sequence , verbosity = 1 ) : text = """" for i in xrange ( len ( sequence ) ) : pattern = sequence [ i ] if pattern == None : text += ""<reset>"" if i < len ( sequence ) - 1 : text += ""\n"" else : text += self . patternMachine . prettyPrintPattern ( pattern , verbosity = verbosity ) return text",Pretty print a sequence .
"def ROCCurve ( y_true , y_score ) : y_true = np . ravel ( y_true ) classes = np . unique ( y_true ) if classes . shape [ 0 ] != 2 : raise ValueError ( ""ROC is defined for binary classification only"" ) y_score = np . ravel ( y_score ) n_pos = float ( np . sum ( y_true == classes [ 1 ] ) ) n_neg = float ( np . sum ( y_true == classes [ 0 ] ) ) thresholds = np . unique ( y_score ) neg_value , pos_value = classes [ 0 ] , classes [ 1 ] tpr = np . empty ( thresholds . size , dtype = np . float ) fpr = np . empty ( thresholds . size , dtype = np . float ) current_pos_count = current_neg_count = sum_pos = sum_neg = idx = 0 signal = np . c_ [ y_score , y_true ] sorted_signal = signal [ signal [ : , 0 ] . argsort ( ) , : ] [ : : - 1 ] last_score = sorted_signal [ 0 ] [ 0 ] for score , value in sorted_signal : if score == last_score : if value == pos_value : current_pos_count += 1 else : current_neg_count += 1 else : tpr [ idx ] = ( sum_pos + current_pos_count ) / n_pos fpr [ idx ] = ( sum_neg + current_neg_count ) / n_neg sum_pos += current_pos_count sum_neg += current_neg_count current_pos_count = 1 if value == pos_value else 0 current_neg_count = 1 if value == neg_value else 0 idx += 1 last_score = score else : tpr [ - 1 ] = ( sum_pos + current_pos_count ) / n_pos fpr [ - 1 ] = ( sum_neg + current_neg_count ) / n_neg if fpr . shape [ 0 ] == 2 : fpr = np . array ( [ 0.0 , fpr [ 0 ] , fpr [ 1 ] ] ) tpr = np . array ( [ 0.0 , tpr [ 0 ] , tpr [ 1 ] ] ) elif fpr . shape [ 0 ] == 1 : fpr = np . array ( [ 0.0 , fpr [ 0 ] , 1.0 ] ) tpr = np . array ( [ 0.0 , tpr [ 0 ] , 1.0 ] ) return fpr , tpr , thresholds",compute Receiver operating characteristic ( ROC )
"def AreaUnderCurve ( x , y ) : if x . shape [ 0 ] != y . shape [ 0 ] : raise ValueError ( 'x and y should have the same shape' ' to compute area under curve,' ' but x.shape = %s and y.shape = %s.' % ( x . shape , y . shape ) ) if x . shape [ 0 ] < 2 : raise ValueError ( 'At least 2 points are needed to compute' ' area under curve, but x.shape = %s' % x . shape ) order = np . argsort ( x ) x = x [ order ] y = y [ order ] h = np . diff ( x ) area = np . sum ( h * ( y [ 1 : ] + y [ : - 1 ] ) ) / 2.0 return area",Compute Area Under the Curve ( AUC ) using the trapezoidal rule
"def mmPrettyPrintTraces ( traces , breakOnResets = None ) : assert len ( traces ) > 0 , ""No traces found"" table = PrettyTable ( [ ""#"" ] + [ trace . prettyPrintTitle ( ) for trace in traces ] ) for i in xrange ( len ( traces [ 0 ] . data ) ) : if breakOnResets and breakOnResets . data [ i ] : table . add_row ( [ ""<reset>"" ] * ( len ( traces ) + 1 ) ) table . add_row ( [ i ] + [ trace . prettyPrintDatum ( trace . data [ i ] ) for trace in traces ] ) return table . get_string ( ) . encode ( ""utf-8"" )",Returns pretty - printed table of traces .
"def mmPrettyPrintMetrics ( metrics , sigFigs = 5 ) : assert len ( metrics ) > 0 , ""No metrics found"" table = PrettyTable ( [ ""Metric"" , ""mean"" , ""standard deviation"" , ""min"" , ""max"" , ""sum"" , ] ) for metric in metrics : table . add_row ( [ metric . prettyPrintTitle ( ) ] + metric . getStats ( ) ) return table . get_string ( ) . encode ( ""utf-8"" )",Returns pretty - printed table of metrics .
"def mmGetCellTracePlot ( self , cellTrace , cellCount , activityType , title = """" , showReset = False , resetShading = 0.25 ) : plot = Plot ( self , title ) resetTrace = self . mmGetTraceResets ( ) . data data = numpy . zeros ( ( cellCount , 1 ) ) for i in xrange ( len ( cellTrace ) ) : if showReset and resetTrace [ i ] : activity = numpy . ones ( ( cellCount , 1 ) ) * resetShading else : activity = numpy . zeros ( ( cellCount , 1 ) ) activeIndices = cellTrace [ i ] activity [ list ( activeIndices ) ] = 1 data = numpy . concatenate ( ( data , activity ) , 1 ) plot . add2DArray ( data , xlabel = ""Time"" , ylabel = activityType , name = title ) return plot",Returns plot of the cell activity . Note that if many timesteps of activities are input matplotlib s image interpolation may omit activities ( columns in the image ) .
"def estimateAnomalyLikelihoods ( anomalyScores , averagingWindow = 10 , skipRecords = 0 , verbosity = 0 ) : if verbosity > 1 : print ( ""In estimateAnomalyLikelihoods."" ) print ( ""Number of anomaly scores:"" , len ( anomalyScores ) ) print ( ""Skip records="" , skipRecords ) print ( ""First 20:"" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) if len ( anomalyScores ) == 0 : raise ValueError ( ""Must have at least one anomalyScore"" ) aggRecordList , historicalValues , total = _anomalyScoreMovingAverage ( anomalyScores , windowSize = averagingWindow , verbosity = verbosity ) s = [ r [ 2 ] for r in aggRecordList ] dataValues = numpy . array ( s ) if len ( aggRecordList ) <= skipRecords : distributionParams = nullDistribution ( verbosity = verbosity ) else : distributionParams = estimateNormal ( dataValues [ skipRecords : ] ) s = [ r [ 1 ] for r in aggRecordList ] if all ( [ isinstance ( r [ 1 ] , numbers . Number ) for r in aggRecordList ] ) : metricValues = numpy . array ( s ) metricDistribution = estimateNormal ( metricValues [ skipRecords : ] , performLowerBoundCheck = False ) if metricDistribution [ ""variance"" ] < 1.5e-5 : distributionParams = nullDistribution ( verbosity = verbosity ) likelihoods = numpy . array ( dataValues , dtype = float ) for i , s in enumerate ( dataValues ) : likelihoods [ i ] = tailProbability ( s , distributionParams ) filteredLikelihoods = numpy . array ( _filterLikelihoods ( likelihoods ) ) params = { ""distribution"" : distributionParams , ""movingAverage"" : { ""historicalValues"" : historicalValues , ""total"" : total , ""windowSize"" : averagingWindow , } , ""historicalLikelihoods"" : list ( likelihoods [ - min ( averagingWindow , len ( likelihoods ) ) : ] ) , } if verbosity > 1 : print ( ""Discovered params="" ) print ( params ) print ( ""Number of likelihoods:"" , len ( likelihoods ) ) print ( ""First 20 likelihoods:"" , ( filteredLikelihoods [ 0 : min ( 20 , len ( filteredLikelihoods ) ) ] ) ) print ( ""leaving estimateAnomalyLikelihoods"" ) return ( filteredLikelihoods , aggRecordList , params )",Given a series of anomaly scores compute the likelihood for each score . This function should be called once on a bunch of historical anomaly scores for an initial estimate of the distribution . It should be called again every so often ( say every 50 records ) to update the estimate .
"def updateAnomalyLikelihoods ( anomalyScores , params , verbosity = 0 ) : if verbosity > 3 : print ( ""In updateAnomalyLikelihoods."" ) print ( ""Number of anomaly scores:"" , len ( anomalyScores ) ) print ( ""First 20:"" , anomalyScores [ 0 : min ( 20 , len ( anomalyScores ) ) ] ) print ( ""Params:"" , params ) if len ( anomalyScores ) == 0 : raise ValueError ( ""Must have at least one anomalyScore"" ) if not isValidEstimatorParams ( params ) : raise ValueError ( ""'params' is not a valid params structure"" ) if ""historicalLikelihoods"" not in params : params [ ""historicalLikelihoods"" ] = [ 1.0 ] historicalValues = params [ ""movingAverage"" ] [ ""historicalValues"" ] total = params [ ""movingAverage"" ] [ ""total"" ] windowSize = params [ ""movingAverage"" ] [ ""windowSize"" ] aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) for i , v in enumerate ( anomalyScores ) : newAverage , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) ) aggRecordList [ i ] = newAverage likelihoods [ i ] = tailProbability ( newAverage , params [ ""distribution"" ] ) likelihoods2 = params [ ""historicalLikelihoods"" ] + list ( likelihoods ) filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] newParams = { ""distribution"" : params [ ""distribution"" ] , ""movingAverage"" : { ""historicalValues"" : historicalValues , ""total"" : total , ""windowSize"" : windowSize , } , ""historicalLikelihoods"" : historicalLikelihoods , } assert len ( newParams [ ""historicalLikelihoods"" ] ) <= windowSize if verbosity > 3 : print ( ""Number of likelihoods:"" , len ( likelihoods ) ) print ( ""First 20 likelihoods:"" , likelihoods [ 0 : min ( 20 , len ( likelihoods ) ) ] ) print ( ""Leaving updateAnomalyLikelihoods."" ) return ( likelihoods , aggRecordList , newParams )",Compute updated probabilities for anomalyScores using the given params .
"def _filterLikelihoods ( likelihoods , redThreshold = 0.99999 , yellowThreshold = 0.999 ) : redThreshold = 1.0 - redThreshold yellowThreshold = 1.0 - yellowThreshold filteredLikelihoods = [ likelihoods [ 0 ] ] for i , v in enumerate ( likelihoods [ 1 : ] ) : if v <= redThreshold : if likelihoods [ i ] > redThreshold : filteredLikelihoods . append ( v ) else : filteredLikelihoods . append ( yellowThreshold ) else : filteredLikelihoods . append ( v ) return filteredLikelihoods",Filter the list of raw ( pre - filtered ) likelihoods so that we only preserve sharp increases in likelihood . likelihoods can be a numpy array of floats or a list of floats .
"def _anomalyScoreMovingAverage ( anomalyScores , windowSize = 10 , verbosity = 0 , ) : historicalValues = [ ] total = 0.0 averagedRecordList = [ ] for record in anomalyScores : if not isinstance ( record , ( list , tuple ) ) or len ( record ) != 3 : if verbosity >= 1 : print ( ""Malformed record:"" , record ) continue avg , historicalValues , total = ( MovingAverage . compute ( historicalValues , total , record [ 2 ] , windowSize ) ) averagedRecordList . append ( [ record [ 0 ] , record [ 1 ] , avg ] ) if verbosity > 2 : print ( ""Aggregating input record:"" , record ) print ( ""Result:"" , [ record [ 0 ] , record [ 1 ] , avg ] ) return averagedRecordList , historicalValues , total",Given a list of anomaly scores return a list of averaged records . anomalyScores is assumed to be a list of records of the form : [ datetime . datetime ( 2013 8 10 23 0 ) 6 . 0 1 . 0 ]
"def estimateNormal ( sampleData , performLowerBoundCheck = True ) : params = { ""name"" : ""normal"" , ""mean"" : numpy . mean ( sampleData ) , ""variance"" : numpy . var ( sampleData ) , } if performLowerBoundCheck : if params [ ""mean"" ] < 0.03 : params [ ""mean"" ] = 0.03 if params [ ""variance"" ] < 0.0003 : params [ ""variance"" ] = 0.0003 if params [ ""variance"" ] > 0 : params [ ""stdev"" ] = math . sqrt ( params [ ""variance"" ] ) else : params [ ""stdev"" ] = 0 return params",: param sampleData : : type sampleData : Numpy array . : param performLowerBoundCheck : : type performLowerBoundCheck : bool : returns : A dict containing the parameters of a normal distribution based on the sampleData .
"def tailProbability ( x , distributionParams ) : if ""mean"" not in distributionParams or ""stdev"" not in distributionParams : raise RuntimeError ( ""Insufficient parameters to specify the distribution."" ) if x < distributionParams [ ""mean"" ] : xp = 2 * distributionParams [ ""mean"" ] - x return tailProbability ( xp , distributionParams ) z = ( x - distributionParams [ ""mean"" ] ) / distributionParams [ ""stdev"" ] return 0.5 * math . erfc ( z / 1.4142 )",Given the normal distribution specified by the mean and standard deviation in distributionParams return the probability of getting samples further from the mean . For values above the mean this is the probability of getting samples > x and for values below the mean the probability of getting samples < x . This is the Q - function : the tail probability of the normal distribution .
"def isValidEstimatorParams ( p ) : if not isinstance ( p , dict ) : return False if ""distribution"" not in p : return False if ""movingAverage"" not in p : return False dist = p [ ""distribution"" ] if not ( ""mean"" in dist and ""name"" in dist and ""variance"" in dist and ""stdev"" in dist ) : return False return True",: returns : True if p is a valid estimator params as might be returned by estimateAnomalyLikelihoods () or updateAnomalyLikelihoods False otherwise . Just does some basic validation .
"def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : numShiftedOut = max ( 0 , numIngested - windowSize ) return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) )",Return the value of skipRecords for passing to estimateAnomalyLikelihoods
"def read ( cls , proto ) : anomalyLikelihood = object . __new__ ( cls ) anomalyLikelihood . _iteration = proto . iteration anomalyLikelihood . _historicalScores = collections . deque ( maxlen = proto . historicWindowSize ) for i , score in enumerate ( proto . historicalScores ) : anomalyLikelihood . _historicalScores . append ( ( i , score . value , score . anomalyScore ) ) if proto . distribution . name : anomalyLikelihood . _distribution = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) anomalyLikelihood . _distribution [ 'distribution' ] [ ""name"" ] = proto . distribution . name anomalyLikelihood . _distribution [ 'distribution' ] [ ""mean"" ] = proto . distribution . mean anomalyLikelihood . _distribution [ 'distribution' ] [ ""variance"" ] = proto . distribution . variance anomalyLikelihood . _distribution [ 'distribution' ] [ ""stdev"" ] = proto . distribution . stdev anomalyLikelihood . _distribution [ ""movingAverage"" ] = { } anomalyLikelihood . _distribution [ ""movingAverage"" ] [ ""windowSize"" ] = proto . distribution . movingAverage . windowSize anomalyLikelihood . _distribution [ ""movingAverage"" ] [ ""historicalValues"" ] = [ ] for value in proto . distribution . movingAverage . historicalValues : anomalyLikelihood . _distribution [ ""movingAverage"" ] [ ""historicalValues"" ] . append ( value ) anomalyLikelihood . _distribution [ ""movingAverage"" ] [ ""total"" ] = proto . distribution . movingAverage . total anomalyLikelihood . _distribution [ ""historicalLikelihoods"" ] = [ ] for likelihood in proto . distribution . historicalLikelihoods : anomalyLikelihood . _distribution [ ""historicalLikelihoods"" ] . append ( likelihood ) else : anomalyLikelihood . _distribution = None anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod anomalyLikelihood . _learningPeriod = proto . learningPeriod anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod return anomalyLikelihood",capnp deserialization method for the anomaly likelihood object
"def write ( self , proto ) : proto . iteration = self . _iteration pHistScores = proto . init ( 'historicalScores' , len ( self . _historicalScores ) ) for i , score in enumerate ( list ( self . _historicalScores ) ) : _ , value , anomalyScore = score record = pHistScores [ i ] record . value = float ( value ) record . anomalyScore = float ( anomalyScore ) if self . _distribution : proto . distribution . name = self . _distribution [ ""distribution"" ] [ ""name"" ] proto . distribution . mean = float ( self . _distribution [ ""distribution"" ] [ ""mean"" ] ) proto . distribution . variance = float ( self . _distribution [ ""distribution"" ] [ ""variance"" ] ) proto . distribution . stdev = float ( self . _distribution [ ""distribution"" ] [ ""stdev"" ] ) proto . distribution . movingAverage . windowSize = float ( self . _distribution [ ""movingAverage"" ] [ ""windowSize"" ] ) historicalValues = self . _distribution [ ""movingAverage"" ] [ ""historicalValues"" ] pHistValues = proto . distribution . movingAverage . init ( ""historicalValues"" , len ( historicalValues ) ) for i , value in enumerate ( historicalValues ) : pHistValues [ i ] = float ( value ) proto . distribution . movingAverage . total = float ( self . _distribution [ ""movingAverage"" ] [ ""total"" ] ) historicalLikelihoods = self . _distribution [ ""historicalLikelihoods"" ] pHistLikelihoods = proto . distribution . init ( ""historicalLikelihoods"" , len ( historicalLikelihoods ) ) for i , likelihood in enumerate ( historicalLikelihoods ) : pHistLikelihoods [ i ] = float ( likelihood ) proto . probationaryPeriod = self . _probationaryPeriod proto . learningPeriod = self . _learningPeriod proto . reestimationPeriod = self . _reestimationPeriod proto . historicWindowSize = self . _historicalScores . maxlen",capnp serialization method for the anomaly likelihood object
"def anomalyProbability ( self , value , anomalyScore , timestamp = None ) : if timestamp is None : timestamp = self . _iteration dataPoint = ( timestamp , value , anomalyScore ) if self . _iteration < self . _probationaryPeriod : likelihood = 0.5 else : if ( ( self . _distribution is None ) or ( self . _iteration % self . _reestimationPeriod == 0 ) ) : numSkipRecords = self . _calcSkipRecords ( numIngested = self . _iteration , windowSize = self . _historicalScores . maxlen , learningPeriod = self . _learningPeriod ) _ , _ , self . _distribution = estimateAnomalyLikelihoods ( self . _historicalScores , skipRecords = numSkipRecords ) likelihoods , _ , self . _distribution = updateAnomalyLikelihoods ( [ dataPoint ] , self . _distribution ) likelihood = 1.0 - likelihoods [ 0 ] self . _historicalScores . append ( dataPoint ) self . _iteration += 1 return likelihood",Compute the probability that the current value plus anomaly score represents an anomaly given the historical distribution of anomaly scores . The closer the number is to 1 the higher the chance it is an anomaly .
"def _getImpl ( self , model ) : impl = _IterationPhaseLearnOnly ( model = model , nIters = self . __nIters ) return impl",Creates and returns the _IterationPhase - based instance corresponding to this phase specification
"def _getImpl ( self , model ) : impl = _IterationPhaseInferOnly ( model = model , nIters = self . __nIters , inferenceArgs = self . __inferenceArgs ) return impl",Creates and returns the _IterationPhase - based instance corresponding to this phase specification
"def replaceIterationCycle ( self , phaseSpecs ) : self . __phaseManager = _PhaseManager ( model = self . __model , phaseSpecs = phaseSpecs ) return",Replaces the Iteration Cycle phases
"def handleInputRecord ( self , inputRecord ) : assert inputRecord , ""Invalid inputRecord: %r"" % inputRecord results = self . __phaseManager . handleInputRecord ( inputRecord ) metrics = self . __metricsMgr . update ( results ) for cb in self . __userCallbacks [ 'postIter' ] : cb ( self . __model ) results . metrics = metrics return results",Processes the given record according to the current iteration cycle phase
def __advancePhase ( self ) : self . __currentPhase = self . __phaseCycler . next ( ) self . __currentPhase . enterPhase ( ) return,Advance to the next iteration cycle phase
"def handleInputRecord ( self , inputRecord ) : results = self . __model . run ( inputRecord ) shouldContinue = self . __currentPhase . advance ( ) if not shouldContinue : self . __advancePhase ( ) return results",Processes the given record according to the current phase
def enterPhase ( self ) : self . __iter = iter ( xrange ( self . __nIters ) ) self . __iter . next ( ),Performs initialization that is necessary upon entry to the phase . Must be called before handleInputRecord () at the beginning of each phase
def advance ( self ) : hasMore = True try : self . __iter . next ( ) except StopIteration : self . __iter = None hasMore = False return hasMore,Advances the iteration ;
"def enterPhase ( self ) : super ( _IterationPhaseLearnOnly , self ) . enterPhase ( ) self . __model . enableLearning ( ) self . __model . disableInference ( ) return",[ _IterationPhase method implementation ] Performs initialization that is necessary upon entry to the phase . Must be called before handleInputRecord () at the beginning of each phase
"def enterPhase ( self ) : super ( _IterationPhaseInferCommon , self ) . enterPhase ( ) self . _model . enableInference ( inferenceArgs = self . _inferenceArgs ) return",[ _IterationPhase method implementation ] Performs initialization that is necessary upon entry to the phase . Must be called before handleInputRecord () at the beginning of each phase
"def write ( self , proto ) : super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) proto . fieldNames = self . _fieldNames proto . fieldTypes = self . _fieldTypes if self . _predictedField : proto . predictedField = self . _predictedField proto . predictionSteps = self . _predictionSteps",Serialize via capnp
"def read ( cls , proto ) : instance = object . __new__ ( cls ) super ( PreviousValueModel , instance ) . __init__ ( proto = proto . modelBase ) instance . _logger = opf_utils . initLogger ( instance ) if len ( proto . predictedField ) : instance . _predictedField = proto . predictedField else : instance . _predictedField = None instance . _fieldNames = list ( proto . fieldNames ) instance . _fieldTypes = list ( proto . fieldTypes ) instance . _predictionSteps = list ( proto . predictionSteps ) return instance",Deserialize via capnp
"def lscsum ( lx , epsilon = None ) : lx = numpy . asarray ( lx ) base = lx . max ( ) if numpy . isinf ( base ) : return base if ( epsilon is not None ) and ( base < epsilon ) : return epsilon x = numpy . exp ( lx - base ) ssum = x . sum ( ) result = numpy . log ( ssum ) + base return result",Accepts log - values as input exponentiates them computes the sum then converts the sum back to log - space and returns the result . Handles underflow by rescaling so that the largest values is exactly 1 . 0 .
"def lscsum0 ( lx ) : lx = numpy . asarray ( lx ) bases = lx . max ( 0 ) x = numpy . exp ( lx - bases ) ssum = x . sum ( 0 ) result = numpy . log ( ssum ) + bases try : conventional = numpy . log ( numpy . exp ( lx ) . sum ( 0 ) ) if not similar ( result , conventional ) : if numpy . isinf ( conventional ) . any ( ) and not numpy . isinf ( result ) . any ( ) : pass else : import sys print >> sys . stderr , ""Warning: scaled log sum down axis 0 did not match."" print >> sys . stderr , ""Scaled log result:"" print >> sys . stderr , result print >> sys . stderr , ""Conventional result:"" print >> sys . stderr , conventional except FloatingPointError , e : pass return result",Accepts log - values as input exponentiates them sums down the rows ( first dimension ) then converts the sum back to log - space and returns the result . Handles underflow by rescaling so that the largest values is exactly 1 . 0 .
"def normalize ( lx ) : lx = numpy . asarray ( lx ) base = lx . max ( ) x = numpy . exp ( lx - base ) result = x / x . sum ( ) conventional = ( numpy . exp ( lx ) / numpy . exp ( lx ) . sum ( ) ) assert similar ( result , conventional ) return result",Accepts log - values as input exponentiates them normalizes and returns the result . Handles underflow by rescaling so that the largest values is exactly 1 . 0 .
"def nsum0 ( lx ) : lx = numpy . asarray ( lx ) base = lx . max ( ) x = numpy . exp ( lx - base ) ssum = x . sum ( 0 ) result = ssum / ssum . sum ( ) conventional = ( numpy . exp ( lx ) . sum ( 0 ) / numpy . exp ( lx ) . sum ( ) ) assert similar ( result , conventional ) return result",Accepts log - values as input exponentiates them sums down the rows ( first dimension ) normalizes and returns the result . Handles underflow by rescaling so that the largest values is exactly 1 . 0 .
"def logSumExp ( A , B , out = None ) : if out is None : out = numpy . zeros ( A . shape ) indicator1 = A >= B indicator2 = numpy . logical_not ( indicator1 ) out [ indicator1 ] = A [ indicator1 ] + numpy . log1p ( numpy . exp ( B [ indicator1 ] - A [ indicator1 ] ) ) out [ indicator2 ] = B [ indicator2 ] + numpy . log1p ( numpy . exp ( A [ indicator2 ] - B [ indicator2 ] ) ) return out",returns log ( exp ( A ) + exp ( B )) . A and B are numpy arrays
"def logDiffExp ( A , B , out = None ) : if out is None : out = numpy . zeros ( A . shape ) indicator1 = A >= B assert indicator1 . all ( ) , ""Values in the first array should be greater than the values in the second"" out [ indicator1 ] = A [ indicator1 ] + numpy . log ( 1 - numpy . exp ( B [ indicator1 ] - A [ indicator1 ] ) ) return out",returns log ( exp ( A ) - exp ( B )) . A and B are numpy arrays . values in A should be greater than or equal to corresponding values in B
"def _getFieldIndexBySpecial ( fields , special ) : for i , field in enumerate ( fields ) : if field . special == special : return i return None",Return index of the field matching the field meta special value . : param fields : sequence of nupic . data . fieldmeta . FieldMetaInfo objects representing the fields of a stream : param special : one of the special field attribute values from nupic . data . fieldmeta . FieldMetaSpecial : returns : first zero - based index of the field tagged with the target field meta special attribute ; None if no such field
"def encode ( self , inputRow ) : result = dict ( zip ( self . _fieldNames , inputRow ) ) if self . _categoryFieldIndex is not None : if isinstance ( inputRow [ self . _categoryFieldIndex ] , int ) : result [ '_category' ] = [ inputRow [ self . _categoryFieldIndex ] ] else : result [ '_category' ] = ( inputRow [ self . _categoryFieldIndex ] if inputRow [ self . _categoryFieldIndex ] else [ None ] ) else : result [ '_category' ] = [ None ] if self . _resetFieldIndex is not None : result [ '_reset' ] = int ( bool ( inputRow [ self . _resetFieldIndex ] ) ) else : result [ '_reset' ] = 0 if self . _learningFieldIndex is not None : result [ '_learning' ] = int ( bool ( inputRow [ self . _learningFieldIndex ] ) ) result [ '_timestampRecordIdx' ] = None if self . _timestampFieldIndex is not None : result [ '_timestamp' ] = inputRow [ self . _timestampFieldIndex ] result [ '_timestampRecordIdx' ] = self . _computeTimestampRecordIdx ( inputRow [ self . _timestampFieldIndex ] ) else : result [ '_timestamp' ] = None hasReset = self . _resetFieldIndex is not None hasSequenceId = self . _sequenceFieldIndex is not None if hasReset and not hasSequenceId : if result [ '_reset' ] : self . _sequenceId += 1 sequenceId = self . _sequenceId elif not hasReset and hasSequenceId : sequenceId = inputRow [ self . _sequenceFieldIndex ] result [ '_reset' ] = int ( sequenceId != self . _sequenceId ) self . _sequenceId = sequenceId elif hasReset and hasSequenceId : sequenceId = inputRow [ self . _sequenceFieldIndex ] else : sequenceId = 0 if sequenceId is not None : result [ '_sequenceId' ] = hash ( sequenceId ) else : result [ '_sequenceId' ] = None return result",Encodes the given input row as a dict with the keys being the field names . This also adds in some meta fields : _category : The value from the category field ( if any ) _reset : True if the reset field was True ( if any ) _sequenceId : the value from the sequenceId field ( if any )
"def _computeTimestampRecordIdx ( self , recordTS ) : if self . _aggregationPeriod is None : return None if self . _aggregationPeriod [ 'months' ] > 0 : assert self . _aggregationPeriod [ 'seconds' ] == 0 result = int ( ( recordTS . year * 12 + ( recordTS . month - 1 ) ) / self . _aggregationPeriod [ 'months' ] ) elif self . _aggregationPeriod [ 'seconds' ] > 0 : delta = recordTS - datetime . datetime ( year = 1 , month = 1 , day = 1 ) deltaSecs = delta . days * 24 * 60 * 60 + delta . seconds + delta . microseconds / 1000000.0 result = int ( deltaSecs / self . _aggregationPeriod [ 'seconds' ] ) else : result = None return result",Give the timestamp of a record ( a datetime object ) compute the record s timestamp index - this is the timestamp divided by the aggregation period .
"def getNextRecordDict ( self ) : values = self . getNextRecord ( ) if values is None : return None if not values : return dict ( ) if self . _modelRecordEncoder is None : self . _modelRecordEncoder = ModelRecordEncoder ( fields = self . getFields ( ) , aggregationPeriod = self . getAggregationMonthsAndSeconds ( ) ) return self . _modelRecordEncoder . encode ( values )",Returns next available data record from the storage as a dict with the keys being the field names . This also adds in some meta fields :
"def getFieldMin ( self , fieldName ) : stats = self . getStats ( ) if stats == None : return None minValues = stats . get ( 'min' , None ) if minValues == None : return None index = self . getFieldNames ( ) . index ( fieldName ) return minValues [ index ]",If underlying implementation does not support min / max stats collection or if a field type does not support min / max ( non scalars ) the return value will be None .
"def getFieldMax ( self , fieldName ) : stats = self . getStats ( ) if stats == None : return None maxValues = stats . get ( 'max' , None ) if maxValues == None : return None index = self . getFieldNames ( ) . index ( fieldName ) return maxValues [ index ]",If underlying implementation does not support min / max stats collection or if a field type does not support min / max ( non scalars ) the return value will be None .
"def debug ( self , msg , * args , * * kwargs ) : self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with severity DEBUG .
"def info ( self , msg , * args , * * kwargs ) : self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with severity INFO .
"def warning ( self , msg , * args , * * kwargs ) : self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with severity WARNING .
"def error ( self , msg , * args , * * kwargs ) : self . _baseLogger . error ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with severity ERROR .
"def critical ( self , msg , * args , * * kwargs ) : self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with severity CRITICAL .
"def log ( self , level , msg , * args , * * kwargs ) : self . _baseLogger . log ( self , level , self . getExtendedMsg ( msg ) , * args , * * kwargs )",Log msg % args with the integer severity level .
"def initFilter ( input , filterInfo = None ) : if filterInfo is None : return None filterList = [ ] for i , fieldName in enumerate ( input . getFieldNames ( ) ) : fieldFilter = filterInfo . get ( fieldName , None ) if fieldFilter == None : continue var = dict ( ) var [ 'acceptValues' ] = None min = fieldFilter . get ( 'min' , None ) max = fieldFilter . get ( 'max' , None ) var [ 'min' ] = min var [ 'max' ] = max if fieldFilter [ 'type' ] == 'category' : var [ 'acceptValues' ] = fieldFilter [ 'acceptValues' ] fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] in x [ 'acceptValues' ] ) elif fieldFilter [ 'type' ] == 'number' : if min != None and max != None : fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] >= x [ 'min' ] and x [ 'value' ] <= x [ 'max' ] ) elif min != None : fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] >= x [ 'min' ] ) else : fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] <= x [ 'max' ] ) filterList . append ( ( i , fp , var ) ) return ( _filterRecord , filterList )",Initializes internal filter variables for further processing . Returns a tuple ( function to call parameters for the filter call )
"def _filterRecord ( filterList , record ) : for ( fieldIdx , fp , params ) in filterList : x = dict ( ) x [ 'value' ] = record [ fieldIdx ] x [ 'acceptValues' ] = params [ 'acceptValues' ] x [ 'min' ] = params [ 'min' ] x [ 'max' ] = params [ 'max' ] if not fp ( x ) : return False return True",Takes a record and returns true if record meets filter criteria false otherwise
def _aggr_sum ( inList ) : aggrMean = _aggr_mean ( inList ) if aggrMean == None : return None aggrSum = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem else : aggrSum += aggrMean return aggrSum,Returns sum of the elements in the list . Missing items are replaced with the mean value
def _aggr_mean ( inList ) : aggrSum = 0 nonNone = 0 for elem in inList : if elem != SENTINEL_VALUE_FOR_MISSING_DATA : aggrSum += elem nonNone += 1 if nonNone != 0 : return aggrSum / nonNone else : return None,Returns mean of non - None elements of the list
"def _aggr_mode ( inList ) : valueCounts = dict ( ) nonNone = 0 for elem in inList : if elem == SENTINEL_VALUE_FOR_MISSING_DATA : continue nonNone += 1 if elem in valueCounts : valueCounts [ elem ] += 1 else : valueCounts [ elem ] = 1 if nonNone == 0 : return None sortedCounts = valueCounts . items ( ) sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) return sortedCounts [ 0 ] [ 0 ]",Returns most common value seen in the non - None elements of the list
"def _aggr_weighted_mean ( inList , params ) : assert ( len ( inList ) == len ( params ) ) weightsSum = sum ( params ) if weightsSum == 0 : return None weightedMean = 0 for i , elem in enumerate ( inList ) : weightedMean += elem * params [ i ] return weightedMean / weightsSum",Weighted mean uses params ( must be the same size as inList ) and makes weighed mean of inList
"def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : inputFullPath = resource_filename ( ""nupic.datafiles"" , inputFilename ) inputObj = FileRecordStream ( inputFullPath ) aggregator = Aggregator ( aggregationInfo = aggregationInfo , inputFields = inputObj . getFields ( ) ) if aggregator . isNullAggregation ( ) : return inputFullPath if outputFilename is None : outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if aggregationInfo . get ( k , 0 ) > 0 : outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) outputFilename += '.csv' outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) lockFilePath = outputFilename + '.please_wait' if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : while os . path . isfile ( lockFilePath ) : print 'Waiting for %s to be fully written by another process' % lockFilePath time . sleep ( 1 ) return outputFilename lockFD = open ( lockFilePath , 'w' ) outputObj = FileRecordStream ( streamID = outputFilename , write = True , fields = inputObj . getFields ( ) ) while True : inRecord = inputObj . getNextRecord ( ) ( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) if aggRecord is None and inRecord is None : break if aggRecord is not None : outputObj . appendRecord ( aggRecord ) return outputFilename",Generate a dataset of aggregated values
"def getFilename ( aggregationInfo , inputFile ) : inputFile = resource_filename ( ""nupic.datafiles"" , inputFile ) a = defaultdict ( lambda : 0 , aggregationInfo ) outputDir = os . path . dirname ( inputFile ) outputFile = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFile ) ) [ 0 ] noAggregation = True timePeriods = 'years months weeks days ' 'hours minutes seconds milliseconds microseconds' for k in timePeriods . split ( ) : if a [ k ] > 0 : noAggregation = False outputFile += '_%s_%d' % ( k , a [ k ] ) if noAggregation : return inputFile outputFile += '.csv' outputFile = os . path . join ( outputDir , outputFile ) return outputFile",Generate the filename for aggregated dataset
"def _getEndTime ( self , t ) : assert isinstance ( t , datetime . datetime ) if self . _aggTimeDelta : return t + self . _aggTimeDelta else : year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 return t . replace ( year = year , month = month )",Add the aggregation period to the input time t and return a datetime object
"def _getFuncPtrAndParams ( self , funcName ) : params = None if isinstance ( funcName , basestring ) : if funcName == 'sum' : fp = _aggr_sum elif funcName == 'first' : fp = _aggr_first elif funcName == 'last' : fp = _aggr_last elif funcName == 'mean' : fp = _aggr_mean elif funcName == 'max' : fp = max elif funcName == 'min' : fp = min elif funcName == 'mode' : fp = _aggr_mode elif funcName . startswith ( 'wmean:' ) : fp = _aggr_weighted_mean paramsName = funcName [ 6 : ] params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) else : fp = funcName return ( fp , params )",Given the name of an aggregation function returns the function pointer and param .
"def _createAggregateRecord ( self ) : record = [ ] for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : if aggFP is None : continue values = self . _slice [ i ] refIndex = None if paramIdx is not None : record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) else : record . append ( aggFP ( values ) ) return record",Generate the aggregated output record
"def next ( self , record , curInputBookmark ) : outRecord = None retInputBookmark = None if record is not None : self . _inIdx += 1 if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : return ( None , None ) if self . _nullAggregation : return ( record , curInputBookmark ) t = record [ self . _timeFieldIdx ] if self . _firstSequenceStartTime == None : self . _firstSequenceStartTime = t if self . _startTime is None : self . _startTime = t if self . _endTime is None : self . _endTime = self . _getEndTime ( t ) assert self . _endTime > t if self . _resetFieldIdx is not None : resetSignal = record [ self . _resetFieldIdx ] else : resetSignal = None if self . _sequenceIdFieldIdx is not None : currSequenceId = record [ self . _sequenceIdFieldIdx ] else : currSequenceId = None newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 if newSequence : self . _sequenceId = currSequenceId sliceEnded = ( t >= self . _endTime or t < self . _startTime ) if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) for j , f in enumerate ( self . _fields ) : index = f [ 0 ] self . _slice [ j ] . append ( record [ index ] ) self . _aggrInputBookmark = curInputBookmark if newSequence : self . _startTime = t self . _endTime = self . _getEndTime ( t ) if sliceEnded : if t < self . _startTime : self . _endTime = self . _firstSequenceStartTime while t >= self . _endTime : self . _startTime = self . _endTime self . _endTime = self . _getEndTime ( self . _endTime ) if outRecord is not None : return ( outRecord , retInputBookmark ) elif self . _slice : for j , f in enumerate ( self . _fields ) : index = f [ 0 ] if index == self . _timeFieldIdx : self . _slice [ j ] [ 0 ] = self . _startTime break outRecord = self . _createAggregateRecord ( ) retInputBookmark = self . _aggrInputBookmark self . _slice = defaultdict ( list ) return ( outRecord , retInputBookmark )",Return the next aggregated record if any
"def processClubAttendance ( f , clubs ) : try : line = f . next ( ) while line == ',,,,,,,,,,,,,,,,,,,\n' : line = f . next ( ) name = line . split ( ',' ) [ 0 ] if name not in clubs : clubs [ name ] = Club ( name ) c = clubs [ name ] c . processAttendance ( f ) return True except StopIteration : return False",Process the attendance data of one club If the club already exists in the list update its data . If the club is new create a new Club object and add it to the dict The next step is to iterate over all the lines and add a record for each line . When reaching an empty line it means there are no more records for this club . Along the way some redundant lines are skipped . When the file ends the f . next () call raises a StopIteration exception and that s the sign to return False which indicates to the caller that there are no more clubs to process .
"def processClubConsumption ( f , clubs ) : try : line = f . next ( ) assert line . endswith ( '""   "",""SITE_LOCATION_NAME"",""TIMESTAMP"",""TOTAL_KWH""\n' ) valid_times = range ( 24 ) t = 0 club = None clubName = None lastDate = None while True : assert t in valid_times consumption = 0 for x in range ( 4 ) : line = f . next ( ) [ : - 1 ] fields = line . split ( ',' ) assert len ( fields ) == 4 for i , field in enumerate ( fields ) : assert field [ 0 ] == '""' and field [ - 1 ] == '""' fields [ i ] = field [ 1 : - 1 ] name = fields [ 1 ] partialNames = ( 'Melbourne Central' , 'North Sydney' , 'Park St' , 'Pitt St' ) for pn in partialNames : if pn in name : name = pn if name != clubName : clubName = name club = clubs [ name ] tokens = fields [ 2 ] . split ( ) if len ( tokens ) == 1 : assert consumption == 0 and t == 0 date = tokens [ 0 ] consumption += float ( fields [ 3 ] ) club . updateRecord ( date , t , consumption ) t += 1 t %= 24 except StopIteration : return",Process the consumption a club - Skip the header line - Iterate over lines - Read 4 records at a time - Parse each line : club date time consumption - Get club object from dictionary if needed - Aggregate consumption - Call club . processConsumption () with data
"def run ( self , inputRecord ) : predictionNumber = self . _numPredictions self . _numPredictions += 1 result = opf_utils . ModelResult ( predictionNumber = predictionNumber , rawInput = inputRecord ) return result",Run one iteration of this model .
"def _getModelCheckpointFilePath ( checkpointDir ) : path = os . path . join ( checkpointDir , ""model.data"" ) path = os . path . abspath ( path ) return path",Return the absolute path of the model s checkpoint file .
"def writeToCheckpoint ( self , checkpointDir ) : proto = self . getSchema ( ) . new_message ( ) self . write ( proto ) checkpointPath = self . _getModelCheckpointFilePath ( checkpointDir ) if os . path . exists ( checkpointDir ) : if not os . path . isdir ( checkpointDir ) : raise Exception ( ( ""Existing filesystem entry <%s> is not a model"" "" checkpoint -- refusing to delete (not a directory)"" ) % checkpointDir ) if not os . path . isfile ( checkpointPath ) : raise Exception ( ( ""Existing filesystem entry <%s> is not a model"" "" checkpoint -- refusing to delete"" "" (%s missing or not a file)"" ) % ( checkpointDir , checkpointPath ) ) shutil . rmtree ( checkpointDir ) self . __makeDirectoryFromAbsolutePath ( checkpointDir ) with open ( checkpointPath , 'wb' ) as f : proto . write ( f )",Serializes model using capnproto and writes data to checkpointDir
"def readFromCheckpoint ( cls , checkpointDir ) : checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) with open ( checkpointPath , 'r' ) as f : proto = cls . getSchema ( ) . read ( f , traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) model = cls . read ( proto ) return model",Deserializes model from checkpointDir using capnproto
"def writeBaseToProto ( self , proto ) : inferenceType = self . getInferenceType ( ) inferenceType = inferenceType [ : 1 ] . lower ( ) + inferenceType [ 1 : ] proto . inferenceType = inferenceType proto . numPredictions = self . _numPredictions proto . learningEnabled = self . __learningEnabled proto . inferenceEnabled = self . __inferenceEnabled proto . inferenceArgs = json . dumps ( self . __inferenceArgs )",Save the state maintained by the Model base class
"def save ( self , saveModelDir ) : logger = self . _getLogger ( ) logger . debug ( ""(%s) Creating local checkpoint in %r..."" , self , saveModelDir ) modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) if os . path . exists ( saveModelDir ) : if not os . path . isdir ( saveModelDir ) : raise Exception ( ( ""Existing filesystem entry <%s> is not a model"" "" checkpoint -- refusing to delete (not a directory)"" ) % saveModelDir ) if not os . path . isfile ( modelPickleFilePath ) : raise Exception ( ( ""Existing filesystem entry <%s> is not a model"" "" checkpoint -- refusing to delete"" "" (%s missing or not a file)"" ) % ( saveModelDir , modelPickleFilePath ) ) shutil . rmtree ( saveModelDir ) self . __makeDirectoryFromAbsolutePath ( saveModelDir ) with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : logger . debug ( ""(%s) Pickling Model instance..."" , self ) pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) logger . debug ( ""(%s) Finished pickling Model instance"" , self ) self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) logger . debug ( ""(%s) Finished creating local checkpoint"" , self ) return",Save the model in the given directory .
"def load ( cls , savedModelDir ) : logger = opf_utils . initLogger ( cls ) logger . debug ( ""Loading model from local checkpoint at %r..."" , savedModelDir ) modelPickleFilePath = Model . _getModelPickleFilePath ( savedModelDir ) with open ( modelPickleFilePath , 'rb' ) as modelPickleFile : logger . debug ( ""Unpickling Model instance..."" ) model = pickle . load ( modelPickleFile ) logger . debug ( ""Finished unpickling Model instance"" ) model . _deSerializeExtraData ( extraDataDir = Model . _getModelExtraDataDir ( savedModelDir ) ) logger . debug ( ""Finished Loading model from local checkpoint"" ) return model",Load saved model .
"def _getModelPickleFilePath ( saveModelDir ) : path = os . path . join ( saveModelDir , ""model.pkl"" ) path = os . path . abspath ( path ) return path",Return the absolute path of the model s pickle file .
"def _getModelExtraDataDir ( saveModelDir ) : path = os . path . join ( saveModelDir , ""modelextradata"" ) path = os . path . abspath ( path ) return path",Return the absolute path to the directory where the model s own extra data are stored ( i . e . data that s too big for pickling ) .
"def runExperiment ( args , model = None ) : opt = _parseCommandLineOptions ( args ) model = _runExperimentImpl ( opt , model ) return model",Run a single OPF experiment .
"def _parseCommandLineOptions ( args ) : usageStr = ( ""%prog [options] descriptionPyDirectory\n"" ""This script runs a single OPF Model described by description.py "" ""located in the given directory."" ) parser = optparse . OptionParser ( usage = usageStr ) parser . add_option ( ""-c"" , help = ""Create a model and save it under the given "" ""<CHECKPOINT> name, but don't run it"" , dest = ""createCheckpointName"" , action = ""store"" , type = ""string"" , default = """" , metavar = ""<CHECKPOINT>"" ) parser . add_option ( ""--listCheckpoints"" , help = ""List all available checkpoints"" , dest = ""listAvailableCheckpoints"" , action = ""store_true"" , default = False ) parser . add_option ( ""--listTasks"" , help = ""List all task labels in description.py"" , dest = ""listTasks"" , action = ""store_true"" , default = False ) parser . add_option ( ""--load"" , help = ""Load a model from the given <CHECKPOINT> and run it. "" ""Run with --listCheckpoints flag for more details. "" , dest = ""runCheckpointName"" , action = ""store"" , type = ""string"" , default = """" , metavar = ""<CHECKPOINT>"" ) parser . add_option ( ""--newSerialization"" , help = ""Use new capnproto serialization"" , dest = ""newSerialization"" , action = ""store_true"" , default = False ) parser . add_option ( ""--tasks"" , help = ""Run the tasks with the given TASK LABELS "" ""in the order they are given.  Either end of "" ""arg-list, or a standalone dot ('.') arg or "" ""the next short or long option name (-a or "" ""--blah) terminates the list. NOTE: FAILS "" ""TO RECOGNIZE task label names with one or more "" ""leading dashes. [default: run all of the tasks in "" ""description.py]"" , dest = ""taskLabels"" , default = [ ] , action = ""callback"" , callback = reapVarArgsCallback , metavar = ""TASK_LABELS"" ) parser . add_option ( ""--testMode"" , help = ""Reduce iteration count for testing"" , dest = ""testMode"" , action = ""store_true"" , default = False ) parser . add_option ( ""--noCheckpoint"" , help = ""Don't checkpoint the model after running each task."" , dest = ""checkpointModel"" , action = ""store_false"" , default = True ) options , experiments = parser . parse_args ( args ) mutuallyExclusiveOptionCount = sum ( [ bool ( options . createCheckpointName ) , options . listAvailableCheckpoints , options . listTasks , bool ( options . runCheckpointName ) ] ) if mutuallyExclusiveOptionCount > 1 : _reportCommandLineUsageErrorAndExit ( parser , ""Options: -c, --listCheckpoints, --listTasks, and --load are "" ""mutually exclusive. Please select only one"" ) mutuallyExclusiveOptionCount = sum ( [ bool ( not options . checkpointModel ) , bool ( options . createCheckpointName ) ] ) if mutuallyExclusiveOptionCount > 1 : _reportCommandLineUsageErrorAndExit ( parser , ""Options: -c and --noCheckpoint are "" ""mutually exclusive. Please select only one"" ) if len ( experiments ) != 1 : _reportCommandLineUsageErrorAndExit ( parser , ""Exactly ONE experiment must be specified, but got %s (%s)"" % ( len ( experiments ) , experiments ) ) parser . destroy ( ) experimentDir = os . path . abspath ( experiments [ 0 ] ) privateOptions = dict ( ) privateOptions [ 'createCheckpointName' ] = options . createCheckpointName privateOptions [ 'listAvailableCheckpoints' ] = options . listAvailableCheckpoints privateOptions [ 'listTasks' ] = options . listTasks privateOptions [ 'runCheckpointName' ] = options . runCheckpointName privateOptions [ 'newSerialization' ] = options . newSerialization privateOptions [ 'testMode' ] = options . testMode privateOptions [ 'taskLabels' ] = options . taskLabels privateOptions [ 'checkpointModel' ] = options . checkpointModel result = ParseCommandLineOptionsResult ( experimentDir = experimentDir , privateOptions = privateOptions ) return result",Parse command line options
"def reapVarArgsCallback ( option , optStr , value , parser ) : newValues = [ ] gotDot = False for arg in parser . rargs : if arg . startswith ( ""--"" ) and len ( arg ) > 2 : break if arg . startswith ( ""-"" ) and len ( arg ) > 1 : break if arg == ""."" : gotDot = True break newValues . append ( arg ) if not newValues : raise optparse . OptionValueError ( ( ""Empty arg list for option %r expecting one or more args "" ""(remaining tokens: %r)"" ) % ( optStr , parser . rargs ) ) del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] value = getattr ( parser . values , option . dest , [ ] ) if value is None : value = [ ] value . extend ( newValues ) setattr ( parser . values , option . dest , value )",Used as optparse callback for reaping a variable number of option args . The option may be specified multiple times and all the args associated with that option name will be accumulated in the order that they are encountered
"def _reportCommandLineUsageErrorAndExit ( parser , message ) : print parser . get_usage ( ) print message sys . exit ( 1 )",Report usage error and exit program with error indication .
"def _runExperimentImpl ( options , model = None ) : json_helpers . validate ( options . privateOptions , schemaDict = g_parsedPrivateCommandLineOptionsSchema ) experimentDir = options . experimentDir descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( experimentDir ) expIface = helpers . getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) if options . privateOptions [ 'listAvailableCheckpoints' ] : _printAvailableCheckpoints ( experimentDir ) return None experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) if ( len ( experimentTasks ) == 0 and expIface . getModelControl ( ) [ 'environment' ] == OpfEnvironment . Nupic ) : expIface . convertNupicEnvToOPF ( ) experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) expIface . normalizeStreamSources ( ) newSerialization = options . privateOptions [ 'newSerialization' ] if options . privateOptions [ 'listTasks' ] : print ""Available tasks:"" for label in [ t [ 'taskLabel' ] for t in experimentTasks ] : print ""\t"" , label return None if options . privateOptions [ 'runCheckpointName' ] : assert model is None checkpointName = options . privateOptions [ 'runCheckpointName' ] model = ModelFactory . loadFromCheckpoint ( savedModelDir = _getModelCheckpointDir ( experimentDir , checkpointName ) , newSerialization = newSerialization ) elif model is not None : print ""Skipping creation of OPFExperiment instance: caller provided his own"" else : modelDescription = expIface . getModelDescription ( ) model = ModelFactory . create ( modelDescription ) if options . privateOptions [ 'createCheckpointName' ] : checkpointName = options . privateOptions [ 'createCheckpointName' ] _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = checkpointName , newSerialization = newSerialization ) return model taskIndexList = range ( len ( experimentTasks ) ) customTaskExecutionLabelsList = options . privateOptions [ 'taskLabels' ] if customTaskExecutionLabelsList : taskLabelsList = [ t [ 'taskLabel' ] for t in experimentTasks ] taskLabelsSet = set ( taskLabelsList ) customTaskExecutionLabelsSet = set ( customTaskExecutionLabelsList ) assert customTaskExecutionLabelsSet . issubset ( taskLabelsSet ) , ( ""Some custom-provided task execution labels don't correspond "" ""to actual task labels: mismatched labels: %r; actual task "" ""labels: %r."" ) % ( customTaskExecutionLabelsSet - taskLabelsSet , customTaskExecutionLabelsList ) taskIndexList = [ taskLabelsList . index ( label ) for label in customTaskExecutionLabelsList ] print ""#### Executing custom task list: %r"" % [ taskLabelsList [ i ] for i in taskIndexList ] for taskIndex in taskIndexList : task = experimentTasks [ taskIndex ] taskRunner = _TaskRunner ( model = model , task = task , cmdOptions = options ) taskRunner . run ( ) del taskRunner if options . privateOptions [ 'checkpointModel' ] : _saveModel ( model = model , experimentDir = experimentDir , checkpointLabel = task [ 'taskLabel' ] , newSerialization = newSerialization ) return model",Creates and runs the experiment
"def _saveModel ( model , experimentDir , checkpointLabel , newSerialization = False ) : checkpointDir = _getModelCheckpointDir ( experimentDir , checkpointLabel ) if newSerialization : model . writeToCheckpoint ( checkpointDir ) else : model . save ( saveModelDir = checkpointDir )",Save model
"def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , checkpointLabel + g_defaultCheckpointExtension ) checkpointDir = os . path . abspath ( checkpointDir ) return checkpointDir",Creates directory for serialization of the model
"def getCheckpointParentDir ( experimentDir ) : baseDir = os . path . join ( experimentDir , ""savedmodels"" ) baseDir = os . path . abspath ( baseDir ) return baseDir",Get checkpoint parent dir .
def _checkpointLabelFromCheckpointDir ( checkpointDir ) : assert checkpointDir . endswith ( g_defaultCheckpointExtension ) lastSegment = os . path . split ( checkpointDir ) [ 1 ] checkpointLabel = lastSegment [ 0 : - len ( g_defaultCheckpointExtension ) ] return checkpointLabel,Returns a checkpoint label string for the given model checkpoint directory
def _isCheckpointDir ( checkpointDir ) : lastSegment = os . path . split ( checkpointDir ) [ 1 ] if lastSegment [ 0 ] == '.' : return False if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : return False if not os . path . isdir ( checkpointDir ) : return False return True,Return true iff checkpointDir appears to be a checkpoint directory .
"def _printAvailableCheckpoints ( experimentDir ) : checkpointParentDir = getCheckpointParentDir ( experimentDir ) if not os . path . exists ( checkpointParentDir ) : print ""No available checkpoints."" return checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] if not checkpointDirs : print ""No available checkpoints."" return print ""Available checkpoints:"" checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) for x in checkpointDirs ] for checkpoint in sorted ( checkpointList ) : print ""\t"" , checkpoint print print ""To start from a checkpoint:"" print ""  python run_opf_experiment.py experiment --load <CHECKPOINT>"" print ""For example, to start from the checkpoint \""MyCheckpoint\"":"" print ""  python run_opf_experiment.py experiment --load MyCheckpoint""",List available checkpoints for the specified experiment .
"def run ( self ) : self . __logger . debug ( ""run(): Starting task <%s>"" , self . __task [ 'taskLabel' ] ) if self . __cmdOptions . privateOptions [ 'testMode' ] : numIters = 10 else : numIters = self . __task [ 'iterationCount' ] if numIters >= 0 : iterTracker = iter ( xrange ( numIters ) ) else : iterTracker = iter ( itertools . count ( ) ) periodic = PeriodicActivityMgr ( requestedActivities = self . _createPeriodicActivities ( ) ) self . __model . resetSequenceStates ( ) self . __taskDriver . setup ( ) while True : try : next ( iterTracker ) except StopIteration : break try : inputRecord = self . __datasetReader . next ( ) except StopIteration : break result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) if InferenceElement . encodings in result . inferences : result . inferences . pop ( InferenceElement . encodings ) self . __predictionLogger . writeRecord ( result ) periodic . tick ( ) self . _getAndEmitExperimentMetrics ( final = True ) self . __taskDriver . finalize ( ) self . __model . resetSequenceStates ( )",Runs a single experiment task
"def _createPeriodicActivities ( self ) : periodicActivities = [ ] class MetricsReportCb ( object ) : def __init__ ( self , taskRunner ) : self . __taskRunner = taskRunner return def __call__ ( self ) : self . __taskRunner . _getAndEmitExperimentMetrics ( ) reportMetrics = PeriodicActivityRequest ( repeating = True , period = 1000 , cb = MetricsReportCb ( self ) ) periodicActivities . append ( reportMetrics ) class IterationProgressCb ( object ) : PROGRESS_UPDATE_PERIOD_TICKS = 1000 def __init__ ( self , taskLabel , requestedIterationCount , logger ) : self . __taskLabel = taskLabel self . __requestedIterationCount = requestedIterationCount self . __logger = logger self . __numIterationsSoFar = 0 def __call__ ( self ) : self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS self . __logger . debug ( ""%s: ITERATION PROGRESS: %s of %s"" % ( self . __taskLabel , self . __numIterationsSoFar , self . __requestedIterationCount ) ) iterationProgressCb = IterationProgressCb ( taskLabel = self . __task [ 'taskLabel' ] , requestedIterationCount = self . __task [ 'iterationCount' ] , logger = self . __logger ) iterationProgressReporter = PeriodicActivityRequest ( repeating = True , period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , cb = iterationProgressCb ) periodicActivities . append ( iterationProgressReporter ) return periodicActivities",Creates and returns a list of activites for this TaskRunner instance
"def _generateFile ( filename , data ) : print ""Creating %s..."" % ( filename ) numRecords , numFields = data . shape fields = [ ( 'field%d' % ( i + 1 ) , 'float' , '' ) for i in range ( numFields ) ] outFile = File ( filename , fields ) for i in xrange ( numRecords ) : outFile . write ( data [ i ] . tolist ( ) ) outFile . close ( )",Parameters : ---------------------------------------------------------------- filename : name of . csv file to generate
"def corruptVector ( v1 , noiseLevel , numActiveCols ) : size = len ( v1 ) v2 = np . zeros ( size , dtype = ""uint32"" ) bitsToSwap = int ( noiseLevel * numActiveCols ) for i in range ( size ) : v2 [ i ] = v1 [ i ] for _ in range ( bitsToSwap ) : i = random . randrange ( size ) if v2 [ i ] == 1 : v2 [ i ] = 0 else : v2 [ i ] = 1 return v2",Corrupts a copy of a binary vector by inverting noiseLevel percent of its bits .
"def showPredictions ( ) : for k in range ( 6 ) : tm . reset ( ) print ""--- "" + ""ABCDXY"" [ k ] + "" ---"" tm . compute ( set ( seqT [ k ] [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = False ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] print ( ""Active cols: "" + str ( np . nonzero ( currentColumns ) [ 0 ] ) ) print ( ""Predicted cols: "" + str ( np . nonzero ( predictedColumns ) [ 0 ] ) ) print """"",Shows predictions of the TM when presented with the characters A B C D X and Y without any contextual information that is not embedded within a sequence .
"def trainTM ( sequence , timeSteps , noiseLevel ) : currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = ""uint32"" ) predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = ""uint32"" ) ts = 0 for t in range ( timeSteps ) : tm . reset ( ) for k in range ( 4 ) : v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] acc = accuracy ( currentColumns , predictedColumns ) x . append ( ts ) y . append ( acc ) ts += 1 predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ]",Trains the TM with given sequence for a given number of time steps and level of input corruption
"def encodeIntoArray ( self , inputVal , outputVal ) : if len ( inputVal ) != len ( outputVal ) : raise ValueError ( ""Different input (%i) and output (%i) sizes."" % ( len ( inputVal ) , len ( outputVal ) ) ) if self . w is not None and sum ( inputVal ) != self . w : raise ValueError ( ""Input has %i bits but w was set to %i."" % ( sum ( inputVal ) , self . w ) ) outputVal [ : ] = inputVal [ : ] if self . verbosity >= 2 : print ""input:"" , inputVal , ""output:"" , outputVal print ""decoded:"" , self . decodedToStr ( self . decode ( outputVal ) )",See method description in base . py
"def decode ( self , encoded , parentFieldName = """" ) : if parentFieldName != """" : fieldName = ""%s.%s"" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( [ [ 0 , 0 ] ] , ""input"" ) } , [ fieldName ] )",See the function description in base . py
"def getBucketInfo ( self , buckets ) : return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ]",See the function description in base . py
"def topDownCompute ( self , encoded ) : return EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) )",See the function description in base . py
"def closenessScores ( self , expValues , actValues , * * kwargs ) : ratio = 1.0 esum = int ( expValues . sum ( ) ) asum = int ( actValues . sum ( ) ) if asum > esum : diff = asum - esum if diff < esum : ratio = 1 - diff / float ( esum ) else : ratio = 1 / float ( diff ) olap = expValues & actValues osum = int ( olap . sum ( ) ) if esum == 0 : r = 0.0 else : r = osum / float ( esum ) r = r * ratio return numpy . array ( [ r ] )",Does a bitwise compare of the two bitmaps and returns a fractonal value between 0 and 1 of how similar they are .
"def getCallerInfo ( depth = 2 ) : f = sys . _getframe ( depth ) method_name = f . f_code . co_name filename = f . f_code . co_filename arg_class = None args = inspect . getargvalues ( f ) if len ( args [ 0 ] ) > 0 : arg_name = args [ 0 ] [ 0 ] arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ return ( method_name , filename , arg_class )",Utility function to get information about function callers
"def title ( s = None , additional = '' , stream = sys . stdout ) : if s is None : callable_name , file_name , class_name = getCallerInfo ( 2 ) s = callable_name if class_name is not None : s = class_name + '.' + callable_name lines = ( s + additional ) . split ( '\n' ) length = max ( len ( line ) for line in lines ) print >> stream , '-' * length print >> stream , s + additional print >> stream , '-' * length",Utility function to display nice titles
"def getArgumentDescriptions ( f ) : argspec = inspect . getargspec ( f ) docstring = f . __doc__ descriptions = { } if docstring : lines = docstring . split ( '\n' ) i = 0 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : i += 1 continue indentLevel = lines [ i ] . index ( stripped [ 0 ] ) firstWord = stripped . split ( ) [ 0 ] if firstWord . endswith ( ':' ) : firstWord = firstWord [ : - 1 ] if firstWord in argspec . args : argName = firstWord restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) argLines = [ restOfLine ] i += 1 while i < len ( lines ) : stripped = lines [ i ] . lstrip ( ) if not stripped : break if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : break argLines . append ( lines [ i ] . strip ( ) ) i += 1 descriptions [ argName ] = ' ' . join ( argLines ) else : i += 1 args = [ ] if argspec . defaults : defaultCount = len ( argspec . defaults ) else : defaultCount = 0 nonDefaultArgCount = len ( argspec . args ) - defaultCount for i , argName in enumerate ( argspec . args ) : if i >= nonDefaultArgCount : defaultValue = argspec . defaults [ i - nonDefaultArgCount ] args . append ( ( argName , descriptions . get ( argName , """" ) , defaultValue ) ) else : args . append ( ( argName , descriptions . get ( argName , """" ) ) ) return args",Get the arguments default values and argument descriptions for a function .
"def initLogging ( verbose = False , console = 'stdout' , consoleLevel = 'DEBUG' ) : global gLoggingInitialized if gLoggingInitialized : if verbose : print >> sys . stderr , ""Logging already initialized, doing nothing."" return consoleStreamMappings = { 'stdout' : 'stdoutConsoleHandler' , 'stderr' : 'stderrConsoleHandler' , } consoleLogLevels = [ 'DEBUG' , 'INFO' , 'WARNING' , 'WARN' , 'ERROR' , 'CRITICAL' , 'FATAL' ] assert console is None or console in consoleStreamMappings . keys ( ) , ( 'Unexpected console arg value: %r' ) % ( console , ) assert consoleLevel in consoleLogLevels , ( 'Unexpected consoleLevel arg value: %r' ) % ( consoleLevel ) configFilename = 'nupic-logging.conf' configFilePath = resource_filename ( ""nupic.support"" , configFilename ) configLogDir = os . environ . get ( 'NTA_LOG_DIR' , None ) if verbose : print >> sys . stderr , ( ""Using logging configuration file: %s"" ) % ( configFilePath ) replacements = dict ( ) def makeKey ( name ) : """""" Makes replacement key """""" return ""$$%s$$"" % ( name ) platform = sys . platform . lower ( ) if platform . startswith ( 'java' ) : import java . lang platform = java . lang . System . getProperty ( ""os.name"" ) . lower ( ) if platform . startswith ( 'mac os x' ) : platform = 'darwin' if platform . startswith ( 'darwin' ) : replacements [ makeKey ( 'SYSLOG_HANDLER_ADDRESS' ) ] = '""/var/run/syslog""' elif platform . startswith ( 'linux' ) : replacements [ makeKey ( 'SYSLOG_HANDLER_ADDRESS' ) ] = '""/dev/log""' elif platform . startswith ( 'win' ) : replacements [ makeKey ( 'SYSLOG_HANDLER_ADDRESS' ) ] = '""log""' else : raise RuntimeError ( ""This platform is neither darwin, win32, nor linux: %s"" % ( sys . platform , ) ) replacements [ makeKey ( 'PERSISTENT_LOG_HANDLER' ) ] = 'fileHandler' if platform . startswith ( 'win' ) : replacements [ makeKey ( 'FILE_HANDLER_LOG_FILENAME' ) ] = '""NUL""' else : replacements [ makeKey ( 'FILE_HANDLER_LOG_FILENAME' ) ] = '""/dev/null""' handlers = list ( ) if configLogDir is not None : logFilePath = _genLoggingFilePath ( ) makeDirectoryFromAbsolutePath ( os . path . dirname ( logFilePath ) ) replacements [ makeKey ( 'FILE_HANDLER_LOG_FILENAME' ) ] = repr ( logFilePath ) handlers . append ( replacements [ makeKey ( 'PERSISTENT_LOG_HANDLER' ) ] ) if console is not None : handlers . append ( consoleStreamMappings [ console ] ) replacements [ makeKey ( 'ROOT_LOGGER_HANDLERS' ) ] = "", "" . join ( handlers ) replacements [ makeKey ( 'CONSOLE_LOG_LEVEL' ) ] = consoleLevel customConfig = StringIO ( ) loggingFileContents = resource_string ( __name__ , configFilename ) for lineNum , line in enumerate ( loggingFileContents . splitlines ( ) ) : if ""$$"" in line : for ( key , value ) in replacements . items ( ) : line = line . replace ( key , value ) if ""$$"" in line and ""$$<key>$$"" not in line : raise RuntimeError ( ( ""The text %r, found at line #%d of file %r, "" ""contains a string not found in our replacement "" ""dict."" ) % ( line , lineNum , configFilePath ) ) customConfig . write ( ""%s\n"" % line ) customConfig . seek ( 0 ) if python_version ( ) [ : 3 ] >= '2.6' : logging . config . fileConfig ( customConfig , disable_existing_loggers = False ) else : logging . config . fileConfig ( customConfig ) gLoggingInitialized = True",Initilize NuPic logging by reading in from the logging configuration file . The logging configuration file is named nupic - logging . conf and is expected to be in the format defined by the python logging module .
"def _genLoggingFilePath ( ) : appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' appLogDir = os . path . abspath ( os . path . join ( os . environ [ 'NTA_LOG_DIR' ] , 'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , appName ) ) appLogFileName = '%s-%s-%s.log' % ( appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) return os . path . join ( appLogDir , appLogFileName )",Generate a filepath for the calling app
"def aggregationToMonthsSeconds ( interval ) : seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 seconds += interval . get ( 'seconds' , 0 ) seconds += interval . get ( 'minutes' , 0 ) * 60 seconds += interval . get ( 'hours' , 0 ) * 60 * 60 seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 months = interval . get ( 'months' , 0 ) months += 12 * interval . get ( 'years' , 0 ) return { 'months' : months , 'seconds' : seconds }",Return the number of months and seconds from an aggregation dict that represents a date and time .
"def aggregationDivide ( dividend , divisor ) : dividendMonthSec = aggregationToMonthsSeconds ( dividend ) divisorMonthSec = aggregationToMonthsSeconds ( divisor ) if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : raise RuntimeError ( ""Aggregation dicts with months/years can only be "" ""inter-operated with other aggregation dicts that contain "" ""months/years"" ) if dividendMonthSec [ 'months' ] > 0 : return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] else : return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ]",Return the result from dividing two dicts that represent date and time .
"def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , ""jsonschema"" , opfJsonSchemaFilename ) jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) return",Validate a python object against an OPF json schema file
"def initLogger ( obj ) : if inspect . isclass ( obj ) : myClass = obj else : myClass = obj . __class__ logger = logging . getLogger ( ""."" . join ( [ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) return logger",Helper function to create a logger object for the current object with the standard Numenta prefix .
"def matchPatterns ( patterns , keys ) : results = [ ] if patterns : for pattern in patterns : prog = re . compile ( pattern ) for key in keys : if prog . match ( key ) : results . append ( key ) else : return None return results",Returns a subset of the keys that match any of the given patterns
"def _getScaledValue ( self , inpt ) : if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : return None else : val = inpt if val < self . minval : val = self . minval elif val > self . maxval : val = self . maxval scaledVal = math . log10 ( val ) return scaledVal",Convert the input which is in normal space into log space
"def getBucketIndices ( self , inpt ) : scaledVal = self . _getScaledValue ( inpt ) if scaledVal is None : return [ None ] else : return self . encoder . getBucketIndices ( scaledVal )",See the function description in base . py
"def encodeIntoArray ( self , inpt , output ) : scaledVal = self . _getScaledValue ( inpt ) if scaledVal is None : output [ 0 : ] = 0 else : self . encoder . encodeIntoArray ( scaledVal , output ) if self . verbosity >= 2 : print ""input:"" , inpt , ""scaledVal:"" , scaledVal , ""output:"" , output print ""decoded:"" , self . decodedToStr ( self . decode ( output ) )",See the function description in base . py
"def decode ( self , encoded , parentFieldName = '' ) : ( fieldsDict , fieldNames ) = self . encoder . decode ( encoded ) if len ( fieldsDict ) == 0 : return ( fieldsDict , fieldNames ) assert ( len ( fieldsDict ) == 1 ) ( inRanges , inDesc ) = fieldsDict . values ( ) [ 0 ] outRanges = [ ] for ( minV , maxV ) in inRanges : outRanges . append ( ( math . pow ( 10 , minV ) , math . pow ( 10 , maxV ) ) ) desc = """" numRanges = len ( outRanges ) for i in xrange ( numRanges ) : if outRanges [ i ] [ 0 ] != outRanges [ i ] [ 1 ] : desc += ""%.2f-%.2f"" % ( outRanges [ i ] [ 0 ] , outRanges [ i ] [ 1 ] ) else : desc += ""%.2f"" % ( outRanges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += "", "" if parentFieldName != '' : fieldName = ""%s.%s"" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( outRanges , desc ) } , [ fieldName ] )",See the function description in base . py
"def getBucketValues ( self ) : if self . _bucketValues is None : scaledValues = self . encoder . getBucketValues ( ) self . _bucketValues = [ ] for scaledValue in scaledValues : value = math . pow ( 10 , scaledValue ) self . _bucketValues . append ( value ) return self . _bucketValues",See the function description in base . py
"def getBucketInfo ( self , buckets ) : scaledResult = self . encoder . getBucketInfo ( buckets ) [ 0 ] scaledValue = scaledResult . value value = math . pow ( 10 , scaledValue ) return [ EncoderResult ( value = value , scalar = value , encoding = scaledResult . encoding ) ]",See the function description in base . py
"def topDownCompute ( self , encoded ) : scaledResult = self . encoder . topDownCompute ( encoded ) [ 0 ] scaledValue = scaledResult . value value = math . pow ( 10 , scaledValue ) return EncoderResult ( value = value , scalar = value , encoding = scaledResult . encoding )",See the function description in base . py
"def closenessScores ( self , expValues , actValues , fractional = True ) : if expValues [ 0 ] > 0 : expValue = math . log10 ( expValues [ 0 ] ) else : expValue = self . minScaledValue if actValues [ 0 ] > 0 : actValue = math . log10 ( actValues [ 0 ] ) else : actValue = self . minScaledValue if fractional : err = abs ( expValue - actValue ) pctErr = err / ( self . maxScaledValue - self . minScaledValue ) pctErr = min ( 1.0 , pctErr ) closeness = 1.0 - pctErr else : err = abs ( expValue - actValue ) closeness = err return numpy . array ( [ closeness ] )",See the function description in base . py
"def export ( self ) : graph = nx . MultiDiGraph ( ) regions = self . network . getRegions ( ) for idx in xrange ( regions . getCount ( ) ) : regionPair = regions . getByIndex ( idx ) regionName = regionPair [ 0 ] graph . add_node ( regionName , label = regionName ) for linkName , link in self . network . getLinks ( ) : graph . add_edge ( link . getSrcRegionName ( ) , link . getDestRegionName ( ) , src = link . getSrcOutputName ( ) , dest = link . getDestInputName ( ) ) return graph",Exports a network as a networkx MultiDiGraph intermediate representation suitable for visualization .
"def bitsToString ( arr ) : s = array ( 'c' , '.' * len ( arr ) ) for i in xrange ( len ( arr ) ) : if arr [ i ] == 1 : s [ i ] = '*' return s",Returns a string representing a numpy array of 0 s and 1 s
"def percentOverlap ( x1 , x2 , size ) : nonZeroX1 = np . count_nonzero ( x1 ) nonZeroX2 = np . count_nonzero ( x2 ) minX1X2 = min ( nonZeroX1 , nonZeroX2 ) percentOverlap = 0 if minX1X2 > 0 : percentOverlap = float ( np . dot ( x1 , x2 ) ) / float ( minX1X2 ) return percentOverlap",Computes the percentage of overlap between vectors x1 and x2 .
"def resetVector ( x1 , x2 ) : size = len ( x1 ) for i in range ( size ) : x2 [ i ] = x1 [ i ]",Copies the contents of vector x1 into vector x2 .
"def runCPU ( ) : model = ModelFactory . create ( model_params . MODEL_PARAMS ) model . enableInference ( { 'predictedField' : 'cpu' } ) shifter = InferenceShifter ( ) actHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) predHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) actline , = plt . plot ( range ( WINDOW ) , actHistory ) predline , = plt . plot ( range ( WINDOW ) , predHistory ) actline . axes . set_ylim ( 0 , 100 ) predline . axes . set_ylim ( 0 , 100 ) while True : s = time . time ( ) cpu = psutil . cpu_percent ( ) modelInput = { 'cpu' : cpu } result = shifter . shift ( model . run ( modelInput ) ) inference = result . inferences [ 'multiStepBestPredictions' ] [ 5 ] if inference is not None : actHistory . append ( result . rawInput [ 'cpu' ] ) predHistory . append ( inference ) actline . set_ydata ( actHistory ) predline . set_ydata ( predHistory ) plt . draw ( ) plt . legend ( ( 'actual' , 'predicted' ) ) try : plt . pause ( SECONDS_PER_STEP ) except : pass",Poll CPU usage make predictions and plot the results . Runs forever .
"def _extractCallingMethodArgs ( ) : import inspect import copy callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) argNames . remove ( ""self"" ) args = copy . copy ( frameLocalVarDict ) for varName in frameLocalVarDict : if varName not in argNames : args . pop ( varName ) return args",Returns args dictionary from the calling method
"def write ( self , proto ) : super ( BacktrackingTMCPP , self ) . write ( proto . baseTM ) self . cells4 . write ( proto . cells4 ) proto . makeCells4Ephemeral = self . makeCells4Ephemeral proto . seed = self . seed proto . checkSynapseConsistency = self . checkSynapseConsistency proto . initArgs = json . dumps ( self . _initArgsDict )",Populate serialization proto instance .
"def read ( cls , proto ) : obj = BacktrackingTM . read ( proto . baseTM ) obj . __class__ = cls newCells4 = Cells4 . read ( proto . cells4 ) print newCells4 obj . cells4 = newCells4 obj . makeCells4Ephemeral = proto . makeCells4Ephemeral obj . seed = proto . seed obj . checkSynapseConsistency = proto . checkSynapseConsistency obj . _initArgsDict = json . loads ( proto . initArgs ) obj . _initArgsDict [ ""outputType"" ] = str ( obj . _initArgsDict [ ""outputType"" ] ) obj . allocateStatesInCPP = False obj . retrieveLearningStates = False obj . _setStatePointers ( ) return obj",Deserialize from proto instance .
def _getEphemeralMembers ( self ) : e = BacktrackingTM . _getEphemeralMembers ( self ) if self . makeCells4Ephemeral : e . extend ( [ 'cells4' ] ) return e,List of our member variables that we don t need to be saved
def _initEphemerals ( self ) : BacktrackingTM . _initEphemerals ( self ) self . allocateStatesInCPP = False self . retrieveLearningStates = False if self . makeCells4Ephemeral : self . _initCells4 ( ),Initialize all ephemeral members after being restored to a pickled state .
"def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : assert ( bottomUpInput . dtype == numpy . dtype ( 'float32' ) ) or ( bottomUpInput . dtype == numpy . dtype ( 'uint32' ) ) or ( bottomUpInput . dtype == numpy . dtype ( 'int32' ) ) self . iterationIdx = self . iterationIdx + 1 if enableInference is None : if enableLearn : enableInference = False else : enableInference = True self . _setStatePointers ( ) y = self . cells4 . compute ( bottomUpInput , enableInference , enableLearn ) self . currentOutput = y . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . avgLearnedSeqLength = self . cells4 . getAvgLearnedSeqLength ( ) self . _copyAllocatedStates ( ) if self . collectStats : activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . compute .
"def _copyAllocatedStates ( self ) : if self . verbosity > 1 or self . retrieveLearningStates : ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) if self . allocateStatesInCPP : assert False ( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , confidenceT1 ) = self . cells4 . getStates ( ) self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) )",If state is allocated in CPP copy over the data into our numpy arrays .
"def _setStatePointers ( self ) : if not self . allocateStatesInCPP : self . cells4 . setStatePointers ( self . infActiveState [ ""t"" ] , self . infActiveState [ ""t-1"" ] , self . infPredictedState [ ""t"" ] , self . infPredictedState [ ""t-1"" ] , self . colConfidence [ ""t"" ] , self . colConfidence [ ""t-1"" ] , self . cellConfidence [ ""t"" ] , self . cellConfidence [ ""t-1"" ] )",If we are having CPP use numpy - allocated buffers set these buffer pointers . This is a relatively fast operation and for safety should be done before every call to the cells4 compute methods . This protects us in situations where code can cause Python or numpy to create copies .
"def reset ( self ) : if self . verbosity >= 3 : print ""TM Reset"" self . _setStatePointers ( ) self . cells4 . reset ( ) BacktrackingTM . reset ( self )",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . reset .
"def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = 0.0 if minNumSyns is None : minNumSyns = 0 if self . verbosity >= 5 : print ""Cells, all segments:"" self . printCells ( predictedOnly = False ) return self . cells4 . trimSegments ( minPermanence = minPermanence , minNumSyns = minNumSyns )",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . trimSegments .
"def printSegmentUpdates ( self ) : assert False print ""=== SEGMENT UPDATES ===, Num = "" , len ( self . segmentUpdates ) for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] print c , i , updateList",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . printSegmentUpdates .
"def _slowIsSegmentActive ( self , seg , timeStep ) : numSyn = seg . size ( ) numActiveSyns = 0 for synIdx in xrange ( numSyn ) : if seg . getPermanence ( synIdx ) < self . connectedPerm : continue sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( synIdx ) ) if self . infActiveState [ timeStep ] [ sc , si ] : numActiveSyns += 1 if numActiveSyns >= self . activationThreshold : return True return numActiveSyns >= self . activationThreshold",A segment is active if it has > = activationThreshold connected synapses that are active due to infActiveState .
"def printCell ( self , c , i , onlyActiveSegments = False ) : nSegs = self . cells4 . nSegmentsOnCell ( c , i ) if nSegs > 0 : segList = self . cells4 . getNonEmptySegList ( c , i ) gidx = c * self . cellsPerColumn + i print ""Column"" , c , ""Cell"" , i , ""(%d)"" % ( gidx ) , "":"" , nSegs , ""segment(s)"" for k , segIdx in enumerate ( segList ) : seg = self . cells4 . getSegment ( c , i , segIdx ) isActive = self . _slowIsSegmentActive ( seg , 't' ) if onlyActiveSegments and not isActive : continue isActiveStr = ""*"" if isActive else "" "" print ""  %sSeg #%-3d"" % ( isActiveStr , segIdx ) , print seg . size ( ) , print seg . isSequenceSegment ( ) , ""%9.7f"" % ( seg . dutyCycle ( self . cells4 . getNLrnIterations ( ) , False , True ) ) , print ""(%4d/%-4d)"" % ( seg . getPositiveActivations ( ) , seg . getTotalActivations ( ) ) , print ""%4d"" % ( self . cells4 . getNLrnIterations ( ) - seg . getLastActiveIteration ( ) ) , numSyn = seg . size ( ) for s in xrange ( numSyn ) : sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( s ) ) print ""[%d,%d]%4.2f"" % ( sc , si , seg . getPermanence ( s ) ) , print",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . printCell .
"def getColCellIdx ( self , idx ) : c = idx // self . cellsPerColumn i = idx - c * self . cellsPerColumn return c , i",Get column and cell within column from a global cell index . The global index is idx = colIdx * nCellsPerCol () + cellIdxInCol : param idx : ( int ) global cell index : returns : ( tuple ) ( colIdx cellIdxInCol )
"def getSegmentOnCell ( self , c , i , segIdx ) : segList = self . cells4 . getNonEmptySegList ( c , i ) seg = self . cells4 . getSegment ( c , i , segList [ segIdx ] ) numSyn = seg . size ( ) assert numSyn != 0 result = [ ] result . append ( [ int ( segIdx ) , bool ( seg . isSequenceSegment ( ) ) , seg . getPositiveActivations ( ) , seg . getTotalActivations ( ) , seg . getLastActiveIteration ( ) , seg . getLastPosDutyCycle ( ) , seg . getLastPosDutyCycleIteration ( ) ] ) for s in xrange ( numSyn ) : sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( s ) ) result . append ( [ int ( sc ) , int ( si ) , seg . getPermanence ( s ) ] ) return result",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . getSegmentOnCell .
"def getSegmentInfo ( self , collectActiveData = False ) : assert collectActiveData == False nSegments , nSynapses = self . getNumSegments ( ) , self . cells4 . nSynapses ( ) distSegSizes , distNSegsPerCell = { } , { } nActiveSegs , nActiveSynapses = 0 , 0 distPermValues = { } numAgeBuckets = 20 distAges = [ ] ageBucketSize = int ( ( self . iterationIdx + 20 ) / 20 ) for i in range ( numAgeBuckets ) : distAges . append ( [ '%d-%d' % ( i * ageBucketSize , ( i + 1 ) * ageBucketSize - 1 ) , 0 ] ) for c in xrange ( self . numberOfCols ) : for i in xrange ( self . cellsPerColumn ) : nSegmentsThisCell = self . getNumSegmentsInCell ( c , i ) if nSegmentsThisCell > 0 : if distNSegsPerCell . has_key ( nSegmentsThisCell ) : distNSegsPerCell [ nSegmentsThisCell ] += 1 else : distNSegsPerCell [ nSegmentsThisCell ] = 1 segList = self . cells4 . getNonEmptySegList ( c , i ) for segIdx in xrange ( nSegmentsThisCell ) : seg = self . getSegmentOnCell ( c , i , segIdx ) nSynapsesThisSeg = len ( seg ) - 1 if nSynapsesThisSeg > 0 : if distSegSizes . has_key ( nSynapsesThisSeg ) : distSegSizes [ nSynapsesThisSeg ] += 1 else : distSegSizes [ nSynapsesThisSeg ] = 1 for syn in seg [ 1 : ] : p = int ( syn [ 2 ] * 10 ) if distPermValues . has_key ( p ) : distPermValues [ p ] += 1 else : distPermValues [ p ] = 1 segObj = self . cells4 . getSegment ( c , i , segList [ segIdx ] ) age = self . iterationIdx - segObj . getLastActiveIteration ( ) ageBucket = int ( age / ageBucketSize ) distAges [ ageBucket ] [ 1 ] += 1 return ( nSegments , nSynapses , nActiveSegs , nActiveSynapses , distSegSizes , distNSegsPerCell , distPermValues , distAges )",Overrides : meth : nupic . algorithms . backtracking_tm . BacktrackingTM . getSegmentInfo .
"def main ( argv ) : parser = OptionParser ( helpString ) parser . add_option ( ""--jobID"" , action = ""store"" , type = ""int"" , default = None , help = ""jobID of the job within the dbTable [default: %default]."" ) parser . add_option ( ""--modelID"" , action = ""store"" , type = ""str"" , default = None , help = ( ""Tell worker to re-run this model ID. When specified, jobID "" ""must also be specified [default: %default]."" ) ) parser . add_option ( ""--workerID"" , action = ""store"" , type = ""str"" , default = None , help = ( ""workerID of the scheduler's SlotAgent (GenericWorker) that "" ""hosts this SpecializedWorker [default: %default]."" ) ) parser . add_option ( ""--params"" , action = ""store"" , default = None , help = ""Create and execute a new hypersearch request using this JSON "" ""format params string. This is helpful for unit tests and debugging. "" ""When specified jobID must NOT be specified. [default: %default]."" ) parser . add_option ( ""--clearModels"" , action = ""store_true"" , default = False , help = ""clear out the models table before starting [default: %default]."" ) parser . add_option ( ""--resetJobStatus"" , action = ""store_true"" , default = False , help = ""Reset the job status before starting  [default: %default]."" ) parser . add_option ( ""--logLevel"" , action = ""store"" , type = ""int"" , default = None , help = ""override default log level. Pass in an integer value that "" ""represents the desired logging level (10=logging.DEBUG, "" ""20=logging.INFO, etc.) [default: %default]."" ) ( options , args ) = parser . parse_args ( argv [ 1 : ] ) if len ( args ) != 0 : raise RuntimeError ( ""Expected no command line arguments, but got: %s"" % ( args ) ) if ( options . jobID and options . params ) : raise RuntimeError ( ""--jobID and --params can not be used at the same time"" ) if ( options . jobID is None and options . params is None ) : raise RuntimeError ( ""Either --jobID or --params must be specified."" ) initLogging ( verbose = True ) hst = HypersearchWorker ( options , argv [ 1 : ] ) if options . params is None : try : jobID = hst . run ( ) except Exception , e : jobID = options . jobID msg = StringIO . StringIO ( ) print >> msg , ""%s: Exception occurred in Hypersearch Worker: %r"" % ( ErrorCodes . hypersearchLogicErr , e ) traceback . print_exc ( None , msg ) completionReason = ClientJobsDAO . CMPL_REASON_ERROR completionMsg = msg . getvalue ( ) hst . logger . error ( completionMsg ) jobsDAO = ClientJobsDAO . get ( ) workerCmpReason = jobsDAO . jobGetFields ( options . jobID , [ 'workerCompletionReason' ] ) [ 0 ] if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : jobsDAO . jobSetFields ( options . jobID , fields = dict ( cancel = True , workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , workerCompletionMsg = completionMsg ) , useConnectionID = False , ignoreUnchanged = True ) else : jobID = None completionReason = ClientJobsDAO . CMPL_REASON_SUCCESS completionMsg = ""Success"" try : jobID = hst . run ( ) except Exception , e : jobID = hst . _options . jobID completionReason = ClientJobsDAO . CMPL_REASON_ERROR completionMsg = ""ERROR: %s"" % ( e , ) raise finally : if jobID is not None : cjDAO = ClientJobsDAO . get ( ) cjDAO . jobSetCompleted ( jobID = jobID , completionReason = completionReason , completionMsg = completionMsg ) return jobID",The main function of the HypersearchWorker script . This parses the command line arguments instantiates a HypersearchWorker instance and then runs it .
"def _processUpdatedModels ( self , cjDAO ) : curModelIDCtrList = cjDAO . modelsGetUpdateCounters ( self . _options . jobID ) if len ( curModelIDCtrList ) == 0 : return self . logger . debug ( ""current modelID/updateCounters: %s"" % ( str ( curModelIDCtrList ) ) ) self . logger . debug ( ""last modelID/updateCounters: %s"" % ( str ( self . _modelIDCtrList ) ) ) curModelIDCtrList = sorted ( curModelIDCtrList ) numItems = len ( curModelIDCtrList ) changedEntries = filter ( lambda x : x [ 1 ] [ 1 ] != x [ 2 ] [ 1 ] , itertools . izip ( xrange ( numItems ) , curModelIDCtrList , self . _modelIDCtrList ) ) if len ( changedEntries ) > 0 : self . logger . debug ( ""changedEntries: %s"" , str ( changedEntries ) ) for entry in changedEntries : ( idx , ( modelID , curCtr ) , ( _ , oldCtr ) ) = entry self . _modelIDCtrDict [ modelID ] = curCtr assert ( self . _modelIDCtrList [ idx ] [ 0 ] == modelID ) assert ( curCtr != oldCtr ) self . _modelIDCtrList [ idx ] [ 1 ] = curCtr changedModelIDs = [ x [ 1 ] [ 0 ] for x in changedEntries ] modelResults = cjDAO . modelsGetResultAndStatus ( changedModelIDs ) for mResult in modelResults : results = mResult . results if results is not None : results = json . loads ( results ) self . _hs . recordModelProgress ( modelID = mResult . modelId , modelParams = None , modelParamsHash = mResult . engParamsHash , results = results , completed = ( mResult . status == cjDAO . STATUS_COMPLETED ) , completionReason = mResult . completionReason , matured = mResult . engMatured , numRecords = mResult . numRecords ) curModelIDSet = set ( [ x [ 0 ] for x in curModelIDCtrList ] ) newModelIDs = curModelIDSet . difference ( self . _modelIDSet ) if len ( newModelIDs ) > 0 : self . _modelIDSet . update ( newModelIDs ) curModelIDCtrDict = dict ( curModelIDCtrList ) modelInfos = cjDAO . modelsGetResultAndStatus ( newModelIDs ) modelInfos . sort ( ) modelParamsAndHashs = cjDAO . modelsGetParams ( newModelIDs ) modelParamsAndHashs . sort ( ) for ( mResult , mParamsAndHash ) in itertools . izip ( modelInfos , modelParamsAndHashs ) : modelID = mResult . modelId assert ( modelID == mParamsAndHash . modelId ) self . _modelIDCtrDict [ modelID ] = curModelIDCtrDict [ modelID ] self . _modelIDCtrList . append ( [ modelID , curModelIDCtrDict [ modelID ] ] ) results = mResult . results if results is not None : results = json . loads ( mResult . results ) self . _hs . recordModelProgress ( modelID = modelID , modelParams = json . loads ( mParamsAndHash . params ) , modelParamsHash = mParamsAndHash . engParamsHash , results = results , completed = ( mResult . status == cjDAO . STATUS_COMPLETED ) , completionReason = ( mResult . completionReason ) , matured = mResult . engMatured , numRecords = mResult . numRecords ) self . _modelIDCtrList . sort ( )",For all models that modified their results since last time this method was called send their latest results to the Hypersearch implementation .
"def run ( self ) : options = self . _options self . logger . info ( ""Connecting to the jobs database"" ) cjDAO = ClientJobsDAO . get ( ) self . _workerID = cjDAO . getConnectionID ( ) if options . clearModels : cjDAO . modelsClearAll ( ) if options . params is not None : options . jobID = cjDAO . jobInsert ( client = 'hwTest' , cmdLine = ""echo 'test mode'"" , params = options . params , alreadyRunning = True , minimumWorkers = 1 , maximumWorkers = 1 , jobType = cjDAO . JOB_TYPE_HS ) if options . workerID is not None : wID = options . workerID else : wID = self . _workerID buildID = Configuration . get ( 'nupic.software.buildNumber' , 'N/A' ) logPrefix = '<BUILDID=%s, WORKER=HW, WRKID=%s, JOBID=%s> ' % ( buildID , wID , options . jobID ) ExtendedLogger . setLogPrefix ( logPrefix ) if options . resetJobStatus : cjDAO . jobSetFields ( options . jobID , fields = { 'workerCompletionReason' : ClientJobsDAO . CMPL_REASON_SUCCESS , 'cancel' : False , } , useConnectionID = False , ignoreUnchanged = True ) jobInfo = cjDAO . jobInfo ( options . jobID ) self . logger . info ( ""Job info retrieved: %s"" % ( str ( clippedObj ( jobInfo ) ) ) ) jobParams = json . loads ( jobInfo . params ) jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , ""jsonschema"" , ""jobParamsSchema.json"" ) validate ( jobParams , schemaPath = jsonSchemaPath ) hsVersion = jobParams . get ( 'hsVersion' , None ) if hsVersion == 'v2' : self . _hs = HypersearchV2 ( searchParams = jobParams , workerID = self . _workerID , cjDAO = cjDAO , jobID = options . jobID , logLevel = options . logLevel ) else : raise RuntimeError ( ""Invalid Hypersearch implementation (%s) specified"" % ( hsVersion ) ) try : exit = False numModelsTotal = 0 print >> sys . stderr , ""reporter:status:Evaluating first model..."" while not exit : batchSize = 10 modelIDToRun = None while modelIDToRun is None : if options . modelID is None : self . _processUpdatedModels ( cjDAO ) ( exit , newModels ) = self . _hs . createModels ( numModels = batchSize ) if exit : break if len ( newModels ) == 0 : continue for ( modelParams , modelParamsHash , particleHash ) in newModels : jsonModelParams = json . dumps ( modelParams ) ( modelID , ours ) = cjDAO . modelInsertAndStart ( options . jobID , jsonModelParams , modelParamsHash , particleHash ) if not ours : mParamsAndHash = cjDAO . modelsGetParams ( [ modelID ] ) [ 0 ] mResult = cjDAO . modelsGetResultAndStatus ( [ modelID ] ) [ 0 ] results = mResult . results if results is not None : results = json . loads ( results ) modelParams = json . loads ( mParamsAndHash . params ) particleHash = cjDAO . modelsGetFields ( modelID , [ 'engParticleHash' ] ) [ 0 ] particleInst = ""%s.%s"" % ( modelParams [ 'particleState' ] [ 'id' ] , modelParams [ 'particleState' ] [ 'genIdx' ] ) self . logger . info ( ""Adding model %d to our internal DB "" ""because modelInsertAndStart() failed to insert it: "" ""paramsHash=%s, particleHash=%s, particleId='%s'"" , modelID , mParamsAndHash . engParamsHash . encode ( 'hex' ) , particleHash . encode ( 'hex' ) , particleInst ) self . _hs . recordModelProgress ( modelID = modelID , modelParams = modelParams , modelParamsHash = mParamsAndHash . engParamsHash , results = results , completed = ( mResult . status == cjDAO . STATUS_COMPLETED ) , completionReason = mResult . completionReason , matured = mResult . engMatured , numRecords = mResult . numRecords ) else : modelIDToRun = modelID break else : modelIDToRun = int ( options . modelID ) mParamsAndHash = cjDAO . modelsGetParams ( [ modelIDToRun ] ) [ 0 ] modelParams = json . loads ( mParamsAndHash . params ) modelParamsHash = mParamsAndHash . engParamsHash cjDAO . modelSetFields ( modelIDToRun , dict ( engWorkerConnId = self . _workerID ) ) if False : for attempt in range ( 1000 ) : paramsHash = hashlib . md5 ( ""OrphanParams.%d.%d"" % ( modelIDToRun , attempt ) ) . digest ( ) particleHash = hashlib . md5 ( ""OrphanParticle.%d.%d"" % ( modelIDToRun , attempt ) ) . digest ( ) try : cjDAO . modelSetFields ( modelIDToRun , dict ( engParamsHash = paramsHash , engParticleHash = particleHash ) ) success = True except : success = False if success : break if not success : raise RuntimeError ( ""Unexpected failure to change paramsHash and "" ""particleHash of orphaned model"" ) ( modelIDToRun , ours ) = cjDAO . modelInsertAndStart ( options . jobID , mParamsAndHash . params , modelParamsHash ) if exit : break self . logger . info ( ""RUNNING MODEL GID=%d, paramsHash=%s, params=%s"" , modelIDToRun , modelParamsHash . encode ( 'hex' ) , modelParams ) persistentJobGUID = jobParams [ 'persistentJobGUID' ] assert persistentJobGUID , ""persistentJobGUID: %r"" % ( persistentJobGUID , ) modelCheckpointGUID = jobInfo . client + ""_"" + persistentJobGUID + ( '_' + str ( modelIDToRun ) ) self . _hs . runModel ( modelID = modelIDToRun , jobID = options . jobID , modelParams = modelParams , modelParamsHash = modelParamsHash , jobsDAO = cjDAO , modelCheckpointGUID = modelCheckpointGUID ) numModelsTotal += 1 self . logger . info ( ""COMPLETED MODEL GID=%d; EVALUATED %d MODELs"" , modelIDToRun , numModelsTotal ) print >> sys . stderr , ""reporter:status:Evaluated %d models..."" % ( numModelsTotal ) print >> sys . stderr , ""reporter:counter:HypersearchWorker,numModels,1"" if options . modelID is not None : exit = True finally : self . _hs . close ( ) self . logger . info ( ""FINISHED. Evaluated %d models."" % ( numModelsTotal ) ) print >> sys . stderr , ""reporter:status:Finished, evaluated %d models"" % ( numModelsTotal ) return options . jobID",Run this worker .
"def getBucketIndices ( self , x ) : if ( ( isinstance ( x , float ) and math . isnan ( x ) ) or x == SENTINEL_VALUE_FOR_MISSING_DATA ) : return [ None ] if self . _offset is None : self . _offset = x bucketIdx = ( ( self . _maxBuckets / 2 ) + int ( round ( ( x - self . _offset ) / self . resolution ) ) ) if bucketIdx < 0 : bucketIdx = 0 elif bucketIdx >= self . _maxBuckets : bucketIdx = self . _maxBuckets - 1 return [ bucketIdx ]",See method description in base . py
"def mapBucketIndexToNonZeroBits ( self , index ) : if index < 0 : index = 0 if index >= self . _maxBuckets : index = self . _maxBuckets - 1 if not self . bucketMap . has_key ( index ) : if self . verbosity >= 2 : print ""Adding additional buckets to handle index="" , index self . _createBucket ( index ) return self . bucketMap [ index ]",Given a bucket index return the list of non - zero bits . If the bucket index does not exist it is created . If the index falls outside our range we clip it .
"def encodeIntoArray ( self , x , output ) : if x is not None and not isinstance ( x , numbers . Number ) : raise TypeError ( ""Expected a scalar input but got input of type %s"" % type ( x ) ) bucketIdx = self . getBucketIndices ( x ) [ 0 ] output [ 0 : self . n ] = 0 if bucketIdx is not None : output [ self . mapBucketIndexToNonZeroBits ( bucketIdx ) ] = 1",See method description in base . py
"def _createBucket ( self , index ) : if index < self . minIndex : if index == self . minIndex - 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , index ) self . minIndex = index else : self . _createBucket ( index + 1 ) self . _createBucket ( index ) else : if index == self . maxIndex + 1 : self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , index ) self . maxIndex = index else : self . _createBucket ( index - 1 ) self . _createBucket ( index )",Create the given bucket index . Recursively create as many in - between bucket indices as necessary .
"def _newRepresentation ( self , index , newIndex ) : newRepresentation = self . bucketMap [ index ] . copy ( ) ri = newIndex % self . w newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit while newBit in self . bucketMap [ index ] or not self . _newRepresentationOK ( newRepresentation , newIndex ) : self . numTries += 1 newBit = self . random . getUInt32 ( self . n ) newRepresentation [ ri ] = newBit return newRepresentation",Return a new representation for newIndex that overlaps with the representation at index by exactly w - 1 bits
"def _newRepresentationOK ( self , newRep , newIndex ) : if newRep . size != self . w : return False if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : raise ValueError ( ""newIndex must be within one of existing indices"" ) newRepBinary = numpy . array ( [ False ] * self . n ) newRepBinary [ newRep ] = True midIdx = self . _maxBuckets / 2 runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : return False for i in range ( self . minIndex + 1 , midIdx + 1 ) : newBit = ( i - 1 ) % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False for i in range ( midIdx + 1 , self . maxIndex + 1 ) : newBit = i % self . w if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : runningOverlap -= 1 if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : runningOverlap += 1 if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : return False return True",Return True if this new candidate representation satisfies all our overlap rules . Since we know that neighboring representations differ by at most one bit we compute running overlaps .
"def _countOverlapIndices ( self , i , j ) : if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : iRep = self . bucketMap [ i ] jRep = self . bucketMap [ j ] return self . _countOverlap ( iRep , jRep ) else : raise ValueError ( ""Either i or j don't exist"" )",Return the overlap between bucket indices i and j
"def _countOverlap ( rep1 , rep2 ) : overlap = 0 for e in rep1 : if e in rep2 : overlap += 1 return overlap",Return the overlap between two representations . rep1 and rep2 are lists of non - zero indices .
"def _overlapOK ( self , i , j , overlap = None ) : if overlap is None : overlap = self . _countOverlapIndices ( i , j ) if abs ( i - j ) < self . w : if overlap == ( self . w - abs ( i - j ) ) : return True else : return False else : if overlap <= self . _maxOverlap : return True else : return False",Return True if the given overlap between bucket indices i and j are acceptable . If overlap is not specified calculate it from the bucketMap
"def _initializeBucketMap ( self , maxBuckets , offset ) : self . _maxBuckets = maxBuckets self . minIndex = self . _maxBuckets / 2 self . maxIndex = self . _maxBuckets / 2 self . _offset = offset self . bucketMap = { } def _permutation ( n ) : r = numpy . arange ( n , dtype = numpy . uint32 ) self . random . shuffle ( r ) return r self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] self . numTries = 0",Initialize the bucket map assuming the given number of maxBuckets .
"def retrySQL ( timeoutSec = 60 * 5 , logger = None ) : if logger is None : logger = logging . getLogger ( __name__ ) def retryFilter ( e , args , kwargs ) : if isinstance ( e , ( pymysql . InternalError , pymysql . OperationalError ) ) : if e . args and e . args [ 0 ] in _ALL_RETRIABLE_ERROR_CODES : return True elif isinstance ( e , pymysql . Error ) : if ( e . args and inspect . isclass ( e . args [ 0 ] ) and issubclass ( e . args [ 0 ] , socket_error ) ) : return True return False retryExceptions = tuple ( [ pymysql . InternalError , pymysql . OperationalError , pymysql . Error , ] ) return make_retry_decorator ( timeoutSec = timeoutSec , initialRetryDelaySec = 0.1 , maxRetryDelaySec = 10 , retryExceptions = retryExceptions , retryFilter = retryFilter , logger = logger )",Return a closure suitable for use as a decorator for retrying a pymysql DAO function on certain failures that warrant retries ( e . g . RDS / MySQL server down temporarily transaction deadlock etc . ) . We share this function across multiple scripts ( e . g . ClientJobsDAO StreamMgr ) for consitent behavior .
"def create ( * args , * * kwargs ) : impl = kwargs . pop ( 'implementation' , None ) if impl is None : impl = Configuration . get ( 'nupic.opf.sdrClassifier.implementation' ) if impl == 'py' : return SDRClassifier ( * args , * * kwargs ) elif impl == 'cpp' : return FastSDRClassifier ( * args , * * kwargs ) elif impl == 'diff' : return SDRClassifierDiff ( * args , * * kwargs ) else : raise ValueError ( 'Invalid classifier implementation (%r). Value must be ' '""py"", ""cpp"" or ""diff"".' % impl )",Create a SDR classifier factory . The implementation of the SDR Classifier can be specified with the implementation keyword argument .
"def read ( proto ) : impl = proto . implementation if impl == 'py' : return SDRClassifier . read ( proto . sdrClassifier ) elif impl == 'cpp' : return FastSDRClassifier . read ( proto . sdrClassifier ) elif impl == 'diff' : return SDRClassifierDiff . read ( proto . sdrClassifier ) else : raise ValueError ( 'Invalid classifier implementation (%r). Value must be ' '""py"", ""cpp"" or ""diff"".' % impl )",: param proto : SDRClassifierRegionProto capnproto object
def cross_list ( * sequences ) : result = [ [ ] ] for seq in sequences : result = [ sublist + [ item ] for sublist in result for item in seq ] return result,From : http : // book . opensourceproject . org . cn / lamp / python / pythoncook2 / opensource / 0596007973 / pythoncook2 - chp - 19 - sect - 9 . html
"def cross ( * sequences ) : wheels = map ( iter , sequences ) digits = [ it . next ( ) for it in wheels ] while True : yield tuple ( digits ) for i in range ( len ( digits ) - 1 , - 1 , - 1 ) : try : digits [ i ] = wheels [ i ] . next ( ) break except StopIteration : wheels [ i ] = iter ( sequences [ i ] ) digits [ i ] = wheels [ i ] . next ( ) else : break",From : http : // book . opensourceproject . org . cn / lamp / python / pythoncook2 / opensource / 0596007973 / pythoncook2 - chp - 19 - sect - 9 . html
"def dcross ( * * keywords ) : keys = keywords . keys ( ) sequences = [ keywords [ key ] for key in keys ] wheels = map ( iter , sequences ) digits = [ it . next ( ) for it in wheels ] while True : yield dict ( zip ( keys , digits ) ) for i in range ( len ( digits ) - 1 , - 1 , - 1 ) : try : digits [ i ] = wheels [ i ] . next ( ) break except StopIteration : wheels [ i ] = iter ( sequences [ i ] ) digits [ i ] = wheels [ i ] . next ( ) else : break",Similar to cross () but generates output dictionaries instead of tuples .
"def mmGetMetricFromTrace ( self , trace ) : return Metric . createFromTrace ( trace . makeCountsTrace ( ) , excludeResets = self . mmGetTraceResets ( ) )",Convenience method to compute a metric over an indices trace excluding resets .
"def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : self . _mmComputeTransitionTraces ( ) numCellsPerColumn = [ ] for predictedActiveCells in ( self . _mmData [ ""predictedActiveCellsForSequence"" ] . values ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] return Metric ( self , ""# predicted => active cells per column for each sequence"" , numCellsPerColumn )",Metric for number of predicted = > active cells per column for each sequence
"def mmGetMetricSequencesPredictedActiveCellsShared ( self ) : self . _mmComputeTransitionTraces ( ) numSequencesForCell = defaultdict ( lambda : 0 ) for predictedActiveCells in ( self . _mmData [ ""predictedActiveCellsForSequence"" ] . values ( ) ) : for cell in predictedActiveCells : numSequencesForCell [ cell ] += 1 return Metric ( self , ""# sequences each predicted => active cells appears in"" , numSequencesForCell . values ( ) )",Metric for number of sequences each predicted = > active cell appears in
"def mmPrettyPrintConnections ( self ) : text = """" text += ( ""Segments: (format => "" ""(#) [(source cell=permanence ...),       ...]\n"" ) text += ""------------------------------------\n"" columns = range ( self . numberOfColumns ( ) ) for column in columns : cells = self . cellsForColumn ( column ) for cell in cells : segmentDict = dict ( ) for seg in self . connections . segmentsForCell ( cell ) : synapseList = [ ] for synapse in self . connections . synapsesForSegment ( seg ) : synapseData = self . connections . dataForSynapse ( synapse ) synapseList . append ( ( synapseData . presynapticCell , synapseData . permanence ) ) synapseList . sort ( ) synapseStringList = [ ""{0:3}={1:.2f}"" . format ( sourceCell , permanence ) for sourceCell , permanence in synapseList ] segmentDict [ seg ] = ""({0})"" . format ( "" "" . join ( synapseStringList ) ) text += ( ""Column {0:3} / Cell {1:3}:\t({2}) {3}\n"" . format ( column , cell , len ( segmentDict . values ( ) ) , ""[{0}]"" . format ( "",       "" . join ( segmentDict . values ( ) ) ) ) ) if column < len ( columns ) - 1 : text += ""\n"" text += ""------------------------------------\n"" return text",Pretty print the connections in the temporal memory .
"def mmPrettyPrintSequenceCellRepresentations ( self , sortby = ""Column"" ) : self . _mmComputeTransitionTraces ( ) table = PrettyTable ( [ ""Pattern"" , ""Column"" , ""predicted=>active cells"" ] ) for sequenceLabel , predictedActiveCells in ( self . _mmData [ ""predictedActiveCellsForSequence"" ] . iteritems ( ) ) : cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) for column , cells in cellsForColumn . iteritems ( ) : table . add_row ( [ sequenceLabel , column , list ( cells ) ] ) return table . get_string ( sortby = sortby ) . encode ( ""utf-8"" )",Pretty print the cell representations for sequences in the history .
"def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , temporalParams = _TM_PARAMS , verbosity = _VERBOSITY ) : inputFilePath = recordParams [ ""inputFilePath"" ] scalarEncoderArgs = recordParams [ ""scalarEncoderArgs"" ] dateEncoderArgs = recordParams [ ""dateEncoderArgs"" ] scalarEncoder = ScalarEncoder ( * * scalarEncoderArgs ) dateEncoder = DateEncoder ( * * dateEncoderArgs ) encoder = MultiEncoder ( ) encoder . addEncoder ( scalarEncoderArgs [ ""name"" ] , scalarEncoder ) encoder . addEncoder ( dateEncoderArgs [ ""name"" ] , dateEncoder ) network = Network ( ) network . addRegion ( ""sensor"" , ""py.RecordSensor"" , json . dumps ( { ""verbosity"" : verbosity } ) ) sensor = network . regions [ ""sensor"" ] . getSelf ( ) sensor . encoder = encoder sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) spatialParams [ ""inputWidth"" ] = sensor . encoder . getWidth ( ) network . addRegion ( ""spatialPoolerRegion"" , ""py.SPRegion"" , json . dumps ( spatialParams ) ) network . link ( ""sensor"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" ) network . link ( ""sensor"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" , srcOutput = ""resetOut"" , destInput = ""resetIn"" ) network . link ( ""spatialPoolerRegion"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""spatialTopDownOut"" , destInput = ""spatialTopDownIn"" ) network . link ( ""spatialPoolerRegion"" , ""sensor"" , ""UniformLink"" , """" , srcOutput = ""temporalTopDownOut"" , destInput = ""temporalTopDownIn"" ) network . addRegion ( ""temporalPoolerRegion"" , ""py.TMRegion"" , json . dumps ( temporalParams ) ) network . link ( ""spatialPoolerRegion"" , ""temporalPoolerRegion"" , ""UniformLink"" , """" ) network . link ( ""temporalPoolerRegion"" , ""spatialPoolerRegion"" , ""UniformLink"" , """" , srcOutput = ""topDownOut"" , destInput = ""topDownIn"" ) spatialPoolerRegion = network . regions [ ""spatialPoolerRegion"" ] spatialPoolerRegion . setParameter ( ""learningMode"" , True ) spatialPoolerRegion . setParameter ( ""anomalyMode"" , False ) temporalPoolerRegion = network . regions [ ""temporalPoolerRegion"" ] temporalPoolerRegion . setParameter ( ""topDownMode"" , True ) temporalPoolerRegion . setParameter ( ""learningMode"" , True ) temporalPoolerRegion . setParameter ( ""inferenceMode"" , True ) temporalPoolerRegion . setParameter ( ""anomalyMode"" , True ) return network",Generates a Network with connected RecordSensor SP TM .
"def runNetwork ( network , writer ) : sensorRegion = network . regions [ ""sensor"" ] temporalPoolerRegion = network . regions [ ""temporalPoolerRegion"" ] for i in xrange ( _NUM_RECORDS ) : network . run ( 1 ) anomalyScore = temporalPoolerRegion . getOutputData ( ""anomalyScore"" ) [ 0 ] consumption = sensorRegion . getOutputData ( ""sourceOut"" ) [ 0 ] writer . writerow ( ( i , consumption , anomalyScore ) )",Run the network and write output to writer .
"def __appendActivities ( self , periodicActivities ) : for req in periodicActivities : act = self . Activity ( repeating = req . repeating , period = req . period , cb = req . cb , iteratorHolder = [ iter ( xrange ( req . period - 1 ) ) ] ) self . __activities . append ( act ) return",periodicActivities : A sequence of PeriodicActivityRequest elements
"def add ( reader , writer , column , start , stop , value ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( value ) ( row [ column ] ) + value writer . appendRecord ( row )",Adds a value over a range of rows .
"def scale ( reader , writer , column , start , stop , multiple ) : for i , row in enumerate ( reader ) : if i >= start and i <= stop : row [ column ] = type ( multiple ) ( row [ column ] ) * multiple writer . appendRecord ( row )",Multiplies a value over a range of rows .
"def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : assert stop >= start startRows = [ ] copyRows = [ ] ts = None inc = None if tsCol is None : tsCol = reader . getTimestampFieldIdx ( ) for i , row in enumerate ( reader ) : if ts is None : ts = row [ tsCol ] elif inc is None : inc = row [ tsCol ] - ts if i >= start and i <= stop : copyRows . append ( row ) startRows . append ( row ) if insertLocation is None : insertLocation = stop + 1 startRows [ insertLocation : insertLocation ] = copyRows for row in startRows : row [ tsCol ] = ts writer . appendRecord ( row ) ts += inc",Copies a range of values to a new location in the data set .
"def sample ( reader , writer , n , start = None , stop = None , tsCol = None , writeSampleOnly = True ) : rows = list ( reader ) if tsCol is not None : ts = rows [ 0 ] [ tsCol ] inc = rows [ 1 ] [ tsCol ] - ts if start is None : start = 0 if stop is None : stop = len ( rows ) - 1 initialN = stop - start + 1 numDeletes = initialN - n for i in xrange ( numDeletes ) : delIndex = random . randint ( start , stop - i ) del rows [ delIndex ] if writeSampleOnly : rows = rows [ start : start + n ] if tsCol is not None : ts = rows [ 0 ] [ tsCol ] for row in rows : if tsCol is not None : row [ tsCol ] = ts ts += inc writer . appendRecord ( row )",Samples n rows .
"def _initEncoder ( self , w , minval , maxval , n , radius , resolution ) : if n != 0 : if ( radius != 0 or resolution != 0 ) : raise ValueError ( ""Only one of n/radius/resolution can be specified for a ScalarEncoder"" ) assert n > w self . n = n if ( minval is not None and maxval is not None ) : if not self . periodic : self . resolution = float ( self . rangeInternal ) / ( self . n - self . w ) else : self . resolution = float ( self . rangeInternal ) / ( self . n ) self . radius = self . w * self . resolution if self . periodic : self . range = self . rangeInternal else : self . range = self . rangeInternal + self . resolution else : if radius != 0 : if ( resolution != 0 ) : raise ValueError ( ""Only one of radius/resolution can be specified for a ScalarEncoder"" ) self . radius = radius self . resolution = float ( self . radius ) / w elif resolution != 0 : self . resolution = float ( resolution ) self . radius = self . resolution * self . w else : raise Exception ( ""One of n, radius, resolution must be specified for a ScalarEncoder"" ) if ( minval is not None and maxval is not None ) : if self . periodic : self . range = self . rangeInternal else : self . range = self . rangeInternal + self . resolution nfloat = self . w * ( self . range / self . radius ) + 2 * self . padding self . n = int ( math . ceil ( nfloat ) )",( helper function ) There are three different ways of thinking about the representation . Handle each case here .
"def _getFirstOnBit ( self , input ) : if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] else : if input < self . minval : if self . clipInput and not self . periodic : if self . verbosity > 0 : print ""Clipped input %s=%.2f to minval %.2f"" % ( self . name , input , self . minval ) input = self . minval else : raise Exception ( 'input (%s) less than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : if input >= self . maxval : raise Exception ( 'input (%s) greater than periodic range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) else : if input > self . maxval : if self . clipInput : if self . verbosity > 0 : print ""Clipped input %s=%.2f to maxval %.2f"" % ( self . name , input , self . maxval ) input = self . maxval else : raise Exception ( 'input (%s) greater than range (%s - %s)' % ( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) if self . periodic : centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding else : centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding minbin = centerbin - self . halfwidth return [ minbin ]",Return the bit offset of the first bit to be set in the encoder output . For periodic encoders this can be a negative number when the encoded output wraps around .
"def getBucketIndices ( self , input ) : if type ( input ) is float and math . isnan ( input ) : input = SENTINEL_VALUE_FOR_MISSING_DATA if input == SENTINEL_VALUE_FOR_MISSING_DATA : return [ None ] minbin = self . _getFirstOnBit ( input ) [ 0 ] if self . periodic : bucketIdx = minbin + self . halfwidth if bucketIdx < 0 : bucketIdx += self . n else : bucketIdx = minbin return [ bucketIdx ]",See method description in base . py
"def encodeIntoArray ( self , input , output , learn = True ) : if input is not None and not isinstance ( input , numbers . Number ) : raise TypeError ( ""Expected a scalar input but got input of type %s"" % type ( input ) ) if type ( input ) is float and math . isnan ( input ) : input = SENTINEL_VALUE_FOR_MISSING_DATA bucketIdx = self . _getFirstOnBit ( input ) [ 0 ] if bucketIdx is None : output [ 0 : self . n ] = 0 else : output [ : self . n ] = 0 minbin = bucketIdx maxbin = minbin + 2 * self . halfwidth if self . periodic : if maxbin >= self . n : bottombins = maxbin - self . n + 1 output [ : bottombins ] = 1 maxbin = self . n - 1 if minbin < 0 : topbins = - minbin output [ self . n - topbins : self . n ] = 1 minbin = 0 assert minbin >= 0 assert maxbin < self . n output [ minbin : maxbin + 1 ] = 1 if self . verbosity >= 2 : print print ""input:"" , input print ""range:"" , self . minval , ""-"" , self . maxval print ""n:"" , self . n , ""w:"" , self . w , ""resolution:"" , self . resolution , ""radius"" , self . radius , ""periodic:"" , self . periodic print ""output:"" , self . pprint ( output ) print ""input desc:"" , self . decodedToStr ( self . decode ( output ) )",See method description in base . py
"def decode ( self , encoded , parentFieldName = '' ) : tmpOutput = numpy . array ( encoded [ : self . n ] > 0 ) . astype ( encoded . dtype ) if not tmpOutput . any ( ) : return ( dict ( ) , [ ] ) maxZerosInARow = self . halfwidth for i in xrange ( maxZerosInARow ) : searchStr = numpy . ones ( i + 3 , dtype = encoded . dtype ) searchStr [ 1 : - 1 ] = 0 subLen = len ( searchStr ) if self . periodic : for j in xrange ( self . n ) : outputIndices = numpy . arange ( j , j + subLen ) outputIndices %= self . n if numpy . array_equal ( searchStr , tmpOutput [ outputIndices ] ) : tmpOutput [ outputIndices ] = 1 else : for j in xrange ( self . n - subLen + 1 ) : if numpy . array_equal ( searchStr , tmpOutput [ j : j + subLen ] ) : tmpOutput [ j : j + subLen ] = 1 if self . verbosity >= 2 : print ""raw output:"" , encoded [ : self . n ] print ""filtered output:"" , tmpOutput nz = tmpOutput . nonzero ( ) [ 0 ] runs = [ ] run = [ nz [ 0 ] , 1 ] i = 1 while ( i < len ( nz ) ) : if nz [ i ] == run [ 0 ] + run [ 1 ] : run [ 1 ] += 1 else : runs . append ( run ) run = [ nz [ i ] , 1 ] i += 1 runs . append ( run ) if self . periodic and len ( runs ) > 1 : if runs [ 0 ] [ 0 ] == 0 and runs [ - 1 ] [ 0 ] + runs [ - 1 ] [ 1 ] == self . n : runs [ - 1 ] [ 1 ] += runs [ 0 ] [ 1 ] runs = runs [ 1 : ] ranges = [ ] for run in runs : ( start , runLen ) = run if runLen <= self . w : left = right = start + runLen / 2 else : left = start + self . halfwidth right = start + runLen - 1 - self . halfwidth if not self . periodic : inMin = ( left - self . padding ) * self . resolution + self . minval inMax = ( right - self . padding ) * self . resolution + self . minval else : inMin = ( left - self . padding ) * self . range / self . nInternal + self . minval inMax = ( right - self . padding ) * self . range / self . nInternal + self . minval if self . periodic : if inMin >= self . maxval : inMin -= self . range inMax -= self . range if inMin < self . minval : inMin = self . minval if inMax < self . minval : inMax = self . minval if self . periodic and inMax >= self . maxval : ranges . append ( [ inMin , self . maxval ] ) ranges . append ( [ self . minval , inMax - self . range ] ) else : if inMax > self . maxval : inMax = self . maxval if inMin > self . maxval : inMin = self . maxval ranges . append ( [ inMin , inMax ] ) desc = self . _generateRangeDescription ( ranges ) if parentFieldName != '' : fieldName = ""%s.%s"" % ( parentFieldName , self . name ) else : fieldName = self . name return ( { fieldName : ( ranges , desc ) } , [ fieldName ] )",See the function description in base . py
"def _generateRangeDescription ( self , ranges ) : desc = """" numRanges = len ( ranges ) for i in xrange ( numRanges ) : if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : desc += ""%.2f-%.2f"" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) else : desc += ""%.2f"" % ( ranges [ i ] [ 0 ] ) if i < numRanges - 1 : desc += "", "" return desc",generate description from a text description of the ranges
"def _getTopDownMapping ( self ) : if self . _topDownMappingM is None : if self . periodic : self . _topDownValues = numpy . arange ( self . minval + self . resolution / 2.0 , self . maxval , self . resolution ) else : self . _topDownValues = numpy . arange ( self . minval , self . maxval + self . resolution / 2.0 , self . resolution ) numCategories = len ( self . _topDownValues ) self . _topDownMappingM = SM32 ( numCategories , self . n ) outputSpace = numpy . zeros ( self . n , dtype = GetNTAReal ( ) ) for i in xrange ( numCategories ) : value = self . _topDownValues [ i ] value = max ( value , self . minval ) value = min ( value , self . maxval ) self . encodeIntoArray ( value , outputSpace , learn = False ) self . _topDownMappingM . setRowFromDense ( i , outputSpace ) return self . _topDownMappingM",Return the interal _topDownMappingM matrix used for handling the bucketInfo () and topDownCompute () methods . This is a matrix one row per category ( bucket ) where each row contains the encoded output for that category .
def getBucketValues ( self ) : if self . _bucketValues is None : topDownMappingM = self . _getTopDownMapping ( ) numBuckets = topDownMappingM . nRows ( ) self . _bucketValues = [ ] for bucketIdx in range ( numBuckets ) : self . _bucketValues . append ( self . getBucketInfo ( [ bucketIdx ] ) [ 0 ] . value ) return self . _bucketValues,See the function description in base . py
"def getBucketInfo ( self , buckets ) : topDownMappingM = self . _getTopDownMapping ( ) category = buckets [ 0 ] encoding = self . _topDownMappingM . getRow ( category ) if self . periodic : inputVal = ( self . minval + ( self . resolution / 2.0 ) + ( category * self . resolution ) ) else : inputVal = self . minval + ( category * self . resolution ) return [ EncoderResult ( value = inputVal , scalar = inputVal , encoding = encoding ) ]",See the function description in base . py
"def topDownCompute ( self , encoded ) : topDownMappingM = self . _getTopDownMapping ( ) category = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) return self . getBucketInfo ( [ category ] )",See the function description in base . py
"def closenessScores ( self , expValues , actValues , fractional = True ) : expValue = expValues [ 0 ] actValue = actValues [ 0 ] if self . periodic : expValue = expValue % self . maxval actValue = actValue % self . maxval err = abs ( expValue - actValue ) if self . periodic : err = min ( err , self . maxval - err ) if fractional : pctErr = float ( err ) / ( self . maxval - self . minval ) pctErr = min ( 1.0 , pctErr ) closeness = 1.0 - pctErr else : closeness = err return numpy . array ( [ closeness ] )",See the function description in base . py
"def _initEphemerals ( self ) : self . segmentUpdates = { } self . resetStats ( ) self . _prevInfPatterns = [ ] self . _prevLrnPatterns = [ ] stateShape = ( self . numberOfCols , self . cellsPerColumn ) self . lrnActiveState = { } self . lrnActiveState [ ""t"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . lrnActiveState [ ""t-1"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . lrnPredictedState = { } self . lrnPredictedState [ ""t"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . lrnPredictedState [ ""t-1"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infActiveState = { } self . infActiveState [ ""t"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infActiveState [ ""t-1"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infActiveState [ ""backup"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infActiveState [ ""candidate"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infPredictedState = { } self . infPredictedState [ ""t"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infPredictedState [ ""t-1"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infPredictedState [ ""backup"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . infPredictedState [ ""candidate"" ] = numpy . zeros ( stateShape , dtype = ""int8"" ) self . cellConfidence = { } self . cellConfidence [ ""t"" ] = numpy . zeros ( stateShape , dtype = ""float32"" ) self . cellConfidence [ ""t-1"" ] = numpy . zeros ( stateShape , dtype = ""float32"" ) self . cellConfidence [ ""candidate"" ] = numpy . zeros ( stateShape , dtype = ""float32"" ) self . colConfidence = { } self . colConfidence [ ""t"" ] = numpy . zeros ( self . numberOfCols , dtype = ""float32"" ) self . colConfidence [ ""t-1"" ] = numpy . zeros ( self . numberOfCols , dtype = ""float32"" ) self . colConfidence [ ""candidate"" ] = numpy . zeros ( self . numberOfCols , dtype = ""float32"" )",Initialize all ephemeral members after being restored to a pickled state .
"def write ( self , proto ) : proto . version = TM_VERSION self . _random . write ( proto . random ) proto . numberOfCols = self . numberOfCols proto . cellsPerColumn = self . cellsPerColumn proto . initialPerm = float ( self . initialPerm ) proto . connectedPerm = float ( self . connectedPerm ) proto . minThreshold = self . minThreshold proto . newSynapseCount = self . newSynapseCount proto . permanenceInc = float ( self . permanenceInc ) proto . permanenceDec = float ( self . permanenceDec ) proto . permanenceMax = float ( self . permanenceMax ) proto . globalDecay = float ( self . globalDecay ) proto . activationThreshold = self . activationThreshold proto . doPooling = self . doPooling proto . segUpdateValidDuration = self . segUpdateValidDuration proto . burnIn = self . burnIn proto . collectStats = self . collectStats proto . verbosity = self . verbosity proto . pamLength = self . pamLength proto . maxAge = self . maxAge proto . maxInfBacktrack = self . maxInfBacktrack proto . maxLrnBacktrack = self . maxLrnBacktrack proto . maxSeqLength = self . maxSeqLength proto . maxSegmentsPerCell = self . maxSegmentsPerCell proto . maxSynapsesPerSegment = self . maxSynapsesPerSegment proto . outputType = self . outputType proto . activeColumns = self . activeColumns cellListProto = proto . init ( ""cells"" , len ( self . cells ) ) for i , columnSegments in enumerate ( self . cells ) : columnSegmentsProto = cellListProto . init ( i , len ( columnSegments ) ) for j , cellSegments in enumerate ( columnSegments ) : cellSegmentsProto = columnSegmentsProto . init ( j , len ( cellSegments ) ) for k , segment in enumerate ( cellSegments ) : segment . write ( cellSegmentsProto [ k ] ) proto . lrnIterationIdx = self . lrnIterationIdx proto . iterationIdx = self . iterationIdx proto . segID = self . segID if self . currentOutput is None : proto . currentOutput . none = None else : proto . currentOutput . list = self . currentOutput . tolist ( ) proto . pamCounter = self . pamCounter proto . collectSequenceStats = self . collectSequenceStats proto . resetCalled = self . resetCalled proto . avgInputDensity = self . avgInputDensity or - 1.0 proto . learnedSeqLength = self . learnedSeqLength proto . avgLearnedSeqLength = self . avgLearnedSeqLength proto . prevLrnPatterns = self . _prevLrnPatterns proto . prevInfPatterns = self . _prevInfPatterns segmentUpdatesListProto = proto . init ( ""segmentUpdates"" , len ( self . segmentUpdates ) ) for i , ( key , updates ) in enumerate ( self . segmentUpdates . iteritems ( ) ) : cellSegmentUpdatesProto = segmentUpdatesListProto [ i ] cellSegmentUpdatesProto . columnIdx = key [ 0 ] cellSegmentUpdatesProto . cellIdx = key [ 1 ] segmentUpdatesProto = cellSegmentUpdatesProto . init ( ""segmentUpdates"" , len ( updates ) ) for j , ( lrnIterationIdx , segmentUpdate ) in enumerate ( updates ) : segmentUpdateWrapperProto = segmentUpdatesProto [ j ] segmentUpdateWrapperProto . lrnIterationIdx = lrnIterationIdx segmentUpdate . write ( segmentUpdateWrapperProto . segmentUpdate ) proto . cellConfidenceT = self . cellConfidence [ ""t"" ] . tolist ( ) proto . cellConfidenceT1 = self . cellConfidence [ ""t-1"" ] . tolist ( ) proto . cellConfidenceCandidate = self . cellConfidence [ ""candidate"" ] . tolist ( ) proto . colConfidenceT = self . colConfidence [ ""t"" ] . tolist ( ) proto . colConfidenceT1 = self . colConfidence [ ""t-1"" ] . tolist ( ) proto . colConfidenceCandidate = self . colConfidence [ ""candidate"" ] . tolist ( ) proto . lrnActiveStateT = self . lrnActiveState [ ""t"" ] . tolist ( ) proto . lrnActiveStateT1 = self . lrnActiveState [ ""t-1"" ] . tolist ( ) proto . infActiveStateT = self . infActiveState [ ""t"" ] . tolist ( ) proto . infActiveStateT1 = self . infActiveState [ ""t-1"" ] . tolist ( ) proto . infActiveStateBackup = self . infActiveState [ ""backup"" ] . tolist ( ) proto . infActiveStateCandidate = self . infActiveState [ ""candidate"" ] . tolist ( ) proto . lrnPredictedStateT = self . lrnPredictedState [ ""t"" ] . tolist ( ) proto . lrnPredictedStateT1 = self . lrnPredictedState [ ""t-1"" ] . tolist ( ) proto . infPredictedStateT = self . infPredictedState [ ""t"" ] . tolist ( ) proto . infPredictedStateT1 = self . infPredictedState [ ""t-1"" ] . tolist ( ) proto . infPredictedStateBackup = self . infPredictedState [ ""backup"" ] . tolist ( ) proto . infPredictedStateCandidate = self . infPredictedState [ ""candidate"" ] . tolist ( ) proto . consolePrinterVerbosity = self . consolePrinterVerbosity",Populate serialization proto instance .
"def read ( cls , proto ) : assert proto . version == TM_VERSION obj = object . __new__ ( cls ) obj . _random = Random ( ) obj . _random . read ( proto . random ) obj . numberOfCols = int ( proto . numberOfCols ) obj . cellsPerColumn = int ( proto . cellsPerColumn ) obj . _numberOfCells = obj . numberOfCols * obj . cellsPerColumn obj . initialPerm = numpy . float32 ( proto . initialPerm ) obj . connectedPerm = numpy . float32 ( proto . connectedPerm ) obj . minThreshold = int ( proto . minThreshold ) obj . newSynapseCount = int ( proto . newSynapseCount ) obj . permanenceInc = numpy . float32 ( proto . permanenceInc ) obj . permanenceDec = numpy . float32 ( proto . permanenceDec ) obj . permanenceMax = numpy . float32 ( proto . permanenceMax ) obj . globalDecay = numpy . float32 ( proto . globalDecay ) obj . activationThreshold = int ( proto . activationThreshold ) obj . doPooling = proto . doPooling obj . segUpdateValidDuration = int ( proto . segUpdateValidDuration ) obj . burnIn = int ( proto . burnIn ) obj . collectStats = proto . collectStats obj . verbosity = int ( proto . verbosity ) obj . pamLength = int ( proto . pamLength ) obj . maxAge = int ( proto . maxAge ) obj . maxInfBacktrack = int ( proto . maxInfBacktrack ) obj . maxLrnBacktrack = int ( proto . maxLrnBacktrack ) obj . maxSeqLength = int ( proto . maxSeqLength ) obj . maxSegmentsPerCell = proto . maxSegmentsPerCell obj . maxSynapsesPerSegment = proto . maxSynapsesPerSegment obj . outputType = proto . outputType obj . activeColumns = [ int ( col ) for col in proto . activeColumns ] obj . cells = [ [ ] for _ in xrange ( len ( proto . cells ) ) ] for columnSegments , columnSegmentsProto in zip ( obj . cells , proto . cells ) : columnSegments . extend ( [ [ ] for _ in xrange ( len ( columnSegmentsProto ) ) ] ) for cellSegments , cellSegmentsProto in zip ( columnSegments , columnSegmentsProto ) : for segmentProto in cellSegmentsProto : segment = Segment . read ( segmentProto , obj ) cellSegments . append ( segment ) obj . lrnIterationIdx = int ( proto . lrnIterationIdx ) obj . iterationIdx = int ( proto . iterationIdx ) obj . segID = int ( proto . segID ) obj . pamCounter = int ( proto . pamCounter ) obj . collectSequenceStats = proto . collectSequenceStats obj . resetCalled = proto . resetCalled avgInputDensity = proto . avgInputDensity if avgInputDensity < 0.0 : obj . avgInputDensity = None else : obj . avgInputDensity = avgInputDensity obj . learnedSeqLength = int ( proto . learnedSeqLength ) obj . avgLearnedSeqLength = proto . avgLearnedSeqLength obj . _initEphemerals ( ) if proto . currentOutput . which ( ) == ""none"" : obj . currentOutput = None else : obj . currentOutput = numpy . array ( proto . currentOutput . list , dtype = 'float32' ) for pattern in proto . prevLrnPatterns : obj . prevLrnPatterns . append ( [ v for v in pattern ] ) for pattern in proto . prevInfPatterns : obj . prevInfPatterns . append ( [ v for v in pattern ] ) for cellWrapperProto in proto . segmentUpdates : key = ( cellWrapperProto . columnIdx , cellWrapperProto . cellIdx ) value = [ ] for updateWrapperProto in cellWrapperProto . segmentUpdates : segmentUpdate = SegmentUpdate . read ( updateWrapperProto . segmentUpdate , obj ) value . append ( ( int ( updateWrapperProto . lrnIterationIdx ) , segmentUpdate ) ) obj . segmentUpdates [ key ] = value numpy . copyto ( obj . cellConfidence [ ""t"" ] , proto . cellConfidenceT ) numpy . copyto ( obj . cellConfidence [ ""t-1"" ] , proto . cellConfidenceT1 ) numpy . copyto ( obj . cellConfidence [ ""candidate"" ] , proto . cellConfidenceCandidate ) numpy . copyto ( obj . colConfidence [ ""t"" ] , proto . colConfidenceT ) numpy . copyto ( obj . colConfidence [ ""t-1"" ] , proto . colConfidenceT1 ) numpy . copyto ( obj . colConfidence [ ""candidate"" ] , proto . colConfidenceCandidate ) numpy . copyto ( obj . lrnActiveState [ ""t"" ] , proto . lrnActiveStateT ) numpy . copyto ( obj . lrnActiveState [ ""t-1"" ] , proto . lrnActiveStateT1 ) numpy . copyto ( obj . infActiveState [ ""t"" ] , proto . infActiveStateT ) numpy . copyto ( obj . infActiveState [ ""t-1"" ] , proto . infActiveStateT1 ) numpy . copyto ( obj . infActiveState [ ""backup"" ] , proto . infActiveStateBackup ) numpy . copyto ( obj . infActiveState [ ""candidate"" ] , proto . infActiveStateCandidate ) numpy . copyto ( obj . lrnPredictedState [ ""t"" ] , proto . lrnPredictedStateT ) numpy . copyto ( obj . lrnPredictedState [ ""t-1"" ] , proto . lrnPredictedStateT1 ) numpy . copyto ( obj . infPredictedState [ ""t"" ] , proto . infPredictedStateT ) numpy . copyto ( obj . infPredictedState [ ""t-1"" ] , proto . infPredictedStateT1 ) numpy . copyto ( obj . infPredictedState [ ""backup"" ] , proto . infPredictedStateBackup ) numpy . copyto ( obj . infPredictedState [ ""candidate"" ] , proto . infPredictedStateCandidate ) obj . consolePrinterVerbosity = int ( proto . consolePrinterVerbosity ) return obj",Deserialize from proto instance .
"def reset ( self , ) : if self . verbosity >= 3 : print ""\n==== RESET ====="" self . lrnActiveState [ 't-1' ] . fill ( 0 ) self . lrnActiveState [ 't' ] . fill ( 0 ) self . lrnPredictedState [ 't-1' ] . fill ( 0 ) self . lrnPredictedState [ 't' ] . fill ( 0 ) self . infActiveState [ 't-1' ] . fill ( 0 ) self . infActiveState [ 't' ] . fill ( 0 ) self . infPredictedState [ 't-1' ] . fill ( 0 ) self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't-1' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . segmentUpdates = { } self . _internalStats [ 'nInfersSinceReset' ] = 0 self . _internalStats [ 'curPredictionScore' ] = 0 self . _internalStats [ 'curPredictionScore2' ] = 0 self . _internalStats [ 'curFalseNegativeScore' ] = 0 self . _internalStats [ 'curFalsePositiveScore' ] = 0 self . _internalStats [ 'curMissing' ] = 0 self . _internalStats [ 'curExtra' ] = 0 self . _internalStats [ 'prevSequenceSignature' ] = None if self . collectSequenceStats : if self . _internalStats [ 'confHistogram' ] . sum ( ) > 0 : sig = self . _internalStats [ 'confHistogram' ] . copy ( ) sig . reshape ( self . numberOfCols * self . cellsPerColumn ) self . _internalStats [ 'prevSequenceSignature' ] = sig self . _internalStats [ 'confHistogram' ] . fill ( 0 ) self . resetCalled = True self . _prevInfPatterns = [ ] self . _prevLrnPatterns = [ ]",Reset the state of all cells .
"def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , colConfidence ) : if not self . collectStats : return stats [ 'nInfersSinceReset' ] += 1 ( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( patternNZs = [ bottomUpNZ ] , output = predictedState , colConfidence = colConfidence ) predictionScore , positivePredictionScore , negativePredictionScore = ( confidences2 [ 0 ] ) stats [ 'curPredictionScore2' ] = float ( predictionScore ) stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) stats [ 'curMissing' ] = numMissing2 stats [ 'curExtra' ] = numExtra2 if stats [ 'nInfersSinceReset' ] <= self . burnIn : return stats [ 'nPredictions' ] += 1 numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) stats [ 'totalMissing' ] += numMissing2 stats [ 'totalExtra' ] += numExtra2 stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) if self . collectSequenceStats : cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] sconf = cc . sum ( axis = 1 ) for c in range ( self . numberOfCols ) : if sconf [ c ] > 0 : cc [ c , : ] /= sconf [ c ] self . _internalStats [ 'confHistogram' ] += cc",Called at the end of learning and inference this routine will update a number of stats in our _internalStats dictionary including our computed prediction score .
"def printState ( self , aState ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatRow ( aState , i )",Print an integer array that is the same shape as activeState .
"def printConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var , i ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += '   ' s += ' %5.3f' % var [ c , i ] s += ' ' return s for i in xrange ( self . cellsPerColumn ) : print formatFPRow ( aState , i )",Print a floating point array that is the same shape as activeState .
"def printColConfidence ( self , aState , maxCols = 20 ) : def formatFPRow ( var ) : s = '' for c in range ( min ( maxCols , self . numberOfCols ) ) : if c > 0 and c % 10 == 0 : s += '   ' s += ' %5.3f' % var [ c ] s += ' ' return s print formatFPRow ( aState )",Print up to maxCols number from a flat floating point array .
"def printStates ( self , printPrevious = True , printLearnState = True ) : def formatRow ( var , i ) : s = '' for c in range ( self . numberOfCols ) : if c > 0 and c % 10 == 0 : s += ' ' s += str ( var [ c , i ] ) s += ' ' return s print ""\nInference Active state"" for i in xrange ( self . cellsPerColumn ) : if printPrevious : print formatRow ( self . infActiveState [ 't-1' ] , i ) , print formatRow ( self . infActiveState [ 't' ] , i ) print ""Inference Predicted state"" for i in xrange ( self . cellsPerColumn ) : if printPrevious : print formatRow ( self . infPredictedState [ 't-1' ] , i ) , print formatRow ( self . infPredictedState [ 't' ] , i ) if printLearnState : print ""\nLearn Active state"" for i in xrange ( self . cellsPerColumn ) : if printPrevious : print formatRow ( self . lrnActiveState [ 't-1' ] , i ) , print formatRow ( self . lrnActiveState [ 't' ] , i ) print ""Learn Predicted state"" for i in xrange ( self . cellsPerColumn ) : if printPrevious : print formatRow ( self . lrnPredictedState [ 't-1' ] , i ) , print formatRow ( self . lrnPredictedState [ 't' ] , i )",TODO : document : param printPrevious : : param printLearnState : : return :
"def printOutput ( self , y ) : print ""Output"" for i in xrange ( self . cellsPerColumn ) : for c in xrange ( self . numberOfCols ) : print int ( y [ c , i ] ) , print",TODO : document : param y : : return :
"def printInput ( self , x ) : print ""Input"" for c in xrange ( self . numberOfCols ) : print int ( x [ c ] ) , print",TODO : document : param x : : return :
"def printParameters ( self ) : print ""numberOfCols="" , self . numberOfCols print ""cellsPerColumn="" , self . cellsPerColumn print ""minThreshold="" , self . minThreshold print ""newSynapseCount="" , self . newSynapseCount print ""activationThreshold="" , self . activationThreshold print print ""initialPerm="" , self . initialPerm print ""connectedPerm="" , self . connectedPerm print ""permanenceInc="" , self . permanenceInc print ""permanenceDec="" , self . permanenceDec print ""permanenceMax="" , self . permanenceMax print ""globalDecay="" , self . globalDecay print print ""doPooling="" , self . doPooling print ""segUpdateValidDuration="" , self . segUpdateValidDuration print ""pamLength="" , self . pamLength",Print the parameter settings for the TM .
"def printActiveIndices ( self , state , andValues = False ) : if len ( state . shape ) == 2 : ( cols , cellIdxs ) = state . nonzero ( ) else : cols = state . nonzero ( ) [ 0 ] cellIdxs = numpy . zeros ( len ( cols ) ) if len ( cols ) == 0 : print ""NONE"" return prevCol = - 1 for ( col , cellIdx ) in zip ( cols , cellIdxs ) : if col != prevCol : if prevCol != - 1 : print ""] "" , print ""Col %d: ["" % ( col ) , prevCol = col if andValues : if len ( state . shape ) == 2 : value = state [ col , cellIdx ] else : value = state [ col ] print ""%d: %s,"" % ( cellIdx , value ) , else : print ""%d,"" % ( cellIdx ) , print ""]""",Print the list of [ column cellIdx ] indices for each of the active cells in state .
"def printComputeEnd ( self , output , learn = False ) : if self . verbosity >= 3 : print ""----- computeEnd summary: "" print ""learn:"" , learn print ""numBurstingCols: %s, "" % ( self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , print ""curPredScore2: %s, "" % ( self . _internalStats [ 'curPredictionScore2' ] ) , print ""curFalsePosScore: %s, "" % ( self . _internalStats [ 'curFalsePositiveScore' ] ) , print ""1-curFalseNegScore: %s, "" % ( 1 - self . _internalStats [ 'curFalseNegativeScore' ] ) print ""numSegments: "" , self . getNumSegments ( ) , print ""avgLearnedSeqLength: "" , self . avgLearnedSeqLength print ""----- infActiveState (%d on) ------"" % ( self . infActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infActiveState [ 't' ] ) print ""----- infPredictedState (%d on)-----"" % ( self . infPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . infPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . infPredictedState [ 't' ] ) print ""----- lrnActiveState (%d on) ------"" % ( self . lrnActiveState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnActiveState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnActiveState [ 't' ] ) print ""----- lrnPredictedState (%d on)-----"" % ( self . lrnPredictedState [ 't' ] . sum ( ) ) self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) if self . verbosity >= 6 : self . printState ( self . lrnPredictedState [ 't' ] ) print ""----- cellConfidence -----"" self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) if self . verbosity >= 6 : self . printConfidence ( self . cellConfidence [ 't' ] ) print ""----- colConfidence -----"" self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) print ""----- cellConfidence[t-1] for currently active cells -----"" cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] self . printActiveIndices ( cc , andValues = True ) if self . verbosity == 4 : print ""Cells, predicted segments only:"" self . printCells ( predictedOnly = True ) elif self . verbosity >= 5 : print ""Cells, all segments:"" self . printCells ( predictedOnly = False ) print elif self . verbosity >= 1 : print ""TM: learn:"" , learn print ""TM: active outputs(%d):"" % len ( output . nonzero ( ) [ 0 ] ) , self . printActiveIndices ( output . reshape ( self . numberOfCols , self . cellsPerColumn ) )",Called at the end of inference to print out various diagnostic information based on the current verbosity level .
"def printCell ( self , c , i , onlyActiveSegments = False ) : if len ( self . cells [ c ] [ i ] ) > 0 : print ""Column"" , c , ""Cell"" , i , "":"" , print len ( self . cells [ c ] [ i ] ) , ""segment(s)"" for j , s in enumerate ( self . cells [ c ] [ i ] ) : isActive = self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) if not onlyActiveSegments or isActive : isActiveStr = ""*"" if isActive else "" "" print ""  %sSeg #%-3d"" % ( isActiveStr , j ) , s . debugPrint ( )",TODO : document : param c : : param i : : param onlyActiveSegments : : return :
"def printCells ( self , predictedOnly = False ) : if predictedOnly : print ""--- PREDICTED CELLS ---"" else : print ""--- ALL CELLS ---"" print ""Activation threshold="" , self . activationThreshold , print ""min threshold="" , self . minThreshold , print ""connected perm="" , self . connectedPerm for c in xrange ( self . numberOfCols ) : for i in xrange ( self . cellsPerColumn ) : if not predictedOnly or self . infPredictedState [ 't' ] [ c , i ] : self . printCell ( c , i , predictedOnly )",TODO : document : param predictedOnly : : return :
"def getSegmentOnCell ( self , c , i , segIdx ) : seg = self . cells [ c ] [ i ] [ segIdx ] retlist = [ [ seg . segID , seg . isSequenceSeg , seg . positiveActivations , seg . totalActivations , seg . lastActiveIteration , seg . _lastPosDutyCycle , seg . _lastPosDutyCycleIteration ] ] retlist += seg . syns return retlist",: param c : ( int ) column index : param i : ( int ) cell index in column : param segIdx : ( int ) segment index to match
"def _addToSegmentUpdates ( self , c , i , segUpdate ) : if segUpdate is None or len ( segUpdate . activeSynapses ) == 0 : return key = ( c , i ) if self . segmentUpdates . has_key ( key ) : self . segmentUpdates [ key ] += [ ( self . lrnIterationIdx , segUpdate ) ] else : self . segmentUpdates [ key ] = [ ( self . lrnIterationIdx , segUpdate ) ]",Store a dated potential segment update . The date ( iteration index ) is used later to determine whether the update is too old and should be forgotten . This is controlled by parameter segUpdateValidDuration .
"def _computeOutput ( self ) : if self . outputType == 'activeState1CellPerCol' : mostActiveCellPerCol = self . cellConfidence [ 't' ] . argmax ( axis = 1 ) self . currentOutput = numpy . zeros ( self . infActiveState [ 't' ] . shape , dtype = 'float32' ) numCols = self . currentOutput . shape [ 0 ] self . currentOutput [ ( xrange ( numCols ) , mostActiveCellPerCol ) ] = 1 activeCols = self . infActiveState [ 't' ] . max ( axis = 1 ) inactiveCols = numpy . where ( activeCols == 0 ) [ 0 ] self . currentOutput [ inactiveCols , : ] = 0 elif self . outputType == 'activeState' : self . currentOutput = self . infActiveState [ 't' ] elif self . outputType == 'normal' : self . currentOutput = numpy . logical_or ( self . infPredictedState [ 't' ] , self . infActiveState [ 't' ] ) else : raise RuntimeError ( ""Unimplemented outputType"" ) return self . currentOutput . reshape ( - 1 ) . astype ( 'float32' )",Computes output for both learning and inference . In both cases the output is the boolean OR of activeState and predictedState at t . Stores currentOutput for checkPrediction . : returns : TODO : document
"def predict ( self , nSteps ) : pristineTPDynamicState = self . _getTPDynamicState ( ) assert ( nSteps > 0 ) multiStepColumnPredictions = numpy . zeros ( ( nSteps , self . numberOfCols ) , dtype = ""float32"" ) step = 0 while True : multiStepColumnPredictions [ step , : ] = self . topDownCompute ( ) if step == nSteps - 1 : break step += 1 self . infActiveState [ 't-1' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 't' ] [ : , : ] self . cellConfidence [ 't-1' ] [ : , : ] = self . cellConfidence [ 't' ] [ : , : ] self . infActiveState [ 't' ] [ : , : ] = self . infPredictedState [ 't-1' ] [ : , : ] self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0.0 ) self . _inferPhase2 ( ) self . _setTPDynamicState ( pristineTPDynamicState ) return multiStepColumnPredictions",This function gives the future predictions for <nSteps > timesteps starting from the current TM state . The TM is returned to its original state at the end before returning .
"def _getTPDynamicState ( self , ) : tpDynamicState = dict ( ) for variableName in self . _getTPDynamicStateVariableNames ( ) : tpDynamicState [ variableName ] = copy . deepcopy ( self . __dict__ [ variableName ] ) return tpDynamicState",Parameters : -------------------------------------------- retval : A dict with all the dynamic state variable names as keys and their values at this instant as values .
"def _setTPDynamicState ( self , tpDynamicState ) : for variableName in self . _getTPDynamicStateVariableNames ( ) : self . __dict__ [ variableName ] = tpDynamicState . pop ( variableName )",Set all the dynamic state variables from the <tpDynamicState > dict .
"def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : if self . lrnIterationIdx < 100 : alpha = 0.5 else : alpha = 0.1 self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + ( alpha * prevSeqLength ) )",Update our moving average of learned sequence length .
"def _inferBacktrack ( self , activeColumns ) : numPrevPatterns = len ( self . _prevInfPatterns ) if numPrevPatterns <= 0 : return currentTimeStepsOffset = numPrevPatterns - 1 self . infActiveState [ 'backup' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] self . infPredictedState [ 'backup' ] [ : , : ] = self . infPredictedState [ 't-1' ] [ : , : ] badPatterns = [ ] inSequence = False candConfidence = None candStartOffset = None for startOffset in range ( 0 , numPrevPatterns ) : if startOffset == currentTimeStepsOffset and candConfidence is not None : break if self . verbosity >= 3 : print ( ""Trying to lock-on using startCell state from %d steps ago:"" % ( numPrevPatterns - 1 - startOffset ) , self . _prevInfPatterns [ startOffset ] ) inSequence = False for offset in range ( startOffset , numPrevPatterns ) : if offset == currentTimeStepsOffset : totalConfidence = self . colConfidence [ 't' ] [ activeColumns ] . sum ( ) self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 't' ] [ : , : ] inSequence = self . _inferPhase1 ( self . _prevInfPatterns [ offset ] , useStartCells = ( offset == startOffset ) ) if not inSequence : break if self . verbosity >= 3 : print ( ""  backtrack: computing predictions from "" , self . _prevInfPatterns [ offset ] ) inSequence = self . _inferPhase2 ( ) if not inSequence : break if not inSequence : badPatterns . append ( startOffset ) continue candConfidence = totalConfidence candStartOffset = startOffset if self . verbosity >= 3 and startOffset != currentTimeStepsOffset : print ( ""  # Prediction confidence of current input after starting %d "" ""steps ago:"" % ( numPrevPatterns - 1 - startOffset ) , totalConfidence ) if candStartOffset == currentTimeStepsOffset : break self . infActiveState [ 'candidate' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] self . infPredictedState [ 'candidate' ] [ : , : ] = ( self . infPredictedState [ 't' ] [ : , : ] ) self . cellConfidence [ 'candidate' ] [ : , : ] = self . cellConfidence [ 't' ] [ : , : ] self . colConfidence [ 'candidate' ] [ : ] = self . colConfidence [ 't' ] [ : ] break if candStartOffset is None : if self . verbosity >= 3 : print ""Failed to lock on. Falling back to bursting all unpredicted."" self . infActiveState [ 't' ] [ : , : ] = self . infActiveState [ 'backup' ] [ : , : ] self . _inferPhase2 ( ) else : if self . verbosity >= 3 : print ( ""Locked on to current input by using start cells from %d "" "" steps ago:"" % ( numPrevPatterns - 1 - candStartOffset ) , self . _prevInfPatterns [ candStartOffset ] ) if candStartOffset != currentTimeStepsOffset : self . infActiveState [ 't' ] [ : , : ] = self . infActiveState [ 'candidate' ] [ : , : ] self . infPredictedState [ 't' ] [ : , : ] = ( self . infPredictedState [ 'candidate' ] [ : , : ] ) self . cellConfidence [ 't' ] [ : , : ] = self . cellConfidence [ 'candidate' ] [ : , : ] self . colConfidence [ 't' ] [ : ] = self . colConfidence [ 'candidate' ] [ : ] for i in range ( numPrevPatterns ) : if ( i in badPatterns or ( candStartOffset is not None and i <= candStartOffset ) ) : if self . verbosity >= 3 : print ( ""Removing useless pattern from history:"" , self . _prevInfPatterns [ 0 ] ) self . _prevInfPatterns . pop ( 0 ) else : break self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 'backup' ] [ : , : ]",This backtracks our inference state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to N steps ago on start cells .
"def _inferPhase1 ( self , activeColumns , useStartCells ) : self . infActiveState [ 't' ] . fill ( 0 ) numPredictedColumns = 0 if useStartCells : for c in activeColumns : self . infActiveState [ 't' ] [ c , 0 ] = 1 else : for c in activeColumns : predictingCells = numpy . where ( self . infPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictingCells = len ( predictingCells ) if numPredictingCells > 0 : self . infActiveState [ 't' ] [ c , predictingCells ] = 1 numPredictedColumns += 1 else : self . infActiveState [ 't' ] [ c , : ] = 1 if useStartCells or numPredictedColumns >= 0.50 * len ( activeColumns ) : return True else : return False",Update the inference active state from the last set of predictions and the current bottom - up .
"def _inferPhase2 ( self ) : self . infPredictedState [ 't' ] . fill ( 0 ) self . cellConfidence [ 't' ] . fill ( 0 ) self . colConfidence [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : for i in xrange ( self . cellsPerColumn ) : for s in self . cells [ c ] [ i ] : numActiveSyns = self . _getSegmentActivityLevel ( s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) if numActiveSyns < self . activationThreshold : continue if self . verbosity >= 6 : print ""incorporating DC from cell[%d,%d]:   "" % ( c , i ) , s . debugPrint ( ) dc = s . dutyCycle ( ) self . cellConfidence [ 't' ] [ c , i ] += dc self . colConfidence [ 't' ] [ c ] += dc if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : self . infPredictedState [ 't' ] [ c , i ] = 1 sumConfidences = self . colConfidence [ 't' ] . sum ( ) if sumConfidences > 0 : self . colConfidence [ 't' ] /= sumConfidences self . cellConfidence [ 't' ] /= sumConfidences numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) if numPredictedCols >= 0.5 * self . avgInputDensity : return True else : return False",Phase 2 for the inference state . The computes the predicted state then checks to insure that the predicted state is not over - saturated i . e . look too close like a burst . This indicates that there were so many separate paths learned from the current input columns to the predicted input columns that bursting on the current input columns is most likely generated mix and match errors on cells in the predicted columns . If we detect this situation we instead turn on only the start cells in the current active columns and re - generate the predicted state from those .
"def _updateInferenceState ( self , activeColumns ) : self . infActiveState [ 't-1' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 't' ] [ : , : ] self . cellConfidence [ 't-1' ] [ : , : ] = self . cellConfidence [ 't' ] [ : , : ] self . colConfidence [ 't-1' ] [ : ] = self . colConfidence [ 't' ] [ : ] if self . maxInfBacktrack > 0 : if len ( self . _prevInfPatterns ) > self . maxInfBacktrack : self . _prevInfPatterns . pop ( 0 ) self . _prevInfPatterns . append ( activeColumns ) inSequence = self . _inferPhase1 ( activeColumns , self . resetCalled ) if not inSequence : if self . verbosity >= 3 : print ( ""Too much unpredicted input, re-tracing back to try and lock on "" ""at an earlier timestep."" ) self . _inferBacktrack ( activeColumns ) return inSequence = self . _inferPhase2 ( ) if not inSequence : if self . verbosity >= 3 : print ( ""Not enough predictions going forward, "" ""re-tracing back to try and lock on at an earlier timestep."" ) self . _inferBacktrack ( activeColumns )",Update the inference state . Called from : meth : compute on every iteration .
"def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : numPrevPatterns = len ( self . _prevLrnPatterns ) currentTimeStepsOffset = numPrevPatterns - 1 if not readOnly : self . segmentUpdates = { } if self . verbosity >= 3 : if readOnly : print ( ""Trying to lock-on using startCell state from %d steps ago:"" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) else : print ( ""Locking on using startCell state from %d steps ago:"" % ( numPrevPatterns - 1 - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) inSequence = True for offset in range ( startOffset , numPrevPatterns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] inputColumns = self . _prevLrnPatterns [ offset ] if not readOnly : self . _processSegmentUpdates ( inputColumns ) if offset == startOffset : self . lrnActiveState [ 't' ] . fill ( 0 ) for c in inputColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 inSequence = True else : inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) if not inSequence or offset == currentTimeStepsOffset : break if self . verbosity >= 3 : print ""  backtrack: computing predictions from "" , inputColumns self . _learnPhase2 ( readOnly = readOnly ) return inSequence",A utility method called from learnBacktrack . This will backtrack starting from the given startOffset in our prevLrnPatterns queue .
"def _learnBacktrack ( self ) : numPrevPatterns = len ( self . _prevLrnPatterns ) - 1 if numPrevPatterns <= 0 : if self . verbosity >= 3 : print ""lrnBacktrack: No available history to backtrack from"" return False badPatterns = [ ] inSequence = False for startOffset in range ( 0 , numPrevPatterns ) : inSequence = self . _learnBacktrackFrom ( startOffset , readOnly = True ) if inSequence : break badPatterns . append ( startOffset ) if not inSequence : if self . verbosity >= 3 : print ( ""Failed to lock on. Falling back to start cells on current "" ""time step."" ) self . _prevLrnPatterns = [ ] return False if self . verbosity >= 3 : print ( ""Discovered path to current input by using start cells from %d "" ""steps ago:"" % ( numPrevPatterns - startOffset ) , self . _prevLrnPatterns [ startOffset ] ) self . _learnBacktrackFrom ( startOffset , readOnly = False ) for i in range ( numPrevPatterns ) : if i in badPatterns or i <= startOffset : if self . verbosity >= 3 : print ( ""Removing useless pattern from history:"" , self . _prevLrnPatterns [ 0 ] ) self . _prevLrnPatterns . pop ( 0 ) else : break return numPrevPatterns - startOffset",This backtracks our learning state trying to see if we can lock onto the current set of inputs by assuming the sequence started up to N steps ago on start cells .
"def _learnPhase1 ( self , activeColumns , readOnly = False ) : self . lrnActiveState [ 't' ] . fill ( 0 ) numUnpredictedColumns = 0 for c in activeColumns : predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] numPredictedCells = len ( predictingCells ) assert numPredictedCells <= 1 if numPredictedCells == 1 : i = predictingCells [ 0 ] self . lrnActiveState [ 't' ] [ c , i ] = 1 continue numUnpredictedColumns += 1 if readOnly : continue i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) if s is not None and s . isSequenceSegment ( ) : if self . verbosity >= 4 : print ""Learn branch 0, found segment match. Learning on col="" , c self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) s . totalActivations += 1 trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , minNumSyns = 0 ) else : i = self . _getCellForNewSegment ( c ) if ( self . verbosity >= 4 ) : print ""Learn branch 1, no match. Learning on col="" , c , print "", newCellIdxInCol="" , i self . lrnActiveState [ 't' ] [ c , i ] = 1 segUpdate = self . _getSegmentActiveSynapses ( c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) segUpdate . sequenceSegment = True self . _adaptSegment ( segUpdate ) numBottomUpColumns = len ( activeColumns ) if numUnpredictedColumns < numBottomUpColumns / 2 : return True else : return False",Compute the learning active state given the predicted state and the bottom - up input .
"def _learnPhase2 ( self , readOnly = False ) : self . lrnPredictedState [ 't' ] . fill ( 0 ) for c in xrange ( self . numberOfCols ) : i , s , numActive = self . _getBestMatchingCell ( c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) if i is None : continue self . lrnPredictedState [ 't' ] [ c , i ] = 1 if readOnly : continue segUpdate = self . _getSegmentActiveSynapses ( c , i , s , activeState = self . lrnActiveState [ 't' ] , newSynapses = ( numActive < self . newSynapseCount ) ) s . totalActivations += 1 self . _addToSegmentUpdates ( c , i , segUpdate ) if self . doPooling : predSegment = self . _getBestMatchingSegment ( c , i , self . lrnActiveState [ 't-1' ] ) segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , self . lrnActiveState [ 't-1' ] , newSynapses = True ) self . _addToSegmentUpdates ( c , i , segUpdate )",Compute the predicted segments given the current set of active cells .
"def _updateLearningState ( self , activeColumns ) : self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] if self . maxLrnBacktrack > 0 : if len ( self . _prevLrnPatterns ) > self . maxLrnBacktrack : self . _prevLrnPatterns . pop ( 0 ) self . _prevLrnPatterns . append ( activeColumns ) if self . verbosity >= 4 : print ""Previous learn patterns: \n"" print self . _prevLrnPatterns self . _processSegmentUpdates ( activeColumns ) if self . pamCounter > 0 : self . pamCounter -= 1 self . learnedSeqLength += 1 if not self . resetCalled : inSequence = self . _learnPhase1 ( activeColumns ) if inSequence : self . pamCounter = self . pamLength if self . verbosity >= 3 : print ""pamCounter = "" , self . pamCounter , ""seqLength = "" , self . learnedSeqLength if ( self . resetCalled or self . pamCounter == 0 or ( self . maxSeqLength != 0 and self . learnedSeqLength >= self . maxSeqLength ) ) : if self . verbosity >= 3 : if self . resetCalled : print ""Starting over:"" , activeColumns , ""(reset was called)"" elif self . pamCounter == 0 : print ""Starting over:"" , activeColumns , ""(PAM counter expired)"" else : print ""Starting over:"" , activeColumns , ""(reached maxSeqLength)"" if self . pamCounter == 0 : seqLength = self . learnedSeqLength - self . pamLength else : seqLength = self . learnedSeqLength if self . verbosity >= 3 : print ""  learned sequence length was:"" , seqLength self . _updateAvgLearnedSeqLength ( seqLength ) backSteps = 0 if not self . resetCalled : backSteps = self . _learnBacktrack ( ) if self . resetCalled or backSteps is None or backSteps == 0 : backSteps = 0 self . lrnActiveState [ 't' ] . fill ( 0 ) for c in activeColumns : self . lrnActiveState [ 't' ] [ c , 0 ] = 1 self . _prevLrnPatterns = [ ] self . pamCounter = self . pamLength self . learnedSeqLength = backSteps self . segmentUpdates = { } self . _learnPhase2 ( )",Update the learning state . Called from compute () on every iteration : param activeColumns List of active column indices
"def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : if enableInference is None : if enableLearn : enableInference = False else : enableInference = True assert ( enableLearn or enableInference ) activeColumns = bottomUpInput . nonzero ( ) [ 0 ] if enableLearn : self . lrnIterationIdx += 1 self . iterationIdx += 1 if self . verbosity >= 3 : print ""\n==== PY Iteration: %d ====="" % ( self . iterationIdx ) print ""Active cols:"" , activeColumns if enableLearn : if self . lrnIterationIdx in Segment . dutyCycleTiers : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . avgInputDensity is None : self . avgInputDensity = len ( activeColumns ) else : self . avgInputDensity = ( 0.99 * self . avgInputDensity + 0.01 * len ( activeColumns ) ) if enableInference : self . _updateInferenceState ( activeColumns ) if enableLearn : self . _updateLearningState ( activeColumns ) if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : segsToDel = [ ] for segment in self . cells [ c ] [ i ] : age = self . lrnIterationIdx - segment . lastActiveIteration if age <= self . maxAge : continue synsToDel = [ ] for synapse in segment . syns : synapse [ 2 ] = synapse [ 2 ] - self . globalDecay if synapse [ 2 ] <= 0 : synsToDel . append ( synapse ) if len ( synsToDel ) == segment . getNumSynapses ( ) : segsToDel . append ( segment ) elif len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) for seg in segsToDel : self . _cleanUpdatesList ( c , i , seg ) self . cells [ c ] [ i ] . remove ( seg ) if self . collectStats : if enableInference : predictedState = self . infPredictedState [ 't-1' ] else : predictedState = self . lrnPredictedState [ 't-1' ] self . _updateStatsInferEnd ( self . _internalStats , activeColumns , predictedState , self . colConfidence [ 't-1' ] ) output = self . _computeOutput ( ) self . printComputeEnd ( output , learn = enableLearn ) self . resetCalled = False return output",Handle one compute possibly learning .
"def learn ( self , bottomUpInput , enableInference = None ) : return self . compute ( bottomUpInput , enableLearn = True , enableInference = enableInference )",TODO : document
"def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , minNumSyns ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold nSegsRemoved , nSynsRemoved = 0 , 0 segsToDel = [ ] for segment in segList : synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] if len ( synsToDel ) == len ( segment . syns ) : segsToDel . append ( segment ) else : if len ( synsToDel ) > 0 : for syn in synsToDel : segment . syns . remove ( syn ) nSynsRemoved += 1 if len ( segment . syns ) < minNumSyns : segsToDel . append ( segment ) nSegsRemoved += len ( segsToDel ) for seg in segsToDel : self . _cleanUpdatesList ( colIdx , cellIdx , seg ) self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) nSynsRemoved += len ( seg . syns ) return nSegsRemoved , nSynsRemoved",This method goes through a list of segments for a given cell and deletes all synapses whose permanence is less than minPermanence and deletes any segments that have less than minNumSyns synapses remaining .
"def trimSegments ( self , minPermanence = None , minNumSyns = None ) : if minPermanence is None : minPermanence = self . connectedPerm if minNumSyns is None : minNumSyns = self . activationThreshold totalSegsRemoved , totalSynsRemoved = 0 , 0 for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , minPermanence = minPermanence , minNumSyns = minNumSyns ) totalSegsRemoved += segsRemoved totalSynsRemoved += synsRemoved if self . verbosity >= 5 : print ""Cells, all segments:"" self . printCells ( predictedOnly = False ) return totalSegsRemoved , totalSynsRemoved",This method deletes all synapses whose permanence is less than minPermanence and deletes any segments that have less than minNumSyns synapses remaining .
"def _cleanUpdatesList ( self , col , cellIdx , seg ) : for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c == col and i == cellIdx : for update in updateList : if update [ 1 ] . segment == seg : self . _removeSegmentUpdate ( update )",Removes any update that would be for the given col cellIdx segIdx .
"def finishLearning ( self ) : self . trimSegments ( minPermanence = 0.0001 ) for c , i in itertools . product ( xrange ( self . numberOfCols ) , xrange ( self . cellsPerColumn ) ) : for segment in self . cells [ c ] [ i ] : segment . dutyCycle ( ) if self . cellsPerColumn > 1 : for c in xrange ( self . numberOfCols ) : assert self . getNumSegmentsInCell ( c , 0 ) == 0",Called when learning has been completed . This method just calls : meth : trimSegments and then clears out caches .
"def _getBestMatchingCell ( self , c , activeState , minThreshold ) : bestActivityInCol = minThreshold bestSegIdxInCol = - 1 bestCellInCol = - 1 for i in xrange ( self . cellsPerColumn ) : maxSegActivity = 0 maxSegIdx = 0 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState ) if activity > maxSegActivity : maxSegActivity = activity maxSegIdx = j if maxSegActivity >= bestActivityInCol : bestActivityInCol = maxSegActivity bestSegIdxInCol = maxSegIdx bestCellInCol = i if bestCellInCol == - 1 : return ( None , None , None ) else : return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , bestActivityInCol )",Find weakly activated cell in column with at least minThreshold active synapses .
"def _getBestMatchingSegment ( self , c , i , activeState ) : maxActivity , which = self . minThreshold , - 1 for j , s in enumerate ( self . cells [ c ] [ i ] ) : activity = self . _getSegmentActivityLevel ( s , activeState , connectedSynapsesOnly = False ) if activity >= maxActivity : maxActivity , which = activity , j if which == - 1 : return None else : return self . cells [ c ] [ i ] [ which ]",For the given cell find the segment with the largest number of active synapses . This routine is aggressive in finding the best match . The permanence value of synapses is allowed to be below connectedPerm . The number of active synapses is allowed to be below activationThreshold but must be above minThreshold . The routine returns the segment index . If no segments are found then an index of - 1 is returned .
"def _getCellForNewSegment ( self , colIdx ) : if self . maxSegmentsPerCell < 0 : if self . cellsPerColumn > 1 : i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 else : i = 0 return i candidateCellIdxs = [ ] if self . cellsPerColumn == 1 : minIdx = 0 maxIdx = 0 else : minIdx = 1 maxIdx = self . cellsPerColumn - 1 for i in xrange ( minIdx , maxIdx + 1 ) : numSegs = len ( self . cells [ colIdx ] [ i ] ) if numSegs < self . maxSegmentsPerCell : candidateCellIdxs . append ( i ) if len ( candidateCellIdxs ) > 0 : candidateCellIdx = ( candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) if self . verbosity >= 5 : print ""Cell [%d,%d] chosen for new segment, # of segs is %d"" % ( colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) return candidateCellIdx candidateSegment = None candidateSegmentDC = 1.0 for i in xrange ( minIdx , maxIdx + 1 ) : for s in self . cells [ colIdx ] [ i ] : dc = s . dutyCycle ( ) if dc < candidateSegmentDC : candidateCellIdx = i candidateSegmentDC = dc candidateSegment = s if self . verbosity >= 5 : print ( ""Deleting segment #%d for cell[%d,%d] to make room for new "" ""segment"" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) candidateSegment . debugPrint ( ) self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) return candidateCellIdx",Return the index of a cell in this column which is a good candidate for adding a new segment .
"def _getSegmentActiveSynapses ( self , c , i , s , activeState , newSynapses = False ) : activeSynapses = [ ] if s is not None : activeSynapses = [ idx for idx , syn in enumerate ( s . syns ) if activeState [ syn [ 0 ] , syn [ 1 ] ] ] if newSynapses : nSynapsesToAdd = self . newSynapseCount - len ( activeSynapses ) activeSynapses += self . _chooseCellsToLearnFrom ( c , i , s , nSynapsesToAdd , activeState ) update = BacktrackingTM . _SegmentUpdate ( c , i , s , activeSynapses ) return update",Return a segmentUpdate data structure containing a list of proposed changes to segment s . Let activeSynapses be the list of active synapses where the originating cells have their activeState output = 1 at time step t . ( This list is empty if s is None since the segment doesn t exist . ) newSynapses is an optional argument that defaults to false . If newSynapses is true then newSynapseCount - len ( activeSynapses ) synapses are added to activeSynapses . These synapses are randomly chosen from the set of cells that have learnState = 1 at timeStep .
"def _chooseCellsToLearnFrom ( self , c , i , s , n , activeState ) : if n <= 0 : return [ ] tmpCandidates = numpy . where ( activeState == 1 ) if len ( tmpCandidates [ 0 ] ) == 0 : return [ ] if s is None : cands = [ syn for syn in zip ( tmpCandidates [ 0 ] , tmpCandidates [ 1 ] ) ] else : synapsesAlreadyInSegment = set ( ( syn [ 0 ] , syn [ 1 ] ) for syn in s . syns ) cands = [ syn for syn in zip ( tmpCandidates [ 0 ] , tmpCandidates [ 1 ] ) if ( syn [ 0 ] , syn [ 1 ] ) not in synapsesAlreadyInSegment ] if len ( cands ) <= n : return cands if n == 1 : idx = self . _random . getUInt32 ( len ( cands ) ) return [ cands [ idx ] ] indices = numpy . array ( [ j for j in range ( len ( cands ) ) ] , dtype = 'uint32' ) tmp = numpy . zeros ( min ( n , len ( indices ) ) , dtype = 'uint32' ) self . _random . sample ( indices , tmp ) return sorted ( [ cands [ j ] for j in tmp ] )",Choose n random cells to learn from .
"def _processSegmentUpdates ( self , activeColumns ) : removeKeys = [ ] trimSegments = [ ] for key , updateList in self . segmentUpdates . iteritems ( ) : c , i = key [ 0 ] , key [ 1 ] if c in activeColumns : action = 'update' else : if self . doPooling and self . lrnPredictedState [ 't' ] [ c , i ] == 1 : action = 'keep' else : action = 'remove' updateListKeep = [ ] if action != 'remove' : for ( createDate , segUpdate ) in updateList : if self . verbosity >= 4 : print ""_nLrnIterations ="" , self . lrnIterationIdx , print segUpdate if self . lrnIterationIdx - createDate > self . segUpdateValidDuration : continue if action == 'update' : trimSegment = self . _adaptSegment ( segUpdate ) if trimSegment : trimSegments . append ( ( segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment ) ) else : updateListKeep . append ( ( createDate , segUpdate ) ) self . segmentUpdates [ key ] = updateListKeep if len ( updateListKeep ) == 0 : removeKeys . append ( key ) for key in removeKeys : self . segmentUpdates . pop ( key ) for ( c , i , segment ) in trimSegments : self . _trimSegmentsInCell ( c , i , [ segment ] , minPermanence = 0.00001 , minNumSyns = 0 )",Go through the list of accumulated segment updates and process them as follows :
"def _adaptSegment ( self , segUpdate ) : trimSegment = False c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment activeSynapses = segUpdate . activeSynapses synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) if segment is not None : if self . verbosity >= 4 : print ""Reinforcing segment #%d for cell[%d,%d]"" % ( segment . segID , c , i ) print ""  before:"" , segment . debugPrint ( ) segment . lastActiveIteration = self . lrnIterationIdx segment . positiveActivations += 1 segment . dutyCycle ( active = True ) lastSynIndex = len ( segment . syns ) - 1 inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] trimSegment = segment . updateSynapses ( inactiveSynIndices , - self . permanenceDec ) activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] segment . updateSynapses ( activeSynIndices , self . permanenceInc ) synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - self . maxSynapsesPerSegment ) segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) for newSyn in synsToAdd : segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) if self . verbosity >= 4 : print ""   after:"" , segment . debugPrint ( ) else : newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) for synapse in activeSynapses : newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) if self . verbosity >= 3 : print ""New segment #%d for cell[%d,%d]"" % ( self . segID - 1 , c , i ) , newSegment . debugPrint ( ) self . cells [ c ] [ i ] . append ( newSegment ) return trimSegment",This function applies segment update information to a segment in a cell .
"def dutyCycle ( self , active = False , readOnly = False ) : if self . tm . lrnIterationIdx <= self . dutyCycleTiers [ 1 ] : dutyCycle = float ( self . positiveActivations ) / self . tm . lrnIterationIdx if not readOnly : self . _lastPosDutyCycleIteration = self . tm . lrnIterationIdx self . _lastPosDutyCycle = dutyCycle return dutyCycle age = self . tm . lrnIterationIdx - self . _lastPosDutyCycleIteration if age == 0 and not active : return self . _lastPosDutyCycle for tierIdx in range ( len ( self . dutyCycleTiers ) - 1 , 0 , - 1 ) : if self . tm . lrnIterationIdx > self . dutyCycleTiers [ tierIdx ] : alpha = self . dutyCycleAlphas [ tierIdx ] break dutyCycle = pow ( 1.0 - alpha , age ) * self . _lastPosDutyCycle if active : dutyCycle += alpha if not readOnly : self . _lastPosDutyCycleIteration = self . tm . lrnIterationIdx self . _lastPosDutyCycle = dutyCycle return dutyCycle",Compute / update and return the positive activations duty cycle of this segment . This is a measure of how often this segment is providing good predictions .
"def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] )",Add a new synapse
"def logExceptions ( logger = None ) : logger = ( logger if logger is not None else logging . getLogger ( __name__ ) ) def exceptionLoggingDecorator ( func ) : @ functools . wraps ( func ) def exceptionLoggingWrap ( * args , * * kwargs ) : try : return func ( * args , * * kwargs ) except : logger . exception ( ""Unhandled exception %r from %r. Caller stack:\n%s"" , sys . exc_info ( ) [ 1 ] , func , '' . join ( traceback . format_stack ( ) ) , ) raise return exceptionLoggingWrap return exceptionLoggingDecorator",Returns a closure suitable for use as function / method decorator for logging exceptions that leave the scope of the decorated function . Exceptions are logged at ERROR level .
"def logEntryExit ( getLoggerCallback = logging . getLogger , entryExitLogLevel = logging . DEBUG , logArgs = False , logTraceback = False ) : def entryExitLoggingDecorator ( func ) : @ functools . wraps ( func ) def entryExitLoggingWrap ( * args , * * kwargs ) : if entryExitLogLevel is None : enabled = False else : logger = getLoggerCallback ( ) enabled = logger . isEnabledFor ( entryExitLogLevel ) if not enabled : return func ( * args , * * kwargs ) funcName = str ( func ) if logArgs : argsRepr = ', ' . join ( [ repr ( a ) for a in args ] + [ '%s=%r' % ( k , v , ) for k , v in kwargs . iteritems ( ) ] ) else : argsRepr = '' logger . log ( entryExitLogLevel , ""ENTERING: %s(%s)%s"" , funcName , argsRepr , '' if not logTraceback else '; ' + repr ( traceback . format_stack ( ) ) ) try : return func ( * args , * * kwargs ) finally : logger . log ( entryExitLogLevel , ""LEAVING: %s(%s)%s"" , funcName , argsRepr , '' if not logTraceback else '; ' + repr ( traceback . format_stack ( ) ) ) return entryExitLoggingWrap return entryExitLoggingDecorator",Returns a closure suitable for use as function / method decorator for logging entry / exit of function / method .
"def retry ( timeoutSec , initialRetryDelaySec , maxRetryDelaySec , retryExceptions = ( Exception , ) , retryFilter = lambda e , args , kwargs : True , logger = None , clientLabel = """" ) : assert initialRetryDelaySec > 0 , str ( initialRetryDelaySec ) assert timeoutSec >= 0 , str ( timeoutSec ) assert maxRetryDelaySec >= initialRetryDelaySec , ""%r < %r"" % ( maxRetryDelaySec , initialRetryDelaySec ) assert isinstance ( retryExceptions , tuple ) , ( ""retryExceptions must be tuple, but got %r"" ) % ( type ( retryExceptions ) , ) if logger is None : logger = logging . getLogger ( __name__ ) def retryDecorator ( func ) : @ functools . wraps ( func ) def retryWrap ( * args , * * kwargs ) : numAttempts = 0 delaySec = initialRetryDelaySec startTime = time . time ( ) while True : numAttempts += 1 try : result = func ( * args , * * kwargs ) except retryExceptions , e : if not retryFilter ( e , args , kwargs ) : if logger . isEnabledFor ( logging . DEBUG ) : logger . debug ( '[%s] Failure in %r; retries aborted by custom retryFilter. ' 'Caller stack:\n%s' , clientLabel , func , '' . join ( traceback . format_stack ( ) ) , exc_info = True ) raise now = time . time ( ) if now < startTime : startTime = now if ( now - startTime ) >= timeoutSec : logger . exception ( '[%s] Exhausted retry timeout (%s sec.; %s attempts) for %r. ' 'Caller stack:\n%s' , clientLabel , timeoutSec , numAttempts , func , '' . join ( traceback . format_stack ( ) ) ) raise if numAttempts == 1 : logger . warning ( '[%s] First failure in %r; initial retry in %s sec.; ' 'timeoutSec=%s. Caller stack:\n%s' , clientLabel , func , delaySec , timeoutSec , '' . join ( traceback . format_stack ( ) ) , exc_info = True ) else : logger . debug ( '[%s] %r failed %s times; retrying in %s sec.; timeoutSec=%s. ' 'Caller stack:\n%s' , clientLabel , func , numAttempts , delaySec , timeoutSec , '' . join ( traceback . format_stack ( ) ) , exc_info = True ) time . sleep ( delaySec ) delaySec = min ( delaySec * 2 , maxRetryDelaySec ) else : if numAttempts > 1 : logger . info ( '[%s] %r succeeded on attempt # %d' , clientLabel , func , numAttempts ) return result return retryWrap return retryDecorator",Returns a closure suitable for use as function / method decorator for retrying a function being decorated .
"def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : assert ( patternOverlap < numOnes ) numNewBitsInEachPattern = numOnes - patternOverlap numCols = numNewBitsInEachPattern * numPatterns + patternOverlap p = [ ] for i in xrange ( numPatterns ) : x = numpy . zeros ( numCols , dtype = 'float32' ) startBit = i * numNewBitsInEachPattern nextStartBit = startBit + numOnes x [ startBit : nextStartBit ] = 1 p . append ( x ) return p",Very simple patterns . Each pattern has numOnes consecutive bits on . The amount of overlap between consecutive patterns is configurable via the patternOverlap parameter .
"def buildOverlappedSequences ( numSequences = 2 , seqLen = 5 , sharedElements = [ 3 , 4 ] , numOnBitsPerPattern = 3 , patternOverlap = 0 , seqOverlap = 0 , * * kwargs ) : numSharedElements = len ( sharedElements ) numUniqueElements = seqLen - numSharedElements numPatterns = numSharedElements + numUniqueElements * numSequences patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] uniquePatternIndices = range ( numSharedElements , numPatterns ) for _ in xrange ( numSequences ) : sequence = [ ] sharedPatternIndices = range ( numSharedElements ) for j in xrange ( seqLen ) : if j in sharedElements : patIdx = sharedPatternIndices . pop ( 0 ) else : patIdx = uniquePatternIndices . pop ( 0 ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print ""\nTraining sequences"" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )",Create training sequences that share some elements in the middle .
"def buildSequencePool ( numSequences = 10 , seqLen = [ 2 , 3 , 4 ] , numPatterns = 5 , numOnBitsPerPattern = 3 , patternOverlap = 0 , * * kwargs ) : patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) numCols = len ( patterns [ 0 ] ) trainingSequences = [ ] for _ in xrange ( numSequences ) : sequence = [ ] length = random . choice ( seqLen ) for _ in xrange ( length ) : patIdx = random . choice ( xrange ( numPatterns ) ) sequence . append ( patterns [ patIdx ] ) trainingSequences . append ( sequence ) if VERBOSITY >= 3 : print ""\nTraining sequences"" printAllTrainingSequences ( trainingSequences ) return ( numCols , trainingSequences )",Create a bunch of sequences of various lengths all built from a fixed set of patterns .
"def createTMs ( includeCPP = True , includePy = True , numCols = 100 , cellsPerCol = 4 , activationThreshold = 3 , minThreshold = 3 , newSynapseCount = 3 , initialPerm = 0.6 , permanenceInc = 0.1 , permanenceDec = 0.0 , globalDecay = 0.0 , pamLength = 0 , checkSynapseConsistency = True , maxInfBacktrack = 0 , maxLrnBacktrack = 0 , * * kwargs ) : connectedPerm = 0.5 tms = dict ( ) if includeCPP : if VERBOSITY >= 2 : print ""Creating BacktrackingTMCPP instance"" cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , checkSynapseConsistency = checkSynapseConsistency , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) cpp_tm . retrieveLearningStates = True tms [ 'CPP' ] = cpp_tm if includePy : if VERBOSITY >= 2 : print ""Creating PY TM instance"" py_tm = BacktrackingTM ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , initialPerm = initialPerm , connectedPerm = connectedPerm , minThreshold = minThreshold , newSynapseCount = newSynapseCount , permanenceInc = permanenceInc , permanenceDec = permanenceDec , activationThreshold = activationThreshold , globalDecay = globalDecay , burnIn = 1 , seed = SEED , verbosity = VERBOSITY , collectStats = True , pamLength = pamLength , maxInfBacktrack = maxInfBacktrack , maxLrnBacktrack = maxLrnBacktrack , ) tms [ 'PY ' ] = py_tm return tms",Create one or more TM instances placing each into a dict keyed by name .
"def assertNoTMDiffs ( tms ) : if len ( tms ) == 1 : return if len ( tms ) > 2 : raise ""Not implemented for more than 2 TMs"" same = fdrutils . tmDiff2 ( tms . values ( ) , verbosity = VERBOSITY ) assert ( same ) return",Check for diffs among the TM instances in the passed in tms dict and raise an assert if any are detected
"def evalSequences ( tms , trainingSequences , testSequences = None , nTrainRepetitions = 1 , doResets = True , * * kwargs ) : if testSequences == None : testSequences = trainingSequences firstTM = tms . values ( ) [ 0 ] assertNoTMDiffs ( tms ) for trainingNum in xrange ( nTrainRepetitions ) : if VERBOSITY >= 2 : print ""\n##############################################################"" print ""################# Training round #%d of %d #################"" % ( trainingNum , nTrainRepetitions ) for ( name , tm ) in tms . iteritems ( ) : print ""TM parameters for %s: "" % ( name ) print ""---------------------"" tm . printParameters ( ) print numSequences = len ( testSequences ) for sequenceNum , trainingSequence in enumerate ( trainingSequences ) : numTimeSteps = len ( trainingSequence ) if VERBOSITY >= 2 : print ""\n================= Sequence #%d of %d ================"" % ( sequenceNum , numSequences ) if doResets : for tm in tms . itervalues ( ) : tm . reset ( ) for t , x in enumerate ( trainingSequence ) : if VERBOSITY >= 2 : print if VERBOSITY >= 3 : print ""------------------------------------------------------------"" print ""--------- sequence: #%d of %d, timeStep: #%d of %d -----------"" % ( sequenceNum , numSequences , t , numTimeSteps ) firstTM . printInput ( x ) print ""input nzs:"" , x . nonzero ( ) x = numpy . array ( x ) . astype ( 'float32' ) for tm in tms . itervalues ( ) : tm . learn ( x , enableInference = True ) if VERBOSITY >= 3 : for ( name , tm ) in tms . iteritems ( ) : print ""I/O states of %s TM:"" % ( name ) print ""-------------------------------------"" , tm . printStates ( printPrevious = ( VERBOSITY >= 5 ) ) print assertNoTMDiffs ( tms ) if VERBOSITY >= 2 : for ( name , tm ) in tms . iteritems ( ) : stats = tm . getStats ( ) print ""# of unpredicted columns for %s TM: %d of %d"" % ( name , stats [ 'curMissing' ] , x . sum ( ) ) numBurstingCols = tm . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) print ""# of bursting columns for %s TM: %d of %d"" % ( name , numBurstingCols , x . sum ( ) ) if VERBOSITY >= 4 : print ""Sequence %d finished."" % ( sequenceNum ) for ( name , tm ) in tms . iteritems ( ) : print ""All cells of %s TM:"" % ( name ) print ""-------------------------------------"" , tm . printCells ( ) print if VERBOSITY >= 2 : print prevResult = None for ( name , tm ) in tms . iteritems ( ) : stats = tm . getStats ( ) if VERBOSITY >= 1 : print ""Stats for %s TM over all sequences for training round #%d of %d:"" % ( name , trainingNum , nTrainRepetitions ) print ""   total missing:"" , stats [ 'totalMissing' ] print ""   total extra:"" , stats [ 'totalExtra' ] if prevResult is None : prevResult = ( stats [ 'totalMissing' ] , stats [ 'totalExtra' ] ) else : assert ( stats [ 'totalMissing' ] == prevResult [ 0 ] ) assert ( stats [ 'totalExtra' ] == prevResult [ 1 ] ) tm . resetStats ( ) if VERBOSITY >= 3 : print ""Calling trim segments"" prevResult = None for tm in tms . itervalues ( ) : nSegsRemoved , nSynsRemoved = tm . trimSegments ( ) if prevResult is None : prevResult = ( nSegsRemoved , nSynsRemoved ) else : assert ( nSegsRemoved == prevResult [ 0 ] ) assert ( nSynsRemoved == prevResult [ 1 ] ) assertNoTMDiffs ( tms ) if VERBOSITY >= 4 : print ""Training completed. Complete state:"" for ( name , tm ) in tms . iteritems ( ) : print ""%s:"" % ( name ) tm . printCells ( ) print if VERBOSITY >= 2 : print ""\n##############################################################"" print ""########################## Inference #########################"" for tm in tms . itervalues ( ) : tm . resetStats ( ) numSequences = len ( testSequences ) for sequenceNum , testSequence in enumerate ( testSequences ) : numTimeSteps = len ( testSequence ) if VERBOSITY >= 2 : print ""\n================= Sequence %d of %d ================"" % ( sequenceNum , numSequences ) if doResets : for tm in tms . itervalues ( ) : tm . reset ( ) for t , x in enumerate ( testSequence ) : if VERBOSITY >= 2 : print if VERBOSITY >= 3 : print ""------------------------------------------------------------"" print ""--------- sequence: #%d of %d, timeStep: #%d of %d -----------"" % ( sequenceNum , numSequences , t , numTimeSteps ) firstTM . printInput ( x ) print ""input nzs:"" , x . nonzero ( ) for tm in tms . itervalues ( ) : tm . infer ( x ) assertNoTMDiffs ( tms ) if VERBOSITY >= 2 : for ( name , tm ) in tms . iteritems ( ) : stats = tm . getStats ( ) print ""# of unpredicted columns for %s TM: %d of %d"" % ( name , stats [ 'curMissing' ] , x . sum ( ) ) if VERBOSITY >= 3 : for ( name , tm ) in tms . iteritems ( ) : print ""I/O states of %s TM:"" % ( name ) print ""-------------------------------------"" , tm . printStates ( printPrevious = ( VERBOSITY >= 5 ) , printLearnState = False ) print if VERBOSITY >= 4 : print for ( name , tm ) in tms . iteritems ( ) : print ""Interim internal stats for %s TM:"" % ( name ) print ""---------------------------------"" pprint . pprint ( tm . getStats ( ) ) print if VERBOSITY >= 2 : print ""\n##############################################################"" print ""####################### Inference Done #######################"" tmStats = dict ( ) for ( name , tm ) in tms . iteritems ( ) : tmStats [ name ] = stats = tm . getStats ( ) if VERBOSITY >= 2 : print ""Stats for %s TM over all sequences:"" % ( name ) print ""   total missing:"" , stats [ 'totalMissing' ] print ""   total extra:"" , stats [ 'totalExtra' ] for ( name , tm ) in tms . iteritems ( ) : if VERBOSITY >= 3 : print ""\nAll internal stats for %s TM:"" % ( name ) print ""-------------------------------------"" , pprint . pprint ( tmStats [ name ] ) print return tmStats",Train the TMs on the entire training set for nTrainRepetitions in a row . Then run the test set through inference once and return the inference stats .
"def needsquoting ( c , quotetabs , header ) : if c in ' \t' : return quotetabs if c == '_' : return header return c == ESCAPE or not ( ' ' <= c <= '~' )",Decide whether a particular character needs to be quoted .
def quote ( c ) : i = ord ( c ) return ESCAPE + HEX [ i // 16 ] + HEX [ i % 16 ],Quote a single character .
"def encode ( input , output , quotetabs , header = 0 ) : if b2a_qp is not None : data = input . read ( ) odata = b2a_qp ( data , quotetabs = quotetabs , header = header ) output . write ( odata ) return def write ( s , output = output , lineEnd = '\n' ) : if s and s [ - 1 : ] in ' \t' : output . write ( s [ : - 1 ] + quote ( s [ - 1 ] ) + lineEnd ) elif s == '.' : output . write ( quote ( s ) + lineEnd ) else : output . write ( s + lineEnd ) prevline = None while 1 : line = input . readline ( ) if not line : break outline = [ ] stripped = '' if line [ - 1 : ] == '\n' : line = line [ : - 1 ] stripped = '\n' for c in line : if needsquoting ( c , quotetabs , header ) : c = quote ( c ) if header and c == ' ' : outline . append ( '_' ) else : outline . append ( c ) if prevline is not None : write ( prevline ) thisline = EMPTYSTRING . join ( outline ) while len ( thisline ) > MAXLINESIZE : write ( thisline [ : MAXLINESIZE - 1 ] , lineEnd = '=\n' ) thisline = thisline [ MAXLINESIZE - 1 : ] prevline = thisline if prevline is not None : write ( prevline , lineEnd = stripped )",Read input apply quoted - printable encoding and write to output .
"def decode ( input , output , header = 0 ) : if a2b_qp is not None : data = input . read ( ) odata = a2b_qp ( data , header = header ) output . write ( odata ) return new = '' while 1 : line = input . readline ( ) if not line : break i , n = 0 , len ( line ) if n > 0 and line [ n - 1 ] == '\n' : partial = 0 n = n - 1 while n > 0 and line [ n - 1 ] in "" \t\r"" : n = n - 1 else : partial = 1 while i < n : c = line [ i ] if c == '_' and header : new = new + ' ' i = i + 1 elif c != ESCAPE : new = new + c i = i + 1 elif i + 1 == n and not partial : partial = 1 break elif i + 1 < n and line [ i + 1 ] == ESCAPE : new = new + ESCAPE i = i + 2 elif i + 2 < n and ishex ( line [ i + 1 ] ) and ishex ( line [ i + 2 ] ) : new = new + chr ( unhex ( line [ i + 1 : i + 3 ] ) ) i = i + 3 else : new = new + c i = i + 1 if not partial : output . write ( new + '\n' ) new = '' if new : output . write ( new )",Read input apply quoted - printable decoding and write to output . input and output are files with readline () and write () methods . If header is true decode underscore as space ( per RFC 1522 ) .
def unhex ( s ) : bits = 0 for c in s : if '0' <= c <= '9' : i = ord ( '0' ) elif 'a' <= c <= 'f' : i = ord ( 'a' ) - 10 elif 'A' <= c <= 'F' : i = ord ( 'A' ) - 10 else : break bits = bits * 16 + ( ord ( c ) - i ) return bits,Get the integer value of a hexadecimal number .
"def b64encode ( s , altchars = None ) : encoded = binascii . b2a_base64 ( s ) [ : - 1 ] if altchars is not None : return encoded . translate ( string . maketrans ( b'+/' , altchars [ : 2 ] ) ) return encoded",Encode a string using Base64 .
"def b64decode ( s , altchars = None ) : if altchars is not None : s = s . translate ( string . maketrans ( altchars [ : 2 ] , '+/' ) ) try : return binascii . a2b_base64 ( s ) except binascii . Error , msg : raise TypeError ( msg )",Decode a Base64 encoded string .
"def b32encode ( s ) : parts = [ ] quanta , leftover = divmod ( len ( s ) , 5 ) if leftover : s += ( '\0' * ( 5 - leftover ) ) quanta += 1 for i in range ( quanta ) : c1 , c2 , c3 = struct . unpack ( '!HHB' , s [ i * 5 : ( i + 1 ) * 5 ] ) c2 += ( c1 & 1 ) << 16 c3 += ( c2 & 3 ) << 8 parts . extend ( [ _b32tab [ c1 >> 11 ] , _b32tab [ ( c1 >> 6 ) & 0x1f ] , _b32tab [ ( c1 >> 1 ) & 0x1f ] , _b32tab [ c2 >> 12 ] , _b32tab [ ( c2 >> 7 ) & 0x1f ] , _b32tab [ ( c2 >> 2 ) & 0x1f ] , _b32tab [ c3 >> 5 ] , _b32tab [ c3 & 0x1f ] , ] ) encoded = EMPTYSTRING . join ( parts ) if leftover == 1 : return encoded [ : - 6 ] + '======' elif leftover == 2 : return encoded [ : - 4 ] + '====' elif leftover == 3 : return encoded [ : - 3 ] + '===' elif leftover == 4 : return encoded [ : - 1 ] + '=' return encoded",Encode a string using Base32 .
"def b32decode ( s , casefold = False , map01 = None ) : quanta , leftover = divmod ( len ( s ) , 8 ) if leftover : raise TypeError ( 'Incorrect padding' ) if map01 : s = s . translate ( string . maketrans ( b'01' , b'O' + map01 ) ) if casefold : s = s . upper ( ) padchars = 0 mo = re . search ( '(?P<pad>[=]*)$' , s ) if mo : padchars = len ( mo . group ( 'pad' ) ) if padchars > 0 : s = s [ : - padchars ] parts = [ ] acc = 0 shift = 35 for c in s : val = _b32rev . get ( c ) if val is None : raise TypeError ( 'Non-base32 digit found' ) acc += _b32rev [ c ] << shift shift -= 5 if shift < 0 : parts . append ( binascii . unhexlify ( '%010x' % acc ) ) acc = 0 shift = 35 last = binascii . unhexlify ( '%010x' % acc ) if padchars == 0 : last = '' elif padchars == 1 : last = last [ : - 1 ] elif padchars == 3 : last = last [ : - 2 ] elif padchars == 4 : last = last [ : - 3 ] elif padchars == 6 : last = last [ : - 4 ] else : raise TypeError ( 'Incorrect padding' ) parts . append ( last ) return EMPTYSTRING . join ( parts )",Decode a Base32 encoded string .
"def b16decode ( s , casefold = False ) : if casefold : s = s . upper ( ) if re . search ( '[^0-9A-F]' , s ) : raise TypeError ( 'Non-base16 digit found' ) return binascii . unhexlify ( s )",Decode a Base16 encoded string .
"def encode ( input , output ) : while True : s = input . read ( MAXBINSIZE ) if not s : break while len ( s ) < MAXBINSIZE : ns = input . read ( MAXBINSIZE - len ( s ) ) if not ns : break s += ns line = binascii . b2a_base64 ( s ) output . write ( line )",Encode a file .
"def decode ( input , output ) : while True : line = input . readline ( ) if not line : break s = binascii . a2b_base64 ( line ) output . write ( s )",Decode a file .
"def encodestring ( s ) : pieces = [ ] for i in range ( 0 , len ( s ) , MAXBINSIZE ) : chunk = s [ i : i + MAXBINSIZE ] pieces . append ( binascii . b2a_base64 ( chunk ) ) return """" . join ( pieces )",Encode a string into multiple lines of base - 64 data .
"def source_line ( self , lineno ) : line_begins = self . _extract_line_begins ( ) lineno = lineno - self . first_line if lineno >= 0 and lineno + 1 < len ( line_begins ) : first , last = line_begins [ lineno : lineno + 2 ] return self . source [ first : last ] elif lineno >= 0 and lineno < len ( line_begins ) : return self . source [ line_begins [ - 1 ] : ] else : raise IndexError",Returns line lineno from source taking first_line into account or raises : exc : IndexError if lineno is out of range .
"def decompose_position ( self , offset ) : line_begins = self . _extract_line_begins ( ) lineno = bisect . bisect_right ( line_begins , offset ) - 1 if offset >= 0 and offset <= len ( self . source ) : return lineno + self . first_line , offset - line_begins [ lineno ] else : raise IndexError",Returns a line column tuple for a character offset into the source orraises : exc : IndexError if lineno is out of range .
"def chain ( self , expanded_from ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = expanded_from )",Returns a range identical to this one but indicating that it was expanded from the range expanded_from .
"def begin ( self ) : return Range ( self . source_buffer , self . begin_pos , self . begin_pos , expanded_from = self . expanded_from )",Returns a zero - length range located just before the beginning of this range .
"def end ( self ) : return Range ( self . source_buffer , self . end_pos , self . end_pos , expanded_from = self . expanded_from )",Returns a zero - length range located just after the end of this range .
"def column ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return column",Returns a zero - based column number of the beginning of this range .
"def column_range ( self ) : if self . begin ( ) . line ( ) == self . end ( ) . line ( ) : return self . begin ( ) . column ( ) , self . end ( ) . column ( ) else : return self . begin ( ) . column ( ) , len ( self . begin ( ) . source_line ( ) ) - 1",Returns a [ * begin * * end * ) tuple describing the range of columns spanned by this range . If range spans more than one line returned * end * is the last column of the line .
"def line ( self ) : line , column = self . source_buffer . decompose_position ( self . begin_pos ) return line",Returns the line number of the beginning of this range .
"def join ( self , other ) : if self . source_buffer != other . source_buffer : raise ValueError if self . expanded_from == other . expanded_from : expanded_from = self . expanded_from else : expanded_from = None return Range ( self . source_buffer , min ( self . begin_pos , other . begin_pos ) , max ( self . end_pos , other . end_pos ) , expanded_from = expanded_from )",Returns the smallest possible range spanning both this range and other . Raises : exc : ValueError if the ranges do not belong to the same : class : Buffer .
"def source_lines ( self ) : return [ self . source_buffer . source_line ( line ) for line in range ( self . line ( ) , self . end ( ) . line ( ) + 1 ) ]",Returns the lines of source code containing the entirety of this range .
"def rewrite ( self ) : self . _sort ( ) self . _check ( ) rewritten , pos = [ ] , 0 for range , replacement in self . ranges : rewritten . append ( self . buffer . source [ pos : range . begin_pos ] ) rewritten . append ( replacement ) pos = range . end_pos rewritten . append ( self . buffer . source [ pos : ] ) return Buffer ( """" . join ( rewritten ) , self . buffer . name , self . buffer . first_line )",Return the rewritten source . May raise : class : RewriterConflict .
"def compare ( left , right , compare_locs = False ) : if type ( left ) != type ( right ) : return False if isinstance ( left , ast . AST ) : for field in left . _fields : if not compare ( getattr ( left , field ) , getattr ( right , field ) ) : return False if compare_locs : for loc in left . _locs : if getattr ( left , loc ) != getattr ( right , loc ) : return False return True elif isinstance ( left , list ) : if len ( left ) != len ( right ) : return False for left_elt , right_elt in zip ( left , right ) : if not compare ( left_elt , right_elt ) : return False return True else : return left == right",An AST comparison function . Returns True if all fields in left are equal to fields in right ; if compare_locs is true all locations should match as well .
"def visit ( self , obj ) : if isinstance ( obj , list ) : return [ self . visit ( elt ) for elt in obj ] elif isinstance ( obj , ast . AST ) : return self . _visit_one ( obj )",Visit a node or a list of nodes . Other values are ignored
"def generic_visit ( self , node ) : for field_name in node . _fields : setattr ( node , field_name , self . visit ( getattr ( node , field_name ) ) ) return node",Called if no explicit visitor function exists for a node .
"def visit ( self , obj ) : if isinstance ( obj , list ) : return list ( filter ( lambda x : x is not None , map ( self . visit , obj ) ) ) elif isinstance ( obj , ast . AST ) : return self . _visit_one ( obj ) else : return obj",Visit a node or a list of nodes . Other values are ignored
"def start_new_thread ( function , args , kwargs = { } ) : if type ( args ) != type ( tuple ( ) ) : raise TypeError ( ""2nd arg must be a tuple"" ) if type ( kwargs ) != type ( dict ( ) ) : raise TypeError ( ""3rd arg must be a dict"" ) global _main _main = False try : function ( * args , * * kwargs ) except SystemExit : pass except : _traceback . print_exc ( ) _main = True global _interrupt if _interrupt : _interrupt = False raise KeyboardInterrupt",Dummy implementation of thread . start_new_thread () .
"def acquire ( self , waitflag = None ) : if waitflag is None or waitflag : self . locked_status = True return True else : if not self . locked_status : self . locked_status = True return True else : return False",Dummy implementation of acquire () .
def round_to_nearest ( x ) : int_part = int ( x ) frac_part = x - int_part if frac_part > 0.5 or frac_part == 0.5 and int_part & 1 == 1 : int_part += 1 return int_part,Python 3 style round : round a float x to the nearest int but unlike the builtin Python 2 . x round function :
"def float_unpack ( Q , size , le ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( ""invalid size value"" ) if Q >> BITS : raise ValueError ( ""input out of range"" ) sign = Q >> BITS - 1 exp = ( Q & ( ( 1 << BITS - 1 ) - ( 1 << MANT_DIG - 1 ) ) ) >> MANT_DIG - 1 mant = Q & ( ( 1 << MANT_DIG - 1 ) - 1 ) if exp == MAX_EXP - MIN_EXP + 2 : result = float ( 'nan' ) if mant else float ( 'inf' ) elif exp == 0 : result = math . ldexp ( float ( mant ) , MIN_EXP - MANT_DIG ) else : mant += 1 << MANT_DIG - 1 result = math . ldexp ( float ( mant ) , exp + MIN_EXP - MANT_DIG - 1 ) return - result if sign else result",Convert a 32 - bit or 64 - bit integer created by float_pack into a Python float .
"def float_pack ( x , size ) : if size == 8 : MIN_EXP = - 1021 MAX_EXP = 1024 MANT_DIG = 53 BITS = 64 elif size == 4 : MIN_EXP = - 125 MAX_EXP = 128 MANT_DIG = 24 BITS = 32 else : raise ValueError ( ""invalid size value"" ) sign = math . copysign ( 1.0 , x ) < 0.0 if math . isinf ( x ) : mant = 0 exp = MAX_EXP - MIN_EXP + 2 elif math . isnan ( x ) : mant = 1 << ( MANT_DIG - 2 ) exp = MAX_EXP - MIN_EXP + 2 elif x == 0.0 : mant = 0 exp = 0 else : m , e = math . frexp ( abs ( x ) ) exp = e - ( MIN_EXP - 1 ) if exp > 0 : mant = round_to_nearest ( m * ( 1 << MANT_DIG ) ) mant -= 1 << MANT_DIG - 1 else : if exp + MANT_DIG - 1 >= 0 : mant = round_to_nearest ( m * ( 1 << exp + MANT_DIG - 1 ) ) else : mant = 0 exp = 0 assert 0 <= mant <= 1 << MANT_DIG - 1 if mant == 1 << MANT_DIG - 1 : mant = 0 exp += 1 if exp >= MAX_EXP - MIN_EXP + 2 : raise OverflowError ( ""float too large to pack in this format"" ) assert 0 <= mant < 1 << MANT_DIG - 1 assert 0 <= exp <= MAX_EXP - MIN_EXP + 2 assert 0 <= sign <= 1 return ( ( sign << BITS - 1 ) | ( exp << MANT_DIG - 1 ) ) | mant",Convert a Python float x into a 64 - bit unsigned integer with the same byte representation .
"def calcsize ( fmt ) : formatdef , endianness , i = getmode ( fmt ) num = 0 result = 0 while i < len ( fmt ) : num , i = getNum ( fmt , i ) cur = fmt [ i ] try : format = formatdef [ cur ] except KeyError : raise StructError ( ""%s is not a valid format"" % cur ) if num != None : result += num * format [ 'size' ] else : result += format [ 'size' ] num = 0 i += 1 return result",calcsize ( fmt ) - > int Return size of C struct described by format string fmt . See struct . __doc__ for more on format strings .
"def pack ( fmt , * args ) : formatdef , endianness , i = getmode ( fmt ) args = list ( args ) n_args = len ( args ) result = [ ] while i < len ( fmt ) : num , i = getNum ( fmt , i ) cur = fmt [ i ] try : format = formatdef [ cur ] except KeyError : raise StructError ( ""%s is not a valid format"" % cur ) if num == None : num_s = 0 num = 1 else : num_s = num if cur == 'x' : result += [ b'\0' * num ] elif cur == 's' : if isinstance ( args [ 0 ] , bytes ) : padding = num - len ( args [ 0 ] ) result += [ args [ 0 ] [ : num ] + b'\0' * padding ] args . pop ( 0 ) else : raise StructError ( ""arg for string format not a string"" ) elif cur == 'p' : if isinstance ( args [ 0 ] , bytes ) : padding = num - len ( args [ 0 ] ) - 1 if padding > 0 : result += [ bytes ( [ len ( args [ 0 ] ) ] ) + args [ 0 ] [ : num - 1 ] + b'\0' * padding ] else : if num < 255 : result += [ bytes ( [ num - 1 ] ) + args [ 0 ] [ : num - 1 ] ] else : result += [ bytes ( [ 255 ] ) + args [ 0 ] [ : num - 1 ] ] args . pop ( 0 ) else : raise StructError ( ""arg for string format not a string"" ) else : if len ( args ) < num : raise StructError ( ""insufficient arguments to pack"" ) for var in args [ : num ] : result += [ format [ 'pack' ] ( var , format [ 'size' ] , endianness ) ] args = args [ num : ] num = None i += 1 if len ( args ) != 0 : raise StructError ( ""too many arguments for pack format"" ) return b'' . join ( result )",pack ( fmt v1 v2 ... ) - > string Return string containing values v1 v2 ... packed according to fmt . See struct . __doc__ for more on format strings .
"def unpack ( fmt , data ) : formatdef , endianness , i = getmode ( fmt ) j = 0 num = 0 result = [ ] length = calcsize ( fmt ) if length != len ( data ) : raise StructError ( ""unpack str size does not match format"" ) while i < len ( fmt ) : num , i = getNum ( fmt , i ) cur = fmt [ i ] i += 1 try : format = formatdef [ cur ] except KeyError : raise StructError ( ""%s is not a valid format"" % cur ) if not num : num = 1 if cur == 'x' : j += num elif cur == 's' : result . append ( data [ j : j + num ] ) j += num elif cur == 'p' : n = data [ j ] if n >= num : n = num - 1 result . append ( data [ j + 1 : j + n + 1 ] ) j += num else : for n in range ( num ) : result += [ format [ 'unpack' ] ( data , j , format [ 'size' ] , endianness ) ] j += format [ 'size' ] return tuple ( result )",unpack ( fmt string ) - > ( v1 v2 ... ) Unpack the string containing packed C structure data according to fmt . Requires len ( string ) == calcsize ( fmt ) . See struct . __doc__ for more on format strings .
"def render ( self , only_line = False , colored = False ) : source_line = self . location . source_line ( ) . rstrip ( ""\n"" ) highlight_line = bytearray ( re . sub ( r""[^\t]"" , "" "" , source_line ) , ""utf-8"" ) for hilight in self . highlights : if hilight . line ( ) == self . location . line ( ) : lft , rgt = hilight . column_range ( ) highlight_line [ lft : rgt ] = bytearray ( ""~"" , ""utf-8"" ) * ( rgt - lft ) lft , rgt = self . location . column_range ( ) if rgt == lft : rgt = lft + 1 highlight_line [ lft : rgt ] = bytearray ( ""^"" , ""utf-8"" ) * ( rgt - lft ) if only_line : location = ""%s:%s"" % ( self . location . source_buffer . name , self . location . line ( ) ) else : location = str ( self . location ) notes = list ( self . notes ) if self . level != ""note"" : expanded_location = self . location . expanded_from while expanded_location is not None : notes . insert ( 0 , Diagnostic ( ""note"" , ""expanded from here"" , { } , self . location . expanded_from ) ) expanded_location = expanded_location . expanded_from rendered_notes = reduce ( list . __add__ , [ note . render ( only_line , colored ) for note in notes ] , [ ] ) if colored : if self . level in ( ""error"" , ""fatal"" ) : level_color = 31 elif self . level == ""warning"" : level_color = 35 else : level_color = 30 return [ ""\x1b[1;37m{}: \x1b[{}m{}:\x1b[37m {}\x1b[0m"" . format ( location , level_color , self . level , self . message ( ) ) , source_line , ""\x1b[1;32m{}\x1b[0m"" . format ( highlight_line . decode ( ""utf-8"" ) ) ] + rendered_notes else : return [ ""{}: {}: {}"" . format ( location , self . level , self . message ( ) ) , source_line , highlight_line . decode ( ""utf-8"" ) ] + rendered_notes",Returns the human - readable location of the diagnostic in the source the formatted message the source line corresponding to location and a line emphasizing the problematic locations in the source line using ASCII art as a list of lines . Appends the result of calling : meth : render on notes if any .
"def process ( self , diagnostic ) : diagnostic . notes += self . _appended_notes self . render_diagnostic ( diagnostic ) if diagnostic . level == ""fatal"" or ( self . all_errors_are_fatal and diagnostic . level == ""error"" ) : raise Error ( diagnostic )",The default implementation of : meth : process renders non - fatal diagnostics to sys . stderr and raises fatal ones as a : class : Error .
"def context ( self , * notes ) : self . _appended_notes += notes yield del self . _appended_notes [ - len ( notes ) : ]",A context manager that appends note to every diagnostic processed by this engine .
"def decode ( input , output , encoding ) : if encoding == 'base64' : import base64 return base64 . decode ( input , output ) if encoding == 'quoted-printable' : import quopri return quopri . decode ( input , output ) if encoding in ( 'uuencode' , 'x-uuencode' , 'uue' , 'x-uue' ) : import uu return uu . decode ( input , output ) if encoding in ( '7bit' , '8bit' ) : return output . write ( input . read ( ) ) if encoding in decodetab : pipethrough ( input , decodetab [ encoding ] , output ) else : raise ValueError , 'unknown Content-Transfer-Encoding: %s' % encoding",Decode common content - transfer - encodings ( base64 quopri uuencode ) .
"def encode ( input , output , encoding ) : if encoding == 'base64' : import base64 return base64 . encode ( input , output ) if encoding == 'quoted-printable' : import quopri return quopri . encode ( input , output , 0 ) if encoding in ( 'uuencode' , 'x-uuencode' , 'uue' , 'x-uue' ) : import uu return uu . encode ( input , output ) if encoding in ( '7bit' , '8bit' ) : return output . write ( input . read ( ) ) if encoding in encodetab : pipethrough ( input , encodetab [ encoding ] , output ) else : raise ValueError , 'unknown Content-Transfer-Encoding: %s' % encoding",Encode common content - transfer - encodings ( base64 quopri uuencode ) .
"def print_list ( extracted_list , file = None ) : if file is None : file = sys . stderr for filename , lineno , name , line in extracted_list : _print ( file , '  File ""%s"", line %d, in %s' % ( filename , lineno , name ) ) if line : _print ( file , '    %s' % line . strip ( ) )",Print the list of tuples as returned by extract_tb () or extract_stack () as a formatted stack trace to the given file .
"def format_list ( extracted_list ) : list = [ ] for filename , lineno , name , line in extracted_list : item = '  File ""%s"", line %d, in %s\n' % ( filename , lineno , name ) if line : item = item + '    %s\n' % line . strip ( ) list . append ( item ) return list",Format a list of traceback entry tuples for printing .
"def print_tb ( tb , limit = None , file = None ) : if file is None : file = sys . stderr if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit n = 0 while tb is not None and ( limit is None or n < limit ) : f = tb . tb_frame lineno = tb . tb_lineno co = f . f_code filename = co . co_filename name = co . co_name _print ( file , '  File ""%s"", line %d, in %s' % ( filename , lineno , name ) ) linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : _print ( file , '    ' + line . strip ( ) ) tb = tb . tb_next n = n + 1",Print up to limit stack trace entries from the traceback tb .
"def print_exception ( etype , value , tb , limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) if tb : _print ( file , 'Traceback (most recent call last):' ) print_tb ( tb , limit , file ) lines = format_exception_only ( etype , value ) for line in lines : _print ( file , line , '' )",Print exception up to limit stack trace entries from tb to file .
"def format_exception ( etype , value , tb , limit = None ) : if tb : list = [ 'Traceback (most recent call last):\n' ] list = list + format_tb ( tb , limit ) else : list = [ ] list = list + format_exception_only ( etype , value ) return list",Format a stack trace and the exception information .
"def format_exception_only ( etype , value ) : if ( isinstance ( etype , BaseException ) or etype is None or type ( etype ) is str ) : return [ _format_final_exc_line ( etype , value ) ] stype = etype . __name__ if not issubclass ( etype , SyntaxError ) : return [ _format_final_exc_line ( stype , value ) ] lines = [ ] try : msg , ( filename , lineno , offset , badline ) = value . args except Exception : pass else : filename = filename or ""<string>"" lines . append ( '  File ""%s"", line %d\n' % ( filename , lineno ) ) if badline is not None : lines . append ( '    %s\n' % badline . strip ( ) ) if offset is not None : caretspace = badline . rstrip ( '\n' ) offset = min ( len ( caretspace ) , offset ) - 1 caretspace = caretspace [ : offset ] . lstrip ( ) caretspace = ( ( c . isspace ( ) and c or ' ' ) for c in caretspace ) lines . append ( '    %s^\n' % '' . join ( caretspace ) ) value = msg lines . append ( _format_final_exc_line ( stype , value ) ) return lines",Format the exception part of a traceback .
"def _format_final_exc_line ( etype , value ) : valuestr = _some_str ( value ) if value is None or not valuestr : line = ""%s\n"" % etype else : line = ""%s: %s\n"" % ( etype , valuestr ) return line",Return a list of a single line -- normal case for format_exception_only
"def print_exc ( limit = None , file = None ) : if file is None : file = open ( '/dev/stderr' , 'w' ) try : etype , value , tb = sys . exc_info ( ) print_exception ( etype , value , tb , limit , file ) finally : etype = value = tb = None",Shorthand for print_exception ( sys . exc_type sys . exc_value sys . exc_traceback limit file ) . ( In fact it uses sys . exc_info () to retrieve the same information in a thread - safe way . )
"def print_last ( limit = None , file = None ) : if not hasattr ( sys , ""last_type"" ) : raise ValueError ( ""no last exception"" ) if file is None : file = sys . stderr print_exception ( sys . last_type , sys . last_value , sys . last_traceback , limit , file )",This is a shorthand for print_exception ( sys . last_type sys . last_value sys . last_traceback limit file ) .
"def print_stack ( f = None , limit = None , file = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back print_list ( extract_stack ( f , limit ) , file )",Print a stack trace from its invocation point .
"def format_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back return format_list ( extract_stack ( f , limit ) )",Shorthand for format_list ( extract_stack ( f limit )) .
"def extract_stack ( f = None , limit = None ) : if f is None : try : raise ZeroDivisionError except ZeroDivisionError : f = sys . exc_info ( ) [ 2 ] . tb_frame . f_back if limit is None : if hasattr ( sys , 'tracebacklimit' ) : limit = sys . tracebacklimit list = [ ] n = 0 while f is not None and ( limit is None or n < limit ) : lineno = f . f_lineno co = f . f_code filename = co . co_filename name = co . co_name linecache . checkcache ( filename ) line = linecache . getline ( filename , lineno , f . f_globals ) if line : line = line . strip ( ) else : line = None list . append ( ( filename , lineno , name , line ) ) f = f . f_back n = n + 1 list . reverse ( ) return list",Extract the raw traceback from the current stack frame .
"def seed ( self , a = None ) : super ( Random , self ) . seed ( a ) self . gauss_next = None",Initialize internal state of the random number generator .
"def randrange ( self , start , stop = None , step = 1 , _int = int , _maxwidth = 1L << BPF ) : istart = _int ( start ) if istart != start : raise ValueError , ""non-integer arg 1 for randrange()"" if stop is None : if istart > 0 : if istart >= _maxwidth : return self . _randbelow ( istart ) return _int ( self . random ( ) * istart ) raise ValueError , ""empty range for randrange()"" istop = _int ( stop ) if istop != stop : raise ValueError , ""non-integer stop for randrange()"" width = istop - istart if step == 1 and width > 0 : if width >= _maxwidth : return _int ( istart + self . _randbelow ( width ) ) return _int ( istart + _int ( self . random ( ) * width ) ) if step == 1 : raise ValueError , ""empty range for randrange() (%d,%d, %d)"" % ( istart , istop , width ) istep = _int ( step ) if istep != step : raise ValueError , ""non-integer step for randrange()"" if istep > 0 : n = ( width + istep - 1 ) // istep elif istep < 0 : n = ( width + istep + 1 ) // istep else : raise ValueError , ""zero step for randrange()"" if n <= 0 : raise ValueError , ""empty range for randrange()"" if n >= _maxwidth : return istart + istep * self . _randbelow ( n ) return istart + istep * _int ( self . random ( ) * n )",Choose a random item from range ( start stop [ step ] ) .
"def shuffle ( self , x , random = None ) : if random is None : random = self . random _int = int for i in reversed ( xrange ( 1 , len ( x ) ) ) : j = _int ( random ( ) * ( i + 1 ) ) x [ i ] , x [ j ] = x [ j ] , x [ i ]",x random = random . random - > shuffle list x in place ; return None .
"def _slotnames ( cls ) : names = cls . __dict__ . get ( ""__slotnames__"" ) if names is not None : return names names = [ ] if not hasattr ( cls , ""__slots__"" ) : pass else : for c in cls . __mro__ : if ""__slots__"" in c . __dict__ : slots = c . __dict__ [ '__slots__' ] if isinstance ( slots , basestring ) : slots = ( slots , ) for name in slots : if name in ( ""__dict__"" , ""__weakref__"" ) : continue elif name . startswith ( '__' ) and not name . endswith ( '__' ) : names . append ( '_%s%s' % ( c . __name__ , name ) ) else : names . append ( name ) try : cls . __slotnames__ = names except : pass return names",Return a list of slot names for a given class .
"def add_extension ( module , name , code ) : code = int ( code ) if not 1 <= code <= 0x7fffffff : raise ValueError , ""code out of range"" key = ( module , name ) if ( _extension_registry . get ( key ) == code and _inverted_registry . get ( code ) == key ) : return if key in _extension_registry : raise ValueError ( ""key %s is already registered with code %s"" % ( key , _extension_registry [ key ] ) ) if code in _inverted_registry : raise ValueError ( ""code %s is already in use for key %s"" % ( code , _inverted_registry [ code ] ) ) _extension_registry [ key ] = code _inverted_registry [ code ] = key",Register an extension code .
"def remove_extension ( module , name , code ) : key = ( module , name ) if ( _extension_registry . get ( key ) != code or _inverted_registry . get ( code ) != key ) : raise ValueError ( ""key %s is not registered with code %s"" % ( key , code ) ) del _extension_registry [ key ] del _inverted_registry [ code ] if code in _extension_cache : del _extension_cache [ code ]",Unregister an extension code . For testing only .
"def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : for attr in assigned : setattr ( wrapper , attr , getattr ( wrapped , attr ) ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , { } ) ) return wrapper",Update a wrapper function to look like the wrapped function
"def wraps ( wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ) : return partial ( update_wrapper , wrapped = wrapped , assigned = assigned , updated = updated )",Decorator factory to apply update_wrapper () to a wrapper function
"def cmp_to_key ( mycmp ) : class K ( object ) : __slots__ = [ 'obj' ] def __init__ ( self , obj , * args ) : self . obj = obj def __lt__ ( self , other ) : return mycmp ( self . obj , other . obj ) < 0 def __gt__ ( self , other ) : return mycmp ( self . obj , other . obj ) > 0 def __eq__ ( self , other ) : return mycmp ( self . obj , other . obj ) == 0 def __le__ ( self , other ) : return mycmp ( self . obj , other . obj ) <= 0 def __ge__ ( self , other ) : return mycmp ( self . obj , other . obj ) >= 0 def __ne__ ( self , other ) : return mycmp ( self . obj , other . obj ) != 0 def __hash__ ( self ) : raise TypeError ( 'hash not implemented' ) return K",Convert a cmp = function into a key = function
"def unquote ( s ) : if len ( s ) > 1 : if s . startswith ( '""' ) and s . endswith ( '""' ) : return s [ 1 : - 1 ] . replace ( '\\\\' , '\\' ) . replace ( '\\""' , '""' ) if s . startswith ( '<' ) and s . endswith ( '>' ) : return s [ 1 : - 1 ] return s",Remove quotes from a string .
"def parseaddr ( address ) : a = AddressList ( address ) lst = a . addresslist if not lst : return ( None , None ) return lst [ 0 ]",Parse an address into a ( realname mailaddr ) tuple .
"def mktime_tz ( data ) : if data [ 9 ] is None : return time . mktime ( data [ : 8 ] + ( - 1 , ) ) else : t = time . mktime ( data [ : 8 ] + ( 0 , ) ) return t - data [ 9 ] - time . timezone",Turn a 10 - tuple as returned by parsedate_tz () into a UTC timestamp .
"def formatdate ( timeval = None ) : if timeval is None : timeval = time . time ( ) timeval = time . gmtime ( timeval ) return ""%s, %02d %s %04d %02d:%02d:%02d GMT"" % ( ( ""Mon"" , ""Tue"" , ""Wed"" , ""Thu"" , ""Fri"" , ""Sat"" , ""Sun"" ) [ timeval [ 6 ] ] , timeval [ 2 ] , ( ""Jan"" , ""Feb"" , ""Mar"" , ""Apr"" , ""May"" , ""Jun"" , ""Jul"" , ""Aug"" , ""Sep"" , ""Oct"" , ""Nov"" , ""Dec"" ) [ timeval [ 1 ] - 1 ] , timeval [ 0 ] , timeval [ 3 ] , timeval [ 4 ] , timeval [ 5 ] )",Returns time format preferred for Internet standards .
"def rewindbody ( self ) : if not self . seekable : raise IOError , ""unseekable file"" self . fp . seek ( self . startofbody )",Rewind the file to the start of the body ( if seekable ) .
"def readheaders ( self ) : self . dict = { } self . unixfrom = '' self . headers = lst = [ ] self . status = '' headerseen = """" firstline = 1 startofline = unread = tell = None if hasattr ( self . fp , 'unread' ) : unread = self . fp . unread elif self . seekable : tell = self . fp . tell while 1 : if tell : try : startofline = tell ( ) except IOError : startofline = tell = None self . seekable = 0 line = self . fp . readline ( ) if not line : self . status = 'EOF in headers' break if firstline and line . startswith ( 'From ' ) : self . unixfrom = self . unixfrom + line continue firstline = 0 if headerseen and line [ 0 ] in ' \t' : lst . append ( line ) x = ( self . dict [ headerseen ] + ""\n "" + line . strip ( ) ) self . dict [ headerseen ] = x . strip ( ) continue elif self . iscomment ( line ) : continue elif self . islast ( line ) : break headerseen = self . isheader ( line ) if headerseen : lst . append ( line ) self . dict [ headerseen ] = line [ len ( headerseen ) + 1 : ] . strip ( ) continue elif headerseen is not None : continue else : if not self . dict : self . status = 'No headers' else : self . status = 'Non-header line where header expected' if unread : unread ( line ) elif tell : self . fp . seek ( startofline ) else : self . status = self . status + '; bad seek' break",Read header lines .
"def isheader ( self , line ) : i = line . find ( ':' ) if i > - 1 : return line [ : i ] . lower ( ) return None",Determine whether a given line is a legal header .
"def getfirstmatchingheader ( self , name ) : name = name . lower ( ) + ':' n = len ( name ) lst = [ ] hit = 0 for line in self . headers : if hit : if not line [ : 1 ] . isspace ( ) : break elif line [ : n ] . lower ( ) == name : hit = 1 if hit : lst . append ( line ) return lst",Get the first header line matching name .
"def getrawheader ( self , name ) : lst = self . getfirstmatchingheader ( name ) if not lst : return None lst [ 0 ] = lst [ 0 ] [ len ( name ) + 1 : ] return '' . join ( lst )",A higher - level interface to getfirstmatchingheader () .
"def getheader ( self , name , default = None ) : return self . dict . get ( name . lower ( ) , default )",Get the header value for a name .
"def getheaders ( self , name ) : result = [ ] current = '' have_header = 0 for s in self . getallmatchingheaders ( name ) : if s [ 0 ] . isspace ( ) : if current : current = ""%s\n %s"" % ( current , s . strip ( ) ) else : current = s . strip ( ) else : if have_header : result . append ( current ) current = s [ s . find ( "":"" ) + 1 : ] . strip ( ) have_header = 1 if have_header : result . append ( current ) return result",Get all values for a header .
"def getaddrlist ( self , name ) : raw = [ ] for h in self . getallmatchingheaders ( name ) : if h [ 0 ] in ' \t' : raw . append ( h ) else : if raw : raw . append ( ', ' ) i = h . find ( ':' ) if i > 0 : addr = h [ i + 1 : ] raw . append ( addr ) alladdrs = '' . join ( raw ) a = AddressList ( alladdrs ) return a . addresslist",Get a list of addresses from a header .
def gotonext ( self ) : while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS + '\n\r' : self . pos = self . pos + 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) else : break,Parse up to the start of the next address .
def getaddrlist ( self ) : result = [ ] ad = self . getaddress ( ) while ad : result += ad ad = self . getaddress ( ) return result,Parse all addresses .
"def getrouteaddr ( self ) : if self . field [ self . pos ] != '<' : return expectroute = 0 self . pos += 1 self . gotonext ( ) adlist = """" while self . pos < len ( self . field ) : if expectroute : self . getdomain ( ) expectroute = 0 elif self . field [ self . pos ] == '>' : self . pos += 1 break elif self . field [ self . pos ] == '@' : self . pos += 1 expectroute = 1 elif self . field [ self . pos ] == ':' : self . pos += 1 else : adlist = self . getaddrspec ( ) self . pos += 1 break self . gotonext ( ) return adlist",Parse a route address ( Return - path value ) .
"def getaddrspec ( self ) : aslist = [ ] self . gotonext ( ) while self . pos < len ( self . field ) : if self . field [ self . pos ] == '.' : aslist . append ( '.' ) self . pos += 1 elif self . field [ self . pos ] == '""' : aslist . append ( '""%s""' % self . getquote ( ) ) elif self . field [ self . pos ] in self . atomends : break else : aslist . append ( self . getatom ( ) ) self . gotonext ( ) if self . pos >= len ( self . field ) or self . field [ self . pos ] != '@' : return '' . join ( aslist ) aslist . append ( '@' ) self . pos += 1 self . gotonext ( ) return '' . join ( aslist ) + self . getdomain ( )",Parse an RFC 2822 addr - spec .
def getdomain ( self ) : sdlist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] == '[' : sdlist . append ( self . getdomainliteral ( ) ) elif self . field [ self . pos ] == '.' : self . pos += 1 sdlist . append ( '.' ) elif self . field [ self . pos ] in self . atomends : break else : sdlist . append ( self . getatom ( ) ) return '' . join ( sdlist ),Get the complete domain name from an address .
"def getdelimited ( self , beginchar , endchars , allowcomments = 1 ) : if self . field [ self . pos ] != beginchar : return '' slist = [ '' ] quote = 0 self . pos += 1 while self . pos < len ( self . field ) : if quote == 1 : slist . append ( self . field [ self . pos ] ) quote = 0 elif self . field [ self . pos ] in endchars : self . pos += 1 break elif allowcomments and self . field [ self . pos ] == '(' : slist . append ( self . getcomment ( ) ) continue elif self . field [ self . pos ] == '\\' : quote = 1 else : slist . append ( self . field [ self . pos ] ) self . pos += 1 return '' . join ( slist )",Parse a header fragment delimited by special characters .
"def getphraselist ( self ) : plist = [ ] while self . pos < len ( self . field ) : if self . field [ self . pos ] in self . LWS : self . pos += 1 elif self . field [ self . pos ] == '""' : plist . append ( self . getquote ( ) ) elif self . field [ self . pos ] == '(' : self . commentlist . append ( self . getcomment ( ) ) elif self . field [ self . pos ] in self . phraseends : break else : plist . append ( self . getatom ( self . phraseends ) ) return plist",Parse a sequence of RFC 2822 phrases .
"def _days_in_month ( year , month ) : assert 1 <= month <= 12 , month if month == 2 and _is_leap ( year ) : return 29 return _DAYS_IN_MONTH [ month ]",year month - > number of days in that month in that year .
"def _ymd2ord ( year , month , day ) : assert 1 <= month <= 12 , 'month must be in 1..12' dim = _days_in_month ( year , month ) assert 1 <= day <= dim , ( 'day must be in 1..%d' % dim ) return ( _days_before_year ( year ) + _days_before_month ( year , month ) + day )",year month day - > ordinal considering 01 - Jan - 0001 as day 1 .
"def _ord2ymd ( n ) : n -= 1 n400 , n = divmod ( n , _DI400Y ) year = n400 * 400 + 1 n100 , n = divmod ( n , _DI100Y ) n4 , n = divmod ( n , _DI4Y ) n1 , n = divmod ( n , 365 ) year += n100 * 100 + n4 * 4 + n1 if n1 == 4 or n100 == 4 : assert n == 0 return year - 1 , 12 , 31 leapyear = n1 == 3 and ( n4 != 24 or n100 == 3 ) assert leapyear == _is_leap ( year ) month = ( n + 50 ) >> 5 preceding = _DAYS_BEFORE_MONTH [ month ] + ( month > 2 and leapyear ) if preceding > n : month -= 1 preceding -= _DAYS_IN_MONTH [ month ] + ( month == 2 and leapyear ) n -= preceding assert 0 <= n < _days_in_month ( year , month ) return year , month , n + 1",ordinal - > ( year month day ) considering 01 - Jan - 0001 as day 1 .
"def fromtimestamp ( cls , t ) : y , m , d , hh , mm , ss , weekday , jday , dst = _time . localtime ( t ) return cls ( y , m , d )",Construct a date from a POSIX timestamp ( like time . time () ) .
"def fromordinal ( cls , n ) : y , m , d = _ord2ymd ( n ) return cls ( y , m , d )",Contruct a date from a proleptic Gregorian ordinal .
"def ctime ( self ) : weekday = self . toordinal ( ) % 7 or 7 return ""%s %s %2d 00:00:00 %04d"" % ( _DAYNAMES [ weekday ] , _MONTHNAMES [ self . _month ] , self . _day , self . _year )",Return ctime () style string .
"def isoformat ( self ) : return ""%s-%s-%s"" % ( str ( self . _year ) . zfill ( 4 ) , str ( self . _month ) . zfill ( 2 ) , str ( self . _day ) . zfill ( 2 ) )",Return the date formatted according to ISO .
"def replace ( self , year = None , month = None , day = None ) : if year is None : year = self . _year if month is None : month = self . _month if day is None : day = self . _day return date . __new__ ( type ( self ) , year , month , day )",Return a new date with new values for the specified fields .
"def isocalendar ( self ) : year = self . _year week1monday = _isoweek1monday ( year ) today = _ymd2ord ( self . _year , self . _month , self . _day ) week , day = divmod ( today - week1monday , 7 ) if week < 0 : year -= 1 week1monday = _isoweek1monday ( year ) week , day = divmod ( today - week1monday , 7 ) elif week >= 52 : if today >= _isoweek1monday ( year + 1 ) : year += 1 week = 0 return year , week + 1 , day + 1",Return a 3 - tuple containing ISO year week number and weekday .
"def _tzstr ( self , sep = "":"" ) : off = self . _utcoffset ( ) if off is not None : if off < 0 : sign = ""-"" off = - off else : sign = ""+"" hh , mm = divmod ( off , 60 ) assert 0 <= hh < 24 off = ""%s%02d%s%02d"" % ( sign , hh , sep , mm ) return off",Return formatted timezone offset ( + xx : xx ) or None .
"def isoformat ( self ) : s = _format_time ( self . _hour , self . _minute , self . _second , self . _microsecond ) tz = self . _tzstr ( ) if tz : s += tz return s",Return the time formatted according to ISO .
def tzname ( self ) : if self . _tzinfo is None : return None name = self . _tzinfo . tzname ( None ) _check_tzname ( name ) return name,Return the timezone name .
"def dst ( self ) : if self . _tzinfo is None : return None offset = self . _tzinfo . dst ( None ) offset = _check_utc_offset ( ""dst"" , offset ) if offset is not None : offset = timedelta . _create ( 0 , offset * 60 , 0 , True ) return offset",Return 0 if DST is not in effect or the DST offset ( in minutes eastward ) if DST is in effect .
"def replace ( self , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return time . __new__ ( type ( self ) , hour , minute , second , microsecond , tzinfo )",Return a new time with new values for the specified fields .
"def fromtimestamp ( cls , timestamp , tz = None ) : _check_tzinfo_arg ( tz ) converter = _time . localtime if tz is None else _time . gmtime self = cls . _from_timestamp ( converter , timestamp , tz ) if tz is not None : self = tz . fromutc ( self ) return self",Construct a datetime from a POSIX timestamp ( like time . time () ) .
"def now ( cls , tz = None ) : t = _time . time ( ) return cls . fromtimestamp ( t , tz )",Construct a datetime from time . time () and optional time zone info .
"def combine ( cls , date , time ) : if not isinstance ( date , _date_class ) : raise TypeError ( ""date argument must be a date instance"" ) if not isinstance ( time , _time_class ) : raise TypeError ( ""time argument must be a time instance"" ) return cls ( date . year , date . month , date . day , time . hour , time . minute , time . second , time . microsecond , time . tzinfo )",Construct a datetime from a given date and a given time .
"def utctimetuple ( self ) : y , m , d = self . year , self . month , self . day hh , mm , ss = self . hour , self . minute , self . second offset = self . _utcoffset ( ) if offset : mm -= offset y , m , d , hh , mm , ss , _ = _normalize_datetime ( y , m , d , hh , mm , ss , 0 , ignore_overflow = True ) return _build_struct_time ( y , m , d , hh , mm , ss , 0 )",Return UTC time tuple compatible with time . gmtime () .
"def time ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond )",Return the time part with tzinfo None .
"def timetz ( self ) : return time ( self . hour , self . minute , self . second , self . microsecond , self . _tzinfo )",Return the time part with same tzinfo .
"def replace ( self , year = None , month = None , day = None , hour = None , minute = None , second = None , microsecond = None , tzinfo = True ) : if year is None : year = self . year if month is None : month = self . month if day is None : day = self . day if hour is None : hour = self . hour if minute is None : minute = self . minute if second is None : second = self . second if microsecond is None : microsecond = self . microsecond if tzinfo is True : tzinfo = self . tzinfo return datetime . __new__ ( type ( self ) , year , month , day , hour , minute , second , microsecond , tzinfo )",Return a new datetime with new values for the specified fields .
"def ctime ( self ) : weekday = self . toordinal ( ) % 7 or 7 return ""%s %s %2d %02d:%02d:%02d %04d"" % ( _DAYNAMES [ weekday ] , _MONTHNAMES [ self . _month ] , self . _day , self . _hour , self . _minute , self . _second , self . _year )",Return ctime () style string .
"def isoformat ( self , sep = 'T' ) : s = ( ""%04d-%02d-%02d%c"" % ( self . _year , self . _month , self . _day , sep ) + _format_time ( self . _hour , self . _minute , self . _second , self . _microsecond ) ) off = self . _utcoffset ( ) if off is not None : if off < 0 : sign = ""-"" off = - off else : sign = ""+"" hh , mm = divmod ( off , 60 ) s += ""%s%02d:%02d"" % ( sign , hh , mm ) return s",Return the time formatted according to ISO .
"def utcoffset ( self ) : if self . _tzinfo is None : return None offset = self . _tzinfo . utcoffset ( self ) offset = _check_utc_offset ( ""utcoffset"" , offset ) if offset is not None : offset = timedelta . _create ( 0 , offset * 60 , 0 , True ) return offset",Return the timezone offset in minutes east of UTC ( negative west of UTC ) .
"def truediv ( a , b ) : if type ( a ) == int or type ( a ) == long : a = float ( a ) return a / b",Same as a / b .
"def concat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = ""'%s' object can't be concatenated"" % type ( a ) . __name__ raise TypeError ( msg ) return a + b",Same as a + b for a and b sequences .
"def countOf ( a , b ) : count = 0 for i in a : if i == b : count += 1 return count",Return the number of times b occurs in a .
"def indexOf ( a , b ) : for i , j in enumerate ( a ) : if j == b : return i else : raise ValueError ( 'sequence.index(x): x not in sequence' )",Return the first index of b in a .
"def length_hint ( obj , default = 0 ) : if not isinstance ( default , int ) : msg = ( ""'%s' object cannot be interpreted as an integer"" % type ( default ) . __name__ ) raise TypeError ( msg ) try : return len ( obj ) except TypeError : pass try : hint = type ( obj ) . __length_hint__ except AttributeError : return default try : val = hint ( obj ) except TypeError : return default if val is NotImplemented : return default if not isinstance ( val , int ) : msg = ( '__length_hint__ must be integer, not %s' % type ( val ) . __name__ ) raise TypeError ( msg ) if val < 0 : msg = '__length_hint__() should return >= 0' raise ValueError ( msg ) return val",Return an estimate of the number of items in obj . This is useful for presizing containers when building from an iterable . If the object supports len () the result will be exact . Otherwise it may over - or under - estimate by an arbitrary amount . The result will be an integer > = 0 .
"def iconcat ( a , b ) : if not hasattr ( a , '__getitem__' ) : msg = ""'%s' object can't be concatenated"" % type ( a ) . __name__ raise TypeError ( msg ) a += b return a",Same as a + = b for a and b sequences .
"def itruediv ( a , b ) : if type ( a ) == int or type ( a ) == long : a = float ( a ) a /= b return a",Same as a / = b .
"def sniff ( self , sample , delimiters = None ) : quotechar , doublequote , delimiter , skipinitialspace = self . _guess_quote_and_delimiter ( sample , delimiters ) if not delimiter : delimiter , skipinitialspace = self . _guess_delimiter ( sample , delimiters ) if not delimiter : raise Error , ""Could not determine delimiter"" class dialect ( Dialect ) : _name = ""sniffed"" lineterminator = '\r\n' quoting = QUOTE_MINIMAL dialect . doublequote = doublequote dialect . delimiter = delimiter dialect . quotechar = quotechar or '""' dialect . skipinitialspace = skipinitialspace return dialect",Returns a dialect ( or None ) corresponding to the sample
"def _guess_quote_and_delimiter ( self , data , delimiters ) : matches = [ ] for restr in ( '(?P<delim>[^\w\n""\'])(?P<space> ?)(?P<quote>[""\']).*?(?P=quote)(?P=delim)' , '(?:^|\n)(?P<quote>[""\']).*?(?P=quote)(?P<delim>[^\w\n""\'])(?P<space> ?)' , '(?P<delim>>[^\w\n""\'])(?P<space> ?)(?P<quote>[""\']).*?(?P=quote)(?:$|\n)' , '(?:^|\n)(?P<quote>[""\']).*?(?P=quote)(?:$|\n)' ) : regexp = re . compile ( restr , re . DOTALL | re . MULTILINE ) matches = regexp . findall ( data ) if matches : break if not matches : return ( '' , False , None , 0 ) quotes = { } delims = { } spaces = 0 for m in matches : n = regexp . groupindex [ 'quote' ] - 1 key = m [ n ] if key : quotes [ key ] = quotes . get ( key , 0 ) + 1 try : n = regexp . groupindex [ 'delim' ] - 1 key = m [ n ] except KeyError : continue if key and ( delimiters is None or key in delimiters ) : delims [ key ] = delims . get ( key , 0 ) + 1 try : n = regexp . groupindex [ 'space' ] - 1 except KeyError : continue if m [ n ] : spaces += 1 quotechar = reduce ( lambda a , b , quotes = quotes : ( quotes [ a ] > quotes [ b ] ) and a or b , quotes . keys ( ) ) if delims : delim = reduce ( lambda a , b , delims = delims : ( delims [ a ] > delims [ b ] ) and a or b , delims . keys ( ) ) skipinitialspace = delims [ delim ] == spaces if delim == '\n' : delim = '' else : delim = '' skipinitialspace = 0 dq_regexp = re . compile ( r""((%(delim)s)|^)\W*%(quote)s[^%(delim)s\n]*%(quote)s[^%(delim)s\n]*%(quote)s\W*((%(delim)s)|$)"" % { 'delim' : re . escape ( delim ) , 'quote' : quotechar } , re . MULTILINE ) if dq_regexp . search ( data ) : doublequote = True else : doublequote = False return ( quotechar , doublequote , delim , skipinitialspace )",Looks for text enclosed between two identical quotes ( the probable quotechar ) which are preceded and followed by the same character ( the probable delimiter ) . For example : some text The quote with the most wins same with the delimiter . If there is no quotechar the delimiter can t be determined this way .
"def _guess_delimiter ( self , data , delimiters ) : data = filter ( None , data . split ( '\n' ) ) ascii = [ chr ( c ) for c in range ( 127 ) ] chunkLength = min ( 10 , len ( data ) ) iteration = 0 charFrequency = { } modes = { } delims = { } start , end = 0 , min ( chunkLength , len ( data ) ) while start < len ( data ) : iteration += 1 for line in data [ start : end ] : for char in ascii : metaFrequency = charFrequency . get ( char , { } ) freq = line . count ( char ) metaFrequency [ freq ] = metaFrequency . get ( freq , 0 ) + 1 charFrequency [ char ] = metaFrequency for char in charFrequency . keys ( ) : items = charFrequency [ char ] . items ( ) if len ( items ) == 1 and items [ 0 ] [ 0 ] == 0 : continue if len ( items ) > 1 : modes [ char ] = reduce ( lambda a , b : a [ 1 ] > b [ 1 ] and a or b , items ) items . remove ( modes [ char ] ) modes [ char ] = ( modes [ char ] [ 0 ] , modes [ char ] [ 1 ] - reduce ( lambda a , b : ( 0 , a [ 1 ] + b [ 1 ] ) , items ) [ 1 ] ) else : modes [ char ] = items [ 0 ] modeList = modes . items ( ) total = float ( chunkLength * iteration ) consistency = 1.0 threshold = 0.9 while len ( delims ) == 0 and consistency >= threshold : for k , v in modeList : if v [ 0 ] > 0 and v [ 1 ] > 0 : if ( ( v [ 1 ] / total ) >= consistency and ( delimiters is None or k in delimiters ) ) : delims [ k ] = v consistency -= 0.01 if len ( delims ) == 1 : delim = delims . keys ( ) [ 0 ] skipinitialspace = ( data [ 0 ] . count ( delim ) == data [ 0 ] . count ( ""%c "" % delim ) ) return ( delim , skipinitialspace ) start = end end += chunkLength if not delims : return ( '' , 0 ) if len ( delims ) > 1 : for d in self . preferred : if d in delims . keys ( ) : skipinitialspace = ( data [ 0 ] . count ( d ) == data [ 0 ] . count ( ""%c "" % d ) ) return ( d , skipinitialspace ) items = [ ( v , k ) for ( k , v ) in delims . items ( ) ] items . sort ( ) delim = items [ - 1 ] [ 1 ] skipinitialspace = ( data [ 0 ] . count ( delim ) == data [ 0 ] . count ( ""%c "" % delim ) ) return ( delim , skipinitialspace )",The delimiter / should / occur the same number of times on each row . However due to malformed data it may not . We don t want an all or nothing approach so we allow for small variations in this number . 1 ) build a table of the frequency of each character on every line . 2 ) build a table of frequencies of this frequency ( meta - frequency? ) e . g . x occurred 5 times in 10 rows 6 times in 1000 rows 7 times in 2 rows 3 ) use the mode of the meta - frequency to determine the / expected / frequency for that character 4 ) find out how often the character actually meets that goal 5 ) the character that best meets its goal is the delimiter For performance reasons the data is evaluated in chunks so it can try and evaluate the smallest portion of the data possible evaluating additional chunks as necessary .
"def encode_basestring ( s ) : def replace ( match ) : return ESCAPE_DCT [ match . group ( 0 ) ] return '""' + ESCAPE . sub ( replace , s ) + '""'",Return a JSON representation of a Python string
"def sub ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . sub ( repl , string , count )",Return the string obtained by replacing the leftmost non - overlapping occurrences of the pattern in string by the replacement repl . repl can be either a string or a callable ; if a string backslash escapes in it are processed . If it is a callable it s passed the match object and must return a replacement string to be used .
"def subn ( pattern , repl , string , count = 0 , flags = 0 ) : return _compile ( pattern , flags ) . subn ( repl , string , count )",Return a 2 - tuple containing ( new_string number ) . new_string is the string obtained by replacing the leftmost non - overlapping occurrences of the pattern in the source string by the replacement repl . number is the number of substitutions that were made . repl can be either a string or a callable ; if a string backslash escapes in it are processed . If it is a callable it s passed the match object and must return a replacement string to be used .
"def split ( pattern , string , maxsplit = 0 , flags = 0 ) : return _compile ( pattern , flags ) . split ( string , maxsplit )",Split the source string by the occurrences of the pattern returning a list containing the resulting substrings .
"def findall ( pattern , string , flags = 0 ) : return _compile ( pattern , flags ) . findall ( string ) def finditer ( pattern , string , flags = 0 ) : """"""Return an iterator over all non-overlapping matches in the
        string.  For each match, the iterator returns a match object.

        Empty matches are included in the result."""""" return _compile ( pattern , flags ) . finditer ( string )",Return a list of all non - overlapping matches in the string .
"def escape ( pattern ) : s = list ( pattern ) alphanum = _alphanum for i , c in enumerate ( pattern ) : if c not in alphanum : if c == ""\000"" : s [ i ] = ""\\000"" else : s [ i ] = ""\\"" + c return pattern [ : 0 ] . join ( s )",Escape all non - alphanumeric characters in pattern .
"def alloc_temp ( self , type_ = '*πg.Object') : for v in sorted ( self . free_temps , key = lambda k : k . name ) : if v . type_ == type_ : self . free_temps . remove ( v ) self . used_temps . add ( v ) return v self . temp_index += 1 name = 'πTemp{:03d}'. f ormat( s elf. t emp_index) v = expr . GeneratedTempVar ( self , name , type_ ) self . used_temps . add ( v ) return v",Create a new temporary Go variable having type type_ for this block .
"def free_temp ( self , v ) : self . used_temps . remove ( v ) self . free_temps . add ( v )",Release the GeneratedTempVar v so it can be reused .
"def parse_buffer ( buffer , mode = ""exec"" , flags = [ ] , version = None , engine = None ) : if version is None : version = sys . version_info [ 0 : 2 ] if engine is None : engine = pythonparser_diagnostic . Engine ( ) lexer = pythonparser_lexer . Lexer ( buffer , version , engine ) if mode in ( ""single"" , ""eval"" ) : lexer . interactive = True parser = pythonparser_parser . Parser ( lexer , version , engine ) parser . add_flags ( flags ) if mode == ""exec"" : return parser . file_input ( ) , lexer . comments elif mode == ""single"" : return parser . single_input ( ) , lexer . comments elif mode == ""eval"" : return parser . eval_input ( ) , lexer . comments",Like : meth : parse but accepts a : class : source . Buffer instead of source and filename and returns comments as well .
"def parse ( source , filename = ""<unknown>"" , mode = ""exec"" , flags = [ ] , version = None , engine = None ) : ast , comments = parse_buffer ( pythonparser_source . Buffer ( source , filename ) , mode , flags , version , engine ) return ast",Parse a string into an abstract syntax tree . This is the replacement for the built - in : meth : .. ast . parse .
"def encode ( in_file , out_file , name = None , mode = None ) : opened_files = [ ] try : if in_file == '-' : in_file = sys . stdin elif isinstance ( in_file , basestring ) : if name is None : name = os . path . basename ( in_file ) if mode is None : try : mode = os . stat ( in_file ) . st_mode except AttributeError : pass in_file = open ( in_file , 'rb' ) opened_files . append ( in_file ) if out_file == '-' : out_file = sys . stdout elif isinstance ( out_file , basestring ) : out_file = open ( out_file , 'wb' ) opened_files . append ( out_file ) if name is None : name = '-' if mode is None : mode = 0666 out_file . write ( 'begin %o %s\n' % ( ( mode & 0777 ) , name ) ) data = in_file . read ( 45 ) while len ( data ) > 0 : out_file . write ( binascii . b2a_uu ( data ) ) data = in_file . read ( 45 ) out_file . write ( ' \nend\n' ) finally : for f in opened_files : f . close ( )",Uuencode file
"def decode ( in_file , out_file = None , mode = None , quiet = 0 ) : opened_files = [ ] if in_file == '-' : in_file = sys . stdin elif isinstance ( in_file , basestring ) : in_file = open ( in_file ) opened_files . append ( in_file ) try : while True : hdr = in_file . readline ( ) if not hdr : raise Error ( 'No valid begin line found in input file' ) if not hdr . startswith ( 'begin' ) : continue hdrfields = hdr . split ( ' ' , 2 ) if len ( hdrfields ) == 3 and hdrfields [ 0 ] == 'begin' : try : int ( hdrfields [ 1 ] , 8 ) break except ValueError : pass if out_file is None : out_file = hdrfields [ 2 ] . rstrip ( ) if os . path . exists ( out_file ) : raise Error ( 'Cannot overwrite existing file: %s' % out_file ) if mode is None : mode = int ( hdrfields [ 1 ] , 8 ) if out_file == '-' : out_file = sys . stdout elif isinstance ( out_file , basestring ) : fp = open ( out_file , 'wb' ) try : os . path . chmod ( out_file , mode ) except AttributeError : pass out_file = fp opened_files . append ( out_file ) s = in_file . readline ( ) while s and s . strip ( ) != 'end' : try : data = binascii . a2b_uu ( s ) except binascii . Error , v : nbytes = ( ( ( ord ( s [ 0 ] ) - 32 ) & 63 ) * 4 + 5 ) // 3 data = binascii . a2b_uu ( s [ : nbytes ] ) if not quiet : sys . stderr . write ( ""Warning: %s\n"" % v ) out_file . write ( data ) s = in_file . readline ( ) if not s : raise Error ( 'Truncated input file' ) finally : for f in opened_files : f . close ( )",Decode uuencoded file
"def capwords ( s , sep = None ) : return ( sep or ' ' ) . join ( x . capitalize ( ) for x in s . split ( sep ) )",capwords ( s [ sep ] ) - > string
"def maketrans ( fromstr , tostr ) : if len ( fromstr ) != len ( tostr ) : raise ValueError , ""maketrans arguments must have same length"" global _idmapL if not _idmapL : _idmapL = list ( _idmap ) L = _idmapL [ : ] fromstr = map ( ord , fromstr ) for i in range ( len ( fromstr ) ) : L [ fromstr [ i ] ] = tostr [ i ] return '' . join ( L )",maketrans ( frm to ) - > string
"def zfill ( x , width ) : if not isinstance ( x , basestring ) : x = repr ( x ) return x . zfill ( width )",zfill ( x width ) - > string
"def translate ( s , table , deletions = """" ) : if deletions or table is None : return s . translate ( table , deletions ) else : return s . translate ( table + s [ : 0 ] )",translate ( s table [ deletions ] ) - > string
"def replace ( s , old , new , maxreplace = - 1 ) : return s . replace ( old , new , maxreplace )",replace ( str old new [ maxreplace ] ) - > string
"def get_close_matches ( word , possibilities , n = 3 , cutoff = 0.6 ) : if not n > 0 : raise ValueError ( ""n must be > 0: %r"" % ( n , ) ) if not 0.0 <= cutoff <= 1.0 : raise ValueError ( ""cutoff must be in [0.0, 1.0]: %r"" % ( cutoff , ) ) result = [ ] s = SequenceMatcher ( ) s . set_seq2 ( word ) for x in possibilities : s . set_seq1 ( x ) if s . real_quick_ratio ( ) >= cutoff and s . quick_ratio ( ) >= cutoff and s . ratio ( ) >= cutoff : result . append ( ( s . ratio ( ) , x ) ) result = heapq . nlargest ( n , result ) return [ x for score , x in result ]",Use SequenceMatcher to return list of the best good enough matches .
"def _count_leading ( line , ch ) : i , n = 0 , len ( line ) while i < n and line [ i ] == ch : i += 1 return i",Return number of ch characters at the start of line .
"def _format_range_unified ( start , stop ) : beginning = start + 1 length = stop - start if length == 1 : return '%s' % ( beginning ) if not length : beginning -= 1 return '%s,%s' % ( beginning , length )",Convert range to the ed format
"def unified_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '--- %s%s%s' % ( fromfile , fromdate , lineterm ) yield '+++ %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] file1_range = _format_range_unified ( first [ 1 ] , last [ 2 ] ) file2_range = _format_range_unified ( first [ 3 ] , last [ 4 ] ) yield '@@ -%s +%s @@%s' % ( file1_range , file2_range , lineterm ) for tag , i1 , i2 , j1 , j2 in group : if tag == 'equal' : for line in a [ i1 : i2 ] : yield ' ' + line continue if tag in ( 'replace' , 'delete' ) : for line in a [ i1 : i2 ] : yield '-' + line if tag in ( 'replace' , 'insert' ) : for line in b [ j1 : j2 ] : yield '+' + line",r Compare two sequences of lines ; generate the delta as a unified diff .
"def _format_range_context ( start , stop ) : beginning = start + 1 length = stop - start if not length : beginning -= 1 if length <= 1 : return '%s' % ( beginning ) return '%s,%s' % ( beginning , beginning + length - 1 )",Convert range to the ed format
"def context_diff ( a , b , fromfile = '' , tofile = '' , fromfiledate = '' , tofiledate = '' , n = 3 , lineterm = '\n' ) : prefix = dict ( insert = '+ ' , delete = '- ' , replace = '! ' , equal = '  ' ) started = False for group in SequenceMatcher ( None , a , b ) . get_grouped_opcodes ( n ) : if not started : started = True fromdate = '\t%s' % ( fromfiledate ) if fromfiledate else '' todate = '\t%s' % ( tofiledate ) if tofiledate else '' yield '*** %s%s%s' % ( fromfile , fromdate , lineterm ) yield '--- %s%s%s' % ( tofile , todate , lineterm ) first , last = group [ 0 ] , group [ - 1 ] yield '***************' + lineterm file1_range = _format_range_context ( first [ 1 ] , last [ 2 ] ) yield '*** %s ****%s' % ( file1_range , lineterm ) if any ( tag in ( 'replace' , 'delete' ) for tag , _ , _ , _ , _ in group ) : for tag , i1 , i2 , _ , _ in group : if tag != 'insert' : for line in a [ i1 : i2 ] : yield prefix [ tag ] + line file2_range = _format_range_context ( first [ 3 ] , last [ 4 ] ) yield '--- %s ----%s' % ( file2_range , lineterm ) if any ( tag in ( 'replace' , 'insert' ) for tag , _ , _ , _ , _ in group ) : for tag , _ , _ , j1 , j2 in group : if tag != 'delete' : for line in b [ j1 : j2 ] : yield prefix [ tag ] + line",r Compare two sequences of lines ; generate the delta as a context diff .
"def ndiff ( a , b , linejunk = None , charjunk = IS_CHARACTER_JUNK ) : return Differ ( linejunk , charjunk ) . compare ( a , b )",r Compare a and b ( lists of strings ) ; return a Differ - style delta .
"def _mdiff ( fromlines , tolines , context = None , linejunk = None , charjunk = IS_CHARACTER_JUNK ) : import re change_re = re . compile ( '(\++|\-+|\^+)' ) diff_lines_iterator = ndiff ( fromlines , tolines , linejunk , charjunk ) def _make_line ( lines , format_key , side , num_lines = [ 0 , 0 ] ) : """"""Returns line of text with user's change markup and line formatting.

        lines -- list of lines from the ndiff generator to produce a line of
                 text from.  When producing the line of text to return, the
                 lines used are removed from this list.
        format_key -- '+' return first line in list with ""add"" markup around
                          the entire line.
                      '-' return first line in list with ""delete"" markup around
                          the entire line.
                      '?' return first line in list with add/delete/change
                          intraline markup (indices obtained from second line)
                      None return first line in list with no markup
        side -- indice into the num_lines list (0=from,1=to)
        num_lines -- from/to current line number.  This is NOT intended to be a
                     passed parameter.  It is present as a keyword argument to
                     maintain memory of the current line numbers between calls
                     of this function.

        Note, this function is purposefully not defined at the module scope so
        that data it needs from its parent function (within whose context it
        is defined) does not need to be of module scope.
        """""" num_lines [ side ] += 1 if format_key is None : return ( num_lines [ side ] , lines . pop ( 0 ) [ 2 : ] ) if format_key == '?' : text , markers = lines . pop ( 0 ) , lines . pop ( 0 ) sub_info = [ ] def record_sub_info ( match_object , sub_info = sub_info ) : sub_info . append ( [ match_object . group ( 1 ) [ 0 ] , match_object . span ( ) ] ) return match_object . group ( 1 ) change_re . sub ( record_sub_info , markers ) for key , ( begin , end ) in sub_info [ : : - 1 ] : text = text [ 0 : begin ] + '\0' + key + text [ begin : end ] + '\1' + text [ end : ] text = text [ 2 : ] else : text = lines . pop ( 0 ) [ 2 : ] if not text : text = ' ' text = '\0' + format_key + text + '\1' return ( num_lines [ side ] , text ) def _line_iterator ( ) : """"""Yields from/to lines of text with a change indication.

        This function is an iterator.  It itself pulls lines from a
        differencing iterator, processes them and yields them.  When it can
        it yields both a ""from"" and a ""to"" line, otherwise it will yield one
        or the other.  In addition to yielding the lines of from/to text, a
        boolean flag is yielded to indicate if the text line(s) have
        differences in them.

        Note, this function is purposefully not defined at the module scope so
        that data it needs from its parent function (within whose context it
        is defined) does not need to be of module scope.
        """""" lines = [ ] num_blanks_pending , num_blanks_to_yield = 0 , 0 while True : while len ( lines ) < 4 : try : lines . append ( diff_lines_iterator . next ( ) ) except StopIteration : lines . append ( 'X' ) s = '' . join ( [ line [ 0 ] for line in lines ] ) if s . startswith ( 'X' ) : num_blanks_to_yield = num_blanks_pending elif s . startswith ( '-?+?' ) : yield _make_line ( lines , '?' , 0 ) , _make_line ( lines , '?' , 1 ) , True continue elif s . startswith ( '--++' ) : num_blanks_pending -= 1 yield _make_line ( lines , '-' , 0 ) , None , True continue elif s . startswith ( ( '--?+' , '--+' , '- ' ) ) : from_line , to_line = _make_line ( lines , '-' , 0 ) , None num_blanks_to_yield , num_blanks_pending = num_blanks_pending - 1 , 0 elif s . startswith ( '-+?' ) : yield _make_line ( lines , None , 0 ) , _make_line ( lines , '?' , 1 ) , True continue elif s . startswith ( '-?+' ) : yield _make_line ( lines , '?' , 0 ) , _make_line ( lines , None , 1 ) , True continue elif s . startswith ( '-' ) : num_blanks_pending -= 1 yield _make_line ( lines , '-' , 0 ) , None , True continue elif s . startswith ( '+--' ) : num_blanks_pending += 1 yield None , _make_line ( lines , '+' , 1 ) , True continue elif s . startswith ( ( '+ ' , '+-' ) ) : from_line , to_line = None , _make_line ( lines , '+' , 1 ) num_blanks_to_yield , num_blanks_pending = num_blanks_pending + 1 , 0 elif s . startswith ( '+' ) : num_blanks_pending += 1 yield None , _make_line ( lines , '+' , 1 ) , True continue elif s . startswith ( ' ' ) : yield _make_line ( lines [ : ] , None , 0 ) , _make_line ( lines , None , 1 ) , False continue while ( num_blanks_to_yield < 0 ) : num_blanks_to_yield += 1 yield None , ( '' , '\n' ) , True while ( num_blanks_to_yield > 0 ) : num_blanks_to_yield -= 1 yield ( '' , '\n' ) , None , True if s . startswith ( 'X' ) : raise StopIteration else : yield from_line , to_line , True def _line_pair_iterator ( ) : """"""Yields from/to lines of text with a change indication.

        This function is an iterator.  It itself pulls lines from the line
        iterator.  Its difference from that iterator is that this function
        always yields a pair of from/to text lines (with the change
        indication).  If necessary it will collect single from/to lines
        until it has a matching pair from/to pair to yield.

        Note, this function is purposefully not defined at the module scope so
        that data it needs from its parent function (within whose context it
        is defined) does not need to be of module scope.
        """""" line_iterator = _line_iterator ( ) fromlines , tolines = [ ] , [ ] while True : while ( len ( fromlines ) == 0 or len ( tolines ) == 0 ) : from_line , to_line , found_diff = line_iterator . next ( ) if from_line is not None : fromlines . append ( ( from_line , found_diff ) ) if to_line is not None : tolines . append ( ( to_line , found_diff ) ) from_line , fromDiff = fromlines . pop ( 0 ) to_line , to_diff = tolines . pop ( 0 ) yield ( from_line , to_line , fromDiff or to_diff ) line_pair_iterator = _line_pair_iterator ( ) if context is None : while True : yield line_pair_iterator . next ( ) else : context += 1 lines_to_write = 0 while True : index , contextLines = 0 , [ None ] * ( context ) found_diff = False while ( found_diff is False ) : from_line , to_line , found_diff = line_pair_iterator . next ( ) i = index % context contextLines [ i ] = ( from_line , to_line , found_diff ) index += 1 if index > context : yield None , None , None lines_to_write = context else : lines_to_write = index index = 0 while ( lines_to_write ) : i = index % context index += 1 yield contextLines [ i ] lines_to_write -= 1 lines_to_write = context - 1 while ( lines_to_write ) : from_line , to_line , found_diff = line_pair_iterator . next ( ) if found_diff : lines_to_write = context - 1 else : lines_to_write -= 1 yield from_line , to_line , found_diff",r Returns generator yielding marked up from / to side by side differences .
"def restore ( delta , which ) : try : tag = { 1 : ""- "" , 2 : ""+ "" } [ int ( which ) ] except KeyError : raise ValueError , ( 'unknown delta choice (must be 1 or 2): %r' % which ) prefixes = ( ""  "" , tag ) for line in delta : if line [ : 2 ] in prefixes : yield line [ 2 : ]",r Generate one of the two sequences that generated a delta .
"def _make ( cls , iterable , new = tuple . __new__ , len = len ) : result = new ( cls , iterable ) if len ( result ) != 3 : raise TypeError ( 'Expected 3 arguments, got %d' % len ( result ) ) return result",Make a new Match object from a sequence or iterable
"def set_seq1 ( self , a ) : if a is self . a : return self . a = a self . matching_blocks = self . opcodes = None",Set the first sequence to be compared .
"def set_seq2 ( self , b ) : if b is self . b : return self . b = b self . matching_blocks = self . opcodes = None self . fullbcount = None self . __chain_b ( )",Set the second sequence to be compared .
"def find_longest_match ( self , alo , ahi , blo , bhi ) : a , b , b2j , isbjunk = self . a , self . b , self . b2j , self . isbjunk besti , bestj , bestsize = alo , blo , 0 j2len = { } nothing = [ ] for i in xrange ( alo , ahi ) : j2lenget = j2len . get newj2len = { } for j in b2j . get ( a [ i ] , nothing ) : if j < blo : continue if j >= bhi : break k = newj2len [ j ] = j2lenget ( j - 1 , 0 ) + 1 if k > bestsize : besti , bestj , bestsize = i - k + 1 , j - k + 1 , k j2len = newj2len while besti > alo and bestj > blo and not isbjunk ( b [ bestj - 1 ] ) and a [ besti - 1 ] == b [ bestj - 1 ] : besti , bestj , bestsize = besti - 1 , bestj - 1 , bestsize + 1 while besti + bestsize < ahi and bestj + bestsize < bhi and not isbjunk ( b [ bestj + bestsize ] ) and a [ besti + bestsize ] == b [ bestj + bestsize ] : bestsize += 1 while besti > alo and bestj > blo and isbjunk ( b [ bestj - 1 ] ) and a [ besti - 1 ] == b [ bestj - 1 ] : besti , bestj , bestsize = besti - 1 , bestj - 1 , bestsize + 1 while besti + bestsize < ahi and bestj + bestsize < bhi and isbjunk ( b [ bestj + bestsize ] ) and a [ besti + bestsize ] == b [ bestj + bestsize ] : bestsize = bestsize + 1 return Match ( besti , bestj , bestsize )",Find longest matching block in a [ alo : ahi ] and b [ blo : bhi ] .
"def get_matching_blocks ( self ) : if self . matching_blocks is not None : return self . matching_blocks la , lb = len ( self . a ) , len ( self . b ) queue = [ ( 0 , la , 0 , lb ) ] matching_blocks = [ ] while queue : alo , ahi , blo , bhi = queue . pop ( ) i , j , k = x = self . find_longest_match ( alo , ahi , blo , bhi ) if k : matching_blocks . append ( x ) if alo < i and blo < j : queue . append ( ( alo , i , blo , j ) ) if i + k < ahi and j + k < bhi : queue . append ( ( i + k , ahi , j + k , bhi ) ) matching_blocks . sort ( ) i1 = j1 = k1 = 0 non_adjacent = [ ] for i2 , j2 , k2 in matching_blocks : if i1 + k1 == i2 and j1 + k1 == j2 : k1 += k2 else : if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) i1 , j1 , k1 = i2 , j2 , k2 if k1 : non_adjacent . append ( ( i1 , j1 , k1 ) ) non_adjacent . append ( ( la , lb , 0 ) ) self . matching_blocks = map ( Match . _make , non_adjacent ) return self . matching_blocks",Return list of triples describing matching subsequences .
"def get_opcodes ( self ) : if self . opcodes is not None : return self . opcodes i = j = 0 self . opcodes = answer = [ ] for ai , bj , size in self . get_matching_blocks ( ) : tag = '' if i < ai and j < bj : tag = 'replace' elif i < ai : tag = 'delete' elif j < bj : tag = 'insert' if tag : answer . append ( ( tag , i , ai , j , bj ) ) i , j = ai + size , bj + size if size : answer . append ( ( 'equal' , ai , i , bj , j ) ) return answer",Return list of 5 - tuples describing how to turn a into b .
"def get_grouped_opcodes ( self , n = 3 ) : codes = self . get_opcodes ( ) if not codes : codes = [ ( ""equal"" , 0 , 1 , 0 , 1 ) ] if codes [ 0 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ 0 ] codes [ 0 ] = tag , max ( i1 , i2 - n ) , i2 , max ( j1 , j2 - n ) , j2 if codes [ - 1 ] [ 0 ] == 'equal' : tag , i1 , i2 , j1 , j2 = codes [ - 1 ] codes [ - 1 ] = tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) nn = n + n group = [ ] for tag , i1 , i2 , j1 , j2 in codes : if tag == 'equal' and i2 - i1 > nn : group . append ( ( tag , i1 , min ( i2 , i1 + n ) , j1 , min ( j2 , j1 + n ) ) ) yield group group = [ ] i1 , j1 = max ( i1 , i2 - n ) , max ( j1 , j2 - n ) group . append ( ( tag , i1 , i2 , j1 , j2 ) ) if group and not ( len ( group ) == 1 and group [ 0 ] [ 0 ] == 'equal' ) : yield group",Isolate change clusters by eliminating ranges with no changes .
"def ratio ( self ) : matches = reduce ( lambda sum , triple : sum + triple [ - 1 ] , self . get_matching_blocks ( ) , 0 ) return _calculate_ratio ( matches , len ( self . a ) + len ( self . b ) )",Return a measure of the sequences similarity ( float in [ 0 1 ] ) .
"def quick_ratio ( self ) : if self . fullbcount is None : self . fullbcount = fullbcount = { } for elt in self . b : fullbcount [ elt ] = fullbcount . get ( elt , 0 ) + 1 fullbcount = self . fullbcount avail = { } availhas , matches = avail . __contains__ , 0 for elt in self . a : if availhas ( elt ) : numb = avail [ elt ] else : numb = fullbcount . get ( elt , 0 ) avail [ elt ] = numb - 1 if numb > 0 : matches = matches + 1 return _calculate_ratio ( matches , len ( self . a ) + len ( self . b ) )",Return an upper bound on ratio () relatively quickly .
"def real_quick_ratio ( self ) : la , lb = len ( self . a ) , len ( self . b ) return _calculate_ratio ( min ( la , lb ) , la + lb )",Return an upper bound on ratio () very quickly .
"def compare ( self , a , b ) : cruncher = SequenceMatcher ( self . linejunk , a , b ) for tag , alo , ahi , blo , bhi in cruncher . get_opcodes ( ) : if tag == 'replace' : g = self . _fancy_replace ( a , alo , ahi , b , blo , bhi ) elif tag == 'delete' : g = self . _dump ( '-' , a , alo , ahi ) elif tag == 'insert' : g = self . _dump ( '+' , b , blo , bhi ) elif tag == 'equal' : g = self . _dump ( ' ' , a , alo , ahi ) else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in g : yield line",r Compare two sequences of lines ; generate the resulting delta .
"def _dump ( self , tag , x , lo , hi ) : for i in xrange ( lo , hi ) : yield '%s %s' % ( tag , x [ i ] )",Generate comparison results for a same - tagged range .
"def _fancy_replace ( self , a , alo , ahi , b , blo , bhi ) : best_ratio , cutoff = 0.74 , 0.75 cruncher = SequenceMatcher ( self . charjunk ) eqi , eqj = None , None for j in xrange ( blo , bhi ) : bj = b [ j ] cruncher . set_seq2 ( bj ) for i in xrange ( alo , ahi ) : ai = a [ i ] if ai == bj : if eqi is None : eqi , eqj = i , j continue cruncher . set_seq1 ( ai ) if cruncher . real_quick_ratio ( ) > best_ratio and cruncher . quick_ratio ( ) > best_ratio and cruncher . ratio ( ) > best_ratio : best_ratio , best_i , best_j = cruncher . ratio ( ) , i , j if best_ratio < cutoff : if eqi is None : for line in self . _plain_replace ( a , alo , ahi , b , blo , bhi ) : yield line return best_i , best_j , best_ratio = eqi , eqj , 1.0 else : eqi = None for line in self . _fancy_helper ( a , alo , best_i , b , blo , best_j ) : yield line aelt , belt = a [ best_i ] , b [ best_j ] if eqi is None : atags = btags = """" cruncher . set_seqs ( aelt , belt ) for tag , ai1 , ai2 , bj1 , bj2 in cruncher . get_opcodes ( ) : la , lb = ai2 - ai1 , bj2 - bj1 if tag == 'replace' : atags += '^' * la btags += '^' * lb elif tag == 'delete' : atags += '-' * la elif tag == 'insert' : btags += '+' * lb elif tag == 'equal' : atags += ' ' * la btags += ' ' * lb else : raise ValueError , 'unknown tag %r' % ( tag , ) for line in self . _qformat ( aelt , belt , atags , btags ) : yield line else : yield '  ' + aelt for line in self . _fancy_helper ( a , best_i + 1 , ahi , b , best_j + 1 , bhi ) : yield line",r When replacing one block of lines with another search the blocks for * similar * lines ; the best - matching pair ( if any ) is used as a synch point and intraline difference marking is done on the similar pair . Lots of work but often worth it .
"def _qformat ( self , aline , bline , atags , btags ) : common = min ( _count_leading ( aline , ""\t"" ) , _count_leading ( bline , ""\t"" ) ) common = min ( common , _count_leading ( atags [ : common ] , "" "" ) ) common = min ( common , _count_leading ( btags [ : common ] , "" "" ) ) atags = atags [ common : ] . rstrip ( ) btags = btags [ common : ] . rstrip ( ) yield ""- "" + aline if atags : yield ""? %s%s\n"" % ( ""\t"" * common , atags ) yield ""+ "" + bline if btags : yield ""? %s%s\n"" % ( ""\t"" * common , btags )",r Format ? output and deal with leading tabs .
"def make_file ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : return self . _file_template % dict ( styles = self . _styles , legend = self . _legend , table = self . make_table ( fromlines , tolines , fromdesc , todesc , context = context , numlines = numlines ) )",Returns HTML file of side by side comparison with change highlights
"def _tab_newline_replace ( self , fromlines , tolines ) : def expand_tabs ( line ) : line = line . replace ( ' ' , '\0' ) line = line . expandtabs ( self . _tabsize ) line = line . replace ( ' ' , '\t' ) return line . replace ( '\0' , ' ' ) . rstrip ( '\n' ) fromlines = [ expand_tabs ( line ) for line in fromlines ] tolines = [ expand_tabs ( line ) for line in tolines ] return fromlines , tolines",Returns from / to line lists with tabs expanded and newlines removed .
"def _split_line ( self , data_list , line_num , text ) : if not line_num : data_list . append ( ( line_num , text ) ) return size = len ( text ) max = self . _wrapcolumn if ( size <= max ) or ( ( size - ( text . count ( '\0' ) * 3 ) ) <= max ) : data_list . append ( ( line_num , text ) ) return i = 0 n = 0 mark = '' while n < max and i < size : if text [ i ] == '\0' : i += 1 mark = text [ i ] i += 1 elif text [ i ] == '\1' : i += 1 mark = '' else : i += 1 n += 1 line1 = text [ : i ] line2 = text [ i : ] if mark : line1 = line1 + '\1' line2 = '\0' + mark + line2 data_list . append ( ( line_num , line1 ) ) self . _split_line ( data_list , '>' , line2 )",Builds list of text lines by splitting text lines at wrap point
"def _line_wrapper ( self , diffs ) : for fromdata , todata , flag in diffs : if flag is None : yield fromdata , todata , flag continue ( fromline , fromtext ) , ( toline , totext ) = fromdata , todata fromlist , tolist = [ ] , [ ] self . _split_line ( fromlist , fromline , fromtext ) self . _split_line ( tolist , toline , totext ) while fromlist or tolist : if fromlist : fromdata = fromlist . pop ( 0 ) else : fromdata = ( '' , ' ' ) if tolist : todata = tolist . pop ( 0 ) else : todata = ( '' , ' ' ) yield fromdata , todata , flag",Returns iterator that splits ( wraps ) mdiff text lines
"def _collect_lines ( self , diffs ) : fromlist , tolist , flaglist = [ ] , [ ] , [ ] for fromdata , todata , flag in diffs : try : fromlist . append ( self . _format_line ( 0 , flag , * fromdata ) ) tolist . append ( self . _format_line ( 1 , flag , * todata ) ) except TypeError : fromlist . append ( None ) tolist . append ( None ) flaglist . append ( flag ) return fromlist , tolist , flaglist",Collects mdiff output into separate lists
"def _make_prefix ( self ) : fromprefix = ""from%d_"" % HtmlDiff . _default_prefix toprefix = ""to%d_"" % HtmlDiff . _default_prefix HtmlDiff . _default_prefix += 1 self . _prefix = [ fromprefix , toprefix ]",Create unique anchor prefixes
"def _convert_flags ( self , fromlist , tolist , flaglist , context , numlines ) : toprefix = self . _prefix [ 1 ] next_id = [ '' ] * len ( flaglist ) next_href = [ '' ] * len ( flaglist ) num_chg , in_change = 0 , False last = 0 for i , flag in enumerate ( flaglist ) : if flag : if not in_change : in_change = True last = i i = max ( [ 0 , i - numlines ] ) next_id [ i ] = ' id=""difflib_chg_%s_%d""' % ( toprefix , num_chg ) num_chg += 1 next_href [ last ] = '<a href=""#difflib_chg_%s_%d"">n</a>' % ( toprefix , num_chg ) else : in_change = False if not flaglist : flaglist = [ False ] next_id = [ '' ] next_href = [ '' ] last = 0 if context : fromlist = [ '<td></td><td>&nbsp;No Differences Found&nbsp;</td>' ] tolist = fromlist else : fromlist = tolist = [ '<td></td><td>&nbsp;Empty File&nbsp;</td>' ] if not flaglist [ 0 ] : next_href [ 0 ] = '<a href=""#difflib_chg_%s_0"">f</a>' % toprefix next_href [ last ] = '<a href=""#difflib_chg_%s_top"">t</a>' % ( toprefix ) return fromlist , tolist , flaglist , next_href , next_id",Makes list of next links
"def make_table ( self , fromlines , tolines , fromdesc = '' , todesc = '' , context = False , numlines = 5 ) : self . _make_prefix ( ) fromlines , tolines = self . _tab_newline_replace ( fromlines , tolines ) if context : context_lines = numlines else : context_lines = None diffs = _mdiff ( fromlines , tolines , context_lines , linejunk = self . _linejunk , charjunk = self . _charjunk ) if self . _wrapcolumn : diffs = self . _line_wrapper ( diffs ) fromlist , tolist , flaglist = self . _collect_lines ( diffs ) fromlist , tolist , flaglist , next_href , next_id = self . _convert_flags ( fromlist , tolist , flaglist , context , numlines ) s = [ ] fmt = '            <tr><td class=""diff_next""%s>%s</td>%s' + '<td class=""diff_next"">%s</td>%s</tr>\n' for i in range ( len ( flaglist ) ) : if flaglist [ i ] is None : if i > 0 : s . append ( '        </tbody>        \n        <tbody>\n' ) else : s . append ( fmt % ( next_id [ i ] , next_href [ i ] , fromlist [ i ] , next_href [ i ] , tolist [ i ] ) ) if fromdesc or todesc : header_row = '<thead><tr>%s%s%s%s</tr></thead>' % ( '<th class=""diff_next""><br /></th>' , '<th colspan=""2"" class=""diff_header"">%s</th>' % fromdesc , '<th class=""diff_next""><br /></th>' , '<th colspan=""2"" class=""diff_header"">%s</th>' % todesc ) else : header_row = '' table = self . _table_template % dict ( data_rows = '' . join ( s ) , header_row = header_row , prefix = self . _prefix [ 1 ] ) return table . replace ( '\0+' , '<span class=""diff_add"">' ) . replace ( '\0-' , '<span class=""diff_sub"">' ) . replace ( '\0^' , '<span class=""diff_chg"">' ) . replace ( '\1' , '</span>' ) . replace ( '\t' , '&nbsp;' )",Returns HTML table of side by side comparison with change highlights
"def _MakeParallelBenchmark ( p , work_func , * args ) : def Benchmark ( b ) : e = threading . Event ( ) def Target ( ) : e . wait ( ) for _ in xrange ( b . N / p ) : work_func ( * args ) threads = [ ] for _ in xrange ( p ) : t = threading . Thread ( target = Target ) t . start ( ) threads . append ( t ) b . ResetTimer ( ) e . set ( ) for t in threads : t . join ( ) return Benchmark",Create and return a benchmark that runs work_func p times in parallel .
"def visit_function_inline ( self , node ) : func_visitor = block . FunctionBlockVisitor ( node ) for child in node . body : func_visitor . visit ( child ) func_block = block . FunctionBlock ( self . block , node . name , func_visitor . vars , func_visitor . is_generator ) visitor = StatementVisitor ( func_block , self . future_node ) with visitor . writer . indent_block ( ) : visitor . _visit_each ( node . body ) result = self . block . alloc_temp ( ) with self . block . alloc_temp ( '[]πg.Param')   s  unc_args: args = node . args argc = len ( args . args ) self . writer . write ( '{} = make([]πg.Param, {})'. f ormat( func_args . expr , argc ) ) defaults = [ None ] * ( argc - len ( args . defaults ) ) + args . defaults for i , ( a , d ) in enumerate ( zip ( args . args , defaults ) ) : with self . visit_expr ( d ) if d else expr . nil_expr as default : tmpl = '$args[$i] = πg.Param{Name: $name, Def: $default}' self . writer . write_tmpl ( tmpl , args = func_args . expr , i = i , name = util . go_str ( a . arg ) , default = default . expr ) flags = [ ] if args . vararg : flags . append ( 'πg.CodeFlagVarArg') if args . kwarg : flags . append ( 'πg.CodeFlagKWArg') self . writer . write_tmpl ( '$result = πg.NewFunction(πg.NewCode($name, $filename, $args, ' '$flags, func(πF *πg.Frame, πArgs []*πg.Object) ' '(*πg.Object, *πg.BaseException) {', result = result . name , name = util . go_str ( node . name ) , filename = util . go_str ( self . block . root . filename ) , args = func_args . expr , flags = ' | ' . join ( flags ) if flags else 0 ) with self . writer . indent_block ( ) : for var in func_block . vars . values ( ) : if var . type != block . Var . TYPE_GLOBAL : fmt = 'var {0} *πg.Object = {1}; _ = {0}' self . writer . write ( fmt . format ( util . adjust_local_name ( var . name ) , var . init_expr ) ) self . writer . write_temp_decls ( func_block ) self . writer . write ( 'var πR *πg.Object; _ = πR') self . writer . write ( 'var πE *πg.BaseException; _ = πE') if func_block . is_generator : self . writer . write ( 'return πg.NewGenerator(πF, func(πSent *πg.Object) ' '(*πg.Object, *πg.BaseException) {') with self . writer . indent_block ( ) : self . writer . write_block ( func_block , visitor . writer . getvalue ( ) ) self . writer . write ( 'return nil, πE') self . writer . write ( '}).ToObject(), nil' ) else : self . writer . write_block ( func_block , visitor . writer . getvalue ( ) ) self . writer . write ( textwrap . dedent ( """"""\
              if πE != nil {
              \tπR = nil
              } else if πR == nil {
              \tπR = πg.None
              }
              return πR, πE"""""")) self . writer . write ( '}), πF.Globals()).ToObject()') return result",Returns an GeneratedExpr for a function with the given body .
"def _import_and_bind ( self , imp ) : with self . block . alloc_temp ( ) as mod , self . block . alloc_temp ( '[]*πg.Object')   s  od_slice: self . writer . write_checked_call2 ( mod_slice , 'πg.ImportModule(πF, {})',  u il.g o _str(i m p.n a me)) for binding in imp . bindings : if binding . bind_type == imputil . Import . MODULE : self . writer . write ( '{} = {}[{}]' . format ( mod . name , mod_slice . expr , binding . value ) ) self . block . bind_var ( self . writer , binding . alias , mod . expr ) else : self . writer . write ( '{} = {}[{}]' . format ( mod . name , mod_slice . expr , imp . name . count ( '.' ) ) ) with self . block . alloc_temp ( ) as member : self . writer . write_checked_call2 ( member , 'πg.GetAttr(πF, {}, {}, nil)', mod . expr , self . block . root . intern ( binding . value ) ) self . block . bind_var ( self . writer , binding . alias , member . expr )",Generates code that imports a module and binds it to a variable .
"def _write_except_dispatcher ( self , exc , tb , handlers ) : handler_labels = [ ] for i , except_node in enumerate ( handlers ) : handler_labels . append ( self . block . genlabel ( ) ) if except_node . type : with self . visit_expr ( except_node . type ) as type_ , self . block . alloc_temp ( 'bool' ) as is_inst : self . writer . write_checked_call2 ( is_inst , 'πg.IsInstance(πF, {}.ToObject(), {})',  e c,  t pe_.e x pr) self . writer . write_tmpl ( textwrap . dedent ( """"""\
              if $is_inst {
              \tgoto Label$label
              }"""""" ) , is_inst = is_inst . expr , label = handler_labels [ - 1 ] ) else : if i != len ( handlers ) - 1 : msg = ""default 'except:' must be last"" raise util . ParseError ( except_node , msg ) self . writer . write ( 'goto Label{}' . format ( handler_labels [ - 1 ] ) ) if handlers [ - 1 ] . type : self . writer . write ( 'πE = πF.Raise({}.ToObject(), nil, {}.ToObject())'.f o rmat(e x c,  t )) self . writer . write ( 'continue' ) return handler_labels",Outputs a Go code that jumps to the appropriate except handler .
"def listdir ( path ) : try : cached_mtime , list = cache [ path ] del cache [ path ] except KeyError : cached_mtime , list = - 1 , [ ] mtime = os . stat ( path ) . st_mtime if mtime != cached_mtime : list = os . listdir ( path ) list . sort ( ) cache [ path ] = mtime , list return list",List directory contents using cache .
"def annotate ( head , list ) : for i in range ( len ( list ) ) : if os . path . isdir ( os . path . join ( head , list [ i ] ) ) : list [ i ] = list [ i ] + '/'",Add / suffixes to directories .
"def pprint ( o , stream = None , indent = 1 , width = 80 , depth = None ) : printer = PrettyPrinter ( stream = stream , indent = indent , width = width , depth = depth ) printer . pprint ( o )",Pretty - print a Python o to a stream [ default is sys . stdout ] .
"def pformat ( o , indent = 1 , width = 80 , depth = None ) : return PrettyPrinter ( indent = indent , width = width , depth = depth ) . pformat ( o )",Format a Python o into a pretty - printed representation .
"def format ( self , o , context , maxlevels , level ) : return _safe_repr ( o , context , maxlevels , level )",Format o for a specific context returning a string and flags indicating whether the representation is readable and whether the o represents a recursive construct .
"def action ( inner_rule , loc = None ) : def decorator ( mapper ) : @ llrule ( loc , inner_rule . expected ) def outer_rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : return result if isinstance ( result , tuple ) : return mapper ( parser , * result ) else : return mapper ( parser , result ) return outer_rule return decorator",A decorator returning a function that first runs inner_rule and then if its return value is not None maps that value using mapper .
"def Eps ( value = None , loc = None ) : @ llrule ( loc , lambda parser : [ ] ) def rule ( parser ) : return value return rule",A rule that accepts no tokens ( epsilon ) and returns value .
"def Tok ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : return parser . _accept ( kind ) return rule",A rule that accepts a token of kind kind and returns it or returns None .
"def Loc ( kind , loc = None ) : @ llrule ( loc , lambda parser : [ kind ] ) def rule ( parser ) : result = parser . _accept ( kind ) if result is unmatched : return result return result . loc return rule",A rule that accepts a token of kind kind and returns its location or returns None .
"def Rule ( name , loc = None ) : @ llrule ( loc , lambda parser : getattr ( parser , name ) . expected ( parser ) ) def rule ( parser ) : return getattr ( parser , name ) ( ) return rule",A proxy for a rule called name which may not be yet defined .
"def Expect ( inner_rule , loc = None ) : @ llrule ( loc , inner_rule . expected ) def rule ( parser ) : result = inner_rule ( parser ) if result is unmatched : expected = reduce ( list . __add__ , [ rule . expected ( parser ) for rule in parser . _errrules ] ) expected = list ( sorted ( set ( expected ) ) ) if len ( expected ) > 1 : expected = "" or "" . join ( [ "", "" . join ( expected [ 0 : - 1 ] ) , expected [ - 1 ] ] ) elif len ( expected ) == 1 : expected = expected [ 0 ] else : expected = ""(impossible)"" error_tok = parser . _tokens [ parser . _errindex ] error = diagnostic . Diagnostic ( ""fatal"" , ""unexpected {actual}: expected {expected}"" , { ""actual"" : error_tok . kind , ""expected"" : expected } , error_tok . loc ) parser . diagnostic_engine . process ( error ) return result return rule",A rule that executes inner_rule and emits a diagnostic error if it returns None .
"def Seq ( first_rule , * rest_of_rules , * * kwargs ) : @ llrule ( kwargs . get ( ""loc"" , None ) , first_rule . expected ) def rule ( parser ) : result = first_rule ( parser ) if result is unmatched : return result results = [ result ] for rule in rest_of_rules : result = rule ( parser ) if result is unmatched : return result results . append ( result ) return tuple ( results ) return rule",A rule that accepts a sequence of tokens satisfying rules and returns a tuple containing their return values or None if the first rule was not satisfied .
"def SeqN ( n , * inner_rules , * * kwargs ) : @ action ( Seq ( * inner_rules ) , loc = kwargs . get ( ""loc"" , None ) ) def rule ( parser , * values ) : return values [ n ] return rule",A rule that accepts a sequence of tokens satisfying rules and returns the value returned by rule number n or None if the first rule was not satisfied .
"def Alt ( * inner_rules , * * kwargs ) : loc = kwargs . get ( ""loc"" , None ) expected = lambda parser : reduce ( list . __add__ , map ( lambda x : x . expected ( parser ) , inner_rules ) ) if loc is not None : @ llrule ( loc , expected , cases = len ( inner_rules ) ) def rule ( parser ) : data = parser . _save ( ) for idx , inner_rule in enumerate ( inner_rules ) : result = inner_rule ( parser ) if result is unmatched : parser . _restore ( data , rule = inner_rule ) else : rule . covered [ idx ] = True return result return unmatched else : @ llrule ( loc , expected , cases = len ( inner_rules ) ) def rule ( parser ) : data = parser . _save ( ) for inner_rule in inner_rules : result = inner_rule ( parser ) if result is unmatched : parser . _restore ( data , rule = inner_rule ) else : return result return unmatched return rule",A rule that expects a sequence of tokens satisfying one of rules in sequence ( a rule is satisfied when it returns anything but None ) and returns the return value of that rule or None if no rules were satisfied .
"def Star ( inner_rule , loc = None ) : @ llrule ( loc , lambda parser : [ ] ) def rule ( parser ) : results = [ ] while True : data = parser . _save ( ) result = inner_rule ( parser ) if result is unmatched : parser . _restore ( data , rule = inner_rule ) return results results . append ( result ) return rule",A rule that accepts a sequence of tokens satisfying inner_rule zero or more times and returns the returned values in a : class : list .
"def Newline ( loc = None ) : @ llrule ( loc , lambda parser : [ ""newline"" ] ) def rule ( parser ) : result = parser . _accept ( ""newline"" ) if result is unmatched : return result return [ ] return rule",A rule that accepts token of kind newline and returns an empty list .
"def Oper ( klass , * kinds , * * kwargs ) : @ action ( Seq ( * map ( Loc , kinds ) ) , loc = kwargs . get ( ""loc"" , None ) ) def rule ( parser , * tokens ) : return klass ( loc = tokens [ 0 ] . join ( tokens [ - 1 ] ) ) return rule",A rule that accepts a sequence of tokens of kinds kinds and returns an instance of klass with loc encompassing the entire sequence or None if the first token is not of kinds [ 0 ] .
"def single_input ( self , body ) : loc = None if body != [ ] : loc = body [ 0 ] . loc return ast . Interactive ( body = body , loc = loc )",single_input : NEWLINE | simple_stmt | compound_stmt NEWLINE
"def file_input ( parser , body ) : body = reduce ( list . __add__ , body , [ ] ) loc = None if body != [ ] : loc = body [ 0 ] . loc return ast . Module ( body = body , loc = loc )",file_input : ( NEWLINE | stmt ) * ENDMARKER
"def eval_input ( self , expr ) : return ast . Expression ( body = [ expr ] , loc = expr . loc )",eval_input : testlist NEWLINE * ENDMARKER
"def decorator ( self , at_loc , idents , call_opt , newline_loc ) : root = idents [ 0 ] dec_loc = root . loc expr = ast . Name ( id = root . value , ctx = None , loc = root . loc ) for ident in idents [ 1 : ] : dot_loc = ident . loc . begin ( ) dot_loc . begin_pos -= 1 dec_loc = dec_loc . join ( ident . loc ) expr = ast . Attribute ( value = expr , attr = ident . value , ctx = None , loc = expr . loc . join ( ident . loc ) , attr_loc = ident . loc , dot_loc = dot_loc ) if call_opt : call_opt . func = expr call_opt . loc = dec_loc . join ( call_opt . loc ) expr = call_opt return at_loc , expr",decorator :
"def decorated ( self , decorators , classfuncdef ) : classfuncdef . at_locs = list ( map ( lambda x : x [ 0 ] , decorators ) ) classfuncdef . decorator_list = list ( map ( lambda x : x [ 1 ] , decorators ) ) classfuncdef . loc = classfuncdef . loc . join ( decorators [ 0 ] [ 0 ] ) return classfuncdef",decorated : decorators ( classdef | funcdef )
"def funcdef__26 ( self , def_loc , ident_tok , args , colon_loc , suite ) : return ast . FunctionDef ( name = ident_tok . value , args = args , returns = None , body = suite , decorator_list = [ ] , at_locs = [ ] , keyword_loc = def_loc , name_loc = ident_tok . loc , colon_loc = colon_loc , arrow_loc = None , loc = def_loc . join ( suite [ - 1 ] . loc ) )",( 2 . 6 2 . 7 ) funcdef : def NAME parameters : suite
"def varargslist__26 ( self , fparams , args ) : for fparam , default_opt in fparams : if default_opt : equals_loc , default = default_opt args . equals_locs . append ( equals_loc ) args . defaults . append ( default ) elif len ( args . defaults ) > 0 : error = diagnostic . Diagnostic ( ""fatal"" , ""non-default argument follows default argument"" , { } , fparam . loc , [ args . args [ - 1 ] . loc . join ( args . defaults [ - 1 ] . loc ) ] ) self . diagnostic_engine . process ( error ) args . args . append ( fparam ) def fparam_loc ( fparam , default_opt ) : if default_opt : equals_loc , default = default_opt return fparam . loc . join ( default . loc ) else : return fparam . loc if args . loc is None : args . loc = fparam_loc ( * fparams [ 0 ] ) . join ( fparam_loc ( * fparams [ - 1 ] ) ) elif len ( fparams ) > 0 : args . loc = args . loc . join ( fparam_loc ( * fparams [ 0 ] ) ) return args",( 2 . 6 2 . 7 ) varargslist : (( fpdef [ = test ] ) * ( * NAME [ ** NAME ] | ** NAME ) | fpdef [ = test ] ( fpdef [ = test ] ) * [ ] )
"def tfpdef ( self , ident_tok , annotation_opt ) : if annotation_opt : colon_loc , annotation = annotation_opt return self . _arg ( ident_tok , colon_loc , annotation ) return self . _arg ( ident_tok )",( 3 . 0 - ) tfpdef : NAME [ : test ]
"def expr_stmt ( self , lhs , rhs ) : if isinstance ( rhs , ast . AugAssign ) : if isinstance ( lhs , ast . Tuple ) or isinstance ( lhs , ast . List ) : error = diagnostic . Diagnostic ( ""fatal"" , ""illegal expression for augmented assignment"" , { } , rhs . op . loc , [ lhs . loc ] ) self . diagnostic_engine . process ( error ) else : rhs . target = self . _assignable ( lhs ) rhs . loc = rhs . target . loc . join ( rhs . value . loc ) return rhs elif rhs is not None : rhs . targets = list ( map ( self . _assignable , [ lhs ] + rhs . targets ) ) rhs . loc = lhs . loc . join ( rhs . value . loc ) return rhs else : return ast . Expr ( value = lhs , loc = lhs . loc )",( 2 . 6 2 . 7 3 . 0 3 . 1 ) expr_stmt : testlist ( augassign ( yield_expr|testlist ) | ( = ( yield_expr|testlist )) * ) ( 3 . 2 - ) expr_stmt : testlist_star_expr ( augassign ( yield_expr|testlist ) | ( = ( yield_expr|testlist_star_expr )) * )
"def print_stmt ( self , print_loc , stmt ) : stmt . keyword_loc = print_loc if stmt . loc is None : stmt . loc = print_loc else : stmt . loc = print_loc . join ( stmt . loc ) return stmt",( 2 . 6 - 2 . 7 ) print_stmt : print ( [ test ( test ) * [ ] ] | >> test [ ( test ) + [ ] ] )
"def del_stmt ( self , stmt_loc , exprs ) : return ast . Delete ( targets = [ self . _assignable ( expr , is_delete = True ) for expr in exprs ] , loc = stmt_loc . join ( exprs [ - 1 ] . loc ) , keyword_loc = stmt_loc )",del_stmt : del exprlist
"def return_stmt ( self , stmt_loc , values ) : loc = stmt_loc if values : loc = loc . join ( values . loc ) return ast . Return ( value = values , loc = loc , keyword_loc = stmt_loc )",return_stmt : return [ testlist ]
"def yield_stmt ( self , expr ) : return ast . Expr ( value = expr , loc = expr . loc )",yield_stmt : yield_expr
"def raise_stmt__26 ( self , raise_loc , type_opt ) : type_ = inst = tback = None loc = raise_loc if type_opt : type_ , inst_opt = type_opt loc = loc . join ( type_ . loc ) if inst_opt : _ , inst , tback = inst_opt loc = loc . join ( inst . loc ) if tback : loc = loc . join ( tback . loc ) return ast . Raise ( exc = type_ , inst = inst , tback = tback , cause = None , keyword_loc = raise_loc , from_loc = None , loc = loc )",( 2 . 6 2 . 7 ) raise_stmt : raise [ test [ test [ test ]]]
"def raise_stmt__30 ( self , raise_loc , exc_opt ) : exc = from_loc = cause = None loc = raise_loc if exc_opt : exc , cause_opt = exc_opt loc = loc . join ( exc . loc ) if cause_opt : from_loc , cause = cause_opt loc = loc . join ( cause . loc ) return ast . Raise ( exc = exc , inst = None , tback = None , cause = cause , keyword_loc = raise_loc , from_loc = from_loc , loc = loc )",( 3 . 0 - ) raise_stmt : raise [ test [ from test ]]
"def import_name ( self , import_loc , names ) : return ast . Import ( names = names , keyword_loc = import_loc , loc = import_loc . join ( names [ - 1 ] . loc ) )",import_name : import dotted_as_names
"def import_from ( self , from_loc , module_name , import_loc , names ) : ( dots_loc , dots_count ) , dotted_name_opt = module_name module_loc = module = None if dotted_name_opt : module_loc , module = dotted_name_opt lparen_loc , names , rparen_loc = names loc = from_loc . join ( names [ - 1 ] . loc ) if rparen_loc : loc = loc . join ( rparen_loc ) if module == ""__future__"" : self . add_flags ( [ x . name for x in names ] ) return ast . ImportFrom ( names = names , module = module , level = dots_count , keyword_loc = from_loc , dots_loc = dots_loc , module_loc = module_loc , import_loc = import_loc , lparen_loc = lparen_loc , rparen_loc = rparen_loc , loc = loc )",( 2 . 6 2 . 7 ) import_from : ( from ( . * dotted_name | . + ) import ( * | ( import_as_names ) | import_as_names )) ( 3 . 0 - ) # note below : the ( . | ... ) is necessary because ... is tokenized as ELLIPSIS import_from : ( from (( . | ... ) * dotted_name | ( . | ... ) + ) import ( * | ( import_as_names ) | import_as_names ))
"def import_as_name ( self , name_tok , as_name_opt ) : asname_name = asname_loc = as_loc = None loc = name_tok . loc if as_name_opt : as_loc , asname = as_name_opt asname_name = asname . value asname_loc = asname . loc loc = loc . join ( asname . loc ) return ast . alias ( name = name_tok . value , asname = asname_name , loc = loc , name_loc = name_tok . loc , as_loc = as_loc , asname_loc = asname_loc )",import_as_name : NAME [ as NAME ]
"def dotted_as_name ( self , dotted_name , as_name_opt ) : asname_name = asname_loc = as_loc = None dotted_name_loc , dotted_name_name = dotted_name loc = dotted_name_loc if as_name_opt : as_loc , asname = as_name_opt asname_name = asname . value asname_loc = asname . loc loc = loc . join ( asname . loc ) return ast . alias ( name = dotted_name_name , asname = asname_name , loc = loc , name_loc = dotted_name_loc , as_loc = as_loc , asname_loc = asname_loc )",dotted_as_name : dotted_name [ as NAME ]
"def dotted_name ( self , idents ) : return idents [ 0 ] . loc . join ( idents [ - 1 ] . loc ) , ""."" . join ( list ( map ( lambda x : x . value , idents ) ) )",dotted_name : NAME ( . NAME ) *
"def global_stmt ( self , global_loc , names ) : return ast . Global ( names = list ( map ( lambda x : x . value , names ) ) , name_locs = list ( map ( lambda x : x . loc , names ) ) , keyword_loc = global_loc , loc = global_loc . join ( names [ - 1 ] . loc ) )",global_stmt : global NAME ( NAME ) *
"def exec_stmt ( self , exec_loc , body , in_opt ) : in_loc , globals , locals = None , None , None loc = exec_loc . join ( body . loc ) if in_opt : in_loc , globals , locals = in_opt if locals : loc = loc . join ( locals . loc ) else : loc = loc . join ( globals . loc ) return ast . Exec ( body = body , locals = locals , globals = globals , loc = loc , keyword_loc = exec_loc , in_loc = in_loc )",( 2 . 6 2 . 7 ) exec_stmt : exec expr [ in test [ test ]]
"def nonlocal_stmt ( self , nonlocal_loc , names ) : return ast . Nonlocal ( names = list ( map ( lambda x : x . value , names ) ) , name_locs = list ( map ( lambda x : x . loc , names ) ) , keyword_loc = nonlocal_loc , loc = nonlocal_loc . join ( names [ - 1 ] . loc ) )",( 3 . 0 - ) nonlocal_stmt : nonlocal NAME ( NAME ) *
"def assert_stmt ( self , assert_loc , test , msg ) : loc = assert_loc . join ( test . loc ) if msg : loc = loc . join ( msg . loc ) return ast . Assert ( test = test , msg = msg , loc = loc , keyword_loc = assert_loc )",assert_stmt : assert test [ test ]
"def if_stmt ( self , if_loc , test , if_colon_loc , body , elifs , else_opt ) : stmt = ast . If ( orelse = [ ] , else_loc = None , else_colon_loc = None ) if else_opt : stmt . else_loc , stmt . else_colon_loc , stmt . orelse = else_opt for elif_ in reversed ( elifs ) : stmt . keyword_loc , stmt . test , stmt . if_colon_loc , stmt . body = elif_ stmt . loc = stmt . keyword_loc . join ( stmt . body [ - 1 ] . loc ) if stmt . orelse : stmt . loc = stmt . loc . join ( stmt . orelse [ - 1 ] . loc ) stmt = ast . If ( orelse = [ stmt ] , else_loc = None , else_colon_loc = None ) stmt . keyword_loc , stmt . test , stmt . if_colon_loc , stmt . body = if_loc , test , if_colon_loc , body stmt . loc = stmt . keyword_loc . join ( stmt . body [ - 1 ] . loc ) if stmt . orelse : stmt . loc = stmt . loc . join ( stmt . orelse [ - 1 ] . loc ) return stmt",if_stmt : if test : suite ( elif test : suite ) * [ else : suite ]
"def while_stmt ( self , while_loc , test , while_colon_loc , body , else_opt ) : stmt = ast . While ( test = test , body = body , orelse = [ ] , keyword_loc = while_loc , while_colon_loc = while_colon_loc , else_loc = None , else_colon_loc = None , loc = while_loc . join ( body [ - 1 ] . loc ) ) if else_opt : stmt . else_loc , stmt . else_colon_loc , stmt . orelse = else_opt stmt . loc = stmt . loc . join ( stmt . orelse [ - 1 ] . loc ) return stmt",while_stmt : while test : suite [ else : suite ]
"def for_stmt ( self , for_loc , target , in_loc , iter , for_colon_loc , body , else_opt ) : stmt = ast . For ( target = self . _assignable ( target ) , iter = iter , body = body , orelse = [ ] , keyword_loc = for_loc , in_loc = in_loc , for_colon_loc = for_colon_loc , else_loc = None , else_colon_loc = None , loc = for_loc . join ( body [ - 1 ] . loc ) ) if else_opt : stmt . else_loc , stmt . else_colon_loc , stmt . orelse = else_opt stmt . loc = stmt . loc . join ( stmt . orelse [ - 1 ] . loc ) return stmt",for_stmt : for exprlist in testlist : suite [ else : suite ]
"def try_stmt ( self , try_loc , try_colon_loc , body , stmt ) : stmt . keyword_loc , stmt . try_colon_loc , stmt . body = try_loc , try_colon_loc , body stmt . loc = stmt . loc . join ( try_loc ) return stmt",try_stmt : ( try : suite (( except_clause : suite ) + [ else : suite ] [ finally : suite ] | finally : suite ))
"def with_stmt__26 ( self , with_loc , context , with_var , colon_loc , body ) : if with_var : as_loc , optional_vars = with_var item = ast . withitem ( context_expr = context , optional_vars = optional_vars , as_loc = as_loc , loc = context . loc . join ( optional_vars . loc ) ) else : item = ast . withitem ( context_expr = context , optional_vars = None , as_loc = None , loc = context . loc ) return ast . With ( items = [ item ] , body = body , keyword_loc = with_loc , colon_loc = colon_loc , loc = with_loc . join ( body [ - 1 ] . loc ) )",( 2 . 6 3 . 0 ) with_stmt : with test [ with_var ] : suite
"def with_stmt__27 ( self , with_loc , items , colon_loc , body ) : return ast . With ( items = items , body = body , keyword_loc = with_loc , colon_loc = colon_loc , loc = with_loc . join ( body [ - 1 ] . loc ) )",( 2 . 7 3 . 1 - ) with_stmt : with with_item ( with_item ) * : suite
"def with_item ( self , context , as_opt ) : if as_opt : as_loc , optional_vars = as_opt return ast . withitem ( context_expr = context , optional_vars = optional_vars , as_loc = as_loc , loc = context . loc . join ( optional_vars . loc ) ) else : return ast . withitem ( context_expr = context , optional_vars = None , as_loc = None , loc = context . loc )",( 2 . 7 3 . 1 - ) with_item : test [ as expr ]
"def except_clause ( self , except_loc , exc_opt ) : type_ = name = as_loc = name_loc = None loc = except_loc if exc_opt : type_ , name_opt = exc_opt loc = loc . join ( type_ . loc ) if name_opt : as_loc , name_tok , name_node = name_opt if name_tok : name = name_tok . value name_loc = name_tok . loc else : name = name_node name_loc = name_node . loc loc = loc . join ( name_loc ) return ast . ExceptHandler ( type = type_ , name = name , except_loc = except_loc , as_loc = as_loc , name_loc = name_loc , loc = loc )",( 2 . 6 2 . 7 ) except_clause : except [ test [ ( as | ) test ]] ( 3 . 0 - ) except_clause : except [ test [ as NAME ]]
"def old_lambdef ( self , lambda_loc , args_opt , colon_loc , body ) : if args_opt is None : args_opt = self . _arguments ( ) args_opt . loc = colon_loc . begin ( ) return ast . Lambda ( args = args_opt , body = body , lambda_loc = lambda_loc , colon_loc = colon_loc , loc = lambda_loc . join ( body . loc ) )",( 2 . 6 2 . 7 ) old_lambdef : lambda [ varargslist ] : old_test
"def comparison ( self , lhs , rhs ) : if len ( rhs ) > 0 : return ast . Compare ( left = lhs , ops = list ( map ( lambda x : x [ 0 ] , rhs ) ) , comparators = list ( map ( lambda x : x [ 1 ] , rhs ) ) , loc = lhs . loc . join ( rhs [ - 1 ] [ 1 ] . loc ) ) else : return lhs",( 2 . 6 2 . 7 ) comparison : expr ( comp_op expr ) * ( 3 . 0 3 . 1 ) comparison : star_expr ( comp_op star_expr ) * ( 3 . 2 - ) comparison : expr ( comp_op expr ) *
"def star_expr__30 ( self , star_opt , expr ) : if star_opt : return ast . Starred ( value = expr , ctx = None , star_loc = star_opt , loc = expr . loc . join ( star_opt ) ) return expr",( 3 . 0 3 . 1 ) star_expr : [ * ] expr
"def star_expr__32 ( self , star_loc , expr ) : return ast . Starred ( value = expr , ctx = None , star_loc = star_loc , loc = expr . loc . join ( star_loc ) )",( 3 . 0 - ) star_expr : * expr
"def power ( self , atom , trailers , factor_opt ) : for trailer in trailers : if isinstance ( trailer , ast . Attribute ) or isinstance ( trailer , ast . Subscript ) : trailer . value = atom elif isinstance ( trailer , ast . Call ) : trailer . func = atom trailer . loc = atom . loc . join ( trailer . loc ) atom = trailer if factor_opt : op_loc , factor = factor_opt return ast . BinOp ( left = atom , op = ast . Pow ( loc = op_loc ) , right = factor , loc = atom . loc . join ( factor . loc ) ) return atom",power : atom trailer * [ ** factor ]
"def subscriptlist ( self , subscripts ) : if len ( subscripts ) == 1 : return ast . Subscript ( slice = subscripts [ 0 ] , ctx = None , loc = None ) elif all ( [ isinstance ( x , ast . Index ) for x in subscripts ] ) : elts = [ x . value for x in subscripts ] loc = subscripts [ 0 ] . loc . join ( subscripts [ - 1 ] . loc ) index = ast . Index ( value = ast . Tuple ( elts = elts , ctx = None , begin_loc = None , end_loc = None , loc = loc ) , loc = loc ) return ast . Subscript ( slice = index , ctx = None , loc = None ) else : extslice = ast . ExtSlice ( dims = subscripts , loc = subscripts [ 0 ] . loc . join ( subscripts [ - 1 ] . loc ) ) return ast . Subscript ( slice = extslice , ctx = None , loc = None )",subscriptlist : subscript ( subscript ) * [ ]
"def dictmaker ( self , elts ) : return ast . Dict ( keys = list ( map ( lambda x : x [ 0 ] , elts ) ) , values = list ( map ( lambda x : x [ 2 ] , elts ) ) , colon_locs = list ( map ( lambda x : x [ 1 ] , elts ) ) , loc = None )",( 2 . 6 ) dictmaker : test : test ( test : test ) * [ ]
"def classdef__26 ( self , class_loc , name_tok , bases_opt , colon_loc , body ) : bases , lparen_loc , rparen_loc = [ ] , None , None if bases_opt : lparen_loc , bases , rparen_loc = bases_opt return ast . ClassDef ( name = name_tok . value , bases = bases , keywords = [ ] , starargs = None , kwargs = None , body = body , decorator_list = [ ] , at_locs = [ ] , keyword_loc = class_loc , lparen_loc = lparen_loc , star_loc = None , dstar_loc = None , rparen_loc = rparen_loc , name_loc = name_tok . loc , colon_loc = colon_loc , loc = class_loc . join ( body [ - 1 ] . loc ) )",( 2 . 6 2 . 7 ) classdef : class NAME [ ( [ testlist ] ) ] : suite
"def classdef__30 ( self , class_loc , name_tok , arglist_opt , colon_loc , body ) : arglist , lparen_loc , rparen_loc = [ ] , None , None bases , keywords , starargs , kwargs = [ ] , [ ] , None , None star_loc , dstar_loc = None , None if arglist_opt : lparen_loc , arglist , rparen_loc = arglist_opt bases , keywords , starargs , kwargs = arglist . args , arglist . keywords , arglist . starargs , arglist . kwargs star_loc , dstar_loc = arglist . star_loc , arglist . dstar_loc return ast . ClassDef ( name = name_tok . value , bases = bases , keywords = keywords , starargs = starargs , kwargs = kwargs , body = body , decorator_list = [ ] , at_locs = [ ] , keyword_loc = class_loc , lparen_loc = lparen_loc , star_loc = star_loc , dstar_loc = dstar_loc , rparen_loc = rparen_loc , name_loc = name_tok . loc , colon_loc = colon_loc , loc = class_loc . join ( body [ - 1 ] . loc ) )",( 3 . 0 ) classdef : class NAME [ ( [ testlist ] ) ] : suite
"def arglist ( self , args , call ) : for arg in args : if isinstance ( arg , ast . keyword ) : call . keywords . append ( arg ) elif len ( call . keywords ) > 0 : error = diagnostic . Diagnostic ( ""fatal"" , ""non-keyword arg after keyword arg"" , { } , arg . loc , [ call . keywords [ - 1 ] . loc ] ) self . diagnostic_engine . process ( error ) else : call . args . append ( arg ) return call",arglist : ( argument ) * ( argument [ ] | * test ( argument ) * [ ** test ] | ** test )
"def yield_expr__26 ( self , yield_loc , exprs ) : if exprs is not None : return ast . Yield ( value = exprs , yield_loc = yield_loc , loc = yield_loc . join ( exprs . loc ) ) else : return ast . Yield ( value = None , yield_loc = yield_loc , loc = yield_loc )",( 2 . 6 2 . 7 3 . 0 3 . 1 3 . 2 ) yield_expr : yield [ testlist ]
"def yield_expr__33 ( self , yield_loc , arg ) : if isinstance ( arg , ast . YieldFrom ) : arg . yield_loc = yield_loc arg . loc = arg . loc . join ( arg . yield_loc ) return arg elif arg is not None : return ast . Yield ( value = arg , yield_loc = yield_loc , loc = yield_loc . join ( arg . loc ) ) else : return ast . Yield ( value = None , yield_loc = yield_loc , loc = yield_loc )",( 3 . 3 - ) yield_expr : yield [ yield_arg ]
"def urlparse ( url , scheme = '' , allow_fragments = True ) : tuple = urlsplit ( url , scheme , allow_fragments ) scheme , netloc , url , query , fragment = tuple if scheme in uses_params and ';' in url : url , params = _splitparams ( url ) else : params = '' return ParseResult ( scheme , netloc , url , params , query , fragment )",Parse a URL into 6 components : <scheme > : // <netloc > / <path > ; <params > ?<query > #<fragment > Return a 6 - tuple : ( scheme netloc path params query fragment ) . Note that we don t break the components up in smaller bits ( e . g . netloc is a single string ) and we don t expand % escapes .
"def urlsplit ( url , scheme = '' , allow_fragments = True ) : allow_fragments = bool ( allow_fragments ) key = url , scheme , allow_fragments , type ( url ) , type ( scheme ) cached = _parse_cache . get ( key , None ) if cached : return cached if len ( _parse_cache ) >= MAX_CACHE_SIZE : clear_cache ( ) netloc = query = fragment = '' i = url . find ( ':' ) if i > 0 : if url [ : i ] == 'http' : scheme = url [ : i ] . lower ( ) url = url [ i + 1 : ] if url [ : 2 ] == '//' : netloc , url = _splitnetloc ( url , 2 ) if ( ( '[' in netloc and ']' not in netloc ) or ( ']' in netloc and '[' not in netloc ) ) : raise ValueError ( ""Invalid IPv6 URL"" ) if allow_fragments and '#' in url : url , fragment = url . split ( '#' , 1 ) if '?' in url : url , query = url . split ( '?' , 1 ) v = SplitResult ( scheme , netloc , url , query , fragment ) _parse_cache [ key ] = v return v for c in url [ : i ] : if c not in scheme_chars : break else : rest = url [ i + 1 : ] if not rest or any ( c not in '0123456789' for c in rest ) : scheme , url = url [ : i ] . lower ( ) , rest if url [ : 2 ] == '//' : netloc , url = _splitnetloc ( url , 2 ) if ( ( '[' in netloc and ']' not in netloc ) or ( ']' in netloc and '[' not in netloc ) ) : raise ValueError ( ""Invalid IPv6 URL"" ) if allow_fragments and '#' in url : url , fragment = url . split ( '#' , 1 ) if '?' in url : url , query = url . split ( '?' , 1 ) v = SplitResult ( scheme , netloc , url , query , fragment ) _parse_cache [ key ] = v return v",Parse a URL into 5 components : <scheme > : // <netloc > / <path > ?<query > #<fragment > Return a 5 - tuple : ( scheme netloc path query fragment ) . Note that we don t break the components up in smaller bits ( e . g . netloc is a single string ) and we don t expand % escapes .
"def urlunparse ( data ) : scheme , netloc , url , params , query , fragment = data if params : url = ""%s;%s"" % ( url , params ) return urlunsplit ( ( scheme , netloc , url , query , fragment ) )",Put a parsed URL back together again . This may result in a slightly different but equivalent URL if the URL that was parsed originally had redundant delimiters e . g . a ? with an empty query ( the draft states that these are equivalent ) .
"def urlunsplit ( data ) : scheme , netloc , url , query , fragment = data if netloc or ( scheme and scheme in uses_netloc and url [ : 2 ] != '//' ) : if url and url [ : 1 ] != '/' : url = '/' + url url = '//' + ( netloc or '' ) + url if scheme : url = scheme + ':' + url if query : url = url + '?' + query if fragment : url = url + '#' + fragment return url",Combine the elements of a tuple as returned by urlsplit () into a complete URL as a string . The data argument can be any five - item iterable . This may result in a slightly different but equivalent URL if the URL that was parsed originally had unnecessary delimiters ( for example a ? with an empty query ; the RFC states that these are equivalent ) .
"def urljoin ( base , url , allow_fragments = True ) : if not base : return url if not url : return base bscheme , bnetloc , bpath , bparams , bquery , bfragment = urlparse ( base , '' , allow_fragments ) scheme , netloc , path , params , query , fragment = urlparse ( url , bscheme , allow_fragments ) if scheme != bscheme or scheme not in uses_relative : return url if scheme in uses_netloc : if netloc : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) netloc = bnetloc if path [ : 1 ] == '/' : return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) if not path and not params : path = bpath params = bparams if not query : query = bquery return urlunparse ( ( scheme , netloc , path , params , query , fragment ) ) segments = bpath . split ( '/' ) [ : - 1 ] + path . split ( '/' ) if segments [ - 1 ] == '.' : segments [ - 1 ] = '' while '.' in segments : segments . remove ( '.' ) while 1 : i = 1 n = len ( segments ) - 1 while i < n : if ( segments [ i ] == '..' and segments [ i - 1 ] not in ( '' , '..' ) ) : del segments [ i - 1 : i + 1 ] break i = i + 1 else : break if segments == [ '' , '..' ] : segments [ - 1 ] = '' elif len ( segments ) >= 2 and segments [ - 1 ] == '..' : segments [ - 2 : ] = [ '' ] return urlunparse ( ( scheme , netloc , '/' . join ( segments ) , params , query , fragment ) )",Join a base URL and a possibly relative URL to form an absolute interpretation of the latter .
"def urldefrag ( url ) : if '#' in url : s , n , p , a , q , frag = urlparse ( url ) defrag = urlunparse ( ( s , n , p , a , q , '' ) ) return defrag , frag else : return url , ''",Removes any existing fragment from URL .
"def unquote ( s ) : if _is_unicode ( s ) : if '%' not in s : return s bits = _asciire . split ( s ) res = [ bits [ 0 ] ] append = res . append for i in range ( 1 , len ( bits ) , 2 ) : append ( unquote ( str ( bits [ i ] ) ) . decode ( 'latin1' ) ) append ( bits [ i + 1 ] ) return '' . join ( res ) bits = s . split ( '%' ) if len ( bits ) == 1 : return s res = [ bits [ 0 ] ] append = res . append for item in bits [ 1 : ] : try : append ( _hextochr [ item [ : 2 ] ] ) append ( item [ 2 : ] ) except KeyError : append ( '%' ) append ( item ) return '' . join ( res )",unquote ( abc%20def ) - > abc def .
"def parse_qs ( qs , keep_blank_values = 0 , strict_parsing = 0 ) : dict = { } for name , value in parse_qsl ( qs , keep_blank_values , strict_parsing ) : if name in dict : dict [ name ] . append ( value ) else : dict [ name ] = [ value ] return dict",Parse a query given as a string argument .
"def parse_qsl ( qs , keep_blank_values = 0 , strict_parsing = 0 ) : pairs = [ s2 for s1 in qs . split ( '&' ) for s2 in s1 . split ( ';' ) ] r = [ ] for name_value in pairs : if not name_value and not strict_parsing : continue nv = name_value . split ( '=' , 1 ) if len ( nv ) != 2 : if strict_parsing : raise ValueError , ""bad query field: %r"" % ( name_value , ) if keep_blank_values : nv . append ( '' ) else : continue if len ( nv [ 1 ] ) or keep_blank_values : name = unquote ( nv [ 0 ] . replace ( '+' , ' ' ) ) value = unquote ( nv [ 1 ] . replace ( '+' , ' ' ) ) r . append ( ( name , value ) ) return r",Parse a query given as a string argument .
"def _replace ( _self , * * kwds ) : result = _self . _make ( map ( kwds . pop , ( 'scheme' , 'netloc' , 'path' , 'query' , 'fragment' ) , _self ) ) if kwds : raise ValueError ( 'Got unexpected field names: %r' % kwds . keys ( ) ) return result",Return a new SplitResult object replacing specified fields with new values
"def getlines ( filename , module_globals = None ) : if filename in cache : return cache [ filename ] [ 2 ] try : return updatecache ( filename , module_globals ) except MemoryError : clearcache ( ) return [ ]",Get the lines for a file from the cache . Update the cache if it doesn t contain an entry for this file already .
"def checkcache ( filename = None ) : if filename is None : filenames = cache . keys ( ) else : if filename in cache : filenames = [ filename ] else : return for filename in filenames : size , mtime , lines , fullname = cache [ filename ] if mtime is None : continue try : stat = os . stat ( fullname ) except os . error : del cache [ filename ] continue if size != stat . st_size or mtime != stat . st_mtime : del cache [ filename ]",Discard cache entries that are out of date . ( This is not checked upon each call! )
"def updatecache ( filename , module_globals = None ) : if filename in cache : del cache [ filename ] if not filename or ( filename . startswith ( '<' ) and filename . endswith ( '>' ) ) : return [ ] fullname = filename try : stat = os . stat ( fullname ) except OSError : basename = filename if module_globals and '__loader__' in module_globals : name = module_globals . get ( '__name__' ) loader = module_globals [ '__loader__' ] get_source = getattr ( loader , 'get_source' , None ) if name and get_source : try : data = get_source ( name ) except ( ImportError , IOError ) : pass else : if data is None : return [ ] cache [ filename ] = ( len ( data ) , None , [ line + '\n' for line in data . splitlines ( ) ] , fullname ) return cache [ filename ] [ 2 ] if os . path . isabs ( filename ) : return [ ] for dirname in sys . path : try : fullname = os . path . join ( dirname , basename ) except ( TypeError , AttributeError ) : continue try : stat = os . stat ( fullname ) break except os . error : pass else : return [ ] try : with open ( fullname , 'rU' ) as fp : lines = fp . readlines ( ) except IOError : return [ ] if lines and not lines [ - 1 ] . endswith ( '\n' ) : lines [ - 1 ] += '\n' size , mtime = stat . st_size , stat . st_mtime cache [ filename ] = size , mtime , lines , fullname return lines",Update a cache entry and return its list of lines . If something s wrong print a message discard the cache entry and return an empty list .
def isfile ( path ) : try : st = os . stat ( path ) except os . error : return False return stat . S_ISREG ( st . st_mode ),Test whether a path is a regular file
def isdir ( s ) : try : st = os . stat ( s ) except os . error : return False return stat . S_ISDIR ( st . st_mode ),Return true if the pathname refers to an existing directory .
"def commonprefix ( m ) : if not m : return '' s1 = min ( m ) s2 = max ( m ) for i , c in enumerate ( s1 ) : if c != s2 [ i ] : return s1 [ : i ] return s1",Given a list of pathnames returns the longest common leading component
"def _splitext ( p , sep , altsep , extsep ) : sepIndex = p . rfind ( sep ) if altsep : altsepIndex = p . rfind ( altsep ) sepIndex = max ( sepIndex , altsepIndex ) dotIndex = p . rfind ( extsep ) if dotIndex > sepIndex : filenameIndex = sepIndex + 1 while filenameIndex < dotIndex : if p [ filenameIndex ] != extsep : return p [ : dotIndex ] , p [ dotIndex : ] filenameIndex += 1 return p , ''",Split the extension from a pathname .
"def wrap ( text , width = 70 , * * kwargs ) : w = TextWrapper ( width = width , * * kwargs ) return w . wrap ( text )",Wrap a single paragraph of text returning a list of wrapped lines .
"def fill ( text , width = 70 , * * kwargs ) : w = TextWrapper ( width = width , * * kwargs ) return w . fill ( text )",Fill a single paragraph of text returning a new string .
"def dedent ( text ) : margin = None text = _whitespace_only_re . sub ( '' , text ) indents = _leading_whitespace_re . findall ( text ) for indent in indents : if margin is None : margin = indent elif indent . startswith ( margin ) : pass elif margin . startswith ( indent ) : margin = indent else : for i , ( x , y ) in enumerate ( zip ( margin , indent ) ) : if x != y : margin = margin [ : i ] break else : margin = margin [ : len ( indent ) ] if 0 and margin : for line in text . split ( ""\n"" ) : assert not line or line . startswith ( margin ) , ""line = %r, margin = %r"" % ( line , margin ) if margin : text = re . sub ( r'(?m)^' + margin , '' , text ) return text",Remove any common leading whitespace from every line in text .
"def _munge_whitespace ( self , text ) : if self . expand_tabs : text = ' ' . join ( ( ' ' . join ( text . split ( '\n' ) ) ) . split ( '\t' ) ) if self . replace_whitespace : text = ' ' . join ( ' ' . join ( text . split ( '\n' ) ) . split ( '\t' ) ) return text",_munge_whitespace ( text : string ) - > string
"def _split ( self , text ) : if isinstance ( text , _unicode ) : if self . break_on_hyphens : pat = self . wordsep_re_uni else : pat = self . wordsep_simple_re_uni else : if self . break_on_hyphens : pat = self . wordsep_re else : pat = self . wordsep_simple_re chunks = pat . split ( text ) chunks = [ x for x in chunks if x is not None ] return chunks",_split ( text : string ) - > [ string ]
"def _fix_sentence_endings ( self , chunks ) : i = 0 patsearch = self . sentence_end_re . search while i < len ( chunks ) - 1 : if chunks [ i + 1 ] == "" "" and patsearch ( chunks [ i ] ) : chunks [ i + 1 ] = ""  "" i += 2 else : i += 1",_fix_sentence_endings ( chunks : [ string ] )
"def _handle_long_word ( self , reversed_chunks , cur_line , cur_len , width ) : if width < 1 : space_left = 1 else : space_left = width - cur_len if self . break_long_words : cur_line . append ( reversed_chunks [ - 1 ] [ : space_left ] ) reversed_chunks [ - 1 ] = reversed_chunks [ - 1 ] [ space_left : ] elif not cur_line : cur_line . append ( reversed_chunks . pop ( ) )",_handle_long_word ( chunks : [ string ] cur_line : [ string ] cur_len : int width : int )
"def _wrap_chunks ( self , chunks ) : lines = [ ] if self . width <= 0 : raise ValueError ( ""invalid width %r (must be > 0)"" % self . width ) chunks . reverse ( ) while chunks : cur_line = [ ] cur_len = 0 if lines : indent = self . subsequent_indent else : indent = self . initial_indent width = self . width - len ( indent ) if self . drop_whitespace and chunks [ - 1 ] . strip ( ) == '' and lines : chunks . pop ( ) while chunks : l = len ( chunks [ - 1 ] ) if cur_len + l <= width : cur_line . append ( chunks . pop ( ) ) cur_len += l else : break if chunks and len ( chunks [ - 1 ] ) > width : self . _handle_long_word ( chunks , cur_line , cur_len , width ) if self . drop_whitespace and cur_line and cur_line [ - 1 ] . strip ( ) == '' : cur_line . pop ( ) if cur_line : lines . append ( indent + '' . join ( cur_line ) ) return lines",_wrap_chunks ( chunks : [ string ] ) - > [ string ]
"def wrap ( self , text ) : text = self . _munge_whitespace ( text ) chunks = self . _split ( text ) if self . fix_sentence_endings : self . _fix_sentence_endings ( chunks ) return self . _wrap_chunks ( chunks )",wrap ( text : string ) - > [ string ]
"def _long2bytesBigEndian ( n , blocksize = 0 ) : s = b'' pack = struct . pack while n > 0 : s = pack ( '>I' , n & 0xffffffff ) + s n = n >> 32 for i in range ( len ( s ) ) : if s [ i ] != '\000' : break else : s = '\000' i = 0 s = s [ i : ] if blocksize > 0 and len ( s ) % blocksize : s = ( blocksize - len ( s ) % blocksize ) * '\000' + s return s",Convert a long integer to a byte string .
def _bytelist2longBigEndian ( list ) : imax = len ( list ) // 4 hl = [ 0 ] * imax j = 0 i = 0 while i < imax : b0 = ord ( list [ j ] ) << 24 b1 = ord ( list [ j + 1 ] ) << 16 b2 = ord ( list [ j + 2 ] ) << 8 b3 = ord ( list [ j + 3 ] ) hl [ i ] = b0 | b1 | b2 | b3 i = i + 1 j = j + 4 return hl,Transform a list of characters into a list of longs .
def init ( self ) : self . length = 0 self . input = [ ] self . H0 = 0x67452301 self . H1 = 0xEFCDAB89 self . H2 = 0x98BADCFE self . H3 = 0x10325476 self . H4 = 0xC3D2E1F0,Initialize the message - digest and set all fields to zero .
"def _transform ( self , W ) : for t in range ( 16 , 80 ) : W . append ( _rotateLeft ( W [ t - 3 ] ^ W [ t - 8 ] ^ W [ t - 14 ] ^ W [ t - 16 ] , 1 ) & 0xffffffff ) A = self . H0 B = self . H1 C = self . H2 D = self . H3 E = self . H4 for t in range ( 0 , 20 ) : TEMP = _rotateLeft ( A , 5 ) + ( ( B & C ) | ( ( ~ B ) & D ) ) + E + W [ t ] + K [ 0 ] E = D D = C C = _rotateLeft ( B , 30 ) & 0xffffffff B = A A = TEMP & 0xffffffff for t in range ( 20 , 40 ) : TEMP = _rotateLeft ( A , 5 ) + ( B ^ C ^ D ) + E + W [ t ] + K [ 1 ] E = D D = C C = _rotateLeft ( B , 30 ) & 0xffffffff B = A A = TEMP & 0xffffffff for t in range ( 40 , 60 ) : TEMP = _rotateLeft ( A , 5 ) + ( ( B & C ) | ( B & D ) | ( C & D ) ) + E + W [ t ] + K [ 2 ] E = D D = C C = _rotateLeft ( B , 30 ) & 0xffffffff B = A A = TEMP & 0xffffffff for t in range ( 60 , 80 ) : TEMP = _rotateLeft ( A , 5 ) + ( B ^ C ^ D ) + E + W [ t ] + K [ 3 ] E = D D = C C = _rotateLeft ( B , 30 ) & 0xffffffff B = A A = TEMP & 0xffffffff self . H0 = ( self . H0 + A ) & 0xffffffff self . H1 = ( self . H1 + B ) & 0xffffffff self . H2 = ( self . H2 + C ) & 0xffffffff self . H3 = ( self . H3 + D ) & 0xffffffff self . H4 = ( self . H4 + E ) & 0xffffffff",This loop was unrolled to gain about 10% in speed for t in range ( 0 80 ) : TEMP = _rotateLeft ( A 5 ) + f [ t / 20 ] + E + W [ t ] + K [ t / 20 ] E = D D = C C = _rotateLeft ( B 30 ) & 0xffffffff B = A A = TEMP & 0xffffffff
"def digest ( self ) : H0 = self . H0 H1 = self . H1 H2 = self . H2 H3 = self . H3 H4 = self . H4 input = [ ] + self . input count = [ ] + self . count index = ( self . count [ 1 ] >> 3 ) & 0x3f if index < 56 : padLen = 56 - index else : padLen = 120 - index padding = [ '\200' ] + [ '\000' ] * 63 self . update ( padding [ : padLen ] ) bits = _bytelist2longBigEndian ( self . input [ : 56 ] ) + count self . _transform ( bits ) digest = _long2bytesBigEndian ( self . H0 , 4 ) + _long2bytesBigEndian ( self . H1 , 4 ) + _long2bytesBigEndian ( self . H2 , 4 ) + _long2bytesBigEndian ( self . H3 , 4 ) + _long2bytesBigEndian ( self . H4 , 4 ) self . H0 = H0 self . H1 = H1 self . H2 = H2 self . H3 = H3 self . H4 = H4 self . input = input self . count = count return digest",Terminate the message - digest computation and return digest .
"def next ( self , eof_token = False ) : if len ( self . queue ) == 0 : self . _refill ( eof_token ) return self . queue . pop ( 0 )",Returns token at offset as a : class : Token and advances offset to point past the end of the token where the token has :
"def peek ( self , eof_token = False ) : if len ( self . queue ) == 0 : self . _refill ( eof_token ) return self . queue [ - 1 ]",Same as : meth : next except the token is not dequeued .
"def enterabs ( self , time , priority , action , argument ) : event = Event ( time , priority , action , argument ) heapq . heappush ( self . _queue , event ) return event",Enter a new event in the queue at an absolute time . Returns an ID for the event which can be used to remove it if necessary .
"def run ( self ) : q = self . _queue delayfunc = self . delayfunc timefunc = self . timefunc pop = heapq . heappop while q : checked_event = q [ 0 ] time , priority , action , argument = checked_event . get_fields ( ) now = timefunc ( ) if now < time : delayfunc ( time - now ) else : event = pop ( q ) if event is checked_event : action ( * argument ) delayfunc ( 0 ) else : heapq . heappush ( q , event )",Execute events until the queue is empty . When there is a positive delay until the first event the delay function is called and the event is left in the queue ; otherwise the event is removed from the queue and executed ( its action function is called passing it the argument ) . If the delay function returns prematurely it is simply restarted . It is legal for both the delay function and the action function to modify the queue or to raise an exception ; exceptions are not caught but the scheduler s state remains well - defined so run () may be called again . A questionable hack is added to allow other threads to run : just after an event is executed a delay of 0 is executed to avoid monopolizing the CPU when other threads are also runnable .
"def queue ( self ) : events = self . _queue [ : ] return map ( heapq . heappop , [ events ] * len ( events ) )",An ordered list of upcoming events . Events are named tuples with fields for : time priority action arguments
"def register_dialect ( name , dialect = None , * * kwargs ) : if not isinstance ( name , basestring ) : raise TypeError ( ""dialect name must be a string or unicode"" ) dialect = _call_dialect ( dialect , kwargs ) _dialects [ name ] = dialect",Create a mapping from a string name to a dialect class . dialect = csv . register_dialect ( name dialect )
"def field_size_limit ( limit = undefined ) : global _field_limit old_limit = _field_limit if limit is not undefined : if not isinstance ( limit , ( int , long ) ) : raise TypeError ( ""int expected, got %s"" % ( limit . __class__ . __name__ , ) ) _field_limit = limit return old_limit",Sets an upper limit on parsed fields . csv . field_size_limit ( [ limit ] )
"def copy ( x ) : cls = type ( x ) copier = _copy_dispatch . get ( cls ) if copier : return copier ( x ) copier = getattr ( cls , ""__copy__"" , None ) if copier : return copier ( x ) reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , ""__reduce_ex__"" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , ""__reduce__"" , None ) if reductor : rv = reductor ( ) else : raise Error ( ""un(shallow)copyable object of type %s"" % cls ) return _reconstruct ( x , rv , 0 )",Shallow copy operation on arbitrary Python objects .
"def deepcopy ( x , memo = None , _nil = [ ] ) : if memo is None : memo = { } d = id ( x ) y = memo . get ( d , _nil ) if y is not _nil : return y cls = type ( x ) copier = _deepcopy_dispatch . get ( cls ) if copier : y = copier ( x , memo ) else : try : issc = issubclass ( cls , type ) except TypeError : issc = 0 if issc : y = _deepcopy_atomic ( x , memo ) else : copier = getattr ( x , ""__deepcopy__"" , None ) if copier : y = copier ( memo ) else : reductor = dispatch_table . get ( cls ) if reductor : rv = reductor ( x ) else : reductor = getattr ( x , ""__reduce_ex__"" , None ) if reductor : rv = reductor ( 2 ) else : reductor = getattr ( x , ""__reduce__"" , None ) if reductor : rv = reductor ( ) else : raise Error ( ""un(deep)copyable object of type %s"" % cls ) y = _reconstruct ( x , rv , 1 , memo ) memo [ d ] = y _keep_alive ( x , memo ) return y",Deep copy operation on arbitrary Python objects .
"def _keep_alive ( x , memo ) : try : memo [ id ( memo ) ] . append ( x ) except KeyError : memo [ id ( memo ) ] = [ x ]",Keeps a reference to the object x in the memo .
"def warnpy3k ( message , category = None , stacklevel = 1 ) : if sys . py3kwarning : if category is None : category = DeprecationWarning warn ( message , category , stacklevel + 1 )",Issue a deprecation warning for Python 3 . x related changes .
"def _show_warning ( message , category , filename , lineno , file = None , line = None ) : if file is None : file = sys . stderr if file is None : return try : file . write ( formatwarning ( message , category , filename , lineno , line ) ) except ( IOError , UnicodeError ) : pass",Hook to write a warning to a file ; replace if you like .
"def formatwarning ( message , category , filename , lineno , line = None ) : try : unicodetype = unicode except NameError : unicodetype = ( ) try : message = str ( message ) except UnicodeEncodeError : pass s = ""%s: %s: %s\n"" % ( lineno , category . __name__ , message ) line = linecache . getline ( filename , lineno ) if line is None else line if line : line = line . strip ( ) if isinstance ( s , unicodetype ) and isinstance ( line , str ) : line = unicode ( line , 'latin1' ) s += ""  %s\n"" % line if isinstance ( s , unicodetype ) and isinstance ( filename , str ) : enc = sys . getfilesystemencoding ( ) if enc : try : filename = unicode ( filename , enc ) except UnicodeDecodeError : pass s = ""%s:%s"" % ( filename , s ) return s",Function to format a warning the standard way .
"def filterwarnings ( action , message = """" , category = Warning , module = """" , lineno = 0 , append = 0 ) : assert action in ( ""error"" , ""ignore"" , ""always"" , ""default"" , ""module"" , ""once"" ) , ""invalid action: %r"" % ( action , ) assert isinstance ( message , basestring ) , ""message must be a string"" assert isinstance ( category , type ) , ""category must be a class"" assert issubclass ( category , Warning ) , ""category must be a Warning subclass"" assert isinstance ( module , basestring ) , ""module must be a string"" assert isinstance ( lineno , int ) and lineno >= 0 , ""lineno must be an int >= 0"" item = ( action , re . compile ( message , re . I ) , category , re . compile ( module ) , lineno ) if append : filters . append ( item ) else : filters . insert ( 0 , item )",Insert an entry into the list of warnings filters ( at the front ) .
"def simplefilter ( action , category = Warning , lineno = 0 , append = 0 ) : assert action in ( ""error"" , ""ignore"" , ""always"" , ""default"" , ""module"" , ""once"" ) , ""invalid action: %r"" % ( action , ) assert isinstance ( lineno , int ) and lineno >= 0 , ""lineno must be an int >= 0"" item = ( action , None , category , None , lineno ) if append : filters . append ( item ) else : filters . insert ( 0 , item )",Insert a simple entry into the list of warnings filters ( at the front ) .
"def warn ( message , category = None , stacklevel = 1 ) : if isinstance ( message , Warning ) : category = message . __class__ if category is None : category = UserWarning assert issubclass ( category , Warning ) try : caller = sys . _getframe ( stacklevel ) except ValueError : globals = sys . __dict__ lineno = 1 else : globals = caller . f_globals lineno = caller . f_lineno if '__name__' in globals : module = globals [ '__name__' ] else : module = ""<string>"" filename = globals . get ( '__file__' ) if filename : fnl = filename . lower ( ) if fnl . endswith ( ( "".pyc"" , "".pyo"" ) ) : filename = filename [ : - 1 ] else : if module == ""__main__"" : try : filename = sys . argv [ 0 ] except AttributeError : filename = '__main__' if not filename : filename = module registry = globals . setdefault ( ""__warningregistry__"" , { } ) warn_explicit ( message , category , filename , lineno , module , registry , globals )",Issue a warning or maybe ignore it or raise an exception .
def _hash ( self ) : MAX = sys . maxint MASK = 2 * MAX + 1 n = len ( self ) h = 1927868237 * ( n + 1 ) h &= MASK for x in self : hx = hash ( x ) h ^= ( hx ^ ( hx << 16 ) ^ 89869747 ) * 3644798167 h &= MASK h = h * 69069 + 907133923 h &= MASK if h > MAX : h -= MASK + 1 if h == - 1 : h = 590923713 return h,Compute the hash value of a set .
"def remove ( self , value ) : if value not in self : raise KeyError ( value ) self . discard ( value )",Remove an element . If not a member raise a KeyError .
def pop ( self ) : it = iter ( self ) try : value = next ( it ) except StopIteration : raise KeyError self . discard ( value ) return value,Return the popped value . Raise KeyError if empty .
"def update ( * args , * * kwds ) : if not args : raise TypeError ( ""descriptor 'update' of 'MutableMapping' object "" ""needs an argument"" ) self = args [ 0 ] args = args [ 1 : ] if len ( args ) > 1 : raise TypeError ( 'update expected at most 1 arguments, got %d' % len ( args ) ) if args : other = args [ 0 ] if isinstance ( other , Mapping ) : for key in other : self [ key ] = other [ key ] elif hasattr ( other , ""keys"" ) : for key in other . keys ( ) : self [ key ] = other [ key ] else : for key , value in other : self [ key ] = value for key , value in kwds . items ( ) : self [ key ] = value",D . update ( [ E ] ** F ) - > None . Update D from mapping / iterable E and F . If E present and has a . keys () method does : for k in E : D [ k ] = E [ k ] If E present and lacks . keys () method does : for ( k v ) in E : D [ k ] = v In either case this is followed by : for k v in F . items () : D [ k ] = v
"def index ( self , value ) : for i , v in enumerate ( self ) : if v == value : return i raise ValueError",S . index ( value ) - > integer -- return first index of value . Raises ValueError if the value is not present .
"def reverse ( self ) : n = len ( self ) for i in range ( n // 2 ) : self [ i ] , self [ n - i - 1 ] = self [ n - i - 1 ] , self [ i ]",S . reverse () -- reverse * IN PLACE *
"def go_str ( value ) : io = StringIO . StringIO ( ) io . write ( '""' ) for c in value : if c in _ESCAPES : io . write ( _ESCAPES [ c ] ) elif c in _SIMPLE_CHARS : io . write ( c ) else : io . write ( r'\x{:02x}' . format ( ord ( c ) ) ) io . write ( '""' ) return io . getvalue ( )",Returns value as a valid Go string literal .
"def write_block ( self , block_ , body ) : self . write ( 'for ; πF.State() >= 0; πF.PopCheckpoint() {') with self . indent_block ( ) : self . write ( 'switch πF.State() {') self . write ( 'case 0:' ) for checkpoint in block_ . checkpoints : self . write_tmpl ( 'case $state: goto Label$state' , state = checkpoint ) self . write ( 'default: panic(""unexpected function state"")' ) self . write ( '}' ) with self . indent_block ( - 1 ) : self . write ( body ) self . write ( '}' )",Outputs the boilerplate necessary for code blocks like functions .
"def acquire ( self , blocking = 1 ) : me = _get_ident ( ) if self . __owner == me : self . __count = self . __count + 1 if __debug__ : self . _note ( ""%s.acquire(%s): recursive success"" , self , blocking ) return 1 rc = self . __block . acquire ( blocking ) if rc : self . __owner = me self . __count = 1 if __debug__ : self . _note ( ""%s.acquire(%s): initial success"" , self , blocking ) else : if __debug__ : self . _note ( ""%s.acquire(%s): failure"" , self , blocking ) return rc",Acquire a lock blocking or non - blocking .
"def release ( self ) : if self . __owner != _get_ident ( ) : raise RuntimeError ( ""cannot release un-acquired lock"" ) self . __count = count = self . __count - 1 if not count : self . __owner = None self . __block . release ( ) if __debug__ : self . _note ( ""%s.release(): final release"" , self ) else : if __debug__ : self . _note ( ""%s.release(): non-final release"" , self )",Release a lock decrementing the recursion level .
"def wait ( self , timeout = None ) : if not self . _is_owned ( ) : raise RuntimeError ( ""cannot wait on un-acquired lock"" ) waiter = _allocate_lock ( ) waiter . acquire ( ) self . __waiters . append ( waiter ) saved_state = self . _release_save ( ) try : if timeout is None : waiter . acquire ( ) if __debug__ : self . _note ( ""%s.wait(): got it"" , self ) else : endtime = _time ( ) + timeout delay = 0.0005 while True : gotit = waiter . acquire ( 0 ) if gotit : break remaining = endtime - _time ( ) if remaining <= 0 : break delay = min ( delay * 2 , remaining , .05 ) _sleep ( delay ) if not gotit : if __debug__ : self . _note ( ""%s.wait(%s): timed out"" , self , timeout ) try : self . __waiters . remove ( waiter ) except ValueError : pass else : if __debug__ : self . _note ( ""%s.wait(%s): got it"" , self , timeout ) finally : self . _acquire_restore ( saved_state )",Wait until notified or until a timeout occurs .
"def notify ( self , n = 1 ) : if not self . _is_owned ( ) : raise RuntimeError ( ""cannot notify on un-acquired lock"" ) __waiters = self . __waiters waiters = __waiters [ : n ] if not waiters : if __debug__ : self . _note ( ""%s.notify(): no waiters"" , self ) return self . _note ( ""%s.notify(): notifying %d waiter%s"" , self , n , n != 1 and ""s"" or """" ) for waiter in waiters : waiter . release ( ) try : __waiters . remove ( waiter ) except ValueError : pass",Wake up one or more threads waiting on this condition if any .
"def acquire ( self , blocking = 1 ) : rc = False with self . __cond : while self . __value == 0 : if not blocking : break if __debug__ : self . _note ( ""%s.acquire(%s): blocked waiting, value=%s"" , self , blocking , self . __value ) self . __cond . wait ( ) else : self . __value = self . __value - 1 if __debug__ : self . _note ( ""%s.acquire: success, value=%s"" , self , self . __value ) rc = True return rc",Acquire a semaphore decrementing the internal counter by one .
"def release ( self ) : with self . __cond : self . __value = self . __value + 1 if __debug__ : self . _note ( ""%s.release: success, value=%s"" , self , self . __value ) self . __cond . notify ( )",Release a semaphore incrementing the internal counter by one .
"def release ( self ) : with self . __cond : if self . __value >= self . _initial_value : raise ValueError ( ""Semaphore released too many times"" ) self . __value += 1 self . __cond . notify ( )",Release a semaphore incrementing the internal counter by one .
def set ( self ) : with self . __cond : self . __flag = True self . __cond . notify_all ( ),Set the internal flag to true .
"def wait ( self , timeout = None ) : with self . __cond : if not self . __flag : self . __cond . wait ( timeout ) return self . __flag",Block until the internal flag is true .
"def start ( self ) : if not self . __initialized : raise RuntimeError ( ""thread.__init__() not called"" ) if self . __started . is_set ( ) : raise RuntimeError ( ""threads can only be started once"" ) if __debug__ : self . _note ( ""%s.start(): starting thread"" , self ) with _active_limbo_lock : _limbo [ self ] = self try : _start_new_thread ( self . __bootstrap , ( ) ) except Exception : with _active_limbo_lock : del _limbo [ self ] raise self . __started . wait ( )",Start the thread s activity .
"def run ( self ) : try : if self . __target : self . __target ( * self . __args , * * self . __kwargs ) finally : del self . __target , self . __args , self . __kwargs",Method representing the thread s activity .
"def join ( self , timeout = None ) : if not self . __initialized : raise RuntimeError ( ""Thread.__init__() not called"" ) if not self . __started . is_set ( ) : raise RuntimeError ( ""cannot join thread before it is started"" ) if self is current_thread ( ) : raise RuntimeError ( ""cannot join current thread"" ) if __debug__ : if not self . __stopped : self . _note ( ""%s.join(): waiting until thread stops"" , self ) self . __block . acquire ( ) try : if timeout is None : while not self . __stopped : self . __block . wait ( ) if __debug__ : self . _note ( ""%s.join(): thread stopped"" , self ) else : deadline = _time ( ) + timeout while not self . __stopped : delay = deadline - _time ( ) if delay <= 0 : if __debug__ : self . _note ( ""%s.join(): timed out"" , self ) break self . __block . wait ( delay ) else : if __debug__ : self . _note ( ""%s.join(): thread stopped"" , self ) finally : self . __block . release ( )",Wait until the thread terminates .
"def _dump_registry ( cls , file = None ) : print >> file , ""Class: %s.%s"" % ( cls . __module__ , cls . __name__ ) print >> file , ""Inv.counter: %s"" % ABCMeta . _abc_invalidation_counter for name in sorted ( cls . __dict__ . keys ( ) ) : if name . startswith ( ""_abc_"" ) : value = getattr ( cls , name ) print >> file , ""%s: %r"" % ( name , value )",Debug helper to print the ABC registry .
"def b2a_qp ( data , quotetabs = False , istext = True , header = False ) : MAXLINESIZE = 76 lf = data . find ( '\n' ) crlf = lf > 0 and data [ lf - 1 ] == '\r' inp = 0 linelen = 0 odata = [ ] while inp < len ( data ) : c = data [ inp ] if ( c > '~' or c == '=' or ( header and c == '_' ) or ( c == '.' and linelen == 0 and ( inp + 1 == len ( data ) or data [ inp + 1 ] == '\n' or data [ inp + 1 ] == '\r' ) ) or ( not istext and ( c == '\r' or c == '\n' ) ) or ( ( c == '\t' or c == ' ' ) and ( inp + 1 == len ( data ) ) ) or ( c <= ' ' and c != '\r' and c != '\n' and ( quotetabs or ( not quotetabs and ( c != '\t' and c != ' ' ) ) ) ) ) : linelen += 3 if linelen >= MAXLINESIZE : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 3 odata . append ( '=' + two_hex_digits ( ord ( c ) ) ) inp += 1 else : if ( istext and ( c == '\n' or ( inp + 1 < len ( data ) and c == '\r' and data [ inp + 1 ] == '\n' ) ) ) : linelen = 0 if ( len ( odata ) > 0 and ( odata [ - 1 ] == ' ' or odata [ - 1 ] == '\t' ) ) : ch = ord ( odata [ - 1 ] ) odata [ - 1 ] = '=' odata . append ( two_hex_digits ( ch ) ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) if c == '\r' : inp += 2 else : inp += 1 else : if ( inp + 1 < len ( data ) and data [ inp + 1 ] != '\n' and ( linelen + 1 ) >= MAXLINESIZE ) : odata . append ( '=' ) if crlf : odata . append ( '\r' ) odata . append ( '\n' ) linelen = 0 linelen += 1 if header and c == ' ' : c = '_' odata . append ( c ) inp += 1 return '' . join ( odata )",quotetabs = True means that tab and space characters are always quoted . istext = False means that \ r and \ n are treated as regular characters header = True encodes space characters with _ and requires real _ characters to be quoted .
"def rlecode_hqx ( s ) : if not s : return '' result = [ ] prev = s [ 0 ] count = 1 if s [ - 1 ] == '!' : s = s [ 1 : ] + '?' else : s = s [ 1 : ] + '!' for c in s : if c == prev and count < 255 : count += 1 else : if count == 1 : if prev != '\x90' : result . append ( prev ) else : result += [ '\x90' , '\x00' ] elif count < 4 : if prev != '\x90' : result += [ prev ] * count else : result += [ '\x90' , '\x00' ] * count else : if prev != '\x90' : result += [ prev , '\x90' , chr ( count ) ] else : result += [ '\x90' , '\x00' , '\x90' , chr ( count ) ] count = 1 prev = c return '' . join ( result )",Run length encoding for binhex4 . The CPython implementation does not do run length encoding of \ x90 characters . This implementation does .
"def _match_abbrev ( s , wordmap ) : if s in wordmap : return s else : possibilities = [ word for word in wordmap . keys ( ) if word . startswith ( s ) ] if len ( possibilities ) == 1 : return possibilities [ 0 ] elif not possibilities : raise BadOptionError ( s ) else : possibilities . sort ( ) raise AmbiguousOptionError ( s , possibilities )",_match_abbrev ( s : string wordmap : { string : Option } ) - > string
"def _format_text ( self , text ) : text_width = max ( self . width - self . current_indent , 11 ) indent = "" "" * self . current_indent return textwrap . fill ( text , text_width , initial_indent = indent , subsequent_indent = indent )",Format a paragraph of free - form text for inclusion in the help output at the current indentation level .
"def format_option_strings ( self , option ) : if option . takes_value ( ) : metavar = option . metavar or option . dest . upper ( ) short_opts = [ self . _short_opt_fmt % ( sopt , metavar ) for sopt in option . _short_opts ] long_opts = [ self . _long_opt_fmt % ( lopt , metavar ) for lopt in option . _long_opts ] else : short_opts = option . _short_opts long_opts = option . _long_opts if self . short_first : opts = short_opts + long_opts else : opts = long_opts + short_opts return "", "" . join ( opts )",Return a comma - separated list of option strings & metavariables .
"def _update_careful ( self , dict ) : for attr in dir ( self ) : if attr in dict : dval = dict [ attr ] if dval is not None : setattr ( self , attr , dval )",Update the option values from an arbitrary dictionary but only use keys from dict that already have a corresponding attribute in self . Any keys in dict without a corresponding attribute are silently ignored .
"def add_option ( self , * args , * * kwargs ) : if type ( args [ 0 ] ) in types . StringTypes : option = self . option_class ( * args , * * kwargs ) elif len ( args ) == 1 and not kwargs : option = args [ 0 ] if not isinstance ( option , Option ) : raise TypeError , ""not an Option instance: %r"" % option else : raise TypeError , ""invalid arguments"" self . _check_conflict ( option ) self . option_list . append ( option ) option . container = self for opt in option . _short_opts : self . _short_opt [ opt ] = option for opt in option . _long_opts : self . _long_opt [ opt ] = option if option . dest is not None : if option . default is not NO_DEFAULT : self . defaults [ option . dest ] = option . default elif option . dest not in self . defaults : self . defaults [ option . dest ] = None return option",add_option ( Option ) add_option ( opt_str ... kwarg = val ... )
def destroy ( self ) : OptionContainer . destroy ( self ) for group in self . option_groups : group . destroy ( ) del self . option_list del self . option_groups del self . formatter,Declare that you are done with this OptionParser . This cleans up reference cycles so the OptionParser ( and all objects referenced by it ) can be garbage - collected promptly . After calling destroy () the OptionParser is unusable .
"def parse_args ( self , args = None , values = None ) : rargs = self . _get_args ( args ) if values is None : values = self . get_default_values ( ) self . rargs = rargs self . largs = largs = [ ] self . values = values try : stop = self . _process_args ( largs , rargs , values ) except ( BadOptionError , OptionValueError ) , err : self . error ( str ( err ) ) args = largs + rargs return self . check_values ( values , args )",parse_args ( args : [ string ] = sys . argv [ 1 : ] values : Values = None ) - > ( values : Values args : [ string ] )
"def print_help ( self , file = None ) : if file is None : file = sys . stdout encoding = self . _get_encoding ( file ) file . write ( self . format_help ( ) )",print_help ( file : file = stdout )
"def reverse ( self ) : leftblock = self . left rightblock = self . right leftindex = self . leftndx rightindex = self . rightndx for i in range ( self . length // 2 ) : assert leftblock != rightblock or leftindex < rightindex ( rightblock [ rightindex ] , leftblock [ leftindex ] ) = ( leftblock [ leftindex ] , rightblock [ rightindex ] ) leftindex += 1 if leftindex == n : leftblock = leftblock [ RGTLNK ] assert leftblock is not None leftindex = 0 rightindex -= 1 if rightindex == - 1 : rightblock = rightblock [ LFTLNK ] assert rightblock is not None rightindex = n - 1",reverse * IN PLACE *
"def insort_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 a . insert ( lo , x )",Insert item x in list a and keep it sorted assuming a is sorted .
"def bisect_right ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if x < a [ mid ] : hi = mid else : lo = mid + 1 return lo",Return the index where to insert item x in list a assuming a is sorted .
"def bisect_left ( a , x , lo = 0 , hi = None ) : if lo < 0 : raise ValueError ( 'lo must be non-negative' ) if hi is None : hi = len ( a ) while lo < hi : mid = ( lo + hi ) // 2 if a [ mid ] < x : lo = mid + 1 else : hi = mid return lo",Return the index where to insert item x in list a assuming a is sorted .
"def lock ( self , function , argument ) : if self . testandset ( ) : function ( argument ) else : self . queue . append ( ( function , argument ) )",Lock a mutex call the function with supplied argument when it is acquired . If the mutex is already locked place function and argument in the queue .
"def unlock ( self ) : if self . queue : function , argument = self . queue . popleft ( ) function ( argument ) else : self . locked = False",Unlock a mutex . If the queue is not empty call the next function with its argument .
"def clear ( self ) : root = self . __root root [ : ] = [ root , root , None ] self . __map . clear ( ) dict . clear ( self )",od . clear () - > None . Remove all items from od .
"def popitem ( self , last = True ) : if not self : raise KeyError ( 'dictionary is empty' ) key = next ( reversed ( self ) if last else iter ( self ) ) value = self . pop ( key ) return key , value",od . popitem () - > ( k v ) return and remove a ( key value ) pair . Pairs are returned in LIFO order if last is true or FIFO order if false .
"def fromkeys ( cls , iterable , value = None ) : self = cls ( ) for key in iterable : self [ key ] = value return self",OD . fromkeys ( S [ v ] ) - > New ordered dictionary with keys from S . If not specified the value defaults to None .
"def update ( * args , * * kwds ) : if not args : raise TypeError ( ""descriptor 'update' of 'Counter' object "" ""needs an argument"" ) self = args [ 0 ] args = args [ 1 : ] if len ( args ) > 1 : raise TypeError ( 'expected at most 1 arguments, got %d' % len ( args ) ) iterable = args [ 0 ] if args else None if iterable is not None : if isinstance ( iterable , Mapping ) : if self : self_get = self . get for elem , count in iterable . iteritems ( ) : self [ elem ] = self_get ( elem , 0 ) + count else : super ( Counter , self ) . update ( iterable ) else : self_get = self . get for elem in iterable : self [ elem ] = self_get ( elem , 0 ) + 1 if kwds : self . update ( kwds )",Like dict . update () but add counts instead of replacing them .
"def subtract ( * args , * * kwds ) : if not args : raise TypeError ( ""descriptor 'subtract' of 'Counter' object "" ""needs an argument"" ) self = args [ 0 ] args = args [ 1 : ] if len ( args ) > 1 : raise TypeError ( 'expected at most 1 arguments, got %d' % len ( args ) ) iterable = args [ 0 ] if args else None if iterable is not None : self_get = self . get if isinstance ( iterable , Mapping ) : for elem , count in iterable . items ( ) : self [ elem ] = self_get ( elem , 0 ) - count else : for elem in iterable : self [ elem ] = self_get ( elem , 0 ) - 1 if kwds : self . subtract ( kwds )",Like dict . update () but subtracts counts instead of replacing them . Counts can be reduced below zero . Both the inputs and outputs are allowed to contain zero and negative counts .
"def digest ( self ) : A = self . A B = self . B C = self . C D = self . D input = [ ] + self . input count = [ ] + self . count index = ( self . count [ 0 ] >> 3 ) & 0x3f if index < 56 : padLen = 56 - index else : padLen = 120 - index padding = [ b'\200' ] + [ b'\000' ] * 63 self . update ( padding [ : padLen ] ) bits = _bytelist2long ( self . input [ : 56 ] ) + count self . _transform ( bits ) digest = struct . pack ( ""<IIII"" , self . A , self . B , self . C , self . D ) self . A = A self . B = B self . C = C self . D = D self . input = input self . count = count return digest",Terminate the message - digest computation and return digest .
def copy ( self ) : if 0 : return copy . deepcopy ( self ) clone = self . __class__ ( ) clone . length = self . length clone . count = [ ] + self . count [ : ] clone . input = [ ] + self . input clone . A = self . A clone . B = self . B clone . C = self . C clone . D = self . D return clone,Return a clone object .
"def compile ( pattern , flags , code , groups = 0 , groupindex = { } , indexgroup = [ None ] ) : return SRE_Pattern ( pattern , flags , code , groups , groupindex , indexgroup )",Compiles ( or rather just converts ) a pattern descriptor to a SRE_Pattern object . Actual compilation to opcodes happens in sre_compile .
"def search ( self , string , pos = 0 , endpos = sys . maxint ) : state = _State ( string , pos , endpos , self . flags ) if state . search ( self . _code ) : return SRE_Match ( self , state ) else : return None",Scan through string looking for a location where this regular expression produces a match and return a corresponding MatchObject instance . Return None if no position in the string matches the pattern .
"def findall ( self , string , pos = 0 , endpos = sys . maxint ) : matchlist = [ ] state = _State ( string , pos , endpos , self . flags ) while state . start <= state . end : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break match = SRE_Match ( self , state ) if self . groups == 0 or self . groups == 1 : item = match . group ( self . groups ) else : item = match . groups ( """" ) matchlist . append ( item ) if state . string_position == state . start : state . start += 1 else : state . start = state . string_position return matchlist",Return a list of all non - overlapping matches of pattern in string .
"def sub ( self , repl , string , count = 0 ) : return self . _subx ( repl , string , count , False )",Return the string obtained by replacing the leftmost non - overlapping occurrences of pattern in string by the replacement repl .
"def subn ( self , repl , string , count = 0 ) : return self . _subx ( repl , string , count , True )",Return the tuple ( new_string number_of_subs_made ) found by replacing the leftmost non - overlapping occurrences of pattern with the replacement repl .
"def split ( self , string , maxsplit = 0 ) : splitlist = [ ] state = _State ( string , 0 , sys . maxint , self . flags ) n = 0 last = state . start while not maxsplit or n < maxsplit : state . reset ( ) state . string_position = state . start if not state . search ( self . _code ) : break if state . start == state . string_position : if last == state . end : break state . start += 1 continue splitlist . append ( string [ last : state . start ] ) if self . groups : match = SRE_Match ( self , state ) splitlist += ( list ( match . groups ( None ) ) ) n += 1 last = state . start = state . string_position splitlist . append ( string [ last : state . end ] ) return splitlist",Split string by the occurrences of pattern .
"def finditer ( self , string , pos = 0 , endpos = sys . maxint ) : scanner = self . scanner ( string , pos , endpos ) return iter ( scanner . search , None )",Return a list of all non - overlapping matches of pattern in string .
"def _create_regs ( self , state ) : regs = [ ( state . start , state . string_position ) ] for group in range ( self . re . groups ) : mark_index = 2 * group if mark_index + 1 < len ( state . marks ) and state . marks [ mark_index ] is not None and state . marks [ mark_index + 1 ] is not None : regs . append ( ( state . marks [ mark_index ] , state . marks [ mark_index + 1 ] ) ) else : regs . append ( ( - 1 , - 1 ) ) return tuple ( regs )",Creates a tuple of index pairs representing matched groups .
"def groups ( self , default = None ) : groups = [ ] for indices in self . regs [ 1 : ] : if indices [ 0 ] >= 0 : groups . append ( self . string [ indices [ 0 ] : indices [ 1 ] ] ) else : groups . append ( default ) return tuple ( groups )",Returns a tuple containing all the subgroups of the match . The default argument is used for groups that did not participate in the match ( defaults to None ) .
"def groupdict ( self , default = None ) : groupdict = { } for key , value in self . re . groupindex . items ( ) : groupdict [ key ] = self . _get_slice ( value , default ) return groupdict",Return a dictionary containing all the named subgroups of the match . The default argument is used for groups that did not participate in the match ( defaults to None ) .
"def group ( self , * args ) : if len ( args ) == 0 : args = ( 0 , ) grouplist = [ ] for group in args : grouplist . append ( self . _get_slice ( self . _get_index ( group ) , None ) ) if len ( grouplist ) == 1 : return grouplist [ 0 ] else : return tuple ( grouplist )",Returns one or more subgroups of the match . Each argument is either a group index or a group name .
"def fast_search ( self , pattern_codes ) : flags = pattern_codes [ 2 ] prefix_len = pattern_codes [ 5 ] prefix_skip = pattern_codes [ 6 ] prefix = pattern_codes [ 7 : 7 + prefix_len ] overlap = pattern_codes [ 7 + prefix_len - 1 : pattern_codes [ 1 ] + 1 ] pattern_codes = pattern_codes [ pattern_codes [ 1 ] + 1 : ] i = 0 string_position = self . string_position while string_position < self . end : while True : if ord ( self . string [ string_position ] ) != prefix [ i ] : if i == 0 : break else : i = overlap [ i ] else : i += 1 if i == prefix_len : self . start = string_position + 1 - prefix_len self . string_position = string_position + 1 - prefix_len + prefix_skip if flags & SRE_INFO_LITERAL : return True if self . match ( pattern_codes [ 2 * prefix_skip : ] ) : return True i = overlap [ i ] break string_position += 1 return False",Skips forward in a string as fast as possible using information from an optimization info block .
"def push_new_context ( self , pattern_offset ) : child_context = _MatchContext ( self . state , self . pattern_codes [ self . code_position + pattern_offset : ] ) self . state . context_stack . append ( child_context ) return child_context",Creates a new child context of this context and pushes it on the stack . pattern_offset is the offset off the current code position to start interpreting from .
"def match ( self , context ) : while context . remaining_codes ( ) > 0 and context . has_matched is None : opcode = context . peek_code ( ) if not self . dispatch ( opcode , context ) : return None if context . has_matched is None : context . has_matched = False return context . has_matched",Returns True if the current context matches False if it doesn t and None if matching is not finished ie must be resumed after child contexts have been matched .
"def dispatch ( self , opcode , context ) : if id ( context ) in self . executing_contexts : generator = self . executing_contexts [ id ( context ) ] del self . executing_contexts [ id ( context ) ] has_finished = generator . next ( ) else : method = self . DISPATCH_TABLE . get ( opcode , _OpcodeDispatcher . unknown ) has_finished = method ( self , context ) if hasattr ( has_finished , ""next"" ) : generator = has_finished has_finished = generator . next ( ) if not has_finished : self . executing_contexts [ id ( context ) ] = generator return has_finished",Dispatches a context on a given opcode . Returns True if the context is done matching False if it must be resumed when next encountered .
"def check_charset ( self , ctx , char ) : self . set_dispatcher . reset ( char ) save_position = ctx . code_position result = None while result is None : result = self . set_dispatcher . dispatch ( ctx . peek_code ( ) , ctx ) ctx . code_position = save_position return result",Checks whether a character matches set of arbitrary length . Assumes the code pointer is at the first member of the set .
"def count_repetitions ( self , ctx , maxcount ) : count = 0 real_maxcount = ctx . state . end - ctx . string_position if maxcount < real_maxcount and maxcount != MAXREPEAT : real_maxcount = maxcount code_position = ctx . code_position string_position = ctx . string_position ctx . skip_code ( 4 ) reset_position = ctx . code_position while count < real_maxcount : ctx . code_position = reset_position self . dispatch ( ctx . peek_code ( ) , ctx ) if ctx . has_matched is False : break count += 1 ctx . has_matched = None ctx . code_position = code_position ctx . string_position = string_position return count",Returns the number of repetitions of a single item starting from the current string position . The code pointer is expected to point to a REPEAT_ONE operation ( with the repeated 4 ahead ) .
"def extract ( s ) : res = decoder . match ( s ) if res is None : raise NotANumber , s sign , intpart , fraction , exppart = res . group ( 1 , 2 , 3 , 4 ) if sign == '+' : sign = '' if fraction : fraction = fraction [ 1 : ] if exppart : expo = int ( exppart [ 1 : ] ) else : expo = 0 return sign , intpart , fraction , expo",Return ( sign intpart fraction expo ) or raise an exception : sign is + or - intpart is 0 or more digits beginning with a nonzero fraction is 0 or more digits expo is an integer
"def unexpo ( intpart , fraction , expo ) : if expo > 0 : f = len ( fraction ) intpart , fraction = intpart + fraction [ : expo ] , fraction [ expo : ] if expo > f : intpart = intpart + '0' * ( expo - f ) elif expo < 0 : i = len ( intpart ) intpart , fraction = intpart [ : expo ] , intpart [ expo : ] + fraction if expo < - i : fraction = '0' * ( - expo - i ) + fraction return intpart , fraction",Remove the exponent by changing intpart and fraction .
"def roundfrac ( intpart , fraction , digs ) : f = len ( fraction ) if f <= digs : return intpart , fraction + '0' * ( digs - f ) i = len ( intpart ) if i + digs < 0 : return '0' * - digs , '' total = intpart + fraction nextdigit = total [ i + digs ] if nextdigit >= '5' : n = i + digs - 1 while n >= 0 : if total [ n ] != '9' : break n = n - 1 else : total = '0' + total i = i + 1 n = 0 total = total [ : n ] + chr ( ord ( total [ n ] ) + 1 ) + '0' * ( len ( total ) - n - 1 ) intpart , fraction = total [ : i ] , total [ i : ] if digs >= 0 : return intpart , fraction [ : digs ] else : return intpart [ : digs ] + '0' * - digs , ''",Round or extend the fraction to size digs .
"def fix ( x , digs ) : if type ( x ) != type ( '' ) : x = repr ( x ) try : sign , intpart , fraction , expo = extract ( x ) except NotANumber : return x intpart , fraction = unexpo ( intpart , fraction , expo ) intpart , fraction = roundfrac ( intpart , fraction , digs ) while intpart and intpart [ 0 ] == '0' : intpart = intpart [ 1 : ] if intpart == '' : intpart = '0' if digs > 0 : return sign + intpart + '.' + fraction else : return sign + intpart",Format x as [ - ] ddd . ddd with digs digits after the point and at least one digit before . If digs < = 0 the point is suppressed .
"def sci ( x , digs ) : if type ( x ) != type ( '' ) : x = repr ( x ) sign , intpart , fraction , expo = extract ( x ) if not intpart : while fraction and fraction [ 0 ] == '0' : fraction = fraction [ 1 : ] expo = expo - 1 if fraction : intpart , fraction = fraction [ 0 ] , fraction [ 1 : ] expo = expo - 1 else : intpart = '0' else : expo = expo + len ( intpart ) - 1 intpart , fraction = intpart [ 0 ] , intpart [ 1 : ] + fraction digs = max ( 0 , digs ) intpart , fraction = roundfrac ( intpart , fraction , digs ) if len ( intpart ) > 1 : intpart , fraction , expo = intpart [ 0 ] , intpart [ 1 : ] + fraction [ : - 1 ] , expo + len ( intpart ) - 1 s = sign + intpart if digs > 0 : s = s + '.' + fraction e = repr ( abs ( expo ) ) e = '0' * ( 3 - len ( e ) ) + e if expo < 0 : e = '-' + e else : e = '+' + e return s + 'e' + e",Format x as [ - ] d . dddE [ + - ] ddd with digs digits after the point and exactly one digit before . If digs is < = 0 one digit is kept and the point is suppressed .
"def getrandbits ( self , k ) : if k <= 0 : raise ValueError ( 'number of bits must be greater than zero' ) if k != int ( k ) : raise TypeError ( 'number of bits should be an integer' ) numbytes = ( k + 7 ) // 8 x = _int_from_bytes ( _gorandom ( numbytes ) ) return x >> ( numbytes * 8 - k )",getrandbits ( k ) - > x . Generates an int with k random bits .
"def _randbelow ( self , n ) : k = _int_bit_length ( n ) r = self . getrandbits ( k ) while r >= n : r = self . getrandbits ( k ) return r",Return a random int in the range [ 0 n ) .
"def getopt ( args , shortopts , longopts = [ ] ) : opts = [ ] if type ( longopts ) == type ( """" ) : longopts = [ longopts ] else : longopts = list ( longopts ) while args and args [ 0 ] . startswith ( '-' ) and args [ 0 ] != '-' : if args [ 0 ] == '--' : args = args [ 1 : ] break if args [ 0 ] . startswith ( '--' ) : opts , args = do_longs ( opts , args [ 0 ] [ 2 : ] , longopts , args [ 1 : ] ) else : opts , args = do_shorts ( opts , args [ 0 ] [ 1 : ] , shortopts , args [ 1 : ] ) return opts , args",getopt ( args options [ long_options ] ) - > opts args
"def gnu_getopt ( args , shortopts , longopts = [ ] ) : opts = [ ] prog_args = [ ] if isinstance ( longopts , str ) : longopts = [ longopts ] else : longopts = list ( longopts ) if shortopts . startswith ( '+' ) : shortopts = shortopts [ 1 : ] all_options_first = True elif os . environ . get ( ""POSIXLY_CORRECT"" ) : all_options_first = True else : all_options_first = False while args : if args [ 0 ] == '--' : prog_args += args [ 1 : ] break if args [ 0 ] [ : 2 ] == '--' : opts , args = do_longs ( opts , args [ 0 ] [ 2 : ] , longopts , args [ 1 : ] ) elif args [ 0 ] [ : 1 ] == '-' and args [ 0 ] != '-' : opts , args = do_shorts ( opts , args [ 0 ] [ 1 : ] , shortopts , args [ 1 : ] ) else : if all_options_first : prog_args += args break else : prog_args . append ( args [ 0 ] ) args = args [ 1 : ] return opts , prog_args",getopt ( args options [ long_options ] ) - > opts args
"def filter ( names , pat ) : import os result = [ ] try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) match = re_pat . match if 1 : for name in names : if match ( name ) : result . append ( name ) else : for name in names : if match ( os . path . normcase ( name ) ) : result . append ( name ) return result",Return the subset of the list NAMES that match PAT
"def fnmatchcase ( name , pat ) : try : re_pat = _cache [ pat ] except KeyError : res = translate ( pat ) if len ( _cache ) >= _MAXCACHE : globals ( ) [ '_cache' ] = { } _cache [ pat ] = re_pat = re . compile ( res ) return re_pat . match ( name ) is not None",Test whether FILENAME matches PATTERN including case .
"def translate ( pat ) : i , n = 0 , len ( pat ) res = '' while i < n : c = pat [ i ] i = i + 1 if c == '*' : res = res + '.*' elif c == '?' : res = res + '.' elif c == '[' : j = i if j < n and pat [ j ] == '!' : j = j + 1 if j < n and pat [ j ] == ']' : j = j + 1 while j < n and pat [ j ] != ']' : j = j + 1 if j >= n : res = res + '\\[' else : stuff = pat [ i : j ] . replace ( '\\' , '\\\\' ) i = j + 1 if stuff [ 0 ] == '!' : stuff = '^' + stuff [ 1 : ] elif stuff [ 0 ] == '^' : stuff = '\\' + stuff res = '%s[%s]' % ( res , stuff ) else : res = res + re . escape ( c ) return res + '\Z(?ms)'",Translate a shell PATTERN to a regular expression .
def task_done ( self ) : self . all_tasks_done . acquire ( ) try : unfinished = self . unfinished_tasks - 1 if unfinished <= 0 : if unfinished < 0 : raise ValueError ( 'task_done() called too many times' ) self . all_tasks_done . notify_all ( ) self . unfinished_tasks = unfinished finally : self . all_tasks_done . release ( ),Indicate that a formerly enqueued task is complete .
def qsize ( self ) : self . mutex . acquire ( ) n = self . _qsize ( ) self . mutex . release ( ) return n,Return the approximate size of the queue ( not reliable! ) .
def empty ( self ) : self . mutex . acquire ( ) n = not self . _qsize ( ) self . mutex . release ( ) return n,Return True if the queue is empty False otherwise ( not reliable! ) .
def full ( self ) : self . mutex . acquire ( ) n = 0 < self . maxsize == self . _qsize ( ) self . mutex . release ( ) return n,Return True if the queue is full False otherwise ( not reliable! ) .
"def put ( self , item , block = True , timeout = None ) : self . not_full . acquire ( ) try : if self . maxsize > 0 : if not block : if self . _qsize ( ) == self . maxsize : raise Full elif timeout is None : while self . _qsize ( ) == self . maxsize : self . not_full . wait ( ) elif timeout < 0 : raise ValueError ( ""'timeout' must be a non-negative number"" ) else : endtime = _time ( ) + timeout while self . _qsize ( ) == self . maxsize : remaining = endtime - _time ( ) if remaining <= 0.0 : raise Full self . not_full . wait ( remaining ) self . _put ( item ) self . unfinished_tasks += 1 self . not_empty . notify ( ) finally : self . not_full . release ( )",Put an item into the queue .
"def calculate_transitive_deps ( modname , script , gopath ) : deps = set ( ) def calc ( modname , script ) : if modname in deps : return deps . add ( modname ) for imp in collect_imports ( modname , script , gopath ) : if imp . is_native : deps . add ( imp . name ) continue parts = imp . name . split ( '.' ) calc ( imp . name , imp . script ) if len ( parts ) == 1 : continue package_dir , filename = os . path . split ( imp . script ) if filename == '__init__.py' : package_dir = os . path . dirname ( package_dir ) for i in xrange ( len ( parts ) - 1 , 0 , - 1 ) : modname = '.' . join ( parts [ : i ] ) script = os . path . join ( package_dir , '__init__.py' ) calc ( modname , script ) package_dir = os . path . dirname ( package_dir ) calc ( modname , script ) deps . remove ( modname ) return deps",Determines all modules that script transitively depends upon .
"def _make_future_features ( node ) : assert isinstance ( node , ast . ImportFrom ) assert node . module == '__future__' features = FutureFeatures ( ) for alias in node . names : name = alias . name if name in _FUTURE_FEATURES : if name not in _IMPLEMENTED_FUTURE_FEATURES : msg = 'future feature {} not yet implemented by grumpy' . format ( name ) raise util . ParseError ( node , msg ) setattr ( features , name , True ) elif name == 'braces' : raise util . ParseError ( node , 'not a chance' ) elif name not in _REDUNDANT_FUTURE_FEATURES : msg = 'future feature {} is not defined' . format ( name ) raise util . ParseError ( node , msg ) return features",Processes a future import statement returning set of flags it defines .
"def parse_future_features ( mod ) : assert isinstance ( mod , ast . Module ) found_docstring = False for node in mod . body : if isinstance ( node , ast . ImportFrom ) : if node . module == '__future__' : return node , _make_future_features ( node ) break elif isinstance ( node , ast . Expr ) and not found_docstring : if not isinstance ( node . value , ast . Str ) : break found_docstring = True else : break return None , FutureFeatures ( )",Accumulates a set of flags for the compiler __future__ imports .
"def contextmanager ( func ) : @ wraps ( func ) def helper ( * args , * * kwds ) : return GeneratorContextManager ( func ( * args , * * kwds ) ) return helper",@contextmanager decorator .
"def nested ( * managers ) : warn ( ""With-statements now directly support multiple context managers"" , DeprecationWarning , 3 ) exits = [ ] vars = [ ] exc = ( None , None , None ) try : for mgr in managers : exit = mgr . __exit__ enter = mgr . __enter__ vars . append ( enter ( ) ) exits . append ( exit ) yield vars except : exc = sys . exc_info ( ) finally : while exits : exit = exits . pop ( ) try : if exit ( * exc ) : exc = ( None , None , None ) except : exc = sys . exc_info ( ) if exc != ( None , None , None ) : raise exc [ 0 ] , exc [ 1 ] , exc [ 2 ]",Combine multiple context managers into a single nested context manager .
"def decode ( self , s , _w = WHITESPACE . match ) : obj , end = self . raw_decode ( s , idx = _w ( s , 0 ) . end ( ) ) end = _w ( s , end ) . end ( ) if end != len ( s ) : raise ValueError ( errmsg ( ""Extra data"" , s , end , len ( s ) ) ) return obj",Return the Python representation of s ( a str or unicode instance containing a JSON document )
"def tf_loss ( self , states , internals , reward , update , reference = None ) : prediction = self . predict ( states = states , internals = internals , update = update ) return tf . nn . l2_loss ( t = ( prediction - reward ) )",Creates the TensorFlow operations for calculating the L2 loss between predicted state values and actual rewards .
"def get_variables ( self , include_nontrainable = False ) : if include_nontrainable : return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] else : return [ self . variables [ key ] for key in sorted ( self . variables ) ]",Returns the TensorFlow variables used by the baseline .
"def from_spec ( spec , kwargs = None ) : baseline = util . get_object ( obj = spec , predefined_objects = tensorforce . core . baselines . baselines , kwargs = kwargs ) assert isinstance ( baseline , Baseline ) return baseline",Creates a baseline from a specification dict .
"def reset ( self ) : self . protocol . send ( { ""cmd"" : ""reset"" } , self . socket ) response = self . protocol . recv ( self . socket ) return self . extract_observation ( response )",same as step ( no kwargs to pass ) but needs to block and return observation_dict - stores the received observation in self . last_observation
"def execute ( self , action ) : action_mappings , axis_mappings = [ ] , [ ] if self . discretize_actions : combination = self . discretized_actions [ action ] for key , value in combination : if isinstance ( value , bool ) : action_mappings . append ( ( key , value ) ) else : axis_mappings . append ( ( key , value ) ) elif action : try : action_mappings , axis_mappings = self . translate_abstract_actions_to_keys ( action ) except KeyError as e : raise TensorForceError ( ""Action- or axis-mapping with name '{}' not defined in connected UE4 game!"" . format ( e ) ) message = dict ( cmd = ""step"" , delta_time = self . delta_time , num_ticks = self . num_ticks , actions = action_mappings , axes = axis_mappings ) self . protocol . send ( message , self . socket ) response = self . protocol . recv ( self . socket ) r = response . pop ( b""_reward"" , 0.0 ) is_terminal = response . pop ( b""_is_terminal"" , False ) obs = self . extract_observation ( response ) self . last_observation = obs return obs , is_terminal , r",Executes a single step in the UE4 game . This step may be comprised of one or more actual game ticks for all of which the same given action - and axis - inputs ( or action number in case of discretized actions ) are repeated . UE4 distinguishes between action - mappings which are boolean actions ( e . g . jump or dont - jump ) and axis - mappings which are continuous actions like MoveForward with values between - 1 . 0 ( run backwards ) and 1 . 0 ( run forwards ) 0 . 0 would mean : stop .
"def translate_abstract_actions_to_keys ( self , abstract ) : if len ( abstract ) >= 2 and not isinstance ( abstract [ 1 ] , ( list , tuple ) ) : abstract = list ( ( abstract , ) ) actions , axes = [ ] , [ ] for a in abstract : first_key = self . action_space_desc [ a [ 0 ] ] [ ""keys"" ] [ 0 ] if isinstance ( first_key , ( bytes , str ) ) : actions . append ( ( first_key , a [ 1 ] ) ) elif isinstance ( first_key , tuple ) : axes . append ( ( first_key [ 0 ] , a [ 1 ] * first_key [ 1 ] ) ) else : raise TensorForceError ( ""action_space_desc contains unsupported type for key {}!"" . format ( a [ 0 ] ) ) return actions , axes",Translates a list of tuples ( [ pretty mapping ] [ value ] ) to a list of tuples ( [ some key ] [ translated value ] ) each single item in abstract will undergo the following translation :
"def discretize_action_space_desc ( self ) : unique_list = [ ] for nice , record in self . action_space_desc . items ( ) : list_for_record = [ ] if record [ ""type"" ] == ""axis"" : head_key = record [ ""keys"" ] [ 0 ] [ 0 ] head_value = record [ ""keys"" ] [ 0 ] [ 1 ] list_for_record . append ( ( head_key , 0.0 ) ) set_ = set ( ) for key_and_scale in self . action_space_desc [ nice ] [ ""keys"" ] : if key_and_scale [ 1 ] not in set_ : list_for_record . append ( ( head_key , key_and_scale [ 1 ] / head_value ) ) set_ . add ( key_and_scale [ 1 ] ) else : list_for_record = [ ( record [ ""keys"" ] [ 0 ] , False ) , ( record [ ""keys"" ] [ 0 ] , True ) ] unique_list . append ( list_for_record ) def so ( in_ ) : st = """" for i in in_ : st += str ( i [ 1 ] ) return st combinations = list ( itertools . product ( * unique_list ) ) combinations = list ( map ( lambda x : sorted ( list ( x ) , key = lambda y : y [ 0 ] ) , combinations ) ) combinations = sorted ( combinations , key = so ) self . discretized_actions = combinations",Creates a list of discrete action ( - combinations ) in case we want to learn with a discrete set of actions but only have action - combinations ( maybe even continuous ) available from the env . E . g . the UE4 game has the following action / axis - mappings :
def reset ( self ) : self . level . reset ( ) return self . level . observations ( ) [ self . state_attribute ],Resets the environment to its initialization state . This method needs to be called to start a new episode after the last episode ended .
"def execute ( self , action ) : adjusted_action = list ( ) for action_spec in self . level . action_spec ( ) : if action_spec [ 'min' ] == - 1 and action_spec [ 'max' ] == 1 : adjusted_action . append ( action [ action_spec [ 'name' ] ] - 1 ) else : adjusted_action . append ( action [ action_spec [ 'name' ] ] ) action = np . array ( adjusted_action , dtype = np . intc ) reward = self . level . step ( action = action , num_steps = self . repeat_action ) state = self . level . observations ( ) [ 'RGB_INTERLACED' ] terminal = not self . level . is_running ( ) return state , terminal , reward",Pass action to universe environment return reward next step terminal state and additional info .
"def tf_solve ( self , fn_x , x_init , b ) : return super ( ConjugateGradient , self ) . tf_solve ( fn_x , x_init , b )",Iteratively solves the system of linear equations $A x = b$ .
"def tf_initialize ( self , x_init , b ) : if x_init is None : x_init = [ tf . zeros ( shape = util . shape ( t ) ) for t in b ] initial_args = super ( ConjugateGradient , self ) . tf_initialize ( x_init ) conjugate = residual = [ t - fx for t , fx in zip ( b , self . fn_x ( x_init ) ) ] squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in residual ] ) return initial_args + ( conjugate , residual , squared_residual )",Initialization step preparing the arguments for the first iteration of the loop body : $x_0 0 p_0 r_0 r_0^2$ .
"def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( x , iteration , conjugate , residual , squared_residual ) A_conjugate = self . fn_x ( conjugate ) if self . damping > 0.0 : A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] conjugate_A_conjugate = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] ) alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual",Iteration loop body of the conjugate gradient algorithm .
"def tf_next_step ( self , x , iteration , conjugate , residual , squared_residual ) : next_step = super ( ConjugateGradient , self ) . tf_next_step ( x , iteration , conjugate , residual , squared_residual ) return tf . logical_and ( x = next_step , y = ( squared_residual >= util . epsilon ) )",Termination condition : max number of iterations or residual sufficiently small .
"def tf_step ( self , time , variables , * * kwargs ) : deltas = self . optimizer . step ( time = time , variables = variables , * * kwargs ) with tf . control_dependencies ( control_inputs = deltas ) : clipped_deltas = list ( ) exceeding_deltas = list ( ) for delta in deltas : clipped_delta = tf . clip_by_value ( t = delta , clip_value_min = - self . clipping_value , clip_value_max = self . clipping_value ) clipped_deltas . append ( clipped_delta ) exceeding_deltas . append ( clipped_delta - delta ) applied = self . apply_step ( variables = variables , deltas = exceeding_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : return [ delta + 0.0 for delta in clipped_deltas ]",Creates the TensorFlow operations for performing an optimization step .
"def from_spec ( spec , kwargs = None ) : layer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . networks . layers , kwargs = kwargs ) assert isinstance ( layer , Layer ) return layer",Creates a layer from a specification dict .
"def tf_q_delta ( self , q_value , next_q_value , terminal , reward ) : for _ in range ( util . rank ( q_value ) - 1 ) : terminal = tf . expand_dims ( input = terminal , axis = 1 ) reward = tf . expand_dims ( input = reward , axis = 1 ) multiples = ( 1 , ) + util . shape ( q_value ) [ 1 : ] terminal = tf . tile ( input = terminal , multiples = multiples ) reward = tf . tile ( input = reward , multiples = multiples ) zeros = tf . zeros_like ( tensor = next_q_value ) next_q_value = tf . where ( condition = terminal , x = zeros , y = ( self . discount * next_q_value ) ) return reward + next_q_value - q_value",Creates the deltas ( or advantage ) of the Q values .
"def target_optimizer_arguments ( self ) : variables = self . target_network . get_variables ( ) + [ variable for name in sorted ( self . target_distributions ) for variable in self . target_distributions [ name ] . get_variables ( ) ] source_variables = self . network . get_variables ( ) + [ variable for name in sorted ( self . distributions ) for variable in self . distributions [ name ] . get_variables ( ) ] arguments = dict ( time = self . global_timestep , variables = variables , source_variables = source_variables ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ variable for name in sorted ( self . global_model . target_distributions ) for variable in self . global_model . target_distributions [ name ] . get_variables ( ) ] return arguments",Returns the target optimizer arguments including the time the list of variables to optimize and various functions which the optimizer might require to perform an update step .
"def from_spec ( spec , kwargs ) : env = tensorforce . util . get_object ( obj = spec , predefined_objects = tensorforce . environments . environments , kwargs = kwargs ) assert isinstance ( env , Environment ) return env",Creates an environment from a specification dict .
"def setup ( app ) : global _is_sphinx _is_sphinx = True app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) app . add_source_parser ( '.md' , M2RParser ) app . add_directive ( 'mdinclude' , MdInclude )",When used for spinx extension .
"def output_image_link ( self , m ) : return self . renderer . image_link ( m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) )",Pass through rest role .
"def output_eol_literal_marker ( self , m ) : marker = ':' if m . group ( 1 ) is None else '' return self . renderer . eol_literal_marker ( marker )",Pass through rest link .
"def header ( self , text , level , raw = None ) : return '\n{0}\n{1}\n' . format ( text , self . hmarks [ level ] * len ( text ) )",Rendering header / heading tags like <h1 > <h2 > .
"def list ( self , body , ordered = True ) : mark = '#. ' if ordered else '* ' lines = body . splitlines ( ) for i , line in enumerate ( lines ) : if line and not line . startswith ( self . list_marker ) : lines [ i ] = ' ' * len ( mark ) + line return '\n{}\n' . format ( '\n' . join ( lines ) ) . replace ( self . list_marker , mark )",Rendering list tags like <ul > and <ol > .
"def table ( self , header , body ) : table = '\n.. list-table::\n' if header and not header . isspace ( ) : table = ( table + self . indent + ':header-rows: 1\n\n' + self . _indent_block ( header ) + '\n' ) else : table = table + '\n' table = table + self . _indent_block ( body ) + '\n\n' return table",Rendering table element . Wrap header and body in it .
"def table_row ( self , content ) : contents = content . splitlines ( ) if not contents : return '' clist = [ '* ' + contents [ 0 ] ] if len ( contents ) > 1 : for c in contents [ 1 : ] : clist . append ( '  ' + c ) return '\n' . join ( clist ) + '\n'",Rendering a table row . Like <tr > .
"def codespan ( self , text ) : if '``' not in text : return '\ ``{}``\ ' . format ( text ) else : return self . _raw_html ( '<code class=""docutils literal"">' '<span class=""pre"">{}</span>' '</code>' . format ( text . replace ( '`' , '&#96;' ) ) )",Rendering inline code text .
"def link ( self , link , title , text ) : if title : raise NotImplementedError ( 'sorry' ) return '\ `{text} <{target}>`_\ ' . format ( target = link , text = text )",Rendering a given link with content and title .
"def image ( self , src , title , text ) : return '\n' . join ( [ '' , '.. image:: {}' . format ( src ) , '   :target: {}' . format ( src ) , '   :alt: {}' . format ( text ) , '' , ] )",Rendering a image with title and text .
"def run ( self ) : if not self . state . document . settings . file_insertion_enabled : raise self . warning ( '""%s"" directive disabled.' % self . name ) source = self . state_machine . input_lines . source ( self . lineno - self . state_machine . input_offset - 1 ) source_dir = os . path . dirname ( os . path . abspath ( source ) ) path = rst . directives . path ( self . arguments [ 0 ] ) path = os . path . normpath ( os . path . join ( source_dir , path ) ) path = utils . relative_path ( None , path ) path = nodes . reprunicode ( path ) encoding = self . options . get ( 'encoding' , self . state . document . settings . input_encoding ) e_handler = self . state . document . settings . input_encoding_error_handler tab_width = self . options . get ( 'tab-width' , self . state . document . settings . tab_width ) try : self . state . document . settings . record_dependencies . add ( path ) include_file = io . FileInput ( source_path = path , encoding = encoding , error_handler = e_handler ) except UnicodeEncodeError as error : raise self . severe ( 'Problems with ""%s"" directive path:\n' 'Cannot encode input file path ""%s"" ' '(wrong locale?).' % ( self . name , SafeString ( path ) ) ) except IOError as error : raise self . severe ( 'Problems with ""%s"" directive path:\n%s.' % ( self . name , ErrorString ( error ) ) ) try : rawtext = include_file . read ( ) except UnicodeError as error : raise self . severe ( 'Problem with ""%s"" directive:\n%s' % ( self . name , ErrorString ( error ) ) ) config = self . state . document . settings . env . config converter = M2R ( no_underscore_emphasis = config . no_underscore_emphasis ) include_lines = statemachine . string2lines ( converter ( rawtext ) , tab_width , convert_whitespace = True ) self . state_machine . insert_input ( include_lines , path ) return [ ]",Most of this method is from docutils . parser . rst . Directive .
"def WorkerAgentGenerator ( agent_class ) : if isinstance ( agent_class , str ) : agent_class = AgentsDictionary . get ( agent_class ) if not agent_class and agent_class . find ( '.' ) != - 1 : module_name , function_name = agent_class . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) agent_class = getattr ( module , function_name ) class WorkerAgent ( agent_class ) : """"""
        Worker agent receiving a shared model to avoid creating multiple models.
        """""" def __init__ ( self , model = None , * * kwargs ) : self . model = model if not issubclass ( agent_class , LearningAgent ) : kwargs . pop ( ""network"" ) super ( WorkerAgent , self ) . __init__ ( * * kwargs ) def initialize_model ( self ) : return self . model return WorkerAgent",Worker Agent generator receives an Agent class and creates a Worker Agent class that inherits from that Agent .
"def clone_worker_agent ( agent , factor , environment , network , agent_config ) : ret = [ agent ] for i in xrange ( factor - 1 ) : worker = WorkerAgentGenerator ( type ( agent ) ) ( states = environment . states , actions = environment . actions , network = network , model = agent . model , * * agent_config ) ret . append ( worker ) return ret",Clones a given Agent ( factor times ) and returns a list of the cloned Agents with the original Agent in the first slot .
"def run ( self , num_episodes = - 1 , max_episode_timesteps = - 1 , episode_finished = None , summary_report = None , summary_interval = 0 , num_timesteps = None , deterministic = False , episodes = None , max_timesteps = None , testing = False , sleep = None ) : if episodes is not None : num_episodes = episodes warnings . warn ( ""WARNING: `episodes` parameter is deprecated, use `num_episodes` instead."" , category = DeprecationWarning ) assert isinstance ( num_episodes , int ) if max_timesteps is not None : max_episode_timesteps = max_timesteps warnings . warn ( ""WARNING: `max_timesteps` parameter is deprecated, use `max_episode_timesteps` instead."" , category = DeprecationWarning ) assert isinstance ( max_episode_timesteps , int ) if summary_report is not None : warnings . warn ( ""WARNING: `summary_report` parameter is deprecated, use `episode_finished` callback "" ""instead to generate summaries every n episodes."" , category = DeprecationWarning ) self . reset ( ) self . global_episode = 0 self . global_timestep = 0 self . should_stop = False threads = [ threading . Thread ( target = self . _run_single , args = ( t , self . agent [ t ] , self . environment [ t ] , ) , kwargs = { ""deterministic"" : deterministic , ""max_episode_timesteps"" : max_episode_timesteps , ""episode_finished"" : episode_finished , ""testing"" : testing , ""sleep"" : sleep } ) for t in range ( len ( self . agent ) ) ] self . start_time = time . time ( ) [ t . start ( ) for t in threads ] try : next_summary = 0 next_save = 0 if self . save_frequency_unit != ""s"" else time . time ( ) while any ( [ t . is_alive ( ) for t in threads ] ) and self . global_episode < num_episodes or num_episodes == - 1 : self . time = time . time ( ) if summary_report is not None and self . global_episode > next_summary : summary_report ( self ) next_summary += summary_interval if self . save_path and self . save_frequency is not None : do_save = True current = None if self . save_frequency_unit == ""e"" and self . global_episode > next_save : current = self . global_episode elif self . save_frequency_unit == ""s"" and self . time > next_save : current = self . time elif self . save_frequency_unit == ""t"" and self . global_timestep > next_save : current = self . global_timestep else : do_save = False if do_save : self . agent [ 0 ] . save_model ( self . save_path ) while next_save < current : next_save += self . save_frequency time . sleep ( 1 ) except KeyboardInterrupt : print ( 'Keyboard interrupt, sending stop command to threads' ) self . should_stop = True [ t . join ( ) for t in threads ] print ( 'All threads stopped' )",Executes this runner by starting all Agents in parallel ( each one in one thread ) .
"def _run_single ( self , thread_id , agent , environment , deterministic = False , max_episode_timesteps = - 1 , episode_finished = None , testing = False , sleep = None ) : old_episode_finished = False if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : old_episode_finished = True episode = 0 while not self . should_stop : state = environment . reset ( ) agent . reset ( ) self . global_timestep , self . global_episode = agent . timestep , agent . episode episode_reward = 0 time_step = 0 time_start = time . time ( ) while True : action , internals , states = agent . act ( states = state , deterministic = deterministic , buffered = False ) reward = 0 for repeat in xrange ( self . repeat_actions ) : state , terminal , step_reward = environment . execute ( action = action ) reward += step_reward if terminal : break if not testing : agent . atomic_observe ( states = state , actions = action , internals = internals , reward = reward , terminal = terminal ) if sleep is not None : time . sleep ( sleep ) time_step += 1 episode_reward += reward if terminal or time_step == max_episode_timesteps : break if self . should_stop : return self . global_timestep += time_step self . episode_list_lock . acquire ( ) self . episode_rewards . append ( episode_reward ) self . episode_timesteps . append ( time_step ) self . episode_times . append ( time . time ( ) - time_start ) self . episode_list_lock . release ( ) if episode_finished is not None : if old_episode_finished : summary_data = { ""thread_id"" : thread_id , ""episode"" : episode , ""timestep"" : time_step , ""episode_reward"" : episode_reward } if not episode_finished ( summary_data ) : return elif not episode_finished ( self , thread_id ) : return episode += 1",The target function for a thread runs an agent and environment until signaled to stop . Adds rewards to shared episode rewards list .
"def _int_to_pos ( self , flat_position ) : return flat_position % self . env . action_space . screen_shape [ 0 ] , flat_position % self . env . action_space . screen_shape [ 1 ]",Returns x y from flat_position integer .
"def _wait_state ( self , state , reward , terminal ) : while state == [ None ] or not state : state , terminal , reward = self . _execute ( dict ( key = 0 ) ) return state , terminal , reward",Wait until there is a state .
"def apply_step ( self , variables , deltas ) : if len ( variables ) != len ( deltas ) : raise TensorForceError ( ""Invalid variables and deltas lists."" ) return tf . group ( * ( tf . assign_add ( ref = variable , value = delta ) for variable , delta in zip ( variables , deltas ) ) )",Applies the given ( and already calculated ) step deltas to the variable values .
"def minimize ( self , time , variables , * * kwargs ) : deltas = self . step ( time = time , variables = variables , * * kwargs ) with tf . control_dependencies ( control_inputs = deltas ) : return tf . no_op ( )",Performs an optimization step .
"def from_spec ( spec , kwargs = None ) : optimizer = util . get_object ( obj = spec , predefined_objects = tensorforce . core . optimizers . optimizers , kwargs = kwargs ) assert isinstance ( optimizer , Optimizer ) return optimizer",Creates an optimizer from a specification dict .
"def np_dtype ( dtype ) : if dtype == 'float' or dtype == float or dtype == np . float32 or dtype == tf . float32 : return np . float32 elif dtype == np . float64 or dtype == tf . float64 : return np . float64 elif dtype == np . float16 or dtype == tf . float16 : return np . float16 elif dtype == 'int' or dtype == int or dtype == np . int32 or dtype == tf . int32 : return np . int32 elif dtype == np . int64 or dtype == tf . int64 : return np . int64 elif dtype == np . int16 or dtype == tf . int16 : return np . int16 elif dtype == 'bool' or dtype == bool or dtype == np . bool_ or dtype == tf . bool : return np . bool_ else : raise TensorForceError ( ""Error: Type conversion from type {} not supported."" . format ( str ( dtype ) ) )",Translates dtype specifications in configurations to numpy data types . Args : dtype : String describing a numerical type ( e . g . float ) or numerical type primitive .
def get_tensor_dependencies ( tensor ) : dependencies = set ( ) dependencies . update ( tensor . op . inputs ) for sub_op in tensor . op . inputs : dependencies . update ( get_tensor_dependencies ( sub_op ) ) return dependencies,Utility method to get all dependencies ( including placeholders ) of a tensor ( backwards through the graph ) .
"def get_object ( obj , predefined_objects = None , default_object = None , kwargs = None ) : args = ( ) kwargs = dict ( ) if kwargs is None else kwargs if isinstance ( obj , str ) and os . path . isfile ( obj ) : with open ( obj , 'r' ) as fp : obj = json . load ( fp = fp ) if isinstance ( obj , dict ) : kwargs . update ( obj ) obj = kwargs . pop ( 'type' , None ) if predefined_objects is not None and obj in predefined_objects : obj = predefined_objects [ obj ] elif isinstance ( obj , str ) : if obj . find ( '.' ) != - 1 : module_name , function_name = obj . rsplit ( '.' , 1 ) module = importlib . import_module ( module_name ) obj = getattr ( module , function_name ) else : raise TensorForceError ( ""Error: object {} not found in predefined objects: {}"" . format ( obj , list ( predefined_objects or ( ) ) ) ) elif callable ( obj ) : pass elif default_object is not None : args = ( obj , ) obj = default_object else : return obj return obj ( * args , * * kwargs )",Utility method to map some kind of object specification to its content e . g . optimizer or baseline specifications to the respective classes .
"def prepare_kwargs ( raw , string_parameter = 'name' ) : kwargs = dict ( ) if isinstance ( raw , dict ) : kwargs . update ( raw ) elif isinstance ( raw , str ) : kwargs [ string_parameter ] = raw return kwargs",Utility method to convert raw string / diction input into a dictionary to pass into a function . Always returns a dictionary .
"def register_saver_ops ( self ) : variables = self . get_savable_variables ( ) if variables is None or len ( variables ) == 0 : self . _saver = None return base_scope = self . _get_base_variable_scope ( ) variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } self . _saver = tf . train . Saver ( var_list = variables_map , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )",Registers the saver operations to the graph in context .
"def save ( self , sess , save_path , timestep = None ) : if self . _saver is None : raise TensorForceError ( ""register_saver_ops should be called before save"" ) return self . _saver . save ( sess = sess , save_path = save_path , global_step = timestep , write_meta_graph = False , write_state = True , )",Saves this component s managed variables .
"def restore ( self , sess , save_path ) : if self . _saver is None : raise TensorForceError ( ""register_saver_ops should be called before restore"" ) self . _saver . restore ( sess = sess , save_path = save_path )",Restores the values of the managed variables from disk location .
def reset ( self ) : fetches = [ ] for processor in self . preprocessors : fetches . extend ( processor . reset ( ) or [ ] ) return fetches,Calls reset on all our Preprocessor objects .
"def process ( self , tensor ) : for processor in self . preprocessors : tensor = processor . process ( tensor = tensor ) return tensor",Process state .
"def processed_shape ( self , shape ) : for processor in self . preprocessors : shape = processor . processed_shape ( shape = shape ) return shape",Shape of preprocessed state given original shape .
"def from_spec ( spec , kwargs = None ) : if isinstance ( spec , dict ) : spec = [ spec ] stack = PreprocessorStack ( ) for preprocessor_spec in spec : preprocessor_kwargs = copy . deepcopy ( kwargs ) preprocessor = util . get_object ( obj = preprocessor_spec , predefined_objects = tensorforce . core . preprocessors . preprocessors , kwargs = preprocessor_kwargs ) assert isinstance ( preprocessor , Preprocessor ) stack . preprocessors . append ( preprocessor ) return stack",Creates a preprocessing stack from a specification dict .
"def tf_solve ( self , fn_x , x_init , * args ) : self . fn_x = fn_x args = self . initialize ( x_init , * args ) if self . unroll_loop : for _ in range ( self . max_iterations ) : next_step = self . next_step ( * args ) step = ( lambda : self . step ( * args ) ) do_nothing = ( lambda : args ) args = tf . cond ( pred = next_step , true_fn = step , false_fn = do_nothing ) else : args = tf . while_loop ( cond = self . next_step , body = self . step , loop_vars = args ) return args [ 0 ]",Iteratively solves an equation / optimization for $x$ involving an expression $f ( x ) $ .
"def execute ( self , action ) : next_state , rew , done , _ = self . env . step ( action ) return next_state , rew , done",Executes action observes next state and reward .
"def as_local_model ( self ) : super ( MemoryModel , self ) . as_local_model ( ) self . optimizer_spec = dict ( type = 'global_optimizer' , optimizer = self . optimizer_spec )",Makes sure our optimizer is wrapped into the global_optimizer meta . This is only relevant for distributed RL .
"def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . memory = Memory . from_spec ( spec = self . memory_spec , kwargs = dict ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , summary_labels = self . summary_labels ) ) self . optimizer = Optimizer . from_spec ( spec = self . optimizer_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) self . fn_discounted_cumulative_reward = tf . make_template ( name_ = 'discounted-cumulative-reward' , func_ = self . tf_discounted_cumulative_reward , custom_getter_ = custom_getter ) self . fn_reference = tf . make_template ( name_ = 'reference' , func_ = self . tf_reference , custom_getter_ = custom_getter ) self . fn_loss_per_instance = tf . make_template ( name_ = 'loss-per-instance' , func_ = self . tf_loss_per_instance , custom_getter_ = custom_getter ) self . fn_regularization_losses = tf . make_template ( name_ = 'regularization-losses' , func_ = self . tf_regularization_losses , custom_getter_ = custom_getter ) self . fn_loss = tf . make_template ( name_ = 'loss' , func_ = self . tf_loss , custom_getter_ = custom_getter ) self . fn_optimization = tf . make_template ( name_ = 'optimization' , func_ = self . tf_optimization , custom_getter_ = custom_getter ) self . fn_import_experience = tf . make_template ( name_ = 'import-experience' , func_ = self . tf_import_experience , custom_getter_ = custom_getter ) return custom_getter",Constructs the memory and the optimizer objects . Generates and stores all template functions .
"def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : if discount is None : discount = self . discount def cumulate ( cumulative , reward_terminal_horizon_subtract ) : rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract return tf . where ( condition = is_terminal , x = rew , y = tf . where ( condition = is_over_horizon , x = ( rew + cumulative * discount - sub ) , y = ( rew + cumulative * discount ) ) ) def len_ ( cumulative , term ) : return tf . where ( condition = term , x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , y = cumulative + 1 ) reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) if horizon > 0 : horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) else : horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) reward = tf . scan ( fn = cumulate , elems = ( reward , terminal , off_horizon , horizon_subtractions ) , initializer = final_reward if horizon != 1 else 0.0 ) return tf . reverse ( tensor = reward , axis = ( 0 , ) )",Creates and returns the TensorFlow operations for calculating the sequence of discounted cumulative rewards for a given sequence of single rewards .
"def tf_reference ( self , states , internals , actions , terminal , reward , next_states , next_internals , update ) : return None",Creates the TensorFlow operations for obtaining the reference tensor ( s ) in case of a comparative loss .
"def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : raise NotImplementedError",Creates the TensorFlow operations for calculating the loss per batch instance .
"def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : loss_per_instance = self . fn_loss_per_instance ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) with tf . control_dependencies ( control_inputs = ( updated , ) ) : loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) if 'losses' in self . summary_labels : tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) if len ( losses ) > 0 : loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) if 'regularization' in self . summary_labels : for name in sorted ( losses ) : tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) return loss",Creates the TensorFlow operations for calculating the full loss of a batch .
"def optimizer_arguments ( self , states , internals , actions , terminal , reward , next_states , next_internals ) : arguments = dict ( time = self . global_timestep , variables = self . get_variables ( ) , arguments = dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = tf . constant ( value = True ) ) , fn_reference = self . fn_reference , fn_loss = self . fn_loss ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . get_variables ( ) return arguments",Returns the optimizer arguments including the time the list of variables to optimize and various functions which the optimizer might require to perform an update step .
"def tf_optimization ( self , states , internals , actions , terminal , reward , next_states = None , next_internals = None ) : arguments = self . optimizer_arguments ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) return self . optimizer . minimize ( * * arguments )",Creates the TensorFlow operations for performing an optimization update step based on the given input states and actions batch .
"def tf_observe_timestep ( self , states , internals , actions , terminal , reward ) : stored = self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( stored , ) ) : unit = self . update_mode [ 'unit' ] batch_size = self . update_mode [ 'batch_size' ] frequency = self . update_mode . get ( 'frequency' , batch_size ) first_update = self . update_mode . get ( 'first_update' , 0 ) if unit == 'timesteps' : optimize = tf . logical_and ( x = tf . equal ( x = ( self . timestep % frequency ) , y = 0 ) , y = tf . logical_and ( x = tf . greater_equal ( x = self . timestep , y = batch_size ) , y = tf . greater_equal ( x = self . timestep , y = first_update ) ) ) elif unit == 'episodes' : optimize = tf . logical_and ( x = tf . equal ( x = ( self . episode % frequency ) , y = 0 ) , y = tf . logical_and ( x = tf . greater ( x = tf . count_nonzero ( input_tensor = terminal ) , y = 0 ) , y = tf . logical_and ( x = tf . greater_equal ( x = self . episode , y = batch_size ) , y = tf . greater_equal ( x = self . episode , y = first_update ) ) ) ) elif unit == 'sequences' : sequence_length = self . update_mode . get ( 'length' , 8 ) optimize = tf . logical_and ( x = tf . equal ( x = ( self . timestep % frequency ) , y = 0 ) , y = tf . logical_and ( x = tf . greater_equal ( x = self . timestep , y = ( batch_size + sequence_length - 1 ) ) , y = tf . greater_equal ( x = self . timestep , y = first_update ) ) ) else : raise TensorForceError ( ""Invalid update unit: {}."" . format ( unit ) ) def true_fn ( ) : if unit == 'timesteps' : batch = self . memory . retrieve_timesteps ( n = batch_size ) elif unit == 'episodes' : batch = self . memory . retrieve_episodes ( n = batch_size ) elif unit == 'sequences' : batch = self . memory . retrieve_sequences ( n = batch_size , sequence_length = sequence_length ) batch = util . map_tensors ( fn = ( lambda tensor : tf . stop_gradient ( input = tensor ) ) , tensors = batch ) optimize = self . fn_optimization ( * * batch ) with tf . control_dependencies ( control_inputs = ( optimize , ) ) : return tf . logical_and ( x = True , y = True ) return tf . cond ( pred = optimize , true_fn = true_fn , false_fn = tf . no_op )",Creates and returns the op that - if frequency condition is hit - pulls a batch from the memory and does one optimization step .
"def tf_import_experience ( self , states , internals , actions , terminal , reward ) : return self . memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Imports experiences into the TensorFlow memory structure . Can be used to import off - policy data .
"def import_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )",Stores experiences .
"def tf_step ( self , time , variables , arguments , fn_loss , fn_reference , * * kwargs ) : arguments [ 'reference' ] = fn_reference ( * * arguments ) loss_before = - fn_loss ( * * arguments ) with tf . control_dependencies ( control_inputs = ( loss_before , ) ) : deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , fn_loss = fn_loss , return_estimated_improvement = True , * * kwargs ) if isinstance ( deltas , tuple ) : if len ( deltas ) != 2 : raise TensorForceError ( ""Unexpected output of internal optimizer."" ) deltas , estimated_improvement = deltas estimated_improvement = - estimated_improvement else : estimated_improvement = None with tf . control_dependencies ( control_inputs = deltas ) : loss_step = - fn_loss ( * * arguments ) with tf . control_dependencies ( control_inputs = ( loss_step , ) ) : def evaluate_step ( deltas ) : with tf . control_dependencies ( control_inputs = deltas ) : applied = self . apply_step ( variables = variables , deltas = deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : return - fn_loss ( * * arguments ) return self . solver . solve ( fn_x = evaluate_step , x_init = deltas , base_value = loss_before , target_value = loss_step , estimated_improvement = estimated_improvement )",Creates the TensorFlow operations for performing an optimization step .
"def from_spec ( spec , kwargs = None ) : distribution = util . get_object ( obj = spec , predefined_objects = tensorforce . core . distributions . distributions , kwargs = kwargs ) assert isinstance ( distribution , Distribution ) return distribution",Creates a distribution from a specification dict .
"def reset ( self ) : self . episode , self . timestep , self . next_internals = self . model . reset ( ) self . current_internals = self . next_internals",Resets the agent to its initial state ( e . g . on experiment start ) . Updates the Model s internal episode and time step counter internal states and resets preprocessors .
"def act ( self , states , deterministic = False , independent = False , fetch_tensors = None , buffered = True , index = 0 ) : self . current_internals = self . next_internals if self . unique_state : self . current_states = dict ( state = np . asarray ( states ) ) else : self . current_states = { name : np . asarray ( states [ name ] ) for name in sorted ( states ) } if fetch_tensors is not None : self . current_actions , self . next_internals , self . timestep , self . fetched_tensors = self . model . act ( states = self . current_states , internals = self . current_internals , deterministic = deterministic , independent = independent , fetch_tensors = fetch_tensors , index = index ) if self . unique_action : return self . current_actions [ 'action' ] , self . fetched_tensors else : return self . current_actions , self . fetched_tensors self . current_actions , self . next_internals , self . timestep = self . model . act ( states = self . current_states , internals = self . current_internals , deterministic = deterministic , independent = independent , index = index ) if buffered : if self . unique_action : return self . current_actions [ 'action' ] else : return self . current_actions else : if self . unique_action : return self . current_actions [ 'action' ] , self . current_states , self . current_internals else : return self . current_actions , self . current_states , self . current_internals",Return action ( s ) for given state ( s ) . States preprocessing and exploration are applied if configured accordingly .
"def observe ( self , terminal , reward , index = 0 ) : self . current_terminal = terminal self . current_reward = reward if self . batched_observe : self . observe_terminal [ index ] . append ( self . current_terminal ) self . observe_reward [ index ] . append ( self . current_reward ) if self . current_terminal or len ( self . observe_terminal [ index ] ) >= self . batching_capacity : self . episode = self . model . observe ( terminal = self . observe_terminal [ index ] , reward = self . observe_reward [ index ] , index = index ) self . observe_terminal [ index ] = list ( ) self . observe_reward [ index ] = list ( ) else : self . episode = self . model . observe ( terminal = self . current_terminal , reward = self . current_reward )",Observe experience from the environment to learn from . Optionally pre - processes rewards Child classes should call super to get the processed reward EX : terminal reward = super () ...
"def atomic_observe ( self , states , actions , internals , reward , terminal ) : self . current_terminal = terminal self . current_reward = reward if self . unique_state : states = dict ( state = states ) if self . unique_action : actions = dict ( action = actions ) self . episode = self . model . atomic_observe ( states = states , actions = actions , internals = internals , terminal = self . current_terminal , reward = self . current_reward )",Utility method for unbuffered observing where each tuple is inserted into TensorFlow via a single session call thus avoiding race conditions in multi - threaded mode .
"def save_model ( self , directory = None , append_timestep = True ) : return self . model . save ( directory = directory , append_timestep = append_timestep )",Save TensorFlow model . If no checkpoint directory is given the model s default saver directory is used . Optionally appends current timestep to prevent overwriting previous checkpoint files . Turn off to be able to load model from the same given path argument as given here .
"def restore_model ( self , directory = None , file = None ) : self . model . restore ( directory = directory , file = file )",Restore TensorFlow model . If no checkpoint file is given the latest checkpoint is restored . If no checkpoint directory is given the model s default saver directory is used ( unless file specifies the entire path ) .
"def from_spec ( spec , kwargs ) : agent = util . get_object ( obj = spec , predefined_objects = tensorforce . agents . agents , kwargs = kwargs ) assert isinstance ( agent , Agent ) return agent",Creates an agent from a specification dict .
"def get_named_tensor ( self , name ) : if name in self . named_tensors : return True , self . named_tensors [ name ] else : return False , None",Returns a named tensor if available .
"def from_spec ( spec , kwargs = None ) : network = util . get_object ( obj = spec , default_object = LayeredNetwork , kwargs = kwargs ) assert isinstance ( network , Network ) return network",Creates a network from a specification dict .
"def put ( self , item , priority = None ) : if not self . _isfull ( ) : self . _memory . append ( None ) position = self . _next_position_then_increment ( ) old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) row = _SumRow ( item , priority ) self . _memory [ position ] = row self . _update_internal_nodes ( position , ( row . priority or 0 ) - old_priority )",Stores a transition in replay memory .
"def move ( self , external_index , new_priority ) : index = external_index + ( self . _capacity - 1 ) return self . _move ( index , new_priority )",Change the priority of a leaf node
"def _move ( self , index , new_priority ) : item , old_priority = self . _memory [ index ] old_priority = old_priority or 0 self . _memory [ index ] = _SumRow ( item , new_priority ) self . _update_internal_nodes ( index , new_priority - old_priority )",Change the priority of a leaf node .
"def _update_internal_nodes ( self , index , delta ) : while index > 0 : index = ( index - 1 ) // 2 self . _memory [ index ] += delta",Update internal priority sums when leaf priority has been changed . Args : index : leaf node index delta : change in priority
def _next_position_then_increment ( self ) : start = self . _capacity - 1 position = start + self . _position self . _position = ( self . _position + 1 ) % self . _capacity return position,Similar to position ++ .
"def _sample_with_priority ( self , p ) : parent = 0 while True : left = 2 * parent + 1 if left >= len ( self . _memory ) : return parent left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) if p <= left_p : parent = left else : if left + 1 >= len ( self . _memory ) : raise RuntimeError ( 'Right child is expected to exist.' ) p -= left_p parent = left + 1",Sample random element with priority greater than p .
"def sample_minibatch ( self , batch_size ) : pool_size = len ( self ) if pool_size == 0 : return [ ] delta_p = self . _memory [ 0 ] / batch_size chosen_idx = [ ] if abs ( self . _memory [ 0 ] ) < util . epsilon : chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) else : for i in xrange ( batch_size ) : lower = max ( i * delta_p , 0 ) upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) p = random . uniform ( lower , upper ) chosen_idx . append ( self . _sample_with_priority ( p ) ) return [ ( i , self . _memory [ i ] ) for i in chosen_idx ]",Sample minibatch of size batch_size .
"def get_batch ( self , batch_size , next_states = False ) : if batch_size > len ( self . observations ) : raise TensorForceError ( ""Requested batch size is larger than observations in memory: increase config.first_update."" ) states = { name : np . zeros ( ( batch_size , ) + tuple ( state [ 'shape' ] ) , dtype = util . np_dtype ( state [ 'type' ] ) ) for name , state in self . states_spec . items ( ) } internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) for shape , dtype in self . internals_spec ] actions = { name : np . zeros ( ( batch_size , ) + tuple ( action [ 'shape' ] ) , dtype = util . np_dtype ( action [ 'type' ] ) ) for name , action in self . actions_spec . items ( ) } terminal = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( 'bool' ) ) reward = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( 'float' ) ) if next_states : next_states = { name : np . zeros ( ( batch_size , ) + tuple ( state [ 'shape' ] ) , dtype = util . np_dtype ( state [ 'type' ] ) ) for name , state in self . states_spec . items ( ) } next_internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) for shape , dtype in self . internals_spec ] unseen_indices = list ( xrange ( self . none_priority_index + self . observations . _capacity - 1 , len ( self . observations ) + self . observations . _capacity - 1 ) ) self . batch_indices = unseen_indices [ : batch_size ] remaining = batch_size - len ( self . batch_indices ) if remaining : samples = self . observations . sample_minibatch ( remaining ) sample_indices = [ i for i , o in samples ] self . batch_indices += sample_indices np . random . shuffle ( self . batch_indices ) for n , index in enumerate ( self . batch_indices ) : observation , _ = self . observations . _memory [ index ] for name , state in states . items ( ) : state [ n ] = observation [ 0 ] [ name ] for k , internal in enumerate ( internals ) : internal [ n ] = observation [ 1 ] [ k ] for name , action in actions . items ( ) : action [ n ] = observation [ 2 ] [ name ] terminal [ n ] = observation [ 3 ] reward [ n ] = observation [ 4 ] if next_states : for name , next_state in next_states . items ( ) : next_state [ n ] = observation [ 5 ] [ name ] for k , next_internal in enumerate ( next_internals ) : next_internal [ n ] = observation [ 6 ] [ k ] if next_states : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Samples a batch of the specified size according to priority .
"def update_batch ( self , loss_per_instance ) : if self . batch_indices is None : raise TensorForceError ( ""Need to call get_batch before each update_batch call."" ) for index , loss in zip ( self . batch_indices , loss_per_instance ) : new_priority = ( np . abs ( loss ) + self . prioritization_constant ) ** self . prioritization_weight self . observations . _move ( index , new_priority ) self . none_priority_index += 1",Computes priorities according to loss .
"def import_experience ( self , experiences ) : if isinstance ( experiences , dict ) : if self . unique_state : experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) if self . unique_action : experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) self . model . import_experience ( * * experiences ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for experience in experiences : if self . unique_state : states [ 'state' ] . append ( experience [ 'states' ] ) else : for name in sorted ( states ) : states [ name ] . append ( experience [ 'states' ] [ name ] ) for n , internal in enumerate ( internals ) : internal . append ( experience [ 'internals' ] [ n ] ) if self . unique_action : actions [ 'action' ] . append ( experience [ 'actions' ] ) else : for name in sorted ( actions ) : actions [ name ] . append ( experience [ 'actions' ] [ name ] ) terminal . append ( experience [ 'terminal' ] ) reward . append ( experience [ 'reward' ] ) self . model . import_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Imports experiences .
"def connect ( self , timeout = 600 ) : if self . socket : raise TensorForceError ( ""Already connected to {}:{}. Only one connection allowed at a time. "" + ""Close first by calling `close`!"" . format ( self . host , self . port ) ) self . socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) if timeout < 5 or timeout is None : timeout = 5 err = 0 start_time = time . time ( ) while time . time ( ) - start_time < timeout : self . socket . settimeout ( 5 ) err = self . socket . connect_ex ( ( self . host , self . port ) ) if err == 0 : break time . sleep ( 1 ) if err != 0 : raise TensorForceError ( ""Error when trying to connect to {}:{}: errno={} errcode='{}' '{}'"" . format ( self . host , self . port , err , errno . errorcode [ err ] , os . strerror ( err ) ) )",Starts the server tcp connection on the given host : port .
"def disconnect ( self ) : if not self . socket : logging . warning ( ""No active socket to close!"" ) return self . socket . close ( ) self . socket = None",Ends our server tcp connection .
"def send ( self , message , socket_ ) : if not socket_ : raise TensorForceError ( ""No socket given in call to `send`!"" ) elif not isinstance ( message , dict ) : raise TensorForceError ( ""Message to be sent must be a dict!"" ) message = msgpack . packb ( message ) len_ = len ( message ) socket_ . send ( bytes ( ""{:08d}"" . format ( len_ ) , encoding = ""ascii"" ) + message )",Sends a message ( dict ) to the socket . Message consists of a 8 - byte len header followed by a msgpack - numpy encoded dict .
"def recv ( self , socket_ , encoding = None ) : unpacker = msgpack . Unpacker ( encoding = encoding ) response = socket_ . recv ( 8 ) if response == b"""" : raise TensorForceError ( ""No data received by socket.recv in call to method `recv` "" + ""(listener possibly closed)!"" ) orig_len = int ( response ) received_len = 0 while True : data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) if not data : raise TensorForceError ( ""No data of len {} received by socket.recv in call to method `recv`!"" . format ( orig_len - received_len ) ) data_len = len ( data ) received_len += data_len unpacker . feed ( data ) if received_len == orig_len : break for message in unpacker : sts = message . get ( ""status"" , message . get ( b""status"" ) ) if sts : if sts == ""ok"" or sts == b""ok"" : return message else : raise TensorForceError ( ""RemoteEnvironment server error: {}"" . format ( message . get ( ""message"" , ""not specified"" ) ) ) else : raise TensorForceError ( ""Message without field 'status' received!"" ) raise TensorForceError ( ""No message encoded in data stream (data stream had len={})"" . format ( orig_len ) )",Receives a message as msgpack - numpy encoded byte - string from the given socket object . Blocks until something was received .
"def is_action_available ( self , action ) : temp_state = np . rot90 ( self . _state , action ) return self . _is_action_available_left ( temp_state )",Determines whether action is available . That is executing it would change the state .
"def _is_action_available_left ( self , state ) : for row in range ( 4 ) : has_empty = False for col in range ( 4 ) : has_empty |= state [ row , col ] == 0 if state [ row , col ] != 0 and has_empty : return True if ( state [ row , col ] != 0 and col > 0 and state [ row , col ] == state [ row , col - 1 ] ) : return True return False",Determines whether action Left is available .
"def do_action ( self , action ) : temp_state = np . rot90 ( self . _state , action ) reward = self . _do_action_left ( temp_state ) self . _state = np . rot90 ( temp_state , - action ) self . _score += reward self . add_random_tile ( ) return reward",Execute action add a new tile update the score & return the reward .
"def _do_action_left ( self , state ) : reward = 0 for row in range ( 4 ) : merge_candidate = - 1 merged = np . zeros ( ( 4 , ) , dtype = np . bool ) for col in range ( 4 ) : if state [ row , col ] == 0 : continue if ( merge_candidate != - 1 and not merged [ merge_candidate ] and state [ row , merge_candidate ] == state [ row , col ] ) : state [ row , col ] = 0 merged [ merge_candidate ] = True state [ row , merge_candidate ] += 1 reward += 2 ** state [ row , merge_candidate ] else : merge_candidate += 1 if col != merge_candidate : state [ row , merge_candidate ] = state [ row , col ] state [ row , col ] = 0 return reward",Executes action Left .
"def add_random_tile ( self ) : x_pos , y_pos = np . where ( self . _state == 0 ) assert len ( x_pos ) != 0 empty_index = np . random . choice ( len ( x_pos ) ) value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value",Adds a random tile to the grid . Assumes that it has empty fields .
"def print_state ( self ) : def tile_string ( value ) : """"""Concert value to string."""""" if value > 0 : return '% 5d' % ( 2 ** value , ) return ""     "" separator_line = '-' * 25 print ( separator_line ) for row in range ( 4 ) : print ( ""|"" + ""|"" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + ""|"" ) print ( separator_line )",Prints the current state .
"def setup ( self ) : graph_default_context = self . setup_graph ( ) if self . execution_type == ""distributed"" and self . server is None and self . is_local_model : self . start_server ( ) with tf . device ( device_name_or_function = self . device ) : with tf . variable_scope ( name_or_scope = self . scope , reuse = False ) : self . variables = dict ( ) self . all_variables = dict ( ) self . registered_variables = set ( ) self . setup_placeholders ( ) self . setup_components_and_tf_funcs ( ) self . fn_initialize ( ) if self . summarizer_spec is not None : with tf . name_scope ( name = 'summarizer' ) : self . summarizer = tf . contrib . summary . create_file_writer ( logdir = self . summarizer_spec [ 'directory' ] , max_queue = None , flush_millis = ( self . summarizer_spec . get ( 'flush' , 10 ) * 1000 ) , filename_suffix = None , name = None ) default_summarizer = self . summarizer . as_default ( ) assert 'steps' not in self . summarizer_spec record_summaries = tf . contrib . summary . always_record_summaries ( ) default_summarizer . __enter__ ( ) record_summaries . __enter__ ( ) states = util . map_tensors ( fn = tf . identity , tensors = self . states_input ) internals = util . map_tensors ( fn = tf . identity , tensors = self . internals_input ) actions = util . map_tensors ( fn = tf . identity , tensors = self . actions_input ) terminal = tf . identity ( input = self . terminal_input ) reward = tf . identity ( input = self . reward_input ) deterministic = tf . identity ( input = self . deterministic_input ) independent = tf . identity ( input = self . independent_input ) episode_index = tf . identity ( input = self . episode_index_input ) states , actions , reward = self . fn_preprocess ( states = states , actions = actions , reward = reward ) self . create_operations ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , deterministic = deterministic , independent = independent , index = episode_index ) if 'inputs' in self . summary_labels or 'states' in self . summary_labels : for name in sorted ( states ) : tf . contrib . summary . histogram ( name = ( 'states-' + name ) , tensor = states [ name ] ) if 'inputs' in self . summary_labels or 'actions' in self . summary_labels : for name in sorted ( actions ) : tf . contrib . summary . histogram ( name = ( 'actions-' + name ) , tensor = actions [ name ] ) if 'inputs' in self . summary_labels or 'reward' in self . summary_labels : tf . contrib . summary . histogram ( name = 'reward' , tensor = reward ) if 'graph' in self . summary_labels : with tf . name_scope ( name = 'summarizer' ) : graph_def = self . graph . as_graph_def ( ) graph_str = tf . constant ( value = graph_def . SerializeToString ( ) , dtype = tf . string , shape = ( ) ) self . graph_summary = tf . contrib . summary . graph ( param = graph_str , step = self . global_timestep ) if 'meta_param_recorder_class' in self . summarizer_spec : self . graph_summary = tf . group ( self . graph_summary , * self . summarizer_spec [ 'meta_param_recorder_class' ] . build_metagraph_list ( ) ) if self . summarizer_spec is not None : record_summaries . __exit__ ( None , None , None ) default_summarizer . __exit__ ( None , None , None ) with tf . name_scope ( name = 'summarizer' ) : self . flush_summarizer = tf . contrib . summary . flush ( ) self . summarizer_init_op = tf . contrib . summary . summary_writer_initializer_op ( ) assert len ( self . summarizer_init_op ) == 1 self . summarizer_init_op = self . summarizer_init_op [ 0 ] if self . execution_type == ""distributed"" and not self . is_local_model : return self . setup_saver ( ) self . setup_scaffold ( ) hooks = self . setup_hooks ( ) self . setup_session ( self . server , hooks , graph_default_context )",Sets up the TensorFlow model graph starts the servers ( distributed mode ) creates summarizers and savers initializes ( and enters ) the TensorFlow session .
"def setup_graph ( self ) : graph_default_context = None if self . execution_type == ""single"" : self . graph = tf . Graph ( ) graph_default_context = self . graph . as_default ( ) graph_default_context . __enter__ ( ) self . global_model = None elif self . execution_type == ""distributed"" : if self . distributed_spec [ ""job"" ] == ""ps"" : return None elif self . distributed_spec [ ""job"" ] == ""worker"" : if self . is_local_model : graph = tf . Graph ( ) graph_default_context = graph . as_default ( ) graph_default_context . __enter__ ( ) self . global_model = deepcopy ( self ) self . global_model . is_local_model = False self . global_model . setup ( ) self . graph = graph self . as_local_model ( ) self . scope += '-worker' + str ( self . distributed_spec [ ""task_index"" ] ) else : self . graph = tf . get_default_graph ( ) self . global_model = None self . device = tf . train . replica_device_setter ( worker_device = self . device , cluster = self . distributed_spec [ ""cluster_spec"" ] ) else : raise TensorForceError ( ""Unsupported job type: {}!"" . format ( self . distributed_spec [ ""job"" ] ) ) else : raise TensorForceError ( ""Unsupported distributed type: {}!"" . format ( self . distributed_spec [ ""type"" ] ) ) return graph_default_context",Creates our Graph and figures out which shared / global model to hook up to . If we are in a global - model s setup procedure we do not create a new graph ( return None as the context ) . We will instead use the already existing local replica graph of the model .
"def start_server ( self ) : self . server = tf . train . Server ( server_or_cluster_def = self . distributed_spec [ ""cluster_spec"" ] , job_name = self . distributed_spec [ ""job"" ] , task_index = self . distributed_spec [ ""task_index"" ] , protocol = self . distributed_spec . get ( ""protocol"" ) , config = self . distributed_spec . get ( ""session_config"" ) , start = True ) if self . distributed_spec [ ""job"" ] == ""ps"" : self . server . join ( ) quit ( )",Creates and stores a tf server ( and optionally joins it if we are a parameter - server ) . Only relevant if we are running in distributed mode .
"def setup_placeholders ( self ) : for name in sorted ( self . states_spec ) : self . states_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( self . states_spec [ name ] [ 'type' ] ) , shape = ( None , ) + tuple ( self . states_spec [ name ] [ 'shape' ] ) , name = ( 'state-' + name ) ) if self . states_preprocessing_spec is None : for name in sorted ( self . states_spec ) : self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] elif not isinstance ( self . states_preprocessing_spec , list ) and all ( name in self . states_spec for name in self . states_preprocessing_spec ) : for name in sorted ( self . states_spec ) : if name in self . states_preprocessing_spec : preprocessing = PreprocessorStack . from_spec ( spec = self . states_preprocessing_spec [ name ] , kwargs = dict ( shape = self . states_spec [ name ] [ 'shape' ] ) ) self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] self . states_spec [ name ] [ 'shape' ] = preprocessing . processed_shape ( shape = self . states_spec [ name ] [ 'unprocessed_shape' ] ) self . states_preprocessing [ name ] = preprocessing else : self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] elif ""type"" in self . states_preprocessing_spec : preprocessing = PreprocessorStack . from_spec ( spec = self . states_preprocessing_spec , kwargs = dict ( shape = self . states_spec [ name ] [ 'shape' ] ) ) for name in sorted ( self . states_spec ) : self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] self . states_spec [ name ] [ 'shape' ] = preprocessing . processed_shape ( shape = self . states_spec [ name ] [ 'unprocessed_shape' ] ) self . states_preprocessing [ name ] = preprocessing else : for name in sorted ( self . states_spec ) : preprocessing = PreprocessorStack . from_spec ( spec = self . states_preprocessing_spec , kwargs = dict ( shape = self . states_spec [ name ] [ 'shape' ] ) ) self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] self . states_spec [ name ] [ 'shape' ] = preprocessing . processed_shape ( shape = self . states_spec [ name ] [ 'unprocessed_shape' ] ) self . states_preprocessing [ name ] = preprocessing for name in sorted ( self . actions_spec ) : self . actions_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( self . actions_spec [ name ] [ 'type' ] ) , shape = ( None , ) + tuple ( self . actions_spec [ name ] [ 'shape' ] ) , name = ( 'action-' + name ) ) if self . actions_exploration_spec is None : pass elif all ( name in self . actions_spec for name in self . actions_exploration_spec ) : for name in sorted ( self . actions_spec ) : if name in self . actions_exploration : self . actions_exploration [ name ] = Exploration . from_spec ( spec = self . actions_exploration_spec [ name ] ) else : for name in sorted ( self . actions_spec ) : self . actions_exploration [ name ] = Exploration . from_spec ( spec = self . actions_exploration_spec ) self . terminal_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( None , ) , name = 'terminal' ) self . reward_input = tf . placeholder ( dtype = util . tf_dtype ( 'float' ) , shape = ( None , ) , name = 'reward' ) if self . reward_preprocessing_spec is not None : self . reward_preprocessing = PreprocessorStack . from_spec ( spec = self . reward_preprocessing_spec , kwargs = dict ( shape = ( ) ) ) if self . reward_preprocessing . processed_shape ( shape = ( ) ) != ( ) : raise TensorForceError ( ""Invalid reward preprocessing!"" ) self . deterministic_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( ) , name = 'deterministic' ) self . independent_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( ) , name = 'independent' )",Creates the TensorFlow placeholders variables ops and functions for this model . NOTE : Does not add the internal state placeholders and initialization values to the model yet as that requires the model s Network ( if any ) to be generated first .
"def setup_components_and_tf_funcs ( self , custom_getter = None ) : if custom_getter is None : def custom_getter ( getter , name , registered = False , * * kwargs ) : """"""
                To be passed to tf.make_template() as 'custom_getter_'.
                """""" if registered : self . registered_variables . add ( name ) elif name in self . registered_variables : registered = True variable = getter ( name = name , * * kwargs ) if registered : pass elif name in self . all_variables : assert variable is self . all_variables [ name ] if kwargs . get ( 'trainable' , True ) : assert variable is self . variables [ name ] if 'variables' in self . summary_labels : tf . contrib . summary . histogram ( name = name , tensor = variable ) else : self . all_variables [ name ] = variable if kwargs . get ( 'trainable' , True ) : self . variables [ name ] = variable if 'variables' in self . summary_labels : tf . contrib . summary . histogram ( name = name , tensor = variable ) return variable self . fn_initialize = tf . make_template ( name_ = 'initialize' , func_ = self . tf_initialize , custom_getter_ = custom_getter ) self . fn_preprocess = tf . make_template ( name_ = 'preprocess' , func_ = self . tf_preprocess , custom_getter_ = custom_getter ) self . fn_actions_and_internals = tf . make_template ( name_ = 'actions-and-internals' , func_ = self . tf_actions_and_internals , custom_getter_ = custom_getter ) self . fn_observe_timestep = tf . make_template ( name_ = 'observe-timestep' , func_ = self . tf_observe_timestep , custom_getter_ = custom_getter ) self . fn_action_exploration = tf . make_template ( name_ = 'action-exploration' , func_ = self . tf_action_exploration , custom_getter_ = custom_getter ) return custom_getter",Allows child models to create model s component objects such as optimizer ( s ) memory ( s ) etc .. Creates all tensorflow functions via tf . make_template calls on all the class tf_ - methods .
"def setup_saver ( self ) : if self . execution_type == ""single"" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) for c in self . get_savable_components ( ) : c . register_saver_ops ( ) self . saver = tf . train . Saver ( var_list = global_variables , reshape = False , sharded = False , max_to_keep = 5 , keep_checkpoint_every_n_hours = 10000.0 , name = None , restore_sequentially = False , saver_def = None , builder = None , defer_build = False , allow_empty = True , write_version = tf . train . SaverDef . V2 , pad_step_number = False , save_relative_paths = True )",Creates the tf . train . Saver object and stores it in self . saver .
"def setup_scaffold ( self ) : if self . execution_type == ""single"" : global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) if self . graph_summary is None : ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) ready_for_local_init_op = None local_init_op = None else : ready_op = None ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) local_init_op = self . graph_summary else : global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) init_op = tf . variables_initializer ( var_list = global_variables ) if self . summarizer_init_op is not None : init_op = tf . group ( init_op , self . summarizer_init_op ) ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) if self . graph_summary is None : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) else : local_init_op = tf . group ( tf . variables_initializer ( var_list = local_variables ) , self . graph_summary , * ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( self . get_variables ( include_submodules = True ) , self . global_model . get_variables ( include_submodules = True ) ) ) ) def init_fn ( scaffold , session ) : if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : directory = self . saver_spec [ 'directory' ] file = self . saver_spec . get ( 'file' ) if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = directory , latest_filename = None ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) if file is not None : try : scaffold . saver . restore ( sess = session , save_path = file ) session . run ( fetches = self . list_buffer_index_reset_op ) except tf . errors . NotFoundError : raise TensorForceError ( ""Error: Existing checkpoint could not be loaded! Set \""load\"" to false in saver_spec."" ) self . scaffold = tf . train . Scaffold ( init_op = init_op , init_feed_dict = None , init_fn = init_fn , ready_op = ready_op , ready_for_local_init_op = ready_for_local_init_op , local_init_op = local_init_op , summary_op = None , saver = self . saver , copy_from_scaffold = None )",Creates the tf . train . Scaffold object and assigns it to self . scaffold . Other fields of the Scaffold are generated automatically .
"def setup_hooks ( self ) : hooks = list ( ) if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : self . saver_directory = self . saver_spec [ 'directory' ] hooks . append ( tf . train . CheckpointSaverHook ( checkpoint_dir = self . saver_directory , save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , save_steps = self . saver_spec . get ( 'steps' ) , saver = None , checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , scaffold = self . scaffold , listeners = None ) ) else : self . saver_directory = None return hooks",Creates and returns a list of hooks to use in a session . Populates self . saver_directory .
"def setup_session ( self , server , hooks , graph_default_context ) : if self . execution_type == ""distributed"" : session_creator = tf . train . ChiefSessionCreator ( scaffold = self . scaffold , master = server . target , config = self . session_config , checkpoint_dir = None , checkpoint_filename_with_path = None ) self . monitored_session = tf . train . MonitoredSession ( session_creator = session_creator , hooks = hooks , stop_grace_period_secs = 120 ) if self . tf_session_dump_dir != """" : self . monitored_session = DumpingDebugWrapperSession ( self . monitored_session , self . tf_session_dump_dir ) else : self . monitored_session = tf . train . SingularMonitoredSession ( hooks = hooks , scaffold = self . scaffold , master = '' , config = self . session_config , checkpoint_dir = None ) if graph_default_context : graph_default_context . __exit__ ( None , None , None ) self . graph . finalize ( ) self . monitored_session . __enter__ ( ) self . session = self . monitored_session . _tf_sess ( )",Creates and then enters the session for this model ( finalizes the graph ) .
"def close ( self ) : if self . flush_summarizer is not None : self . monitored_session . run ( fetches = self . flush_summarizer ) if self . saver_directory is not None : self . save ( append_timestep = True ) self . monitored_session . __exit__ ( None , None , None )",Saves the model ( of saver dir is given ) and closes the session .
"def tf_initialize ( self ) : with tf . device ( device_name_or_function = ( self . global_model . device if self . global_model else self . device ) ) : collection = self . graph . get_collection ( name = 'global-timestep' ) if len ( collection ) == 0 : self . global_timestep = tf . get_variable ( name = 'global-timestep' , shape = ( ) , dtype = tf . int64 , trainable = False , initializer = tf . constant_initializer ( value = 0 , dtype = tf . int64 ) , collections = [ 'global-timestep' , tf . GraphKeys . GLOBAL_STEP ] ) else : assert len ( collection ) == 1 self . global_timestep = collection [ 0 ] collection = self . graph . get_collection ( name = 'global-episode' ) if len ( collection ) == 0 : self . global_episode = tf . get_variable ( name = 'global-episode' , shape = ( ) , dtype = tf . int64 , trainable = False , initializer = tf . constant_initializer ( value = 0 , dtype = tf . int64 ) , collections = [ 'global-episode' ] ) else : assert len ( collection ) == 1 self . global_episode = collection [ 0 ] self . timestep = tf . get_variable ( name = 'timestep' , shape = ( ) , dtype = tf . int64 , initializer = tf . constant_initializer ( value = 0 , dtype = tf . int64 ) , trainable = False ) self . episode = tf . get_variable ( name = 'episode' , shape = ( ) , dtype = tf . int64 , initializer = tf . constant_initializer ( value = 0 , dtype = tf . int64 ) , trainable = False ) self . episode_index_input = tf . placeholder ( name = 'episode_index' , shape = ( ) , dtype = tf . int32 , ) for name in sorted ( self . states_spec ) : self . list_states_buffer [ name ] = tf . get_variable ( name = ( 'state-{}' . format ( name ) ) , shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . states_spec [ name ] [ 'shape' ] ) ) , dtype = util . tf_dtype ( self . states_spec [ name ] [ 'type' ] ) , trainable = False ) for name in sorted ( self . internals_spec ) : self . list_internals_buffer [ name ] = tf . get_variable ( name = ( 'internal-{}' . format ( name ) ) , shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . internals_spec [ name ] [ 'shape' ] ) ) , dtype = util . tf_dtype ( self . internals_spec [ name ] [ 'type' ] ) , trainable = False ) for name in sorted ( self . actions_spec ) : self . list_actions_buffer [ name ] = tf . get_variable ( name = ( 'action-{}' . format ( name ) ) , shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . actions_spec [ name ] [ 'shape' ] ) ) , dtype = util . tf_dtype ( self . actions_spec [ name ] [ 'type' ] ) , trainable = False ) self . list_buffer_index = tf . get_variable ( name = 'buffer-index' , shape = ( self . num_parallel , ) , dtype = util . tf_dtype ( 'int' ) , trainable = False )",Creates tf Variables for the local state / internals / action - buffers and for the local and global counters for timestep and episode .
"def tf_preprocess ( self , states , actions , reward ) : for name in sorted ( self . states_preprocessing ) : states [ name ] = self . states_preprocessing [ name ] . process ( tensor = states [ name ] ) if self . reward_preprocessing is not None : reward = self . reward_preprocessing . process ( tensor = reward ) return states , actions , reward",Applies preprocessing ops to the raw states / action / reward inputs .
"def tf_action_exploration ( self , action , exploration , action_spec ) : action_shape = tf . shape ( input = action ) exploration_value = exploration . tf_explore ( episode = self . global_episode , timestep = self . global_timestep , shape = action_spec [ 'shape' ] ) exploration_value = tf . expand_dims ( input = exploration_value , axis = 0 ) if action_spec [ 'type' ] == 'bool' : action = tf . where ( condition = ( tf . random_uniform ( shape = action_shape ) < exploration_value ) , x = ( tf . random_uniform ( shape = action_shape ) < 0.5 ) , y = action ) elif action_spec [ 'type' ] == 'int' : action = tf . where ( condition = ( tf . random_uniform ( shape = action_shape ) < exploration_value ) , x = tf . random_uniform ( shape = action_shape , maxval = action_spec [ 'num_actions' ] , dtype = util . tf_dtype ( 'int' ) ) , y = action ) elif action_spec [ 'type' ] == 'float' : noise = tf . random_normal ( shape = action_shape , dtype = util . tf_dtype ( 'float' ) ) action += noise * exploration_value if 'min_value' in action_spec : action = tf . clip_by_value ( t = action , clip_value_min = action_spec [ 'min_value' ] , clip_value_max = action_spec [ 'max_value' ] ) return action",Applies optional exploration to the action ( post - processor for action outputs ) .
"def create_act_operations ( self , states , internals , deterministic , independent , index ) : operations = list ( ) if self . variable_noise is not None and self . variable_noise > 0.0 : self . fn_actions_and_internals ( states = states , internals = internals , deterministic = deterministic ) noise_deltas = list ( ) for variable in self . get_variables ( ) : noise_delta = tf . random_normal ( shape = util . shape ( variable ) , mean = 0.0 , stddev = self . variable_noise ) noise_deltas . append ( noise_delta ) operations . append ( variable . assign_add ( delta = noise_delta ) ) with tf . control_dependencies ( control_inputs = operations ) : self . actions_output , self . internals_output = self . fn_actions_and_internals ( states = states , internals = internals , deterministic = deterministic ) with tf . control_dependencies ( control_inputs = [ self . actions_output [ name ] for name in sorted ( self . actions_output ) ] ) : operations = list ( ) if self . variable_noise is not None and self . variable_noise > 0.0 : for variable , noise_delta in zip ( self . get_variables ( ) , noise_deltas ) : operations . append ( variable . assign_sub ( delta = noise_delta ) ) with tf . control_dependencies ( control_inputs = operations ) : for name in sorted ( self . actions_exploration ) : self . actions_output [ name ] = tf . cond ( pred = self . deterministic_input , true_fn = ( lambda : self . actions_output [ name ] ) , false_fn = ( lambda : self . fn_action_exploration ( action = self . actions_output [ name ] , exploration = self . actions_exploration [ name ] , action_spec = self . actions_spec [ name ] ) ) ) def independent_act ( ) : """"""
            Does not store state, action, internal in buffer. Hence, does not have any influence on learning.
            Does not increase timesteps.
            """""" return self . global_timestep def normal_act ( ) : """"""
            Stores current states, internals and actions in buffer. Increases timesteps.
            """""" operations = list ( ) batch_size = tf . shape ( input = states [ next ( iter ( sorted ( states ) ) ) ] ) [ 0 ] for name in sorted ( states ) : operations . append ( tf . assign ( ref = self . list_states_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , value = states [ name ] ) ) for name in sorted ( internals ) : operations . append ( tf . assign ( ref = self . list_internals_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , value = internals [ name ] ) ) for name in sorted ( self . actions_output ) : operations . append ( tf . assign ( ref = self . list_actions_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , value = self . actions_output [ name ] ) ) with tf . control_dependencies ( control_inputs = operations ) : operations = list ( ) operations . append ( tf . assign ( ref = self . list_buffer_index [ index : index + 1 ] , value = tf . add ( self . list_buffer_index [ index : index + 1 ] , tf . constant ( [ 1 ] ) ) ) ) operations . append ( tf . assign_add ( ref = self . timestep , value = tf . to_int64 ( x = batch_size ) ) ) operations . append ( tf . assign_add ( ref = self . global_timestep , value = tf . to_int64 ( x = batch_size ) ) ) with tf . control_dependencies ( control_inputs = operations ) : return self . global_timestep + 0 self . timestep_output = tf . cond ( pred = independent , true_fn = independent_act , false_fn = normal_act )",Creates and stores tf operations that are fetched when calling act () : actions_output internals_output and timestep_output .
"def create_observe_operations ( self , terminal , reward , index ) : num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : fn = ( lambda x : tf . stop_gradient ( input = x [ : self . list_buffer_index [ index ] ] ) ) states = util . map_tensors ( fn = fn , tensors = self . list_states_buffer , index = index ) internals = util . map_tensors ( fn = fn , tensors = self . list_internals_buffer , index = index ) actions = util . map_tensors ( fn = fn , tensors = self . list_actions_buffer , index = index ) terminal = tf . stop_gradient ( input = terminal ) reward = tf . stop_gradient ( input = reward ) observation = self . fn_observe_timestep ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( observation , ) ) : reset_index = tf . assign ( ref = self . list_buffer_index [ index ] , value = 0 ) with tf . control_dependencies ( control_inputs = ( reset_index , ) ) : self . episode_output = self . global_episode + 0 self . list_buffer_index_reset_op = tf . group ( * ( tf . assign ( ref = self . list_buffer_index [ n ] , value = 0 ) for n in range ( self . num_parallel ) ) )",Returns the tf op to fetch when an observation batch is passed in ( e . g . an episode s rewards and terminals ) . Uses the filled tf buffers for states actions and internals to run the tf_observe_timestep ( model - dependent ) resets buffer index and increases counters ( episodes timesteps ) .
"def create_atomic_observe_operations ( self , states , actions , internals , terminal , reward , index ) : num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : states = util . map_tensors ( fn = tf . stop_gradient , tensors = states ) internals = util . map_tensors ( fn = tf . stop_gradient , tensors = internals ) actions = util . map_tensors ( fn = tf . stop_gradient , tensors = actions ) terminal = tf . stop_gradient ( input = terminal ) reward = tf . stop_gradient ( input = reward ) observation = self . fn_observe_timestep ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) with tf . control_dependencies ( control_inputs = ( observation , ) ) : self . unbuffered_episode_output = self . global_episode + 0",Returns the tf op to fetch when unbuffered observations are passed in .
"def create_operations ( self , states , internals , actions , terminal , reward , deterministic , independent , index ) : self . create_act_operations ( states = states , internals = internals , deterministic = deterministic , independent = independent , index = index ) self . create_observe_operations ( reward = reward , terminal = terminal , index = index ) self . create_atomic_observe_operations ( states = states , actions = actions , internals = internals , reward = reward , terminal = terminal , index = index )",Creates and stores tf operations for when act () and observe () are called .
"def get_variables ( self , include_submodules = False , include_nontrainable = False ) : if include_nontrainable : model_variables = [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] states_preprocessing_variables = [ variable for name in sorted ( self . states_preprocessing ) for variable in self . states_preprocessing [ name ] . get_variables ( ) ] model_variables += states_preprocessing_variables actions_exploration_variables = [ variable for name in sorted ( self . actions_exploration ) for variable in self . actions_exploration [ name ] . get_variables ( ) ] model_variables += actions_exploration_variables if self . reward_preprocessing is not None : reward_preprocessing_variables = self . reward_preprocessing . get_variables ( ) model_variables += reward_preprocessing_variables else : model_variables = [ self . variables [ key ] for key in sorted ( self . variables ) ] return model_variables",Returns the TensorFlow variables used by the model .
"def reset ( self ) : fetches = [ self . global_episode , self . global_timestep ] for name in sorted ( self . states_preprocessing ) : fetch = self . states_preprocessing [ name ] . reset ( ) if fetch is not None : fetches . extend ( fetch ) if self . flush_summarizer is not None : fetches . append ( self . flush_summarizer ) fetch_list = self . monitored_session . run ( fetches = fetches ) episode , timestep = fetch_list [ : 2 ] return episode , timestep , self . internals_init",Resets the model to its initial state on episode start . This should also reset all preprocessor ( s ) .
"def get_feed_dict ( self , states = None , internals = None , actions = None , terminal = None , reward = None , deterministic = None , independent = None , index = None ) : feed_dict = dict ( ) batched = None if states is not None : if batched is None : name = next ( iter ( states ) ) state = np . asarray ( states [ name ] ) batched = ( state . ndim != len ( self . states_spec [ name ] [ 'unprocessed_shape' ] ) ) if batched : feed_dict . update ( { self . states_input [ name ] : states [ name ] for name in sorted ( self . states_input ) } ) else : feed_dict . update ( { self . states_input [ name ] : ( states [ name ] , ) for name in sorted ( self . states_input ) } ) if internals is not None : if batched is None : name = next ( iter ( internals ) ) internal = np . asarray ( internals [ name ] ) batched = ( internal . ndim != len ( self . internals_spec [ name ] [ 'shape' ] ) ) if batched : feed_dict . update ( { self . internals_input [ name ] : internals [ name ] for name in sorted ( self . internals_input ) } ) else : feed_dict . update ( { self . internals_input [ name ] : ( internals [ name ] , ) for name in sorted ( self . internals_input ) } ) if actions is not None : if batched is None : name = next ( iter ( actions ) ) action = np . asarray ( actions [ name ] ) batched = ( action . ndim != len ( self . actions_spec [ name ] [ 'shape' ] ) ) if batched : feed_dict . update ( { self . actions_input [ name ] : actions [ name ] for name in sorted ( self . actions_input ) } ) else : feed_dict . update ( { self . actions_input [ name ] : ( actions [ name ] , ) for name in sorted ( self . actions_input ) } ) if terminal is not None : if batched is None : terminal = np . asarray ( terminal ) batched = ( terminal . ndim == 1 ) if batched : feed_dict [ self . terminal_input ] = terminal else : feed_dict [ self . terminal_input ] = ( terminal , ) if reward is not None : if batched is None : reward = np . asarray ( reward ) batched = ( reward . ndim == 1 ) if batched : feed_dict [ self . reward_input ] = reward else : feed_dict [ self . reward_input ] = ( reward , ) if deterministic is not None : feed_dict [ self . deterministic_input ] = deterministic if independent is not None : feed_dict [ self . independent_input ] = independent feed_dict [ self . episode_index_input ] = index return feed_dict",Returns the feed - dict for the model s acting and observing tf fetches .
"def act ( self , states , internals , deterministic = False , independent = False , fetch_tensors = None , index = 0 ) : name = next ( iter ( states ) ) state = np . asarray ( states [ name ] ) batched = ( state . ndim != len ( self . states_spec [ name ] [ 'unprocessed_shape' ] ) ) if batched : assert state . shape [ 0 ] <= self . batching_capacity fetches = [ self . actions_output , self . internals_output , self . timestep_output ] if self . network is not None and fetch_tensors is not None : for name in fetch_tensors : valid , tensor = self . network . get_named_tensor ( name ) if valid : fetches . append ( tensor ) else : keys = self . network . get_list_of_named_tensor ( ) raise TensorForceError ( 'Cannot fetch named tensor ""{}"", Available {}.' . format ( name , keys ) ) feed_dict = self . get_feed_dict ( states = states , internals = internals , deterministic = deterministic , independent = independent , index = index ) fetch_list = self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict ) actions , internals , timestep = fetch_list [ 0 : 3 ] if not batched : actions = { name : actions [ name ] [ 0 ] for name in sorted ( actions ) } internals = { name : internals [ name ] [ 0 ] for name in sorted ( internals ) } if self . network is not None and fetch_tensors is not None : fetch_dict = dict ( ) for index_ , tensor in enumerate ( fetch_list [ 3 : ] ) : name = fetch_tensors [ index_ ] fetch_dict [ name ] = tensor return actions , internals , timestep , fetch_dict else : return actions , internals , timestep",Does a forward pass through the model to retrieve action ( outputs ) given inputs for state ( and internal state if applicable ( e . g . RNNs ))
"def observe ( self , terminal , reward , index = 0 ) : fetches = self . episode_output feed_dict = self . get_feed_dict ( terminal = terminal , reward = reward , index = index ) episode = self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict ) return episode",Adds an observation ( reward and is - terminal ) to the model without updating its trainable variables .
"def save ( self , directory = None , append_timestep = True ) : if self . flush_summarizer is not None : self . monitored_session . run ( fetches = self . flush_summarizer ) return self . saver . save ( sess = self . session , save_path = ( self . saver_directory if directory is None else directory ) , global_step = ( self . global_timestep if append_timestep else None ) , meta_graph_suffix = 'meta' , write_meta_graph = True , write_state = True )",Save TensorFlow model . If no checkpoint directory is given the model s default saver directory is used . Optionally appends current timestep to prevent overwriting previous checkpoint files . Turn off to be able to load model from the same given path argument as given here .
"def restore ( self , directory = None , file = None ) : if file is None : file = tf . train . latest_checkpoint ( checkpoint_dir = ( self . saver_directory if directory is None else directory ) , ) elif directory is None : file = os . path . join ( self . saver_directory , file ) elif not os . path . isfile ( file ) : file = os . path . join ( directory , file ) self . saver . restore ( sess = self . session , save_path = file ) self . session . run ( fetches = self . list_buffer_index_reset_op )",Restore TensorFlow model . If no checkpoint file is given the latest checkpoint is restored . If no checkpoint directory is given the model s default saver directory is used ( unless file specifies the entire path ) .
"def get_savable_components ( self ) : components = self . get_components ( ) components = [ components [ name ] for name in sorted ( components ) ] return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) )",Returns the list of all of the components this model consists of that can be individually saved and restored . For instance the network or distribution .
"def save_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) return component . save ( sess = self . session , save_path = save_path )",Saves a component of this model to the designated location .
"def restore_component ( self , component_name , save_path ) : component = self . get_component ( component_name = component_name ) self . _validate_savable ( component = component , component_name = component_name ) component . restore ( sess = self . session , save_path = save_path )",Restores a component s parameters from a save location .
"def get_component ( self , component_name ) : mapping = self . get_components ( ) return mapping [ component_name ] if component_name in mapping else None",Looks up a component by its name .
"def import_demonstrations ( self , demonstrations ) : if isinstance ( demonstrations , dict ) : if self . unique_state : demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) if self . unique_action : demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) self . model . import_demo_experience ( * * demonstrations ) else : if self . unique_state : states = dict ( state = list ( ) ) else : states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } if self . unique_action : actions = dict ( action = list ( ) ) else : actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } terminal = list ( ) reward = list ( ) for demonstration in demonstrations : if self . unique_state : states [ 'state' ] . append ( demonstration [ 'states' ] ) else : for name , state in states . items ( ) : state . append ( demonstration [ 'states' ] [ name ] ) for name , internal in internals . items ( ) : internal . append ( demonstration [ 'internals' ] [ name ] ) if self . unique_action : actions [ 'action' ] . append ( demonstration [ 'actions' ] ) else : for name , action in actions . items ( ) : action . append ( demonstration [ 'actions' ] [ name ] ) terminal . append ( demonstration [ 'terminal' ] ) reward . append ( demonstration [ 'reward' ] ) self . model . import_demo_experience ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Imports demonstrations i . e . expert observations . Note that for large numbers of observations set_demonstrations is more appropriate which directly sets memory contents to an array an expects a different layout .
"def seed ( self , seed ) : if seed is None : self . env . seed = round ( time . time ( ) ) else : self . env . seed = seed return self . env . seed",Sets the random seed of the environment to the given value ( current time if seed = None ) . Naturally deterministic Environments ( e . g . ALE or some gym Envs ) don t have to implement this method .
"def execute ( self , action ) : if self . env . game_over ( ) : return self . env . getScreenRGB ( ) , True , 0 action_space = self . env . getActionSet ( ) reward = self . env . act ( action_space [ action ] ) new_state = self . env . getScreenRGB ( ) done = self . env . game_over ( ) return new_state , done , reward",Executes action observes next state and reward .
"def states ( self ) : screen = self . env . getScreenRGB ( ) return dict ( shape = screen . shape , type = 'int' )",Return the state space . Might include subdicts if multiple states are available simultaneously .
"def sanity_check_states ( states_spec ) : states = copy . deepcopy ( states_spec ) is_unique = ( 'shape' in states ) if is_unique : states = dict ( state = states ) for name , state in states . items ( ) : if isinstance ( state [ 'shape' ] , int ) : state [ 'shape' ] = ( state [ 'shape' ] , ) if 'type' not in state : state [ 'type' ] = 'float' return states , is_unique",Sanity checks a states dict used to define the state space for an MDP . Throws an error or warns if mismatches are found .
"def sanity_check_actions ( actions_spec ) : actions = copy . deepcopy ( actions_spec ) is_unique = ( 'type' in actions ) if is_unique : actions = dict ( action = actions ) for name , action in actions . items ( ) : if 'type' not in action : action [ 'type' ] = 'int' if action [ 'type' ] == 'int' : if 'num_actions' not in action : raise TensorForceError ( ""Action requires value 'num_actions' set!"" ) elif action [ 'type' ] == 'float' : if ( 'min_value' in action ) != ( 'max_value' in action ) : raise TensorForceError ( ""Action requires both values 'min_value' and 'max_value' set!"" ) if 'shape' not in action : action [ 'shape' ] = ( ) if isinstance ( action [ 'shape' ] , int ) : action [ 'shape' ] = ( action [ 'shape' ] , ) return actions , is_unique",Sanity checks an actions dict used to define the action space for an MDP . Throws an error or warns if mismatches are found .
"def sanity_check_execution_spec ( execution_spec ) : def_ = dict ( type = ""single"" , distributed_spec = None , session_config = None ) if execution_spec is None : return def_ assert isinstance ( execution_spec , dict ) , ""ERROR: execution-spec needs to be of type dict (but is of type {})!"" . format ( type ( execution_spec ) . __name__ ) type_ = execution_spec . get ( ""type"" ) if type_ == ""distributed"" : def_ = dict ( job = ""ps"" , task_index = 0 , cluster_spec = { ""ps"" : [ ""localhost:22222"" ] , ""worker"" : [ ""localhost:22223"" ] } ) def_ . update ( execution_spec . get ( ""distributed_spec"" , { } ) ) execution_spec [ ""distributed_spec"" ] = def_ execution_spec [ ""session_config"" ] = execution_spec . get ( ""session_config"" ) return execution_spec elif type_ == ""multi-threaded"" : return execution_spec elif type_ == ""single"" : return execution_spec if execution_spec . get ( 'num_parallel' ) != None : assert type ( execution_spec [ 'num_parallel' ] ) is int , ""ERROR: num_parallel needs to be of type int but is of type {}!"" . format ( type ( execution_spec [ 'num_parallel' ] ) . __name__ ) assert execution_spec [ 'num_parallel' ] > 0 , ""ERROR: num_parallel needs to be > 0 but is equal to {}"" . format ( execution_spec [ 'num_parallel' ] ) return execution_spec raise TensorForceError ( ""Unsupported execution type specified ({})!"" . format ( type_ ) )",Sanity checks a execution_spec dict used to define execution logic ( distributed vs single shared memories etc .. ) and distributed learning behavior of agents / models . Throws an error or warns if mismatches are found .
"def make_game ( ) : return ascii_art . ascii_art_to_game ( GAME_ART , what_lies_beneath = ' ' , sprites = dict ( [ ( 'P' , PlayerSprite ) ] + [ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + [ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , drapes = dict ( X = MarauderDrape , B = BunkerDrape ) , update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) )",Builds and returns an Extraterrestrial Marauders game .
"def _fly ( self , board , layers , things , the_plot ) : if ( self . character in the_plot [ 'bunker_hitters' ] or self . character in the_plot [ 'marauder_hitters' ] ) : return self . _teleport ( ( - 1 , - 1 ) ) self . _north ( board , the_plot )",Handles the behaviour of visible bolts flying toward Marauders .
"def _fire ( self , layers , things , the_plot ) : if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return the_plot [ 'last_player_shot' ] = the_plot . frame row , col = things [ 'P' ] . position self . _teleport ( ( row - 1 , col ) )",Launches a new bolt from the player .
"def _fly ( self , board , layers , things , the_plot ) : if self . character in the_plot [ 'bunker_hitters' ] : return self . _teleport ( ( - 1 , - 1 ) ) if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) self . _south ( board , the_plot )",Handles the behaviour of visible bolts flying toward the player .
"def _fire ( self , layers , the_plot ) : if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return the_plot [ 'last_marauder_shot' ] = the_plot . frame col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 self . _teleport ( ( row , col ) )",Launches a new bolt from a random Marauder .
"def reset ( self , history = None ) : if not history : history = dict ( ) self . episode_rewards = history . get ( ""episode_rewards"" , list ( ) ) self . episode_timesteps = history . get ( ""episode_timesteps"" , list ( ) ) self . episode_times = history . get ( ""episode_times"" , list ( ) )",Resets the Runner s internal stats counters . If history is empty use default values in history . get () .
"def run ( self , num_episodes , num_timesteps , max_episode_timesteps , deterministic , episode_finished , summary_report , summary_interval ) : raise NotImplementedError",Executes this runner by starting to act ( via Agent ( s )) in the given Environment ( s ) . Stops execution according to certain conditions ( e . g . max . number of episodes etc .. ) . Calls callback functions after each episode and / or after some summary criteria are met .
"def tf_retrieve_indices ( self , buffer_elements , priority_indices ) : states = dict ( ) buffer_start = self . buffer_index - buffer_elements buffer_end = self . buffer_index for name in sorted ( self . states_memory ) : buffer_state_memory = self . states_buffer [ name ] buffer_states = buffer_state_memory [ buffer_start : buffer_end ] memory_states = tf . gather ( params = self . states_memory [ name ] , indices = priority_indices ) states [ name ] = tf . concat ( values = ( buffer_states , memory_states ) , axis = 0 ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internal_buffer_memory = self . internals_buffer [ name ] buffer_internals = internal_buffer_memory [ buffer_start : buffer_end ] memory_internals = tf . gather ( params = self . internals_memory [ name ] , indices = priority_indices ) internals [ name ] = tf . concat ( values = ( buffer_internals , memory_internals ) , axis = 0 ) actions = dict ( ) for name in sorted ( self . actions_memory ) : action_buffer_memory = self . actions_buffer [ name ] buffer_action = action_buffer_memory [ buffer_start : buffer_end ] memory_action = tf . gather ( params = self . actions_memory [ name ] , indices = priority_indices ) actions [ name ] = tf . concat ( values = ( buffer_action , memory_action ) , axis = 0 ) buffer_terminal = self . terminal_buffer [ buffer_start : buffer_end ] priority_terminal = tf . gather ( params = self . terminal_memory , indices = priority_indices ) terminal = tf . concat ( values = ( buffer_terminal , priority_terminal ) , axis = 0 ) buffer_reward = self . reward_buffer [ buffer_start : buffer_end ] priority_reward = tf . gather ( params = self . reward_memory , indices = priority_indices ) reward = tf . concat ( values = ( buffer_reward , priority_reward ) , axis = 0 ) if self . include_next_states : assert util . rank ( priority_indices ) == 1 next_priority_indices = ( priority_indices + 1 ) % self . capacity next_buffer_start = ( buffer_start + 1 ) % self . buffer_size next_buffer_end = ( buffer_end + 1 ) % self . buffer_size next_states = dict ( ) for name in sorted ( self . states_memory ) : buffer_state_memory = self . states_buffer [ name ] buffer_next_states = buffer_state_memory [ next_buffer_start : next_buffer_end ] memory_next_states = tf . gather ( params = self . states_memory [ name ] , indices = next_priority_indices ) next_states [ name ] = tf . concat ( values = ( buffer_next_states , memory_next_states ) , axis = 0 ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : buffer_internal_memory = self . internals_buffer [ name ] buffer_next_internals = buffer_internal_memory [ next_buffer_start : next_buffer_end ] memory_next_internals = tf . gather ( params = self . internals_memory [ name ] , indices = next_priority_indices ) next_internals [ name ] = tf . concat ( values = ( buffer_next_internals , memory_next_internals ) , axis = 0 ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Fetches experiences for given indices by combining entries from buffer which have no priorities and entries from priority memory .
"def tf_update_batch ( self , loss_per_instance ) : mask = tf . not_equal ( x = self . batch_indices , y = tf . zeros ( shape = tf . shape ( input = self . batch_indices ) , dtype = tf . int32 ) ) priority_indices = tf . reshape ( tensor = tf . where ( condition = mask ) , shape = [ - 1 ] ) sampled_buffer_batch = self . tf_retrieve_indices ( buffer_elements = self . last_batch_buffer_elems , priority_indices = priority_indices ) states = sampled_buffer_batch [ 'states' ] internals = sampled_buffer_batch [ 'internals' ] actions = sampled_buffer_batch [ 'actions' ] terminal = sampled_buffer_batch [ 'terminal' ] reward = sampled_buffer_batch [ 'reward' ] priorities = loss_per_instance ** self . prioritization_weight assignments = list ( ) memory_end_index = self . memory_index + self . last_batch_buffer_elems memory_insert_indices = tf . range ( start = self . memory_index , limit = memory_end_index ) % self . capacity for name in sorted ( states ) : assignments . append ( tf . scatter_update ( ref = self . states_memory [ name ] , indices = memory_insert_indices , updates = states [ name ] [ 0 : self . last_batch_buffer_elems ] ) ) for name in sorted ( internals ) : assignments . append ( tf . scatter_update ( ref = self . internals_buffer [ name ] , indices = memory_insert_indices , updates = internals [ name ] [ 0 : self . last_batch_buffer_elems ] ) ) assignments . append ( tf . scatter_update ( ref = self . priorities , indices = memory_insert_indices , updates = priorities [ 0 : self . last_batch_buffer_elems ] ) ) assignments . append ( tf . scatter_update ( ref = self . terminal_memory , indices = memory_insert_indices , updates = terminal [ 0 : self . last_batch_buffer_elems ] ) ) assignments . append ( tf . scatter_update ( ref = self . reward_memory , indices = memory_insert_indices , updates = reward [ 0 : self . last_batch_buffer_elems ] ) ) for name in sorted ( actions ) : assignments . append ( tf . scatter_update ( ref = self . actions_memory [ name ] , indices = memory_insert_indices , updates = actions [ name ] [ 0 : self . last_batch_buffer_elems ] ) ) main_memory_priorities = priorities [ self . last_batch_buffer_elems : ] main_memory_priorities = main_memory_priorities [ 0 : tf . shape ( priority_indices ) [ 0 ] ] assignments . append ( tf . scatter_update ( ref = self . priorities , indices = priority_indices , updates = main_memory_priorities ) ) with tf . control_dependencies ( control_inputs = assignments ) : assignments = list ( ) sorted_priorities , sorted_indices = tf . nn . top_k ( input = self . priorities , k = self . capacity , sorted = True ) assignments . append ( tf . assign ( ref = self . priorities , value = sorted_priorities ) ) assignments . append ( tf . scatter_update ( ref = self . terminal_memory , indices = sorted_indices , updates = self . terminal_memory ) ) for name in sorted ( self . states_memory ) : assignments . append ( tf . scatter_update ( ref = self . states_memory [ name ] , indices = sorted_indices , updates = self . states_memory [ name ] ) ) for name in sorted ( self . actions_memory ) : assignments . append ( tf . scatter_update ( ref = self . actions_memory [ name ] , indices = sorted_indices , updates = self . actions_memory [ name ] ) ) for name in sorted ( self . internals_memory ) : assignments . append ( tf . scatter_update ( ref = self . internals_memory [ name ] , indices = sorted_indices , updates = self . internals_memory [ name ] ) ) assignments . append ( tf . scatter_update ( ref = self . reward_memory , indices = sorted_indices , updates = self . reward_memory ) ) with tf . control_dependencies ( control_inputs = assignments ) : assignments = list ( ) assignments . append ( tf . assign_sub ( ref = self . buffer_index , value = self . last_batch_buffer_elems ) ) total_inserted_elements = self . memory_size + self . last_batch_buffer_elems assignments . append ( tf . assign ( ref = self . memory_size , value = tf . minimum ( x = total_inserted_elements , y = self . capacity ) ) ) assignments . append ( tf . assign ( ref = self . memory_index , value = memory_end_index ) ) assignments . append ( tf . assign ( ref = self . batch_indices , value = tf . zeros ( shape = tf . shape ( self . batch_indices ) , dtype = tf . int32 ) ) ) with tf . control_dependencies ( control_inputs = assignments ) : return tf . no_op ( )",Updates priority memory by performing the following steps :
"def setup_components_and_tf_funcs ( self , custom_getter = None ) : self . network = Network . from_spec ( spec = self . network_spec , kwargs = dict ( summary_labels = self . summary_labels ) ) assert len ( self . internals_spec ) == 0 self . internals_spec = self . network . internals_spec ( ) for name in sorted ( self . internals_spec ) : internal = self . internals_spec [ name ] self . internals_input [ name ] = tf . placeholder ( dtype = util . tf_dtype ( internal [ 'type' ] ) , shape = ( None , ) + tuple ( internal [ 'shape' ] ) , name = ( 'internal-' + name ) ) if internal [ 'initialization' ] == 'zeros' : self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) else : raise TensorForceError ( ""Invalid internal initialization value."" ) custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . distributions = self . create_distributions ( ) self . fn_kl_divergence = tf . make_template ( name_ = 'kl-divergence' , func_ = self . tf_kl_divergence , custom_getter_ = custom_getter ) return custom_getter",Creates and stores Network and Distribution objects . Generates and stores all template functions .
"def create_distributions ( self ) : distributions = dict ( ) for name in sorted ( self . actions_spec ) : action = self . actions_spec [ name ] if self . distributions_spec is not None and name in self . distributions_spec : kwargs = dict ( action ) kwargs [ 'scope' ] = name kwargs [ 'summary_labels' ] = self . summary_labels distributions [ name ] = Distribution . from_spec ( spec = self . distributions_spec [ name ] , kwargs = kwargs ) elif action [ 'type' ] == 'bool' : distributions [ name ] = Bernoulli ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'int' : distributions [ name ] = Categorical ( shape = action [ 'shape' ] , num_actions = action [ 'num_actions' ] , scope = name , summary_labels = self . summary_labels ) elif action [ 'type' ] == 'float' : if 'min_value' in action : distributions [ name ] = Beta ( shape = action [ 'shape' ] , min_value = action [ 'min_value' ] , max_value = action [ 'max_value' ] , scope = name , summary_labels = self . summary_labels ) else : distributions [ name ] = Gaussian ( shape = action [ 'shape' ] , scope = name , summary_labels = self . summary_labels ) return distributions",Creates and returns the Distribution objects based on self . distributions_spec .
"def from_spec ( spec ) : exploration = util . get_object ( obj = spec , predefined_objects = tensorforce . core . explorations . explorations ) assert isinstance ( exploration , Exploration ) return exploration",Creates an exploration object from a specification dict .
"def from_spec ( spec , kwargs = None ) : memory = util . get_object ( obj = spec , predefined_objects = tensorforce . core . memories . memories , kwargs = kwargs ) assert isinstance ( memory , Memory ) return memory",Creates a memory from a specification dict .
"def tf_step ( self , time , variables , arguments , * * kwargs ) : arguments_iter = iter ( arguments . values ( ) ) some_argument = next ( arguments_iter ) try : while not isinstance ( some_argument , tf . Tensor ) or util . rank ( some_argument ) == 0 : if isinstance ( some_argument , dict ) : if some_argument : arguments_iter = iter ( some_argument . values ( ) ) some_argument = next ( arguments_iter ) elif isinstance ( some_argument , list ) : if some_argument : arguments_iter = iter ( some_argument ) some_argument = next ( arguments_iter ) elif some_argument is None or util . rank ( some_argument ) == 0 : some_argument = next ( arguments_iter ) else : raise TensorForceError ( ""Invalid argument type."" ) except StopIteration : raise TensorForceError ( ""Invalid argument type."" ) batch_size = tf . shape ( input = some_argument ) [ 0 ] num_samples = tf . cast ( x = ( self . fraction * tf . cast ( x = batch_size , dtype = util . tf_dtype ( 'float' ) ) ) , dtype = util . tf_dtype ( 'int' ) ) num_samples = tf . maximum ( x = num_samples , y = 1 ) indices = tf . random_uniform ( shape = ( num_samples , ) , maxval = batch_size , dtype = tf . int32 ) subsampled_arguments = util . map_tensors ( fn = ( lambda arg : arg if util . rank ( arg ) == 0 else tf . gather ( params = arg , indices = indices ) ) , tensors = arguments ) return self . optimizer . step ( time = time , variables = variables , arguments = subsampled_arguments , * * kwargs )",Creates the TensorFlow operations for performing an optimization step .
"def run ( self , num_timesteps = None , num_episodes = None , max_episode_timesteps = None , deterministic = False , episode_finished = None , summary_report = None , summary_interval = None , timesteps = None , episodes = None , testing = False , sleep = None ) : if timesteps is not None : num_timesteps = timesteps warnings . warn ( ""WARNING: `timesteps` parameter is deprecated, use `num_timesteps` instead."" , category = DeprecationWarning ) if episodes is not None : num_episodes = episodes warnings . warn ( ""WARNING: `episodes` parameter is deprecated, use `num_episodes` instead."" , category = DeprecationWarning ) old_episode_finished = False if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : old_episode_finished = True self . start_time = time . time ( ) self . agent . reset ( ) if num_episodes is not None : num_episodes += self . agent . episode if num_timesteps is not None : num_timesteps += self . agent . timestep with tqdm ( total = num_episodes ) as pbar : while True : episode_start_time = time . time ( ) state = self . environment . reset ( ) self . agent . reset ( ) self . global_episode = self . agent . episode self . global_timestep = self . agent . timestep episode_reward = 0 self . current_timestep = 0 while True : action = self . agent . act ( states = state , deterministic = deterministic ) reward = 0 for _ in xrange ( self . repeat_actions ) : state , terminal , step_reward = self . environment . execute ( action = action ) reward += step_reward if terminal : break if max_episode_timesteps is not None and self . current_timestep >= max_episode_timesteps : terminal = True if not testing : self . agent . observe ( terminal = terminal , reward = reward ) self . global_timestep += 1 self . current_timestep += 1 episode_reward += reward if terminal or self . agent . should_stop ( ) : break if sleep is not None : time . sleep ( sleep ) time_passed = time . time ( ) - episode_start_time self . episode_rewards . append ( episode_reward ) self . episode_timesteps . append ( self . current_timestep ) self . episode_times . append ( time_passed ) self . global_episode += 1 pbar . update ( 1 ) if episode_finished is not None : if old_episode_finished : if not episode_finished ( self ) : break elif not episode_finished ( self , self . id ) : break if ( num_episodes is not None and self . global_episode >= num_episodes ) or ( num_timesteps is not None and self . global_timestep >= num_timesteps ) or self . agent . should_stop ( ) : break pbar . update ( num_episodes - self . global_episode )",Args : timesteps ( int ) : Deprecated ; see num_timesteps . episodes ( int ) : Deprecated ; see num_episodes .
"def tf_retrieve_indices ( self , indices ) : states = dict ( ) for name in sorted ( self . states_memory ) : states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) internals = dict ( ) for name in sorted ( self . internals_memory ) : internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) actions = dict ( ) for name in sorted ( self . actions_memory ) : actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) terminal = tf . gather ( params = self . terminal_memory , indices = indices ) reward = tf . gather ( params = self . reward_memory , indices = indices ) if self . include_next_states : assert util . rank ( indices ) == 1 next_indices = ( indices + 1 ) % self . capacity next_states = dict ( ) for name in sorted ( self . states_memory ) : next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) next_internals = dict ( ) for name in sorted ( self . internals_memory ) : next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals ) else : return dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Fetches experiences for given indices .
"def tf_solve ( self , fn_x , x_init , base_value , target_value , estimated_improvement = None ) : return super ( LineSearch , self ) . tf_solve ( fn_x , x_init , base_value , target_value , estimated_improvement )",Iteratively optimizes $f ( x ) $ for $x$ on the line between $x $ and $x_0$ .
"def tf_initialize ( self , x_init , base_value , target_value , estimated_improvement ) : self . base_value = base_value if estimated_improvement is None : estimated_improvement = tf . abs ( x = base_value ) first_step = super ( LineSearch , self ) . tf_initialize ( x_init ) improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = estimated_improvement , y = util . epsilon ) ) last_improvement = improvement - 1.0 if self . mode == 'linear' : deltas = [ - t * self . parameter for t in x_init ] self . estimated_incr = - estimated_improvement * self . parameter elif self . mode == 'exponential' : deltas = [ - t * self . parameter for t in x_init ] return first_step + ( deltas , improvement , last_improvement , estimated_improvement )",Initialization step preparing the arguments for the first iteration of the loop body .
"def tf_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : x , next_iteration , deltas , improvement , last_improvement , estimated_improvement = super ( LineSearch , self ) . tf_step ( x , iteration , deltas , improvement , last_improvement , estimated_improvement ) next_x = [ t + delta for t , delta in zip ( x , deltas ) ] if self . mode == 'linear' : next_deltas = deltas next_estimated_improvement = estimated_improvement + self . estimated_incr elif self . mode == 'exponential' : next_deltas = [ delta * self . parameter for delta in deltas ] next_estimated_improvement = estimated_improvement * self . parameter target_value = self . fn_x ( next_deltas ) next_improvement = tf . divide ( x = ( target_value - self . base_value ) , y = tf . maximum ( x = next_estimated_improvement , y = util . epsilon ) ) return next_x , next_iteration , next_deltas , next_improvement , improvement , next_estimated_improvement",Iteration loop body of the line search algorithm .
"def tf_next_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : next_step = super ( LineSearch , self ) . tf_next_step ( x , iteration , deltas , improvement , last_improvement , estimated_improvement ) def undo_deltas ( ) : value = self . fn_x ( [ - delta for delta in deltas ] ) with tf . control_dependencies ( control_inputs = ( value , ) ) : return tf . less ( x = value , y = value ) improved = tf . cond ( pred = ( improvement > last_improvement ) , true_fn = ( lambda : True ) , false_fn = undo_deltas ) next_step = tf . logical_and ( x = next_step , y = improved ) next_step = tf . logical_and ( x = next_step , y = ( improvement < self . accept_ratio ) ) return tf . logical_and ( x = next_step , y = ( estimated_improvement > util . epsilon ) )",Termination condition : max number of iterations or no improvement for last step or improvement less than acceptable ratio or estimated value not positive .
"def escape ( text , quote = False , smart_amp = True ) : if smart_amp : text = _escape_pattern . sub ( '&amp;' , text ) else : text = text . replace ( '&' , '&amp;' ) text = text . replace ( '<' , '&lt;' ) text = text . replace ( '>' , '&gt;' ) if quote : text = text . replace ( '""' , '&quot;' ) text = text . replace ( ""'"" , '&#39;' ) return text",Replace special characters & < and > to HTML - safe sequences .
"def escape_link ( url ) : lower_url = url . lower ( ) . strip ( '\x00\x1a \n\r\t' ) for scheme in _scheme_blacklist : if lower_url . startswith ( scheme ) : return '' return escape ( url , quote = True , smart_amp = False )",Remove dangerous URL schemes like javascript : and escape afterwards .
"def markdown ( text , escape = True , * * kwargs ) : return Markdown ( escape = escape , * * kwargs ) ( text )",Render markdown formatted text to html .
"def parse_lheading ( self , m ) : self . tokens . append ( { 'type' : 'heading' , 'level' : 1 if m . group ( 2 ) == '=' else 2 , 'text' : m . group ( 1 ) , } )",Parse setext heading .
def hard_wrap ( self ) : self . linebreak = re . compile ( r'^ *\n(?!\s*$)' ) self . text = re . compile ( r'^[\s\S]+?(?=[\\<!\[_*`~]|https?://| *\n|$)' ),Grammar for hard wrap linebreak . You don t need to add two spaces at the end of a line .
"def block_code ( self , code , lang = None ) : code = code . rstrip ( '\n' ) if not lang : code = escape ( code , smart_amp = False ) return '<pre><code>%s\n</code></pre>\n' % code code = escape ( code , quote = True , smart_amp = False ) return '<pre><code class=""lang-%s"">%s\n</code></pre>\n' % ( lang , code )",Rendering block level code . pre > code .
"def block_html ( self , html ) : if self . options . get ( 'skip_style' ) and html . lower ( ) . startswith ( '<style' ) : return '' if self . options . get ( 'escape' ) : return escape ( html ) return html",Rendering block level pure html content .
"def list ( self , body , ordered = True ) : tag = 'ul' if ordered : tag = 'ol' return '<%s>\n%s</%s>\n' % ( tag , body , tag )",Rendering list tags like <ul > and <ol > .
"def table_cell ( self , content , * * flags ) : if flags [ 'header' ] : tag = 'th' else : tag = 'td' align = flags [ 'align' ] if not align : return '<%s>%s</%s>\n' % ( tag , content , tag ) return '<%s style=""text-align:%s"">%s</%s>\n' % ( tag , align , content , tag )",Rendering a table cell . Like <th > <td > .
"def codespan ( self , text ) : text = escape ( text . rstrip ( ) , smart_amp = False ) return '<code>%s</code>' % text",Rendering inline code text .
"def autolink ( self , link , is_email = False ) : text = link = escape ( link ) if is_email : link = 'mailto:%s' % link return '<a href=""%s"">%s</a>' % ( link , text )",Rendering a given link or email address .
"def link ( self , link , title , text ) : link = escape_link ( link ) if not title : return '<a href=""%s"">%s</a>' % ( link , text ) title = escape ( title , quote = True ) return '<a href=""%s"" title=""%s"">%s</a>' % ( link , title , text )",Rendering a given link with content and title .
"def image ( self , src , title , text ) : src = escape_link ( src ) text = escape ( text , quote = True ) if title : title = escape ( title , quote = True ) html = '<img src=""%s"" alt=""%s"" title=""%s""' % ( src , text , title ) else : html = '<img src=""%s"" alt=""%s""' % ( src , text ) if self . options . get ( 'use_xhtml' ) : return '%s />' % html return '%s>' % html",Rendering a image with title and text .
"def footnote_ref ( self , key , index ) : html = ( '<sup class=""footnote-ref"" id=""fnref-%s"">' '<a href=""#fn-%s"">%d</a></sup>' ) % ( escape ( key ) , escape ( key ) , index ) return html",Rendering the ref anchor of a footnote .
"def footnote_item ( self , key , text ) : back = ( '<a href=""#fnref-%s"" class=""footnote"">&#8617;</a>' ) % escape ( key ) text = text . rstrip ( ) if text . endswith ( '</p>' ) : text = re . sub ( r'<\/p>$' , r'%s</p>' % back , text ) else : text = '%s<p>%s</p>' % ( text , back ) html = '<li id=""fn-%s"">%s</li>\n' % ( escape ( key ) , text ) return html",Rendering a footnote item .
"def build_metagraph_list ( self ) : ops = [ ] self . ignore_unknown_dtypes = True for key in sorted ( self . meta_params ) : value = self . convert_data_to_string ( self . meta_params [ key ] ) if len ( value ) == 0 : continue if isinstance ( value , str ) : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) else : ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) return ops",Convert MetaParams into TF Summary Format and create summary_op .
"def process_docstring ( app , what , name , obj , options , lines ) : markdown = ""\n"" . join ( lines ) rest = m2r ( markdown ) rest . replace ( ""\r\n"" , ""\n"" ) del lines [ : ] lines . extend ( rest . split ( ""\n"" ) )",Enable markdown syntax in docstrings
"def tf_step ( self , time , variables , arguments , fn_loss , fn_kl_divergence , return_estimated_improvement = False , * * kwargs ) : kldiv = fn_kl_divergence ( * * arguments ) kldiv_gradients = tf . gradients ( ys = kldiv , xs = variables ) def fisher_matrix_product ( deltas ) : deltas = [ tf . stop_gradient ( input = delta ) for delta in deltas ] delta_kldiv_gradients = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( delta * grad ) ) for delta , grad in zip ( deltas , kldiv_gradients ) ] ) return tf . gradients ( ys = delta_kldiv_gradients , xs = variables ) loss = fn_loss ( * * arguments ) loss_gradients = tf . gradients ( ys = loss , xs = variables ) deltas = self . solver . solve ( fn_x = fisher_matrix_product , x_init = None , b = [ - grad for grad in loss_gradients ] ) delta_fisher_matrix_product = fisher_matrix_product ( deltas = deltas ) constant = 0.5 * tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( delta_F * delta ) ) for delta_F , delta in zip ( delta_fisher_matrix_product , deltas ) ] ) def natural_gradient_step ( ) : lagrange_multiplier = tf . sqrt ( x = ( constant / self . learning_rate ) ) estimated_deltas = [ delta / lagrange_multiplier for delta in deltas ] estimated_improvement = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( grad * delta ) ) for grad , delta in zip ( loss_gradients , estimated_deltas ) ] ) applied = self . apply_step ( variables = variables , deltas = estimated_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : if return_estimated_improvement : return [ estimated_delta + 0.0 for estimated_delta in estimated_deltas ] , estimated_improvement else : return [ estimated_delta + 0.0 for estimated_delta in estimated_deltas ] def zero_step ( ) : if return_estimated_improvement : return [ tf . zeros_like ( tensor = delta ) for delta in deltas ] , 0.0 else : return [ tf . zeros_like ( tensor = delta ) for delta in deltas ] return tf . cond ( pred = ( constant > 0.0 ) , true_fn = natural_gradient_step , false_fn = zero_step )",Creates the TensorFlow operations for performing an optimization step .
"def tf_step ( self , time , variables , arguments , fn_reference = None , * * kwargs ) : arguments [ 'reference' ] = fn_reference ( * * arguments ) deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , * * kwargs ) if self . unroll_loop : for _ in xrange ( self . num_steps - 1 ) : with tf . control_dependencies ( control_inputs = deltas ) : step_deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , * * kwargs ) deltas = [ delta1 + delta2 for delta1 , delta2 in zip ( deltas , step_deltas ) ] return deltas else : def body ( iteration , deltas ) : with tf . control_dependencies ( control_inputs = deltas ) : step_deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , * * kwargs ) deltas = [ delta1 + delta2 for delta1 , delta2 in zip ( deltas , step_deltas ) ] return iteration + 1 , deltas def cond ( iteration , deltas ) : return iteration < self . num_steps - 1 _ , deltas = tf . while_loop ( cond = cond , body = body , loop_vars = ( 0 , deltas ) ) return deltas",Creates the TensorFlow operations for performing an optimization step .
"def tf_baseline_loss ( self , states , internals , reward , update , reference = None ) : if self . baseline_mode == 'states' : loss = self . baseline . loss ( states = states , internals = internals , reward = reward , update = update , reference = reference ) elif self . baseline_mode == 'network' : loss = self . baseline . loss ( states = self . network . apply ( x = states , internals = internals , update = update ) , internals = internals , reward = reward , update = update , reference = reference ) regularization_loss = self . baseline . regularization_loss ( ) if regularization_loss is not None : loss += regularization_loss return loss",Creates the TensorFlow operations for calculating the baseline loss of a batch .
"def baseline_optimizer_arguments ( self , states , internals , reward ) : arguments = dict ( time = self . global_timestep , variables = self . baseline . get_variables ( ) , arguments = dict ( states = states , internals = internals , reward = reward , update = tf . constant ( value = True ) , ) , fn_reference = self . baseline . reference , fn_loss = self . fn_baseline_loss , ) if self . global_model is not None : arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) return arguments",Returns the baseline optimizer arguments including the time the list of variables to optimize and various functions which the optimizer might require to perform an update step .
"def tf_step ( self , time , variables , source_variables , * * kwargs ) : assert all ( util . shape ( source ) == util . shape ( target ) for source , target in zip ( source_variables , variables ) ) last_sync = tf . get_variable ( name = 'last-sync' , shape = ( ) , dtype = tf . int64 , initializer = tf . constant_initializer ( value = ( - self . sync_frequency ) , dtype = tf . int64 ) , trainable = False ) def sync ( ) : deltas = list ( ) for source_variable , target_variable in zip ( source_variables , variables ) : delta = self . update_weight * ( source_variable - target_variable ) deltas . append ( delta ) applied = self . apply_step ( variables = variables , deltas = deltas ) last_sync_updated = last_sync . assign ( value = time ) with tf . control_dependencies ( control_inputs = ( applied , last_sync_updated ) ) : return [ delta + 0.0 for delta in deltas ] def no_sync ( ) : deltas = list ( ) for variable in variables : delta = tf . zeros ( shape = util . shape ( variable ) ) deltas . append ( delta ) return deltas do_sync = ( time - last_sync >= self . sync_frequency ) return tf . cond ( pred = do_sync , true_fn = sync , false_fn = no_sync )",Creates the TensorFlow operations for performing an optimization step .
"def tf_step ( self , time , variables , arguments , fn_loss , * * kwargs ) : unperturbed_loss = fn_loss ( * * arguments ) perturbations = [ tf . random_normal ( shape = util . shape ( variable ) ) * self . learning_rate for variable in variables ] applied = self . apply_step ( variables = variables , deltas = perturbations ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : perturbed_loss = fn_loss ( * * arguments ) direction = tf . sign ( x = ( unperturbed_loss - perturbed_loss ) ) deltas_sum = [ direction * perturbation for perturbation in perturbations ] if self . unroll_loop : previous_perturbations = perturbations for sample in xrange ( self . num_samples ) : with tf . control_dependencies ( control_inputs = deltas_sum ) : perturbations = [ tf . random_normal ( shape = util . shape ( variable ) ) * self . learning_rate for variable in variables ] perturbation_deltas = [ pert - prev_pert for pert , prev_pert in zip ( perturbations , previous_perturbations ) ] applied = self . apply_step ( variables = variables , deltas = perturbation_deltas ) previous_perturbations = perturbations with tf . control_dependencies ( control_inputs = ( applied , ) ) : perturbed_loss = fn_loss ( * * arguments ) direction = tf . sign ( x = ( unperturbed_loss - perturbed_loss ) ) deltas_sum = [ delta + direction * perturbation for delta , perturbation in zip ( deltas_sum , perturbations ) ] else : def body ( iteration , deltas_sum , previous_perturbations ) : with tf . control_dependencies ( control_inputs = deltas_sum ) : perturbations = [ tf . random_normal ( shape = util . shape ( variable ) ) * self . learning_rate for variable in variables ] perturbation_deltas = [ pert - prev_pert for pert , prev_pert in zip ( perturbations , previous_perturbations ) ] applied = self . apply_step ( variables = variables , deltas = perturbation_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : perturbed_loss = fn_loss ( * * arguments ) direction = tf . sign ( x = ( unperturbed_loss - perturbed_loss ) ) deltas_sum = [ delta + direction * perturbation for delta , perturbation in zip ( deltas_sum , perturbations ) ] return iteration + 1 , deltas_sum , perturbations def cond ( iteration , deltas_sum , previous_perturbation ) : return iteration < self . num_samples - 1 _ , deltas_sum , perturbations = tf . while_loop ( cond = cond , body = body , loop_vars = ( 0 , deltas_sum , perturbations ) ) with tf . control_dependencies ( control_inputs = deltas_sum ) : deltas = [ delta / self . num_samples for delta in deltas_sum ] perturbation_deltas = [ delta - pert for delta , pert in zip ( deltas , perturbations ) ] applied = self . apply_step ( variables = variables , deltas = perturbation_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : return [ delta + 0.0 for delta in deltas ]",Creates the TensorFlow operations for performing an optimization step .
"def tf_step ( self , time , variables , * * kwargs ) : global_variables = kwargs [ ""global_variables"" ] assert all ( util . shape ( global_variable ) == util . shape ( local_variable ) for global_variable , local_variable in zip ( global_variables , variables ) ) local_deltas = self . optimizer . step ( time = time , variables = variables , * * kwargs ) with tf . control_dependencies ( control_inputs = local_deltas ) : applied = self . optimizer . apply_step ( variables = global_variables , deltas = local_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : update_deltas = list ( ) for global_variable , local_variable in zip ( global_variables , variables ) : delta = global_variable - local_variable update_deltas . append ( delta ) applied = self . apply_step ( variables = variables , deltas = update_deltas ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : return [ local_delta + update_delta for local_delta , update_delta in zip ( local_deltas , update_deltas ) ]",Keyword Args : global_variables : List of global variables to apply the proposed optimization step to .
"def computeStatsEigen ( self ) : with tf . device ( '/cpu:0' ) : def removeNone ( tensor_list ) : local_list = [ ] for item in tensor_list : if item is not None : local_list . append ( item ) return local_list def copyStats ( var_list ) : print ( ""copying stats to buffer tensors before eigen decomp"" ) redundant_stats = { } copied_list = [ ] for item in var_list : if item is not None : if item not in redundant_stats : if self . _use_float64 : redundant_stats [ item ] = tf . cast ( tf . identity ( item ) , tf . float64 ) else : redundant_stats [ item ] = tf . identity ( item ) copied_list . append ( redundant_stats [ item ] ) else : copied_list . append ( None ) return copied_list stats_eigen = self . stats_eigen computedEigen = { } eigen_reverse_lookup = { } updateOps = [ ] with tf . control_dependencies ( [ ] ) : for stats_var in stats_eigen : if stats_var not in computedEigen : eigens = tf . self_adjoint_eig ( stats_var ) e = eigens [ 0 ] Q = eigens [ 1 ] if self . _use_float64 : e = tf . cast ( e , tf . float32 ) Q = tf . cast ( Q , tf . float32 ) updateOps . append ( e ) updateOps . append ( Q ) computedEigen [ stats_var ] = { 'e' : e , 'Q' : Q } eigen_reverse_lookup [ e ] = stats_eigen [ stats_var ] [ 'e' ] eigen_reverse_lookup [ Q ] = stats_eigen [ stats_var ] [ 'Q' ] self . eigen_reverse_lookup = eigen_reverse_lookup self . eigen_update_list = updateOps return updateOps",compute the eigen decomp using copied var stats to avoid concurrent read / write from other queue
"def tf_step ( self , time , variables , * * kwargs ) : fn_loss = kwargs [ ""fn_loss"" ] if variables is None : variables = tf . trainable_variables return tf . gradients ( fn_loss , variables )",Creates the TensorFlow operations for performing an optimization step on the given variables including actually changing the values of the variables .
"def apply_step ( self , variables , deltas , loss_sampled ) : update_stats_op = self . compute_and_apply_stats ( loss_sampled , var_list = var_list ) grads = [ ( a , b ) for a , b in zip ( deltas , varlist ) ] kfacOptim , _ = self . apply_gradients_kfac ( grads ) return kfacOptim",Applies the given ( and already calculated ) step deltas to the variable values .
"def minimize ( self , time , variables , * * kwargs ) : loss = kwargs [ ""fn_loss"" ] sampled_loss = kwargs [ ""sampled_loss"" ] min_op , _ = self . minimize_ ( loss , sampled_loss , var_list = variables ) return min_op",Performs an optimization step .
"def setup_components_and_tf_funcs ( self , custom_getter = None ) : custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) self . demo_memory = Replay ( states = self . states_spec , internals = self . internals_spec , actions = self . actions_spec , include_next_states = True , capacity = self . demo_memory_capacity , scope = 'demo-replay' , summary_labels = self . summary_labels ) self . fn_import_demo_experience = tf . make_template ( name_ = 'import-demo-experience' , func_ = self . tf_import_demo_experience , custom_getter_ = custom_getter ) self . fn_demo_loss = tf . make_template ( name_ = 'demo-loss' , func_ = self . tf_demo_loss , custom_getter_ = custom_getter ) self . fn_combined_loss = tf . make_template ( name_ = 'combined-loss' , func_ = self . tf_combined_loss , custom_getter_ = custom_getter ) self . fn_demo_optimization = tf . make_template ( name_ = 'demo-optimization' , func_ = self . tf_demo_optimization , custom_getter_ = custom_getter ) return custom_getter",Constructs the extra Replay memory .
"def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : return self . demo_memory . store ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward )",Imports a single experience to memory .
"def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : embedding = self . network . apply ( x = states , internals = internals , update = update ) deltas = list ( ) for name in sorted ( actions ) : action = actions [ name ] distr_params = self . distributions [ name ] . parameterize ( x = embedding ) state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) if self . actions_spec [ name ] [ 'type' ] == 'bool' : num_actions = 2 action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) else : num_actions = self . actions_spec [ name ] [ 'num_actions' ] one_hot = tf . one_hot ( indices = action , depth = num_actions ) ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) inverted_one_hot = ones - one_hot state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) state_action_values = state_action_values + inverted_one_hot * self . expert_margin supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) delta = supervised_selector - state_action_value action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) deltas . append ( delta ) loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) loss_per_instance = tf . square ( x = loss_per_instance ) return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 )",Extends the q - model loss via the dqfd large - margin loss .
"def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : q_model_loss = self . fn_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , next_states = next_states , next_internals = next_internals , update = update , reference = reference ) demo_loss = self . fn_demo_loss ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward , update = update , reference = reference ) return q_model_loss + self . supervised_weight * demo_loss",Combines Q - loss and demo loss .
"def get_variables ( self , include_submodules = False , include_nontrainable = False ) : model_variables = super ( QDemoModel , self ) . get_variables ( include_submodules = include_submodules , include_nontrainable = include_nontrainable ) if include_nontrainable : demo_memory_variables = self . demo_memory . get_variables ( ) model_variables += demo_memory_variables return model_variables",Returns the TensorFlow variables used by the model .
"def import_demo_experience ( self , states , internals , actions , terminal , reward ) : fetches = self . import_demo_experience_output feed_dict = self . get_feed_dict ( states = states , internals = internals , actions = actions , terminal = terminal , reward = reward ) self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict )",Stores demonstrations in the demo memory .
def demo_update ( self ) : fetches = self . demo_optimization_output self . monitored_session . run ( fetches = fetches ),Performs a demonstration update by calling the demo optimization operation . Note that the batch data does not have to be fetched from the demo memory as this is now part of the TensorFlow operation of the demo update .
"def from_config ( config , kwargs = None ) : return util . get_object ( obj = config , predefined = tensorforce . core . optimizers . solvers . solvers , kwargs = kwargs )",Creates a solver from a specification dict .
"def tf_step ( self , time , variables , * * kwargs ) : arguments = kwargs [ ""arguments"" ] fn_loss = kwargs [ ""fn_loss"" ] loss = fn_loss ( * * arguments ) with tf . control_dependencies ( control_inputs = ( loss , ) ) : previous_variables = [ variable + 0.0 for variable in variables ] with tf . control_dependencies ( control_inputs = previous_variables ) : applied = self . tf_optimizer . minimize ( loss = loss , var_list = variables ) with tf . control_dependencies ( control_inputs = ( applied , ) ) : return [ variable - previous_variable for variable , previous_variable in zip ( variables , previous_variables ) ]",Keyword Args : arguments : Dict of arguments for passing to fn_loss as ** kwargs . fn_loss : A callable taking arguments as kwargs and returning the loss op of the current model .
"def SetClipboardText ( text : str ) -> bool : if ctypes . windll . user32 . OpenClipboard ( 0 ) : ctypes . windll . user32 . EmptyClipboard ( ) textByteLen = ( len ( text ) + 1 ) * 2 hClipboardData = ctypes . windll . kernel32 . GlobalAlloc ( 0 , textByteLen ) hDestText = ctypes . windll . kernel32 . GlobalLock ( hClipboardData ) ctypes . cdll . msvcrt . wcsncpy ( ctypes . c_wchar_p ( hDestText ) , ctypes . c_wchar_p ( text ) , textByteLen // 2 ) ctypes . windll . kernel32 . GlobalUnlock ( hClipboardData ) ctypes . windll . user32 . SetClipboardData ( 13 , hClipboardData ) ctypes . windll . user32 . CloseClipboard ( ) return True return False",Return bool True if succeed otherwise False .
"def SetConsoleColor ( color : int ) -> bool : global _ConsoleOutputHandle global _DefaultConsoleColor if not _DefaultConsoleColor : if not _ConsoleOutputHandle : _ConsoleOutputHandle = ctypes . windll . kernel32 . GetStdHandle ( _StdOutputHandle ) bufferInfo = ConsoleScreenBufferInfo ( ) ctypes . windll . kernel32 . GetConsoleScreenBufferInfo ( _ConsoleOutputHandle , ctypes . byref ( bufferInfo ) ) _DefaultConsoleColor = int ( bufferInfo . wAttributes & 0xFF ) if sys . stdout : sys . stdout . flush ( ) bool ( ctypes . windll . kernel32 . SetConsoleTextAttribute ( _ConsoleOutputHandle , color ) )",Change the text color on console window . color : int a value in class ConsoleColor . Return bool True if succeed otherwise False .
"def ResetConsoleColor ( ) -> bool : if sys . stdout : sys . stdout . flush ( ) bool ( ctypes . windll . kernel32 . SetConsoleTextAttribute ( _ConsoleOutputHandle , _DefaultConsoleColor ) )",Reset to the default text color on console window . Return bool True if succeed otherwise False .
"def WindowFromPoint ( x : int , y : int ) -> int : return ctypes . windll . user32 . WindowFromPoint ( ctypes . wintypes . POINT ( x , y ) )",WindowFromPoint from Win32 . Return int a native window handle .
"def GetCursorPos ( ) -> tuple : point = ctypes . wintypes . POINT ( 0 , 0 ) ctypes . windll . user32 . GetCursorPos ( ctypes . byref ( point ) ) return point . x , point . y",GetCursorPos from Win32 . Get current mouse cursor positon . Return tuple two ints tuple ( x y ) .
"def SetCursorPos ( x : int , y : int ) -> bool : return bool ( ctypes . windll . user32 . SetCursorPos ( x , y ) )",SetCursorPos from Win32 . Set mouse cursor to point x y . x : int . y : int . Return bool True if succeed otherwise False .
"def mouse_event ( dwFlags : int , dx : int , dy : int , dwData : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . mouse_event ( dwFlags , dx , dy , dwData , dwExtraInfo )",mouse_event from Win32 .
"def keybd_event ( bVk : int , bScan : int , dwFlags : int , dwExtraInfo : int ) -> None : ctypes . windll . user32 . keybd_event ( bVk , bScan , dwFlags , dwExtraInfo )",keybd_event from Win32 .
"def PostMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> bool : return bool ( ctypes . windll . user32 . PostMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam ) )",PostMessage from Win32 . Return bool True if succeed otherwise False .
"def SendMessage ( handle : int , msg : int , wParam : int , lParam : int ) -> int : return ctypes . windll . user32 . SendMessageW ( ctypes . c_void_p ( handle ) , msg , wParam , lParam )",SendMessage from Win32 . Return int the return value specifies the result of the message processing ; it depends on the message sent .
"def Click ( x : int , y : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : SetCursorPos ( x , y ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . LeftDown | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( 0.05 ) mouse_event ( MouseEventFlag . LeftUp | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Simulate mouse click at point x y . x : int . y : int . waitTime : float .
"def MiddleClick ( x : int , y : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : SetCursorPos ( x , y ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . MiddleDown | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( 0.05 ) mouse_event ( MouseEventFlag . MiddleUp | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Simulate mouse middle click at point x y . x : int . y : int . waitTime : float .
"def RightClick ( x : int , y : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : SetCursorPos ( x , y ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . RightDown | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( 0.05 ) mouse_event ( MouseEventFlag . RightUp | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Simulate mouse right click at point x y . x : int . y : int . waitTime : float .
"def PressMouse ( x : int , y : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : SetCursorPos ( x , y ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . LeftDown | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Press left mouse . x : int . y : int . waitTime : float .
"def ReleaseMouse ( waitTime : float = OPERATION_WAIT_TIME ) -> None : x , y = GetCursorPos ( ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . LeftUp | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Release left mouse . waitTime : float .
"def RightPressMouse ( x : int , y : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : SetCursorPos ( x , y ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . RightDown | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Press right mouse . x : int . y : int . waitTime : float .
"def RightReleaseMouse ( waitTime : float = OPERATION_WAIT_TIME ) -> None : x , y = GetCursorPos ( ) screenWidth , screenHeight = GetScreenSize ( ) mouse_event ( MouseEventFlag . RightUp | MouseEventFlag . Absolute , x * 65535 // screenWidth , y * 65535 // screenHeight , 0 , 0 ) time . sleep ( waitTime )",Release right mouse . waitTime : float .
"def MoveTo ( x : int , y : int , moveSpeed : float = 1 , waitTime : float = OPERATION_WAIT_TIME ) -> None : if moveSpeed <= 0 : moveTime = 0 else : moveTime = MAX_MOVE_SECOND / moveSpeed curX , curY = GetCursorPos ( ) xCount = abs ( x - curX ) yCount = abs ( y - curY ) maxPoint = max ( xCount , yCount ) screenWidth , screenHeight = GetScreenSize ( ) maxSide = max ( screenWidth , screenHeight ) minSide = min ( screenWidth , screenHeight ) if maxPoint > minSide : maxPoint = minSide if maxPoint < maxSide : maxPoint = 100 + int ( ( maxSide - 100 ) / maxSide * maxPoint ) moveTime = moveTime * maxPoint * 1.0 / maxSide stepCount = maxPoint // 20 if stepCount > 1 : xStep = ( x - curX ) * 1.0 / stepCount yStep = ( y - curY ) * 1.0 / stepCount interval = moveTime / stepCount for i in range ( stepCount ) : cx = curX + int ( xStep * i ) cy = curY + int ( yStep * i ) SetCursorPos ( cx , cy ) time . sleep ( interval ) SetCursorPos ( x , y ) time . sleep ( waitTime )",Simulate mouse move to point x y from current cursor . x : int . y : int . moveSpeed : float 1 normal speed < 1 move slower > 1 move faster . waitTime : float .
"def DragDrop ( x1 : int , y1 : int , x2 : int , y2 : int , moveSpeed : float = 1 , waitTime : float = OPERATION_WAIT_TIME ) -> None : PressMouse ( x1 , y1 , 0.05 ) MoveTo ( x2 , y2 , moveSpeed , 0.05 ) ReleaseMouse ( waitTime )",Simulate mouse left button drag from point x1 y1 drop to point x2 y2 . x1 : int . y1 : int . x2 : int . y2 : int . moveSpeed : float 1 normal speed < 1 move slower > 1 move faster . waitTime : float .
"def RightDragDrop ( x1 : int , y1 : int , x2 : int , y2 : int , moveSpeed : float = 1 , waitTime : float = OPERATION_WAIT_TIME ) -> None : RightPressMouse ( x1 , y1 , 0.05 ) MoveTo ( x2 , y2 , moveSpeed , 0.05 ) RightReleaseMouse ( waitTime )",Simulate mouse right button drag from point x1 y1 drop to point x2 y2 . x1 : int . y1 : int . x2 : int . y2 : int . moveSpeed : float 1 normal speed < 1 move slower > 1 move faster . waitTime : float .
"def WheelUp ( wheelTimes : int = 1 , interval : float = 0.05 , waitTime : float = OPERATION_WAIT_TIME ) -> None : for i in range ( wheelTimes ) : mouse_event ( MouseEventFlag . Wheel , 0 , 0 , 120 , 0 ) time . sleep ( interval ) time . sleep ( waitTime )",Simulate mouse wheel up . wheelTimes : int . interval : float . waitTime : float .
"def GetScreenSize ( ) -> tuple : SM_CXSCREEN = 0 SM_CYSCREEN = 1 w = ctypes . windll . user32 . GetSystemMetrics ( SM_CXSCREEN ) h = ctypes . windll . user32 . GetSystemMetrics ( SM_CYSCREEN ) return w , h",Return tuple two ints tuple ( width height ) .
"def GetPixelColor ( x : int , y : int , handle : int = 0 ) -> int : hdc = ctypes . windll . user32 . GetWindowDC ( ctypes . c_void_p ( handle ) ) bgr = ctypes . windll . gdi32 . GetPixel ( hdc , x , y ) ctypes . windll . user32 . ReleaseDC ( ctypes . c_void_p ( handle ) , hdc ) return bgr",Get pixel color of a native window . x : int . y : int . handle : int the handle of a native window . Return int the bgr value of point ( x y ) . r = bgr & 0x0000FF g = ( bgr & 0x00FF00 ) >> 8 b = ( bgr & 0xFF0000 ) >> 16 If handle is 0 get pixel from Desktop window ( root control ) . Note : Not all devices support GetPixel . An application should call GetDeviceCaps to determine whether a specified device supports this function . For example console window doesn t support .
"def MessageBox ( content : str , title : str , flags : int = MB . Ok ) -> int : return ctypes . windll . user32 . MessageBoxW ( ctypes . c_void_p ( 0 ) , ctypes . c_wchar_p ( content ) , ctypes . c_wchar_p ( title ) , flags )",MessageBox from Win32 . content : str . title : str . flags : int a value or some combined values in class MB . Return int a value in MB whose name starts with Id such as MB . IdOk
def SetForegroundWindow ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . SetForegroundWindow ( ctypes . c_void_p ( handle ) ) ),SetForegroundWindow from Win32 . handle : int the handle of a native window . Return bool True if succeed otherwise False .
def BringWindowToTop ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . BringWindowToTop ( ctypes . c_void_p ( handle ) ) ),BringWindowToTop from Win32 . handle : int the handle of a native window . Return bool True if succeed otherwise False .
"def SwitchToThisWindow ( handle : int ) -> None : ctypes . windll . user32 . SwitchToThisWindow ( ctypes . c_void_p ( handle ) , 1 )",SwitchToThisWindow from Win32 . handle : int the handle of a native window .
"def GetAncestor ( handle : int , flag : int ) -> int : return ctypes . windll . user32 . GetAncestor ( ctypes . c_void_p ( handle ) , flag )",GetAncestor from Win32 . handle : int the handle of a native window . index : int a value in class GAFlag . Return int a native window handle .
def IsTopLevelWindow ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . IsTopLevelWindow ( ctypes . c_void_p ( handle ) ) ),IsTopLevelWindow from Win32 . handle : int the handle of a native window . Return bool . Only available on Windows 7 or Higher .
"def GetWindowLong ( handle : int , index : int ) -> int : return ctypes . windll . user32 . GetWindowLongW ( ctypes . c_void_p ( handle ) , index )",GetWindowLong from Win32 . handle : int the handle of a native window . index : int .
"def SetWindowLong ( handle : int , index : int , value : int ) -> int : return ctypes . windll . user32 . SetWindowLongW ( ctypes . c_void_p ( handle ) , index , value )",SetWindowLong from Win32 . handle : int the handle of a native window . index : int . value : int . Return int the previous value before set .
def IsIconic ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . IsIconic ( ctypes . c_void_p ( handle ) ) ),IsIconic from Win32 . Determine whether a native window is minimized . handle : int the handle of a native window . Return bool .
def IsZoomed ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . IsZoomed ( ctypes . c_void_p ( handle ) ) ),IsZoomed from Win32 . Determine whether a native window is maximized . handle : int the handle of a native window . Return bool .
def IsWindowVisible ( handle : int ) -> bool : return bool ( ctypes . windll . user32 . IsWindowVisible ( ctypes . c_void_p ( handle ) ) ),IsWindowVisible from Win32 . handle : int the handle of a native window . Return bool .
"def ShowWindow ( handle : int , cmdShow : int ) -> bool : return ctypes . windll . user32 . ShowWindow ( ctypes . c_void_p ( handle ) , cmdShow )",ShowWindow from Win32 . handle : int the handle of a native window . cmdShow : int a value in clas SW . Return bool True if succeed otherwise False .
"def MoveWindow ( handle : int , x : int , y : int , width : int , height : int , repaint : int = 1 ) -> bool : return bool ( ctypes . windll . user32 . MoveWindow ( ctypes . c_void_p ( handle ) , x , y , width , height , repaint ) )",MoveWindow from Win32 . handle : int the handle of a native window . x : int . y : int . width : int . height : int . repaint : int use 1 or 0 . Return bool True if succeed otherwise False .
"def SetWindowPos ( handle : int , hWndInsertAfter : int , x : int , y : int , width : int , height : int , flags : int ) -> bool : return ctypes . windll . user32 . SetWindowPos ( ctypes . c_void_p ( handle ) , ctypes . c_void_p ( hWndInsertAfter ) , x , y , width , height , flags )",SetWindowPos from Win32 . handle : int the handle of a native window . hWndInsertAfter : int a value whose name starts with HWND in class SWP . x : int . y : int . width : int . height : int . flags : int values whose name starts with SWP in class SWP . Return bool True if succeed otherwise False .
"def SetWindowTopmost ( handle : int , isTopmost : bool ) -> bool : topValue = SWP . HWND_Topmost if isTopmost else SWP . HWND_NoTopmost return bool ( SetWindowPos ( handle , topValue , 0 , 0 , 0 , 0 , SWP . SWP_NoSize | SWP . SWP_NoMove ) )",handle : int the handle of a native window . isTopmost : bool Return bool True if succeed otherwise False .
"def GetWindowText ( handle : int ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . user32 . GetWindowTextW ( ctypes . c_void_p ( handle ) , values , MAX_PATH ) return values . value",GetWindowText from Win32 . handle : int the handle of a native window . Return str .
"def SetWindowText ( handle : int , text : str ) -> bool : return bool ( ctypes . windll . user32 . SetWindowTextW ( ctypes . c_void_p ( handle ) , ctypes . c_wchar_p ( text ) ) )",SetWindowText from Win32 . handle : int the handle of a native window . text : str . Return bool True if succeed otherwise False .
"def GetEditText ( handle : int ) -> str : textLen = SendMessage ( handle , 0x000E , 0 , 0 ) + 1 arrayType = ctypes . c_wchar * textLen values = arrayType ( ) SendMessage ( handle , 0x000D , textLen , values ) return values . value",Get text of a native Win32 Edit . handle : int the handle of a native window . Return str .
"def GetConsoleOriginalTitle ( ) -> str : if IsNT6orHigher : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleOriginalTitleW ( values , MAX_PATH ) return values . value else : raise RuntimeError ( 'GetConsoleOriginalTitle is not supported on Windows XP or lower.' )",GetConsoleOriginalTitle from Win32 . Return str . Only available on Windows Vista or higher .
"def GetConsoleTitle ( ) -> str : arrayType = ctypes . c_wchar * MAX_PATH values = arrayType ( ) ctypes . windll . kernel32 . GetConsoleTitleW ( values , MAX_PATH ) return values . value",GetConsoleTitle from Win32 . Return str .
def SetConsoleTitle ( text : str ) -> bool : return bool ( ctypes . windll . kernel32 . SetConsoleTitleW ( ctypes . c_wchar_p ( text ) ) ),SetConsoleTitle from Win32 . text : str . Return bool True if succeed otherwise False .
"def IsDesktopLocked ( ) -> bool : isLocked = False desk = ctypes . windll . user32 . OpenDesktopW ( ctypes . c_wchar_p ( 'Default' ) , 0 , 0 , 0x0100 ) if desk : isLocked = not ctypes . windll . user32 . SwitchDesktop ( desk ) ctypes . windll . user32 . CloseDesktop ( desk ) return isLocked",Check if desktop is locked . Return bool . Desktop is locked if press Win + L Ctrl + Alt + Del or in remote desktop mode .
"def PlayWaveFile ( filePath : str = r'C:\Windows\Media\notify.wav' , isAsync : bool = False , isLoop : bool = False ) -> bool : if filePath : SND_ASYNC = 0x0001 SND_NODEFAULT = 0x0002 SND_LOOP = 0x0008 SND_FILENAME = 0x20000 flags = SND_NODEFAULT | SND_FILENAME if isAsync : flags |= SND_ASYNC if isLoop : flags |= SND_LOOP flags |= SND_ASYNC return bool ( ctypes . windll . winmm . PlaySoundW ( ctypes . c_wchar_p ( filePath ) , ctypes . c_void_p ( 0 ) , flags ) ) else : return bool ( ctypes . windll . winmm . PlaySoundW ( ctypes . c_wchar_p ( 0 ) , ctypes . c_void_p ( 0 ) , 0 ) )",Call PlaySound from Win32 . filePath : str if emtpy stop playing the current sound . isAsync : bool if True the sound is played asynchronously and returns immediately . isLoop : bool if True the sound plays repeatedly until PlayWaveFile ( None ) is called again must also set isAsync to True . Return bool True if succeed otherwise False .
"def IsProcess64Bit ( processId : int ) -> bool : try : func = ctypes . windll . ntdll . ZwWow64ReadVirtualMemory64 except Exception as ex : return False try : IsWow64Process = ctypes . windll . kernel32 . IsWow64Process IsWow64Process . argtypes = ( ctypes . c_void_p , ctypes . POINTER ( ctypes . c_int ) ) except Exception as ex : return False hProcess = ctypes . windll . kernel32 . OpenProcess ( 0x1000 , 0 , processId ) if hProcess : is64Bit = ctypes . c_int32 ( ) if IsWow64Process ( hProcess , ctypes . byref ( is64Bit ) ) : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) ) return False if is64Bit . value else True else : ctypes . windll . kernel32 . CloseHandle ( ctypes . c_void_p ( hProcess ) )",Return True if process is 64 bit . Return False if process is 32 bit . Return None if unknown maybe caused by having no acess right to the process .
"def RunScriptAsAdmin ( argv : list , workingDirectory : str = None , showFlag : int = SW . ShowNormal ) -> bool : args = ' ' . join ( '""{}""' . format ( arg ) for arg in argv ) return ctypes . windll . shell32 . ShellExecuteW ( None , ""runas"" , sys . executable , args , workingDirectory , showFlag ) > 32",Run a python script as administrator . System will show a popup dialog askes you whether to elevate as administrator if UAC is enabled . argv : list a str list like sys . argv argv [ 0 ] is the script file argv [ 1 : ] are other arguments . workingDirectory : str the working directory for the script file . showFlag : int a value in class SW . Return bool True if succeed .
"def SendKey ( key : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : keybd_event ( key , 0 , KeyboardEventFlag . KeyDown | KeyboardEventFlag . ExtendedKey , 0 ) keybd_event ( key , 0 , KeyboardEventFlag . KeyUp | KeyboardEventFlag . ExtendedKey , 0 ) time . sleep ( waitTime )",Simulate typing a key . key : int a value in class Keys .
"def PressKey ( key : int , waitTime : float = OPERATION_WAIT_TIME ) -> None : keybd_event ( key , 0 , KeyboardEventFlag . KeyDown | KeyboardEventFlag . ExtendedKey , 0 ) time . sleep ( waitTime )",Simulate a key down for key . key : int a value in class Keys . waitTime : float .
