code,summary
"def get_vid_from_url ( url ) : return match1 ( url , r'youtu\.be/([^?/]+)' ) or match1 ( url , r'youtube\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' )",Extracts video ID from URL .
def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl,str - > list Convert XML to URL List . From Biligrab .
"def makeMimi ( upid ) : strSeed = ""gGddgPfeaf_gzyr"" prehash = upid + ""_"" + strSeed return md5 ( prehash . encode ( 'utf-8' ) ) . hexdigest ( )",From http : // cdn37 . atwikiimg . com / sitescript / pub / dksitescript / FC2 . site . js Also com . hps . util . fc2 . FC2EncrptUtil . makeMimiLocal L110
"def fc2video_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : hostname = urlparse ( url ) . hostname if not ( 'fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname ) : return False upid = match1 ( url , r'.+/content/(\w+)' ) fc2video_download_by_upid ( upid , output_dir , merge , info_only )",wrapper
"def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities"":({.+?}),""' ) ) title = match1 ( html , r'""video_title""\s*:\s*""([^""]+)""' ) or match1 ( html , r'""title""\s*:\s*""([^""]+)""' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ ""url"" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )",Downloads Dailymotion videos by URL .
"def dictify ( r , root = True ) : if root : return { r . tag : dictify ( r , False ) } d = copy ( r . attrib ) if r . text : d [ ""_text"" ] = r . text for x in r . findall ( ""./*"" ) : if x . tag not in d : d [ x . tag ] = [ ] d [ x . tag ] . append ( dictify ( x , False ) ) return d",http : // stackoverflow . com / a / 30923963 / 2946714
"def ucas_download_single ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : html = get_content ( url ) resourceID = re . findall ( r'resourceID"":""([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})' , html ) [ 0 ] assert resourceID != '' , 'Cannot find resourceID!' title = match1 ( html , r'<div class=""bc-h"">(.+)</div>' ) url_lists = _ucas_get_url_lists_by_resourceID ( resourceID ) assert url_lists , 'Cannot find any URL of such class!' for k , part in enumerate ( url_lists ) : part_title = title + '_' + str ( k ) print_info ( site_info , part_title , 'flv' , 0 ) if not info_only : download_urls ( part , part_title , 'flv' , total_size = None , output_dir = output_dir , merge = merge )",video page
"def ucas_download_playlist ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : html = get_content ( url ) parts = re . findall ( r'(getplaytitle.do\?.+)""' , html ) assert parts , 'No part found!' for part_path in parts : ucas_download ( 'http://v.ucas.ac.cn/course/' + part_path , output_dir = output_dir , merge = merge , info_only = info_only )",course page
"def sina_download_by_vid ( vid , title = None , output_dir = '.' , merge = True , info_only = False ) : xml = api_req ( vid ) urls , name , size = video_info ( xml ) if urls is None : log . wtf ( name ) title = name print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( urls , title , 'flv' , size , output_dir = output_dir , merge = merge )",Downloads a Sina video by its unique vid . http : // video . sina . com . cn /
"def sina_download_by_vkey ( vkey , title = None , output_dir = '.' , merge = True , info_only = False ) : url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey type , ext , size = url_info ( url ) print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( [ url ] , title , 'flv' , size , output_dir = output_dir , merge = merge )",Downloads a Sina video by its unique vkey . http : // video . sina . com /
"def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\s*:\s*\'([^\']+)\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\w]vid\s*:\s*\'([^\']+)\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:""?(\d+)""?' ) if vid : sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\s*:\s*""([^""]+)""' ) if vkey is None : vid = match1 ( url , r'#(\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\s*:\s*""([^""]+)""' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )",Downloads Sina videos by URL .
"def yixia_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : hostname = urlparse ( url ) . hostname if 'n.miaopai.com' == hostname : smid = match1 ( url , r'n\.miaopai\.com/media/([^.]+)' ) miaopai_download_by_smid ( smid , output_dir , merge , info_only ) return elif 'miaopai.com' in hostname : yixia_download_by_scid = yixia_miaopai_download_by_scid site_info = ""Yixia Miaopai"" scid = match1 ( url , r'miaopai\.com/show/channel/([^.]+)\.htm' ) or match1 ( url , r'miaopai\.com/show/([^.]+)\.htm' ) or match1 ( url , r'm\.miaopai\.com/show/channel/([^.]+)\.htm' ) or match1 ( url , r'm\.miaopai\.com/show/channel/([^.]+)' ) elif 'xiaokaxiu.com' in hostname : yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid site_info = ""Yixia Xiaokaxiu"" if re . match ( r'http://v.xiaokaxiu.com/v/.+\.html' , url ) : scid = match1 ( url , r'http://v.xiaokaxiu.com/v/(.+)\.html' ) elif re . match ( r'http://m.xiaokaxiu.com/m/.+\.html' , url ) : scid = match1 ( url , r'http://m.xiaokaxiu.com/m/(.+)\.html' ) else : pass yixia_download_by_scid ( scid , output_dir , merge , info_only )",wrapper
"def veoh_download ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : if re . match ( r'http://www.veoh.com/watch/\w+' , url ) : item_id = match1 ( url , r'http://www.veoh.com/watch/(\w+)' ) elif re . match ( r'http://www.veoh.com/m/watch.php\?v=\.*' , url ) : item_id = match1 ( url , r'http://www.veoh.com/m/watch.php\?v=(\w+)' ) else : raise NotImplementedError ( 'Cannot find item ID' ) veoh_download_by_id ( item_id , output_dir = '.' , merge = False , info_only = info_only , * * kwargs )",Get item_id
"def veoh_download_by_id ( item_id , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1' . format ( item_id = item_id ) a = get_content ( webpage_url , decoded = True ) url = match1 ( a , r'<source src=""(.*?)\""\W' ) title = match1 ( a , r'<meta property=""og:title"" content=""([^""]*)""' ) type_ , ext , size = url_info ( url ) print_info ( site_info , title , type_ , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = None , output_dir = output_dir , merge = merge )",Source : Android mobile
"def download_by_id ( self , vid = '' , title = None , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : assert vid self . prepare ( vid = vid , title = title , * * kwargs ) self . extract ( * * kwargs ) self . download ( output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",self str - > None Keyword arguments : self : self vid : The video ID for BokeCC cloud something like FE3BB999594978049C33DC5901307461 Calls the prepare () to download the video . If no title is provided this method shall try to find a proper title with the information providin within the returned content of the API .
"def get_vid_from_url ( self , url ) : hit = re . search ( r'live.qq.com/(\d+)' , url ) if hit is not None : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\d+)' , url ) if hit is not None : return self . get_room_id_from_url ( hit . group ( 1 ) ) html = get_content ( url ) room_id = match1 ( html , r'room_id\"":(\d+)' ) if room_id is None : log . wtf ( 'Unknown page {}' . format ( url ) ) return room_id",Extracts video ID from live . qq . com .
"def sprint ( text , * colors ) : return ""\33[{}m{content}\33[{}m"" . format ( "";"" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text",Format text with color or other effects into ANSI escaped string .
"def print_log ( text , * colors ) : sys . stderr . write ( sprint ( ""{}: {}"" . format ( script_name , text ) , * colors ) + ""\n"" )",Print a log message to standard error .
"def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )",Print an error log message .
"def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )",What a Terrible Failure!
"def detect_os ( ) : syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os",Detect operating system .
"def miaopai_download_by_fid ( fid , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : page_url = 'http://video.weibo.com/show?fid=' + fid + '&type=mp4' mobile_page = get_content ( page_url , headers = fake_headers_mobile ) url = match1 ( mobile_page , r'<video id=.*?src=[\'""](.*?)[\'""]\W' ) if url is None : wb_mp = re . search ( r'<script src=([\'""])(.+?wb_mp\.js)\1>' , mobile_page ) . group ( 2 ) return miaopai_download_by_wbmp ( wb_mp , fid , output_dir = output_dir , merge = merge , info_only = info_only , total_size = None , * * kwargs ) title = match1 ( mobile_page , r'<title>((.|\n)+?)</title>' ) if not title : title = fid title = title . replace ( '\n' , '_' ) ext , size = 'mp4' , url_info ( url ) [ 2 ] print_info ( site_info , title , ext , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = None , output_dir = output_dir , merge = merge )",Source : Android mobile
"def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : channel_id = match1 ( url , r'http://vimeo.com/channels/(\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )",str - > None
"def vimeo_download_by_channel_id ( channel_id , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : html = get_content ( 'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}' . format ( channel_id = channel_id , access_token = access_token ) ) data = loads ( html ) id_list = [ ] for i in data [ 'data' ] : id_list . append ( match1 ( i [ 'uri' ] , r'/videos/(\w+)' ) ) for id in id_list : try : vimeo_download_by_id ( id , None , output_dir , merge , info_only , * * kwargs ) except urllib . error . URLError as e : log . w ( '{} failed with {}' . format ( id , e ) )",str / int - > None
"def vimeo_download_by_id ( id , title = None , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : site = VimeoExtractor ( ) site . download_by_vid ( id , info_only = info_only , output_dir = output_dir , merge = merge , * * kwargs )",try : # normal Vimeo video html = get_content ( https : // vimeo . com / + id ) cfg_patt = r clip_page_config \ s * = \ s * ( \ { . + ? \ } ) ; cfg = json . loads ( match1 ( html cfg_patt )) video_page = get_content ( cfg [ player ] [ config_url ] headers = fake_headers ) title = cfg [ clip ] [ title ] info = loads ( video_page ) except : # embedded player - referer may be required if referer in kwargs : fake_headers [ Referer ] = kwargs [ referer ]
"def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict",str - > dict Information for CKPlayer API content .
"def get_video_url_from_video_id ( video_id ) : data = [ """" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = ""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}"" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return ""https://ib.365yg.com{path}&s={param}"" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( ""="" ) [ - 1 ] [ 0 ] != ""-"" : return url",Splicing URLs according to video ID to get video details
"def get_vid_from_url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\d+/(\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\d+/(\d+).html' ) return vid",Extracts video ID from URL .
"def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = ""{scheme}://{netloc}{path}/"" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : segment_list . append ( base_url + i ) elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list",str - > list of str Give you the real URLs .
"def get_head ( repo_path ) : try : ref = open ( os . path . join ( repo_path , '.git' , 'HEAD' ) , 'r' ) . read ( ) . strip ( ) [ 5 : ] . split ( '/' ) branch = ref [ - 1 ] commit = open ( os . path . join ( repo_path , '.git' , * ref ) , 'r' ) . read ( ) . strip ( ) [ : 7 ] return branch , commit except : return None",Get ( branch commit ) from HEAD of a git repo .
"def legitimize ( text , os = detect_os ( ) ) : text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) if os == 'windows' or os == 'cygwin' or os == 'wsl' : text = text . translate ( { ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\' ) : '-' , ord ( '\""' ) : '\'' , ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\t' ) : ' ' , } ) else : if os == 'mac' : text = text . translate ( { ord ( ':' ) : '-' , } ) if text . startswith ( ""."" ) : text = text [ 1 : ] text = text [ : 80 ] return text",Converts a string to a valid filename .
"def get_terminal_size ( ) : try : import fcntl , termios , struct return struct . unpack ( 'hh' , fcntl . ioctl ( 1 , termios . TIOCGWINSZ , '1234' ) ) except : return ( 40 , 80 )",Get ( width height ) of the current terminal .
"def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\.settings\.pid\s*=\s*\'([^\']+)\'' ) title = match1 ( html , r'video\.settings\.title\s*=\s*\""([^\""]+)\""' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )",Downloads CBS videos by URL .
"def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : stream_id = kwargs [ 'stream_id' ] else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = """" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )",Override the original one Ugly ugly dirty hack
"def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] if 'sourceId' in info : sourceId = info [ 'sourceId' ] if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size if re . search ( r'fid=[0-9A-Z\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass",str str str bool bool - > None
"def main_dev ( * * kwargs ) : head = git . get_head ( kwargs [ 'repo_path' ] ) try : opts , args = getopt . getopt ( sys . argv [ 1 : ] , _short_options , _options ) except getopt . GetoptError as e : log . wtf ( """"""
    [Fatal] {}.
    Try '{} --help' for more options."""""" . format ( e , script_name ) ) if not opts and not args : print ( _help ) else : conf = { } for opt , arg in opts : if opt in ( '-h' , '--help' ) : print ( _help ) elif opt in ( '-V' , '--version' ) : log . println ( ""you-get:"" , log . BOLD ) log . println ( ""    version:  {}"" . format ( __version__ ) ) if head is not None : log . println ( ""    branch:   {}\n    commit:   {}"" . format ( * head ) ) else : log . println ( ""    branch:   {}\n    commit:   {}"" . format ( ""(stable)"" , ""(tag v{})"" . format ( __version__ ) ) ) log . println ( ""    platform: {}"" . format ( platform . platform ( ) ) ) log . println ( ""    python:   {}"" . format ( sys . version . split ( '\n' ) [ 0 ] ) ) elif opt in ( '-g' , '--gui' ) : conf [ 'gui' ] = True elif opt in ( '-f' , '--force' ) : conf [ 'force' ] = True elif opt in ( '-l' , '--playlist' , '--playlists' ) : conf [ 'playlist' ] = True if args : if 'gui' in conf and conf [ 'gui' ] : from . gui import gui_main gui_main ( * args , * * conf ) else : from . console import console_main console_main ( * args , * * conf )",Main entry point . you - get - dev
"def ffmpeg_download_stream ( files , title , ext , params = { } , output_dir = '.' , stream = True ) : output = title + '.' + ext if not ( output_dir == '.' ) : output = output_dir + '/' + output print ( 'Downloading streaming content with FFmpeg, press q to stop recording...' ) if stream : ffmpeg_params = [ FFMPEG ] + [ '-y' , '-re' , '-i' ] else : ffmpeg_params = [ FFMPEG ] + [ '-y' , '-i' ] ffmpeg_params . append ( files ) if FFMPEG == 'avconv' : ffmpeg_params += [ '-c' , 'copy' , output ] else : ffmpeg_params += [ '-c' , 'copy' , '-bsf:a' , 'aac_adtstoasc' ] if params is not None : if len ( params ) > 0 : for k , v in params : ffmpeg_params . append ( k ) ffmpeg_params . append ( v ) ffmpeg_params . append ( output ) print ( ' ' . join ( ffmpeg_params ) ) try : a = subprocess . Popen ( ffmpeg_params , stdin = subprocess . PIPE ) a . communicate ( ) except KeyboardInterrupt : try : a . stdin . write ( 'q' . encode ( 'utf-8' ) ) except : pass return True",str str - > True WARNING : NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!! You can basicly download anything with this function but better leave it alone with
"def match1 ( text , * patterns ) : if len ( patterns ) == 1 : pattern = patterns [ 0 ] match = re . search ( pattern , text ) if match : return match . group ( 1 ) else : return None else : ret = [ ] for pattern in patterns : match = re . search ( pattern , text ) if match : ret . append ( match . group ( 1 ) ) return ret",Scans through a string for substrings matched some patterns ( first - subgroups only ) .
"def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret",Scans through a string for substrings matched some patterns .
"def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None",Parses the query string of a URL and returns the value of a parameter .
def ungzip ( data ) : from io import BytesIO import gzip buffer = BytesIO ( data ) f = gzip . GzipFile ( fileobj = buffer ) return f . read ( ),Decompresses data for Content - Encoding : gzip .
def undeflate ( data ) : import zlib decompressobj = zlib . decompressobj ( - zlib . MAX_WBITS ) return decompressobj . decompress ( data ) + decompressobj . flush ( ),Decompresses data for Content - Encoding : deflate . ( the zlib compression is used . )
"def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data",Gets the content of a URL via sending a HTTP GET request .
"def post_content ( url , headers = { } , post_data = { } , decoded = True , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data",Post the content of a URL via sending a HTTP POST request .
"def parse_host ( host ) : if re . match ( r'^(\d+)$' , host ) is not None : return ( ""0.0.0.0"" , int ( host ) ) if re . match ( r'^(\w+)://' , host ) is None : host = ""//"" + host o = parse . urlparse ( host ) hostname = o . hostname or ""0.0.0.0"" port = o . port or 0 return ( hostname , port )",Parses host name and port number from a string .
"def print_more_compatible ( * args , * * kwargs ) : import builtins as __builtin__ if sys . version_info [ : 2 ] >= ( 3 , 3 ) : return __builtin__ . print ( * args , * * kwargs ) doFlush = kwargs . pop ( 'flush' , False ) ret = __builtin__ . print ( * args , * * kwargs ) if doFlush : kwargs . get ( 'file' , sys . stdout ) . flush ( ) return ret",Overload default print function as py ( <3 . 3 ) does not support flush keyword . Although the function name can be same as print to get itself overloaded automatically I d rather leave it with a different name and only overload it when importing to make less confusion .
"def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'Accept-Charset' : 'UTF-8,*;q=0.5' , 'Accept-Encoding' : 'gzip,deflate,sdch' , 'Accept-Language' : 'en-US,en;q=0.8' , 'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\?room_id\=(\d+)' ) assert roomid return roomid",str - > str
"def showroom_download_by_room_id ( room_id , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : while True : timestamp = str ( int ( time ( ) * 1000 ) ) api_endpoint = 'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}' . format ( room_id = room_id , timestamp = timestamp ) html = get_content ( api_endpoint ) html = json . loads ( html ) if len ( html ) >= 1 : break log . w ( 'The live show is currently offline.' ) sleep ( 1 ) stream_url = [ i [ 'url' ] for i in html [ 'streaming_url_list' ] if i [ 'is_default' ] and i [ 'type' ] == 'hls' ] [ 0 ] assert stream_url title = '' profile_api = 'https://www.showroom-live.com/api/room/profile?room_id={room_id}' . format ( room_id = room_id ) html = loads ( get_content ( profile_api ) ) try : title = html [ 'main_name' ] except KeyError : title = 'Showroom_{room_id}' . format ( room_id = room_id ) type_ , ext , size = url_info ( stream_url ) print_info ( site_info , title , type_ , size ) if not info_only : download_url_ffmpeg ( url = stream_url , title = title , ext = 'mp4' , output_dir = output_dir )",Source : Android mobile
"def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] )",JSON int int int - > str Get a proper title with courseid + topicID + partID .
"def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",int - > None Download a WHOLE course . Reuse the API call to save time .
"def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tIndex , pIndex ) bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , tIndex , pIndex ) bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )",int int int - > None Download ONE PART of the course .
"def get_streams_by_id ( account_number , video_id ) : endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}' . format ( account_number = account_number , video_id = video_id ) fake_header_id = fake_headers fake_header_id [ 'Accept' ] = 'application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ' html = get_content ( endpoint , headers = fake_header_id ) html_json = json . loads ( html ) link_list = [ ] for i in html_json [ 'sources' ] : if 'src' in i : if i [ 'src' ] . startswith ( 'https' ) : link_list . append ( ( str ( i [ 'height' ] ) , i [ 'src' ] ) ) return link_list",int int - > list Get the height of the videos . Since brightcove is using 3 kinds of links : rtmp http and https we will be using the HTTPS one to make it secure . If somehow akamaihd . net is blocked by the Great Fucking Wall change the startswith https to http .
"def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return True",Checks if a task is either queued or running in this executor
"def get_event_buffer ( self , dag_ids = None ) : cleared_events = dict ( ) if dag_ids is None : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events",Returns and flush the event buffer . In case dag_ids is specified it will only return and flush events for the given dag_ids . Otherwise it returns and flushes all
"def _get_conn_params ( self ) : conn = self . get_connection ( self . snowflake_conn_id ) account = conn . extra_dejson . get ( 'account' , None ) warehouse = conn . extra_dejson . get ( 'warehouse' , None ) database = conn . extra_dejson . get ( 'database' , None ) region = conn . extra_dejson . get ( ""region"" , None ) role = conn . extra_dejson . get ( 'role' , None ) conn_config = { ""user"" : conn . login , ""password"" : conn . password or '' , ""schema"" : conn . schema or '' , ""database"" : self . database or database or '' , ""account"" : self . account or account or '' , ""warehouse"" : self . warehouse or warehouse or '' , ""region"" : self . region or region or '' , ""role"" : self . role or role or '' , } """"""
        If private_key_file is specified in the extra json, load the contents of the file as a private
        key and specify that in the connection configuration. The connection password then becomes the
        passphrase for the private key. If your private key file is not encrypted (not recommended), then
        leave the password empty.
        """""" private_key_file = conn . extra_dejson . get ( 'private_key_file' , None ) if private_key_file : with open ( private_key_file , ""rb"" ) as key : passphrase = None if conn . password : passphrase = conn . password . strip ( ) . encode ( ) p_key = serialization . load_pem_private_key ( key . read ( ) , password = passphrase , backend = default_backend ( ) ) pkb = p_key . private_bytes ( encoding = serialization . Encoding . DER , format = serialization . PrivateFormat . PKCS8 , encryption_algorithm = serialization . NoEncryption ( ) ) conn_config [ 'private_key' ] = pkb conn_config . pop ( 'password' , None ) return conn_config",one method to fetch connection params as a dict used in get_uri () and get_connection ()
def get_uri ( self ) : conn_config = self . _get_conn_params ( ) uri = 'snowflake://{user}:{password}@{account}/{database}/' uri += '{schema}?warehouse={warehouse}&role={role}' return uri . format ( * * conn_config ),override DbApiHook get_uri method for get_sqlalchemy_engine ()
def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn,Returns a snowflake . connection object
"def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key",returns aws_access_key_id aws_secret_access_key from extra
"def _get_field ( self , field_name , default = None ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default",Fetches a field from extras and returns it . This is some Airflow magic . The grpc hook type adds custom UI elements to the hook page which allow admins to specify scopes credential pem files etc . They get formatted as shown below .
"def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )",Executes SQL using psycopg2 copy_expert method . Necessary to execute COPY command without access to a superuser .
"def bulk_load ( self , table , tmp_file ) : self . copy_expert ( ""COPY {table} FROM STDIN"" . format ( table = table ) , tmp_file )",Loads a tab - delimited file into a database table
"def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( ""COPY {table} TO STDOUT"" . format ( table = table ) , tmp_file )",Dumps a database table into a tab - delimited file
"def execute ( self , context ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )",Uploads the file to Google cloud storage
"def max_partition ( table , schema = ""default"" , field = None , filter_map = None , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )",Gets the max partition for a table .
"def _closest_date ( target_dt , date_list , before_target = None ) : fb = lambda d : target_dt - d if d <= target_dt else datetime . timedelta . max fa = lambda d : d - target_dt if d >= target_dt else datetime . timedelta . max fnone = lambda d : target_dt - d if d < target_dt else d - target_dt if before_target is None : return min ( date_list , key = fnone ) . date ( ) if before_target : return min ( date_list , key = fb ) . date ( ) else : return min ( date_list , key = fa ) . date ( )",This function finds the date in a list closest to the target date . An optional parameter can be given to get the closest before or after .
"def closest_ds_partition ( table , ds , before = True , schema = ""default"" , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import HiveMetastoreHook if '.' in table : schema , table = table . split ( '.' ) hh = HiveMetastoreHook ( metastore_conn_id = metastore_conn_id ) partitions = hh . get_partitions ( schema = schema , table_name = table ) if not partitions : return None part_vals = [ list ( p . values ( ) ) [ 0 ] for p in partitions ] if ds in part_vals : return ds else : parts = [ datetime . datetime . strptime ( pv , '%Y-%m-%d' ) for pv in part_vals ] target_dt = datetime . datetime . strptime ( ds , '%Y-%m-%d' ) closest_ds = _closest_date ( target_dt , parts , before_target = before ) return closest_ds . isoformat ( )",This function finds the date in a list closest to the target date . An optional parameter can be given to get the closest before or after .
"def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { ""user"" : conn . login , ""passwd"" : conn . password or '' , ""host"" : conn . host or 'localhost' , ""db"" : self . schema or conn . schema or '' } if not conn . port : conn_config [ ""port"" ] = 3306 else : conn_config [ ""port"" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , False ) : conn_config [ ""charset"" ] = conn . extra_dejson [ ""charset"" ] if ( conn_config [ ""charset"" ] ) . lower ( ) == 'utf8' or ( conn_config [ ""charset"" ] ) . lower ( ) == 'utf-8' : conn_config [ ""use_unicode"" ] = True if conn . extra_dejson . get ( 'cursor' , False ) : if ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'sscursor' : conn_config [ ""cursorclass"" ] = MySQLdb . cursors . SSCursor elif ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'dictcursor' : conn_config [ ""cursorclass"" ] = MySQLdb . cursors . DictCursor elif ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ ""cursorclass"" ] = MySQLdb . cursors . SSDictCursor local_infile = conn . extra_dejson . get ( 'local_infile' , False ) if conn . extra_dejson . get ( 'ssl' , False ) : dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ ""local_infile"" ] = 1 conn = MySQLdb . connect ( * * conn_config ) return conn",Returns a mysql connection object
"def bulk_load ( self , table , tmp_file ) : conn = self . get_conn ( ) cur = conn . cursor ( ) cur . execute ( """"""
            LOAD DATA LOCAL INFILE '{tmp_file}'
            INTO TABLE {table}
            """""" . format ( tmp_file = tmp_file , table = table ) ) conn . commit ( )",Loads a tab - delimited file into a database table
"def is_bucket_updated ( self , current_num_objects ) : if current_num_objects > self . previous_num_objects : self . log . info ( '''
                New objects found at {} resetting last_activity_time.
                ''' . format ( os . path . join ( self . bucket , self . prefix ) ) ) self . last_activity_time = get_time ( ) self . inactivity_seconds = 0 self . previous_num_objects = current_num_objects elif current_num_objects < self . previous_num_objects : if self . allow_delete : self . previous_num_objects = current_num_objects self . last_activity_time = get_time ( ) self . log . warning ( '''
                    Objects were deleted during the last
                    poke interval. Updating the file counter and
                    resetting last_activity_time.
                    ''' ) else : raise RuntimeError ( '''
                    Illegal behavior: objects were deleted in {} between pokes.
                    ''' . format ( os . path . join ( self . bucket , self . prefix ) ) ) else : if self . last_activity_time : self . inactivity_seconds = ( get_time ( ) - self . last_activity_time ) . total_seconds ( ) else : self . last_activity_time = get_time ( ) self . inactivity_seconds = 0 if self . inactivity_seconds >= self . inactivity_period : if current_num_objects >= self . min_objects : self . log . info ( '''
                        SUCCESS:
                        Sensor found {} objects at {}.
                        Waited at least {} seconds, with no new objects dropped.
                        ''' . format ( current_num_objects , os . path . join ( self . bucket , self . prefix ) , self . inactivity_period ) ) return True warn_msg = '''
                    FAILURE:
                    Inactivity Period passed,
                    not enough objects found in {}
                    ''' . format ( os . path . join ( self . bucket , self . prefix ) ) self . log . warning ( warn_msg ) return False return False",Checks whether new objects have been uploaded and the inactivity_period has passed and updates the state of the sensor accordingly .
"def sigquit_handler ( sig , frame ) : print ( ""Dumping stack traces for all threads in PID {}"" . format ( os . getpid ( ) ) ) id_to_name = dict ( [ ( th . ident , th . name ) for th in threading . enumerate ( ) ] ) code = [ ] for thread_id , stack in sys . _current_frames ( ) . items ( ) : code . append ( ""\n# Thread: {}({})"" . format ( id_to_name . get ( thread_id , """" ) , thread_id ) ) for filename , line_number , name , line in traceback . extract_stack ( stack ) : code . append ( 'File: ""{}"", line {}, in {}' . format ( filename , line_number , name ) ) if line : code . append ( ""  {}"" . format ( line . strip ( ) ) ) print ( ""\n"" . join ( code ) )",Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT e . g . kill - s QUIT <PID > or CTRL + \
"def trigger_dag ( args ) : log = LoggingMixin ( ) . log try : message = api_client . trigger_dag ( dag_id = args . dag_id , run_id = args . run_id , conf = args . conf , execution_date = args . exec_date ) except IOError as err : log . error ( err ) raise AirflowException ( err ) log . info ( message )",Creates a dag run for the specified dag : param args : : return :
"def delete_dag ( args ) : log = LoggingMixin ( ) . log if args . yes or input ( ""This will drop all existing records related to the specified DAG. "" ""Proceed? (y/n)"" ) . upper ( ) == ""Y"" : try : message = api_client . delete_dag ( dag_id = args . dag_id ) except IOError as err : log . error ( err ) raise AirflowException ( err ) log . info ( message ) else : print ( ""Bail."" )",Deletes all DB records related to the specified dag : param args : : return :
"def task_failed_deps ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) dep_context = DepContext ( deps = SCHEDULER_DEPS ) failed_deps = list ( ti . get_failed_dep_statuses ( dep_context = dep_context ) ) if failed_deps : print ( ""Task instance dependencies not met:"" ) for dep in failed_deps : print ( ""{}: {}"" . format ( dep . dep_name , dep . reason ) ) else : print ( ""Task instance dependencies are all met."" )",Returns the unmet dependencies for a task instance from the perspective of the scheduler ( i . e . why a task instance doesn t get scheduled and then queued by the scheduler and then run by an executor ) . >>> airflow task_failed_deps tutorial sleep 2015 - 01 - 01 Task instance dependencies not met : Dagrun Running : Task instance s dagrun did not exist : Unknown reason Trigger Rule : Task s trigger rule all_success requires all upstream tasks to have succeeded but found 1 non - success ( es ) .
"def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = TaskInstance ( task , args . execution_date ) print ( ti . current_state ( ) )",Returns the state of a TaskInstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success
"def dag_state ( args ) : dag = get_dag ( args ) dr = DagRun . find ( dag . dag_id , execution_date = args . execution_date ) print ( dr [ 0 ] . state if len ( dr ) > 0 else None )",Returns the state of a DagRun at the command line . >>> airflow dag_state tutorial 2015 - 01 - 01T00 : 00 : 00 . 000000 running
"def next_execution ( args ) : dag = get_dag ( args ) if dag . is_paused : print ( ""[INFO] Please be reminded this DAG is PAUSED now."" ) if dag . latest_execution_date : next_execution_dttm = dag . following_schedule ( dag . latest_execution_date ) if next_execution_dttm is None : print ( ""[WARN] No following schedule can be found. "" + ""This DAG may have schedule interval '@once' or `None`."" ) print ( next_execution_dttm ) else : print ( ""[WARN] Only applicable when there is execution record found for the DAG."" ) print ( None )",Returns the next execution datetime of a DAG at the command line . >>> airflow next_execution tutorial 2018 - 08 - 31 10 : 38 : 00
"def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : """"""
        Sleeps until fn is true
        """""" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise AirflowWebServerTimeout ( ""No response from gunicorn master within {0} seconds"" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . SIGTTIN ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while True : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . SIGTTOU ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : log . error ( ( ""%s some workers seem to have died and gunicorn"" ""did not restart them as expected"" ) , state ) time . sleep ( 10 ) if len ( psutil . Process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( AirflowWebServerTimeout , OSError ) as err : log . error ( err ) log . error ( ""Shutting down webserver"" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )",Runs forever monitoring the child processes of
def get_conn ( self ) : if not self . _client : self . _client = Client ( credentials = self . _get_credentials ( ) ) return self . _client,Retrieves connection to Cloud Translate
"def translate ( self , values , target_language , format_ = None , source_language = None , model = None ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )",Translate a string or list of strings .
"def execute ( self , context ) : self . log . info ( 'Tmp dir root location: \n %s' , gettempdir ( ) ) if self . env is None : self . env = os . environ . copy ( ) airflow_context_vars = context_to_airflow_vars ( context , in_env_var_format = True ) self . log . info ( 'Exporting the following env vars:\n%s' , '\n' . join ( [ ""{}={}"" . format ( k , v ) for k , v in airflow_context_vars . items ( ) ] ) ) self . env . update ( airflow_context_vars ) self . lineage_data = self . bash_command with TemporaryDirectory ( prefix = 'airflowtmp' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir , prefix = self . task_id ) as tmp_file : tmp_file . write ( bytes ( self . bash_command , 'utf_8' ) ) tmp_file . flush ( ) script_location = os . path . abspath ( tmp_file . name ) self . log . info ( 'Temporary script location: %s' , script_location ) def pre_exec ( ) : for sig in ( 'SIGPIPE' , 'SIGXFZ' , 'SIGXFSZ' ) : if hasattr ( signal , sig ) : signal . signal ( getattr ( signal , sig ) , signal . SIG_DFL ) os . setsid ( ) self . log . info ( 'Running command: %s' , self . bash_command ) sub_process = Popen ( [ 'bash' , tmp_file . name ] , stdout = PIPE , stderr = STDOUT , cwd = tmp_dir , env = self . env , preexec_fn = pre_exec ) self . sub_process = sub_process self . log . info ( 'Output:' ) line = '' for raw_line in iter ( sub_process . stdout . readline , b'' ) : line = raw_line . decode ( self . output_encoding ) . rstrip ( ) self . log . info ( line ) sub_process . wait ( ) self . log . info ( 'Command exited with return code %s' , sub_process . returncode ) if sub_process . returncode : raise AirflowException ( 'Bash command failed' ) return line",Execute the bash command in a temporary directory which will be cleaned afterwards
"def get_instance ( self , instance , project_id = None ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )",Retrieves a resource containing information about a Cloud SQL instance .
"def create_instance ( self , body , project_id = None ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Creates a new Cloud SQL instance .
"def patch_instance ( self , body , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Updates settings of a Cloud SQL instance .
"def delete_instance ( self , instance , project_id = None ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Deletes a Cloud SQL instance .
"def get_database ( self , instance , database , project_id = None ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )",Retrieves a database resource from a Cloud SQL instance .
"def create_database ( self , instance , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Creates a new database inside a Cloud SQL instance .
"def patch_database ( self , instance , database , body , project_id = None ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Updates a database resource inside a Cloud SQL instance .
"def delete_database ( self , instance , database , project_id = None ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Deletes a database from a Cloud SQL instance .
"def export_instance ( self , instance , body , project_id = None ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except HttpError as ex : raise AirflowException ( 'Exporting instance {} failed: {}' . format ( instance , ex . content ) )",Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump or CSV file .
"def _wait_for_operation_to_complete ( self , project_id , operation_name ) : service = self . get_conn ( ) while True : operation_response = service . operations ( ) . get ( project = project_id , operation = operation_name , ) . execute ( num_retries = self . num_retries ) if operation_response . get ( ""status"" ) == CloudSqlOperationStatus . DONE : error = operation_response . get ( ""error"" ) if error : error_msg = str ( error . get ( ""errors"" ) ) [ 1 : - 1 ] raise AirflowException ( error_msg ) return time . sleep ( TIME_TO_SLEEP_IN_SECONDS )",Waits for the named operation to complete - checks status of the asynchronous call .
"def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise AirflowException ( ""The sql proxy is already running: {}"" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( ""Creating directory %s"" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except OSError : pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( ""Running the command: `%s`"" , "" "" . join ( command_to_run ) ) self . sql_proxy_process = Popen ( command_to_run , stdin = PIPE , stdout = PIPE , stderr = PIPE ) self . log . info ( ""The pid of cloud_sql_proxy: %s"" , self . sql_proxy_process . pid ) while True : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not None : self . sql_proxy_process = None raise AirflowException ( ""The cloud_sql_proxy finished early with return code {}!"" . format ( return_code ) ) if line != '' : self . log . info ( line ) if ""googleapi: Error"" in line or ""invalid instance name:"" in line : self . stop_proxy ( ) raise AirflowException ( ""Error when starting the cloud_sql_proxy {}!"" . format ( line ) ) if ""Ready for new connections"" in line : return",Starts Cloud SQL Proxy .
"def stop_proxy ( self ) : if not self . sql_proxy_process : raise AirflowException ( ""The sql proxy is not started yet"" ) else : self . log . info ( ""Stopping the cloud_sql_proxy pid: %s"" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = None self . log . info ( ""Removing the socket directory: %s"" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = True ) if self . sql_proxy_was_downloaded : self . log . info ( ""Removing downloaded proxy: %s"" , self . sql_proxy_path ) try : os . remove ( self . sql_proxy_path ) except OSError as e : if not e . errno == errno . ENOENT : raise else : self . log . info ( ""Skipped removing proxy - it was not downloaded: %s"" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( ""Removing generated credentials file %s"" , self . credentials_path ) os . remove ( self . credentials_path )",Stops running proxy .
"def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( ""^.*[V|v]ersion ([^;]*);.*$"" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return None",Returns version of the Cloud SQL Proxy .
"def create_connection ( self , session = None ) : connection = Connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( ""Creating connection %s"" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )",Create connection in the Connection table according to whether it uses proxy TCP UNIX sockets SSL . Connection ID will be randomly generated .
"def retrieve_connection ( self , session = None ) : self . log . info ( ""Retrieving connection %s"" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return None",Retrieves the dynamically created connection from the Connection table .
"def delete_connection ( self , session = None ) : self . log . info ( ""Deleting connection %s"" , self . db_conn_id ) connections = session . query ( Connection ) . filter ( Connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( ""Connection was already deleted!"" )",Delete the dynamically created connection from the Connection table .
"def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise AirflowException ( ""Proxy runner can only be retrieved in case of use_proxy = True"" ) return CloudSqlProxyRunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )",Retrieve Cloud SQL Proxy runner . It is used to manage the proxy lifecycle per task .
"def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = PostgresHook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = MySqlHook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook",Retrieve database hook . This is the actual Postgres or MySQL database hook that uses proxy or connects directly to the Google Cloud SQL database .
"def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )",Clean up database hook after it was used .
"def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]",Reserve free TCP port to be used by Cloud SQL Proxy
"def _normalize_mlengine_job_id ( job_id ) : match = re . search ( r'\d|\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\{{2}.+?\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) cleansed_job_id += re . sub ( r'[^0-9a-zA-Z]+' , '_' , job [ tracker : ] ) return cleansed_job_id",Replaces invalid MLEngine job_id characters with _ .
"def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except ValueError : return e",Extract error code from ftp exception
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import sensors_modules for sensors_module in sensors_modules : sys . modules [ sensors_module . __name__ ] = sensors_module globals ( ) [ sensors_module . _name ] = sensors_module,Integrate plugins to the context
"def clear_dag_runs ( ) : session = settings . Session ( ) drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( DAG_IDS ) , ) . all ( ) for dr in drs : logging . info ( 'Deleting DagRun :: {}' . format ( dr ) ) session . delete ( dr )",Remove any existing DAG runs for the perf test DAGs .
def clear_dag_task_instances ( ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) for ti in tis : logging . info ( 'Deleting TaskInstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( ),Remove any existing task instances for the perf test DAGs .
"def set_dags_paused_state ( is_paused ) : session = settings . Session ( ) dms = session . query ( DagModel ) . filter ( DagModel . dag_id . in_ ( DAG_IDS ) ) for dm in dms : logging . info ( 'Setting DAG :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )",Toggle the pause state of the DAGs in the test .
"def print_stats ( self ) : session = settings . Session ( ) TI = TaskInstance tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == State . SUCCESS ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . DataFrame ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'Performance Results' ) print ( '###################' ) for dag_id in DAG_IDS : print ( 'DAG {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( ""WARNING!! The following task instances haven't completed"" ) print ( pd . DataFrame ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != State . SUCCESS , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )",Print operational metrics for the scheduler test .
"def heartbeat ( self ) : super ( SchedulerMetricsJob , self ) . heartbeat ( ) session = settings . Session ( ) TI = TaskInstance successful_tis = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( DAG_IDS ) ) . filter ( TI . state . in_ ( [ State . SUCCESS ] ) ) . all ( ) ) session . commit ( ) dagbag = DagBag ( SUBDIR ) dags = [ dagbag . dags [ dag_id ] for dag_id in DAG_IDS ] num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > MAX_RUNTIME_SECS ) : if len ( successful_tis ) == num_task_instances : self . log . info ( ""All tasks processed! Printing stats."" ) else : self . log . info ( ""Test timeout reached. Printing available stats."" ) self . print_stats ( ) set_dags_paused_state ( True ) sys . exit ( )",Override the scheduler heartbeat to determine when the test is complete
"def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( FunctionName = self . function_name , InvocationType = self . invocation_type , LogType = self . log_type , Payload = payload , Qualifier = self . qualifier ) return response",Invoke Lambda Function
"def get_dag_run_state ( dag_id , execution_date ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = ""Dag id {} not found"" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'Dag Run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise DagRunNotFound ( error_message ) return { 'state' : dagrun . get_state ( ) }",Return the task object identified by the given dag_id and task_id .
"def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = None , project_id = None , region = None , dataflow_options = None , model_uri = None , model_name = None , version_name = None , dag = None ) : if not re . match ( r""^[a-zA-Z][-A-Za-z0-9]*$"" , task_prefix ) : raise AirflowException ( ""Malformed task_id for DataFlowPythonOperator (only alphanumeric "" ""and hyphens are allowed but got: "" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise AirflowException ( ""`metric_fn` param must be callable."" ) if not callable ( validate_fn ) : raise AirflowException ( ""`validate_fn` param must be callable."" ) if dag is not None and dag . default_args is not None : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = MLEngineBatchPredictionOperator ( task_id = ( task_prefix + ""-prediction"" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = True ) ) evaluate_summary = DataFlowPythonOperator ( task_id = ( task_prefix + ""-summary"" ) , py_options = [ ""-m"" ] , py_file = ""airflow.contrib.utils.mlengine_prediction_summary"" , dataflow_default_options = dataflow_options , options = { ""prediction_path"" : prediction_path , ""metric_fn_encoded"" : metric_fn_encoded , ""metric_keys"" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ ""templates_dict"" ] [ ""prediction_path"" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != ""gs"" or not bucket or not obj : raise ValueError ( ""Wrong format prediction_path: %s"" , prediction_path ) summary = os . path . join ( obj . strip ( ""/"" ) , ""prediction.summary.json"" ) gcs_hook = GoogleCloudStorageHook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = PythonOperator ( task_id = ( task_prefix + ""-validation"" ) , python_callable = apply_validate_fn , provide_context = True , templates_dict = { ""prediction_path"" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation",Creates Operators needed for model evaluation and returns .
"def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except OSError : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )",Creates the directory specified by path creating intermediate directories as necessary . If directory already exists this is a no - op .
"def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( ValueError , TypeError ) : ret = s return ret",A small helper function to convert a string to a numeric value if appropriate
def utcnow ( ) : d = dt . datetime . utcnow ( ) d = d . replace ( tzinfo = utc ) return d,Get the current date and time in UTC : return :
"def utc_epoch ( ) : d = dt . datetime ( 1970 , 1 , 1 ) d = d . replace ( tzinfo = utc ) return d",Gets the epoch in the users timezone : return :
"def convert_to_utc ( value ) : if not value : return value if not is_localized ( value ) : value = pendulum . instance ( value , TIMEZONE ) return value . astimezone ( utc )",Returns the datetime with the default timezone added if timezone information was not associated : param value : datetime : return : datetime with tzinfo
"def make_aware ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE if is_localized ( value ) : raise ValueError ( ""make_aware expects a naive datetime, got %s"" % value ) if hasattr ( value , 'fold' ) : value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : return timezone . convert ( value ) else : return value . replace ( tzinfo = timezone )",Make a naive datetime . datetime in a given time zone aware .
"def make_naive ( value , timezone = None ) : if timezone is None : timezone = TIMEZONE if is_naive ( value ) : raise ValueError ( ""make_naive() cannot be applied to a naive datetime"" ) o = value . astimezone ( timezone ) naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive",Make an aware datetime . datetime naive in a given time zone .
"def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = TIMEZONE return dt . datetime ( * args , * * kwargs )",Wrapper around datetime . datetime that adds settings . TIMEZONE if tzinfo not specified
"def _set_env_from_extras ( self , extras ) : key_path = self . _get_field ( extras , 'key_path' , False ) keyfile_json_str = self . _get_field ( extras , 'keyfile_dict' , False ) if not key_path and not keyfile_json_str : self . log . info ( 'Using gcloud with application default credentials.' ) elif key_path : os . environ [ G_APP_CRED ] = key_path else : service_key = tempfile . NamedTemporaryFile ( delete = False ) service_key . write ( keyfile_json_str ) os . environ [ G_APP_CRED ] = service_key . name return service_key",Sets the environment variable GOOGLE_APPLICATION_CREDENTIALS with either :
"def _get_field ( self , extras , field , default = None ) : long_f = 'extra__google_cloud_platform__{}' . format ( field ) if long_f in extras : return extras [ long_f ] else : self . log . info ( 'Field %s not found in extras.' , field ) return default",Fetches a field from extras and returns it . This is some Airflow magic . The google_cloud_platform hook type adds custom UI elements to the hook page which allow admins to specify service_account key_path etc . They get formatted as shown below .
"def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to druid broker on %s' , conn . host ) return druid_broker_conn",Establish a connection to druid broker .
"def get_conn ( self , headers = None ) : session = requests . Session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if ""://"" in conn . host : self . base_url = conn . host else : schema = conn . schema if conn . schema else ""http"" self . base_url = schema + ""://"" + conn . host if conn . port : self . base_url = self . base_url + "":"" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except TypeError : self . log . warn ( 'Connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session",Returns http session for use with requests
"def run ( self , endpoint , data = None , headers = None , extra_options = None ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = None if self . method == 'GET' : req = requests . Request ( self . method , url , params = data , headers = headers ) elif self . method == 'HEAD' : req = requests . Request ( self . method , url , headers = headers ) else : req = requests . Request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( ""Sending '%s' to url: %s"" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )",Performs the request
"def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . HTTPError : self . log . error ( ""HTTP error: %s"" , response . reason ) if self . method not in [ 'GET' , 'HEAD' ] : self . log . error ( response . text ) raise AirflowException ( str ( response . status_code ) + "":"" + response . reason )",Checks the status code and raise an AirflowException exception on non 2XX or 3XX status codes
"def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( ""stream"" , False ) , verify = extra_options . get ( ""verify"" , True ) , proxies = extra_options . get ( ""proxies"" , { } ) , cert = extra_options . get ( ""cert"" ) , timeout = extra_options . get ( ""timeout"" ) , allow_redirects = extra_options . get ( ""allow_redirects"" , True ) ) if extra_options . get ( 'check_response' , True ) : self . check_response ( response ) return response except requests . exceptions . ConnectionError as ex : self . log . warn ( str ( ex ) + ' Tenacity will retry to execute the operation' ) raise ex",Grabs extra options like timeout and actually runs the request checking for the result
"def run_with_advanced_retry ( self , _retry_args , * args , * * kwargs ) : self . _retry_obj = tenacity . Retrying ( * * _retry_args ) self . _retry_obj ( self . run , * args , * * kwargs )",Runs Hook . run () with a Tenacity decorator attached to it . This is useful for connectors which might be disturbed by intermittent issues and should not instantly fail .
def create_session ( ) : session = settings . Session ( ) try : yield session session . commit ( ) except Exception : session . rollback ( ) raise finally : session . close ( ),Contextmanager that will create and teardown a session .
"def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper",Function decorator that provides a session if it isn t provided . If you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you .
"def resetdb ( ) : from airflow import models from alembic . migration import MigrationContext log . info ( ""Dropping tables that exist"" ) models . base . Base . metadata . drop_all ( settings . engine ) mc = MigrationContext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import Base Base . metadata . drop_all ( settings . engine ) initdb ( )",Clear out the database
"def execute ( self , context ) : hook = WasbHook ( wasb_conn_id = self . wasb_conn_id ) self . log . info ( 'Uploading %s to wasb://%s ' 'as %s' . format ( self . file_path , self . container_name , self . blob_name ) ) hook . load_file ( self . file_path , self . container_name , self . blob_name , * * self . load_options )",Upload a file to Azure Blob Storage .
"def get_conn ( self ) : db = self . get_connection ( self . presto_conn_id ) reqkwargs = None if db . password is not None : reqkwargs = { 'auth' : HTTPBasicAuth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra_dejson . get ( 'source' , 'airflow' ) , protocol = db . extra_dejson . get ( 'protocol' , 'http' ) , catalog = db . extra_dejson . get ( 'catalog' , 'hive' ) , requests_kwargs = reqkwargs , schema = db . schema )",Returns a connection object
"def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorName' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorName' ] , message = e . message [ 'message' ] ) ) else : return str ( e )",Parses some DatabaseError to provide a better error message
"def get_records ( self , hql , parameters = None ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) )",Get a set of records from Presto
"def get_pandas_df ( self , hql , parameters = None ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except DatabaseError as e : raise PrestoException ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . DataFrame ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . DataFrame ( ) return df",Get a pandas dataframe from a sql query .
"def run ( self , hql , parameters = None ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )",Execute the statement against Presto . Can be used to create views .
"def insert_rows ( self , table , rows , target_fields = None ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )",A generic way to insert a set of tuples into a table .
"def get_conn ( self ) : if self . cosmos_client is not None : return self . cosmos_client self . cosmos_client = cosmos_client . CosmosClient ( self . endpoint_uri , { 'masterKey' : self . master_key } ) return self . cosmos_client",Return a cosmos db client .
"def does_collection_exist ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ""Collection name cannot be None."" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""query"" : ""SELECT * FROM r WHERE r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return False return True",Checks if a collection exists in CosmosDB .
"def create_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ""Collection name cannot be None."" ) existing_container = list ( self . get_conn ( ) . QueryContainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""query"" : ""SELECT * FROM r WHERE r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : collection_name } ] } ) ) if len ( existing_container ) == 0 : self . get_conn ( ) . CreateContainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""id"" : collection_name } )",Creates a new collection in the CosmosDB database .
"def does_database_exist ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( ""Database name cannot be None."" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { ""query"" : ""SELECT * FROM r WHERE r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : database_name } ] } ) ) if len ( existing_database ) == 0 : return False return True",Checks if a database exists in CosmosDB .
"def create_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( ""Database name cannot be None."" ) existing_database = list ( self . get_conn ( ) . QueryDatabases ( { ""query"" : ""SELECT * FROM r WHERE r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : database_name } ] } ) ) if len ( existing_database ) == 0 : self . get_conn ( ) . CreateDatabase ( { ""id"" : database_name } )",Creates a new database in CosmosDB .
"def delete_database ( self , database_name ) : if database_name is None : raise AirflowBadRequest ( ""Database name cannot be None."" ) self . get_conn ( ) . DeleteDatabase ( get_database_link ( database_name ) )",Deletes an existing database in CosmosDB .
"def delete_collection ( self , collection_name , database_name = None ) : if collection_name is None : raise AirflowBadRequest ( ""Collection name cannot be None."" ) self . get_conn ( ) . DeleteContainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )",Deletes an existing collection in the CosmosDB database .
"def upsert_document ( self , document , database_name = None , collection_name = None , document_id = None ) : if document_id is None : document_id = str ( uuid . uuid4 ( ) ) if document is None : raise AirflowBadRequest ( ""You cannot insert a None document"" ) if 'id' in document : if document [ 'id' ] is None : document [ 'id' ] = document_id else : document [ 'id' ] = document_id created_document = self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , document ) return created_document",Inserts a new document ( or updates an existing one ) into an existing collection in the CosmosDB database .
"def insert_documents ( self , documents , database_name = None , collection_name = None ) : if documents is None : raise AirflowBadRequest ( ""You cannot insert empty documents"" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . CreateItem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents",Insert a list of new documents into an existing collection in the CosmosDB database .
"def delete_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( ""Cannot delete a document without an id"" ) self . get_conn ( ) . DeleteItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )",Delete an existing document out of a collection in the CosmosDB database .
"def get_document ( self , document_id , database_name = None , collection_name = None ) : if document_id is None : raise AirflowBadRequest ( ""Cannot get a document without an id"" ) try : return self . get_conn ( ) . ReadItem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except HTTPFailure : return None",Get a document from an existing collection in the CosmosDB database .
"def get_documents ( self , sql_string , database_name = None , collection_name = None , partition_key = None ) : if sql_string is None : raise AirflowBadRequest ( ""SQL query string cannot be None"" ) query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . QueryItems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except HTTPFailure : return None",Get a list of documents from an existing collection in the CosmosDB database via SQL query .
"def get_code ( dag_id ) : session = settings . Session ( ) DM = models . DagModel dag = session . query ( DM ) . filter ( DM . dag_id == dag_id ) . first ( ) session . close ( ) if dag is None : error_message = ""Dag id {} not found"" . format ( dag_id ) raise DagNotFound ( error_message ) try : with wwwutils . open_maybe_zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except IOError as e : error_message = ""Error {} while reading Dag id {} Code"" . format ( str ( e ) , dag_id ) raise AirflowException ( error_message )",Return python code of a given dag_id .
"def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )",Returns the Cloud Function with the given name .
"def create_new_function ( self , location , body , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",Creates a new function in Cloud Function in the location specified in the body .
"def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updateMask = "","" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",Updates Cloud Functions according to the specified update mask .
"def upload_function_zip ( self , location , zip_path , project_id = None ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateUploadUrl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadUrl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , headers = { 'Content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url",Uploads zip file with sources .
"def delete_function ( self , name ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )",Deletes the specified Cloud Function .
"def _wait_for_operation_to_complete ( self , operation_name ) : service = self . get_conn ( ) while True : operation_response = service . operations ( ) . get ( name = operation_name , ) . execute ( num_retries = self . num_retries ) if operation_response . get ( ""done"" ) : response = operation_response . get ( ""response"" ) error = operation_response . get ( ""error"" ) if error : raise AirflowException ( str ( error ) ) return response time . sleep ( TIME_TO_SLEEP_IN_SECONDS )",Waits for the named operation to complete - checks status of the asynchronous call .
"def publish ( self , project , topic , messages ) : body = { 'messages' : messages } full_topic = _format_topic ( project , topic ) request = self . get_conn ( ) . projects ( ) . topics ( ) . publish ( topic = full_topic , body = body ) try : request . execute ( num_retries = self . num_retries ) except HttpError as e : raise PubSubException ( 'Error publishing to topic {}' . format ( full_topic ) , e )",Publishes messages to a Pub / Sub topic .
"def create_topic ( self , project , topic , fail_if_exists = False ) : service = self . get_conn ( ) full_topic = _format_topic ( project , topic ) try : service . projects ( ) . topics ( ) . create ( name = full_topic , body = { } ) . execute ( num_retries = self . num_retries ) except HttpError as e : if str ( e . resp [ 'status' ] ) == '409' : message = 'Topic already exists: {}' . format ( full_topic ) self . log . warning ( message ) if fail_if_exists : raise PubSubException ( message ) else : raise PubSubException ( 'Error creating topic {}' . format ( full_topic ) , e )",Creates a Pub / Sub topic if it does not already exist .
"def delete_topic ( self , project , topic , fail_if_not_exists = False ) : service = self . get_conn ( ) full_topic = _format_topic ( project , topic ) try : service . projects ( ) . topics ( ) . delete ( topic = full_topic ) . execute ( num_retries = self . num_retries ) except HttpError as e : if str ( e . resp [ 'status' ] ) == '404' : message = 'Topic does not exist: {}' . format ( full_topic ) self . log . warning ( message ) if fail_if_not_exists : raise PubSubException ( message ) else : raise PubSubException ( 'Error deleting topic {}' . format ( full_topic ) , e )",Deletes a Pub / Sub topic if it exists .
"def create_subscription ( self , topic_project , topic , subscription = None , subscription_project = None , ack_deadline_secs = 10 , fail_if_exists = False ) : service = self . get_conn ( ) full_topic = _format_topic ( topic_project , topic ) if not subscription : subscription = 'sub-{}' . format ( uuid4 ( ) ) if not subscription_project : subscription_project = topic_project full_subscription = _format_subscription ( subscription_project , subscription ) body = { 'topic' : full_topic , 'ackDeadlineSeconds' : ack_deadline_secs } try : service . projects ( ) . subscriptions ( ) . create ( name = full_subscription , body = body ) . execute ( num_retries = self . num_retries ) except HttpError as e : if str ( e . resp [ 'status' ] ) == '409' : message = 'Subscription already exists: {}' . format ( full_subscription ) self . log . warning ( message ) if fail_if_exists : raise PubSubException ( message ) else : raise PubSubException ( 'Error creating subscription {}' . format ( full_subscription ) , e ) return subscription",Creates a Pub / Sub subscription if it does not already exist .
"def delete_subscription ( self , project , subscription , fail_if_not_exists = False ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) try : service . projects ( ) . subscriptions ( ) . delete ( subscription = full_subscription ) . execute ( num_retries = self . num_retries ) except HttpError as e : if str ( e . resp [ 'status' ] ) == '404' : message = 'Subscription does not exist: {}' . format ( full_subscription ) self . log . warning ( message ) if fail_if_not_exists : raise PubSubException ( message ) else : raise PubSubException ( 'Error deleting subscription {}' . format ( full_subscription ) , e )",Deletes a Pub / Sub subscription if it exists .
"def pull ( self , project , subscription , max_messages , return_immediately = False ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) body = { 'maxMessages' : max_messages , 'returnImmediately' : return_immediately } try : response = service . projects ( ) . subscriptions ( ) . pull ( subscription = full_subscription , body = body ) . execute ( num_retries = self . num_retries ) return response . get ( 'receivedMessages' , [ ] ) except HttpError as e : raise PubSubException ( 'Error pulling messages from subscription {}' . format ( full_subscription ) , e )",Pulls up to max_messages messages from Pub / Sub subscription .
"def acknowledge ( self , project , subscription , ack_ids ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) try : service . projects ( ) . subscriptions ( ) . acknowledge ( subscription = full_subscription , body = { 'ackIds' : ack_ids } ) . execute ( num_retries = self . num_retries ) except HttpError as e : raise PubSubException ( 'Error acknowledging {} messages pulled from subscription {}' . format ( len ( ack_ids ) , full_subscription ) , e )",Pulls up to max_messages messages from Pub / Sub subscription .
"def get_dep_statuses ( self , ti , session , dep_context = None ) : from airflow . ti_deps . dep_context import DepContext if dep_context is None : dep_context = DepContext ( ) if self . IGNOREABLE and dep_context . ignore_all_deps : yield self . _passing_status ( reason = ""Context specified all dependencies should be ignored."" ) return if self . IS_TASK_DEP and dep_context . ignore_task_deps : yield self . _passing_status ( reason = ""Context specified all task dependencies should be ignored."" ) return for dep_status in self . _get_dep_statuses ( ti , session , dep_context ) : yield dep_status",Wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies .
"def is_met ( self , ti , session , dep_context = None ) : return all ( status . passed for status in self . get_dep_statuses ( ti , session , dep_context ) )",Returns whether or not this dependency is met for a given task instance . A dependency is considered met if all of the dependency statuses it reports are passing .
"def get_failure_reasons ( self , ti , session , dep_context = None ) : for dep_status in self . get_dep_statuses ( ti , session , dep_context ) : if not dep_status . passed : yield dep_status . reason",Returns an iterable of strings that explain why this dependency wasn t met .
"def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = None ) : config = configparser . ConfigParser ( ) if config . read ( config_file_name ) : sections = config . sections ( ) else : raise AirflowException ( ""Couldn't read {0}"" . format ( config_file_name ) ) if config_format is None : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : if profile is not None and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'Credentials' elif conf_format == 'aws' and profile is not None : cred_section = profile else : cred_section = 'default' if conf_format in ( 'boto' , 'aws' ) : key_id_option = 'aws_access_key_id' secret_key_option = 'aws_secret_access_key' else : key_id_option = 'access_key' secret_key_option = 'secret_key' if cred_section not in sections : raise AirflowException ( ""This config file format is not recognized"" ) else : try : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except Exception : logging . warning ( ""Option Error in parsing s3 config file"" ) raise return access_key , secret_key",Parses a config file for s3 credentials . Can currently parse boto s3cmd . conf and AWS SDK config formats
"def get_credentials ( self , region_name = None ) : session , _ = self . _get_credentials ( region_name ) return session . get_credentials ( ) . get_frozen_credentials ( )",Get the underlying botocore . Credentials object .
"def expand_role ( self , role ) : if '/' in role : return role else : return self . get_client_type ( 'iam' ) . get_role ( RoleName = role ) [ 'Role' ] [ 'Arn' ]",If the IAM role is a role name get the Amazon Resource Name ( ARN ) for the role . If IAM role is already an IAM role ARN no change is made .
"def get_conn ( self ) : conn = self . get_connection ( self . vertica_conn_id ) conn_config = { ""user"" : conn . login , ""password"" : conn . password or '' , ""database"" : conn . schema , ""host"" : conn . host or 'localhost' } if not conn . port : conn_config [ ""port"" ] = 5433 else : conn_config [ ""port"" ] = int ( conn . port ) conn = connect ( * * conn_config ) return conn",Returns verticaql connection object
"def set_context ( logger , value ) : _logger = logger while _logger : for handler in _logger . handlers : try : handler . set_context ( value ) except AttributeError : pass if _logger . propagate is True : _logger = _logger . parent else : _logger = None",Walks the tree of loggers and tries to set the context for each handler : param logger : logger : param value : value to set
"def write ( self , message ) : if not message . endswith ( ""\n"" ) : self . _buffer += message else : self . _buffer += message self . logger . log ( self . level , self . _buffer . rstrip ( ) ) self . _buffer = str ( )",Do whatever it takes to actually log the specified logging record : param message : message to log
"def flush ( self ) : if len ( self . _buffer ) > 0 : self . logger . log ( self . level , self . _buffer ) self . _buffer = str ( )",Ensure all logging output has been flushed
"def correct_maybe_zipped ( fileloc ) : _ , archive , filename = re . search ( r'((.*\.zip){})?(.*)' . format ( re . escape ( os . sep ) ) , fileloc ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return archive else : return fileloc",If the path contains a folder with a . zip suffix then the folder is treated as a zip archive and path to zip is returned .
"def list_py_file_paths ( directory , safe_mode = True , include_examples = None ) : if include_examples is None : include_examples = conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) file_paths = [ ] if directory is None : return [ ] elif os . path . isfile ( directory ) : return [ directory ] elif os . path . isdir ( directory ) : patterns_by_dir = { } for root , dirs , files in os . walk ( directory , followlinks = True ) : patterns = patterns_by_dir . get ( root , [ ] ) ignore_file = os . path . join ( root , '.airflowignore' ) if os . path . isfile ( ignore_file ) : with open ( ignore_file , 'r' ) as f : patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\n' ) if p ] dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] for d in dirs : patterns_by_dir [ os . path . join ( root , d ) ] = patterns for f in files : try : file_path = os . path . join ( root , f ) if not os . path . isfile ( file_path ) : continue mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : continue if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : continue might_contain_dag = True if safe_mode and not zipfile . is_zipfile ( file_path ) : with open ( file_path , 'rb' ) as fp : content = fp . read ( ) might_contain_dag = all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) if not might_contain_dag : continue file_paths . append ( file_path ) except Exception : log = LoggingMixin ( ) . log log . exception ( ""Error while examining %s"" , f ) if include_examples : import airflow . example_dags example_dag_folder = airflow . example_dags . __path__ [ 0 ] file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , False ) ) return file_paths",Traverse a directory and look for Python files .
"def construct_task_instance ( self , session = None , lock_for_update = False ) : TI = airflow . models . TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . _dag_id , TI . task_id == self . _task_id , TI . execution_date == self . _execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) return ti",Construct a TaskInstance from the database based on the primary key
"def get_dag ( self , dag_id ) : if dag_id not in self . dag_id_to_simple_dag : raise AirflowException ( ""Unknown DAG ID {}"" . format ( dag_id ) ) return self . dag_id_to_simple_dag [ dag_id ]",: param dag_id : DAG ID : type dag_id : unicode : return : if the given DAG ID exists in the bag return the BaseDag corresponding to that ID . Otherwise throw an Exception : rtype : airflow . utils . dag_processing . SimpleDag
"def start ( self ) : self . _process = self . _launch_process ( self . _dag_directory , self . _file_paths , self . _max_runs , self . _processor_factory , self . _child_signal_conn , self . _stat_queue , self . _result_queue , self . _async_mode ) self . log . info ( ""Launched DagFileProcessorManager with pid: %s"" , self . _process . pid )",Launch DagFileProcessorManager processor and start DAG parsing loop in manager .
"def harvest_simple_dags ( self ) : self . _sync_metadata ( ) self . _heartbeat_manager ( ) simple_dags = [ ] if sys . platform == ""darwin"" : qsize = self . _result_count else : qsize = self . _result_queue . qsize ( ) for _ in range ( qsize ) : simple_dags . append ( self . _result_queue . get ( ) ) self . _result_count = 0 return simple_dags",Harvest DAG parsing results from result queue and sync metadata from stat queue . : return : List of parsing result in SimpleDag format .
def _heartbeat_manager ( self ) : if self . _process and not self . _process . is_alive ( ) and not self . done : self . start ( ),Heartbeat DAG file processor and start it if it is not alive . : return :
def _sync_metadata ( self ) : while not self . _stat_queue . empty ( ) : stat = self . _stat_queue . get ( ) self . _file_paths = stat . file_paths self . _all_pids = stat . all_pids self . _done = stat . done self . _all_files_processed = stat . all_files_processed self . _result_count += stat . result_count,Sync metadata from stat queue and only keep the latest stat . : return :
"def terminate ( self ) : self . log . info ( ""Sending termination message to manager."" ) self . _child_signal_conn . send ( DagParsingSignal . TERMINATE_MANAGER )",Send termination signal to DAG parsing processor manager and expect it to terminate all DAG file processors .
"def end ( self ) : if not self . _process : self . log . warn ( 'Ending without manager process.' ) return this_process = psutil . Process ( os . getpid ( ) ) try : manager_process = psutil . Process ( self . _process . pid ) except psutil . NoSuchProcess : self . log . info ( ""Manager process not running."" ) return if manager_process . is_running ( ) and manager_process . pid in [ x . pid for x in this_process . children ( ) ] : self . log . info ( ""Terminating manager process: %s"" , manager_process . pid ) manager_process . terminate ( ) timeout = 5 self . log . info ( ""Waiting up to %ss for manager process to exit..."" , timeout ) try : psutil . wait_procs ( { manager_process } , timeout ) except psutil . TimeoutExpired : self . log . debug ( ""Ran out of time while waiting for "" ""processes to exit"" ) if manager_process . is_running ( ) and manager_process . pid in [ x . pid for x in this_process . children ( ) ] : self . log . info ( ""Killing manager process: %s"" , manager_process . pid ) manager_process . kill ( ) manager_process . wait ( )",Terminate ( and then kill ) the manager process launched . : return :
"def _exit_gracefully ( self , signum , frame ) : self . log . info ( ""Exiting gracefully upon receiving signal %s"" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( ""Finished terminating DAG processors."" ) sys . exit ( os . EX_OK )",Helper method to clean up DAG file processors to avoid leaving orphan processes .
"def start ( self ) : self . log . info ( ""Processing files using up to %s processes at a time "" , self . _parallelism ) self . log . info ( ""Process each file at most once every %s seconds"" , self . _file_process_interval ) self . log . info ( ""Checking for new files in %s every %s seconds"" , self . _dag_directory , self . dag_dir_list_interval ) if self . _async_mode : self . log . debug ( ""Starting DagFileProcessorManager in async mode"" ) self . start_in_async ( ) else : self . log . debug ( ""Starting DagFileProcessorManager in sync mode"" ) self . start_in_sync ( )",Use multiple processes to parse and generate tasks for the DAGs in parallel . By processing them in separate processes we can get parallelism and isolation from potentially harmful user code .
"def start_in_async ( self ) : while True : loop_start_time = time . time ( ) if self . _signal_conn . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( ""Exiting dag parsing loop as all files "" ""have been processed %s times"" , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( ""Sleeping for %.2f seconds to prevent excessive logging"" , sleep_length ) time . sleep ( sleep_length )",Parse DAG files repeatedly in a standalone loop .
"def start_in_sync ( self ) : while True : agent_signal = self . _signal_conn . recv ( ) if agent_signal == DagParsingSignal . TERMINATE_MANAGER : self . terminate ( ) break elif agent_signal == DagParsingSignal . END_MANAGER : self . end ( ) sys . exit ( os . EX_OK ) elif agent_signal == DagParsingSignal . AGENT_HEARTBEAT : self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not None for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = DagParsingStat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) self . wait_until_finished ( ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) if max_runs_reached : self . log . info ( ""Exiting dag parsing loop as all files "" ""have been processed %s times"" , self . _max_runs ) self . _signal_conn . send ( DagParsingSignal . MANAGER_DONE ) break",Parse DAG files in a loop controlled by DagParsingSignal . Actual DAG parsing loop will run once upon receiving one agent heartbeat message and will report done when finished the loop .
"def _refresh_dag_dir ( self ) : elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) if elapsed_time_since_refresh > self . dag_dir_list_interval : self . log . info ( ""Searching for files in %s"" , self . _dag_directory ) self . _file_paths = list_py_file_paths ( self . _dag_directory ) self . last_dag_dir_refresh_time = timezone . utcnow ( ) self . log . info ( ""There are %s files in %s"" , len ( self . _file_paths ) , self . _dag_directory ) self . set_file_paths ( self . _file_paths ) try : self . log . debug ( ""Removing old import errors"" ) self . clear_nonexistent_import_errors ( ) except Exception : self . log . exception ( ""Error removing old import errors"" )",Refresh file paths from dag dir if we haven t done it for too long .
def _print_stat ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( ),Occasionally print out stats about how fast the files are getting processed
"def clear_nonexistent_import_errors ( self , session ) : query = session . query ( errors . ImportError ) if self . _file_paths : query = query . filter ( ~ errors . ImportError . filename . in_ ( self . _file_paths ) ) query . delete ( synchronize_session = 'fetch' ) session . commit ( )",Clears import errors for files that no longer exist .
"def _log_file_processing_stats ( self , known_file_paths ) : headers = [ ""File Path"" , ""PID"" , ""Runtime"" , ""Last Runtime"" , ""Last Run"" ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : Stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else None ) last_run = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) Stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , ""{:.2f}s"" . format ( runtime ) if runtime else None , ""{:.2f}s"" . format ( last_runtime ) if last_runtime else None , last_run . strftime ( ""%Y-%m-%dT%H:%M:%S"" ) if last_run else None ) ) log_str = ( ""\n"" + ""="" * 80 + ""\n"" + ""DAG File Processing Stats\n\n"" + tabulate ( formatted_rows , headers = headers ) + ""\n"" + ""="" * 80 ) self . log . info ( log_str )",Print out stats about how files are getting processed .
"def get_pid ( self , file_path ) : if file_path in self . _processors : return self . _processors [ file_path ] . pid return None",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the PID of the process processing the given file or None if the specified file is not being processed : rtype : int
"def get_runtime ( self , file_path ) : if file_path in self . _processors : return ( timezone . utcnow ( ) - self . _processors [ file_path ] . start_time ) . total_seconds ( ) return None",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the current runtime ( in seconds ) of the process that s processing the specified file or None if the file is not currently being processed
"def get_start_time ( self , file_path ) : if file_path in self . _processors : return self . _processors [ file_path ] . start_time return None",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the start time of the process that s processing the specified file or None if the file is not currently being processed : rtype : datetime
"def set_file_paths ( self , new_file_paths ) : self . _file_paths = new_file_paths self . _file_path_queue = [ x for x in self . _file_path_queue if x in new_file_paths ] filtered_processors = { } for file_path , processor in self . _processors . items ( ) : if file_path in new_file_paths : filtered_processors [ file_path ] = processor else : self . log . warning ( ""Stopping processor for %s"" , file_path ) processor . terminate ( ) self . _processors = filtered_processors",Update this with a new set of paths to DAG definition files .
"def wait_until_finished ( self ) : for file_path , processor in self . _processors . items ( ) : while not processor . done : time . sleep ( 0.1 )",Sleeps until all the processors are done .
"def heartbeat ( self ) : finished_processors = { } """""":type : dict[unicode, AbstractDagFileProcessor]"""""" running_processors = { } """""":type : dict[unicode, AbstractDagFileProcessor]"""""" for file_path , processor in self . _processors . items ( ) : if processor . done : self . log . debug ( ""Processor for %s finished"" , file_path ) now = timezone . utcnow ( ) finished_processors [ file_path ] = processor self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) self . _last_finish_time [ file_path ] = now self . _run_count [ file_path ] += 1 else : running_processors [ file_path ] = processor self . _processors = running_processors self . log . debug ( ""%s/%s DAG parsing processes running"" , len ( self . _processors ) , self . _parallelism ) self . log . debug ( ""%s file paths queued for processing"" , len ( self . _file_path_queue ) ) simple_dags = [ ] for file_path , processor in finished_processors . items ( ) : if processor . result is None : self . log . warning ( ""Processor for %s exited with return code %s."" , processor . file_path , processor . exit_code ) else : for simple_dag in processor . result : simple_dags . append ( simple_dag ) if len ( self . _file_path_queue ) == 0 : file_paths_in_progress = self . _processors . keys ( ) now = timezone . utcnow ( ) file_paths_recently_processed = [ ] for file_path in self . _file_paths : last_finish_time = self . get_last_finish_time ( file_path ) if ( last_finish_time is not None and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : file_paths_recently_processed . append ( file_path ) files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) for file_path , processor in self . _processors . items ( ) : self . log . debug ( ""File path %s is still being processed (started: %s)"" , processor . file_path , processor . start_time . isoformat ( ) ) self . log . debug ( ""Queuing the following files for processing:\n\t%s"" , ""\n\t"" . join ( files_paths_to_queue ) ) self . _file_path_queue . extend ( files_paths_to_queue ) zombies = self . _find_zombies ( ) while ( self . _parallelism - len ( self . _processors ) > 0 and len ( self . _file_path_queue ) > 0 ) : file_path = self . _file_path_queue . pop ( 0 ) processor = self . _processor_factory ( file_path , zombies ) processor . start ( ) self . log . debug ( ""Started a process (PID: %s) to generate tasks for %s"" , processor . pid , file_path ) self . _processors [ file_path ] = processor self . _run_count [ self . _heart_beat_key ] += 1 return simple_dags",This should be periodically called by the manager loop . This method will kick off new processes to process DAG definition files and read the results from the finished processors .
"def _find_zombies ( self , session ) : now = timezone . utcnow ( ) zombies = [ ] if ( now - self . _last_zombie_query_time ) . total_seconds ( ) > self . _zombie_query_interval : from airflow . jobs import LocalTaskJob as LJ self . log . info ( ""Finding 'running' jobs without a recent heartbeat"" ) TI = airflow . models . TaskInstance limit_dttm = timezone . utcnow ( ) - timedelta ( seconds = self . _zombie_threshold_secs ) self . log . info ( ""Failing jobs without heartbeat after %s"" , limit_dttm ) tis = ( session . query ( TI ) . join ( LJ , TI . job_id == LJ . id ) . filter ( TI . state == State . RUNNING ) . filter ( or_ ( LJ . state != State . RUNNING , LJ . latest_heartbeat < limit_dttm , ) ) . all ( ) ) self . _last_zombie_query_time = timezone . utcnow ( ) for ti in tis : zombies . append ( SimpleTaskInstance ( ti ) ) return zombies",Find zombie task instances which are tasks haven t heartbeated for too long . : return : Zombie task instances in SimpleTaskInstance format .
def max_runs_reached ( self ) : if self . _max_runs == - 1 : return False for file_path in self . _file_paths : if self . _run_count [ file_path ] < self . _max_runs : return False if self . _run_count [ self . _heart_beat_key ] < self . _max_runs : return False return True,: return : whether all file paths have been processed max_runs times
"def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : this_process = psutil . Process ( os . getpid ( ) ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( ""Terminating child PID: %s"" , child . pid ) child . terminate ( ) timeout = 5 self . log . info ( ""Waiting up to %s seconds for processes to exit..."" , timeout ) try : psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'Terminated PID %s' , x . pid ) ) except psutil . TimeoutExpired : self . log . debug ( ""Ran out of time while waiting for processes to exit"" ) child_processes = [ x for x in this_process . children ( recursive = True ) if x . is_running ( ) and x . pid in pids_to_kill ] if len ( child_processes ) > 0 : self . log . info ( ""SIGKILL processes that did not terminate gracefully"" ) for child in child_processes : self . log . info ( ""Killing child PID: %s"" , child . pid ) child . kill ( ) child . wait ( )",Kill all child processes on exit since we don t want to leave them as orphaned .
"def get_conn ( self ) : self . log . debug ( 'Creating SSH client for conn_id: %s' , self . ssh_conn_id ) client = paramiko . SSHClient ( ) if not self . allow_host_key_change : self . log . warning ( 'Remote Identification Change is not verified. ' 'This wont protect against Man-In-The-Middle attacks' ) client . load_system_host_keys ( ) if self . no_host_key_check : self . log . warning ( 'No Host Key Verification. This wont protect ' 'against Man-In-The-Middle attacks' ) client . set_missing_host_key_policy ( paramiko . AutoAddPolicy ( ) ) if self . password and self . password . strip ( ) : client . connect ( hostname = self . remote_host , username = self . username , password = self . password , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) else : client . connect ( hostname = self . remote_host , username = self . username , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) if self . keepalive_interval : client . get_transport ( ) . set_keepalive ( self . keepalive_interval ) self . client = client return client",Opens a ssh connection to the remote host .
"def get_tunnel ( self , remote_port , remote_host = ""localhost"" , local_port = None ) : if local_port : local_bind_address = ( 'localhost' , local_port ) else : local_bind_address = ( 'localhost' , ) if self . password and self . password . strip ( ) : client = SSHTunnelForwarder ( self . remote_host , ssh_port = self . port , ssh_username = self . username , ssh_password = self . password , ssh_pkey = self . key_file , ssh_proxy = self . host_proxy , local_bind_address = local_bind_address , remote_bind_address = ( remote_host , remote_port ) , logger = self . log ) else : client = SSHTunnelForwarder ( self . remote_host , ssh_port = self . port , ssh_username = self . username , ssh_pkey = self . key_file , ssh_proxy = self . host_proxy , local_bind_address = local_bind_address , remote_bind_address = ( remote_host , remote_port ) , host_pkey_directories = [ ] , logger = self . log ) return client",Creates a tunnel between two hosts . Like ssh - L <LOCAL_PORT > : host : <REMOTE_PORT > .
"def create_transfer_job ( self , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return self . get_conn ( ) . transferJobs ( ) . create ( body = body ) . execute ( num_retries = self . num_retries )",Creates a transfer job that runs periodically .
"def get_transfer_job ( self , job_name , project_id = None ) : return ( self . get_conn ( ) . transferJobs ( ) . get ( jobName = job_name , projectId = project_id ) . execute ( num_retries = self . num_retries ) )",Gets the latest state of a long - running operation in Google Storage Transfer Service .
"def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) request = conn . transferJobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not None : response = request . execute ( num_retries = self . num_retries ) jobs . extend ( response [ TRANSFER_JOBS ] ) request = conn . transferJobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs",Lists long - running operations in Google Storage Transfer Service that match the specified filter .
"def update_transfer_job ( self , job_name , body ) : body = self . _inject_project_id ( body , BODY , PROJECT_ID ) return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = body ) . execute ( num_retries = self . num_retries ) )",Updates a transfer job that runs periodically .
"def delete_transfer_job ( self , job_name , project_id ) : return ( self . get_conn ( ) . transferJobs ( ) . patch ( jobName = job_name , body = { PROJECT_ID : project_id , TRANSFER_JOB : { STATUS1 : GcpTransferJobsStatus . DELETED } , TRANSFER_JOB_FIELD_MASK : STATUS1 , } , ) . execute ( num_retries = self . num_retries ) )",Deletes a transfer job . This is a soft delete . After a transfer job is deleted the job and all the transfer executions are subject to garbage collection . Transfer jobs become eligible for garbage collection 30 days after soft delete .
"def cancel_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . cancel ( name = operation_name ) . execute ( num_retries = self . num_retries )",Cancels an transfer operation in Google Storage Transfer Service .
"def get_transfer_operation ( self , operation_name ) : return ( self . get_conn ( ) . transferOperations ( ) . get ( name = operation_name ) . execute ( num_retries = self . num_retries ) )",Gets an transfer operation in Google Storage Transfer Service .
"def list_transfer_operations ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , FILTER , FILTER_PROJECT_ID ) operations = [ ] request = conn . transferOperations ( ) . list ( name = TRANSFER_OPERATIONS , filter = json . dumps ( filter ) ) while request is not None : response = request . execute ( num_retries = self . num_retries ) if OPERATIONS in response : operations . extend ( response [ OPERATIONS ] ) request = conn . transferOperations ( ) . list_next ( previous_request = request , previous_response = response ) return operations",Gets an transfer operation in Google Storage Transfer Service .
"def pause_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . pause ( name = operation_name ) . execute ( num_retries = self . num_retries )",Pauses an transfer operation in Google Storage Transfer Service .
"def resume_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferOperations ( ) . resume ( name = operation_name ) . execute ( num_retries = self . num_retries )",Resumes an transfer operation in Google Storage Transfer Service .
"def wait_for_transfer_job ( self , job , expected_statuses = ( GcpTransferOperationStatus . SUCCESS , ) , timeout = 60 ) : while timeout > 0 : operations = self . list_transfer_operations ( filter = { FILTER_PROJECT_ID : job [ PROJECT_ID ] , FILTER_JOB_NAMES : [ job [ NAME ] ] } ) if GCPTransferServiceHook . operations_contain_expected_statuses ( operations , expected_statuses ) : return time . sleep ( TIME_TO_SLEEP_IN_SECONDS ) timeout -= TIME_TO_SLEEP_IN_SECONDS raise AirflowException ( ""Timeout. The operation could not be completed within the allotted time."" )",Waits until the job reaches the expected state .
"def operations_contain_expected_statuses ( operations , expected_statuses ) : expected_statuses = ( { expected_statuses } if isinstance ( expected_statuses , six . string_types ) else set ( expected_statuses ) ) if len ( operations ) == 0 : return False current_statuses = { operation [ METADATA ] [ STATUS ] for operation in operations } if len ( current_statuses - set ( expected_statuses ) ) != len ( current_statuses ) : return True if len ( NEGATIVE_STATUSES - current_statuses ) != len ( NEGATIVE_STATUSES ) : raise AirflowException ( 'An unexpected operation status was encountered. Expected: {}' . format ( "", "" . join ( expected_statuses ) ) ) return False",Checks whether the operation list has an operation with the expected status then returns true If it encounters operations in FAILED or ABORTED state throw : class : airflow . exceptions . AirflowException .
"def find_for_task_instance ( task_instance , session ) : TR = TaskReschedule return ( session . query ( TR ) . filter ( TR . dag_id == task_instance . dag_id , TR . task_id == task_instance . task_id , TR . execution_date == task_instance . execution_date , TR . try_number == task_instance . try_number ) . order_by ( asc ( TR . id ) ) . all ( ) )",Returns all task reschedules for the task instance and try number in ascending order .
"def _strip_unsafe_kubernetes_special_chars ( string ) : return '' . join ( ch . lower ( ) for ind , ch in enumerate ( string ) if ch . isalnum ( ) )",Kubernetes only supports lowercase alphanumeric characters and - and . in the pod name However there are special rules about how - and . can be used so let s only keep alphanumeric chars see here for detail : https : // kubernetes . io / docs / concepts / overview / working - with - objects / names /
"def _make_safe_pod_id ( safe_dag_id , safe_task_id , safe_uuid ) : MAX_POD_ID_LEN = 253 safe_key = safe_dag_id + safe_task_id safe_pod_id = safe_key [ : MAX_POD_ID_LEN - len ( safe_uuid ) - 1 ] + ""-"" + safe_uuid return safe_pod_id",Kubernetes pod names must be < = 253 chars and must pass the following regex for validation ^ [ a - z0 - 9 ] ( [ - a - z0 - 9 ] * [ a - z0 - 9 ] ) ? ( \\ . [ a - z0 - 9 ] ( [ - a - z0 - 9 ] * [ a - z0 - 9 ] ) ? ) * $
"def _make_safe_label_value ( string ) : MAX_LABEL_LEN = 63 safe_label = re . sub ( r'^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\-\.]|[^a-z0-9A-Z]*$' , '' , string ) if len ( safe_label ) > MAX_LABEL_LEN or string != safe_label : safe_hash = hashlib . md5 ( string . encode ( ) ) . hexdigest ( ) [ : 9 ] safe_label = safe_label [ : MAX_LABEL_LEN - len ( safe_hash ) - 1 ] + ""-"" + safe_hash return safe_label",Valid label values must be 63 characters or less and must be empty or begin and end with an alphanumeric character ( [ a - z0 - 9A - Z ] ) with dashes ( - ) underscores ( _ ) dots ( . ) and alphanumerics between .
"def clear_not_launched_queued_tasks ( self , session = None ) : queued_tasks = session . query ( TaskInstance ) . filter ( TaskInstance . state == State . QUEUED ) . all ( ) self . log . info ( 'When executor started up, found %s queued task instances' , len ( queued_tasks ) ) for task in queued_tasks : dict_string = ( ""dag_id={},task_id={},execution_date={},airflow-worker={}"" . format ( AirflowKubernetesScheduler . _make_safe_label_value ( task . dag_id ) , AirflowKubernetesScheduler . _make_safe_label_value ( task . task_id ) , AirflowKubernetesScheduler . _datetime_to_label_safe_datestring ( task . execution_date ) , self . worker_uuid ) ) kwargs = dict ( label_selector = dict_string ) pod_list = self . kube_client . list_namespaced_pod ( self . kube_config . kube_namespace , * * kwargs ) if len ( pod_list . items ) == 0 : self . log . info ( 'TaskInstance: %s found in queued state but was not launched, ' 'rescheduling' , task ) session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == task . dag_id , TaskInstance . task_id == task . task_id , TaskInstance . execution_date == task . execution_date ) . update ( { TaskInstance . state : State . NONE } )",If the airflow scheduler restarts with pending Queued tasks the tasks may or may not have been launched Thus on starting up the scheduler let s check every Queued task to see if it has been launched ( ie : if there is a corresponding pod on kubernetes )
"def open_slots ( self , session ) : from airflow . models . taskinstance import TaskInstance as TI used_slots = session . query ( func . count ( ) ) . filter ( TI . pool == self . pool ) . filter ( TI . state . in_ ( [ State . RUNNING , State . QUEUED ] ) ) . scalar ( ) return self . slots - used_slots",Returns the number of slots open at the moment
def expand_env_var ( env_var ) : if not env_var : return env_var while True : interpolated = os . path . expanduser ( os . path . expandvars ( str ( env_var ) ) ) if interpolated == env_var : return interpolated else : env_var = interpolated,Expands ( potentially nested ) env vars by repeatedly applying expandvars and expanduser until interpolation stops having any effect .
"def run_command ( command ) : process = subprocess . Popen ( shlex . split ( command ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , close_fds = True ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise AirflowConfigException ( ""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}"" . format ( command , process . returncode , output , stderr ) ) return output",Runs command and returns stdout
"def parameterized_config ( template ) : all_vars = { k : v for d in [ globals ( ) , locals ( ) ] for k , v in d . items ( ) } return template . format ( * * all_vars )",Generates a configuration from the provided template + variables defined in current scope : param template : a config content templated with {{ variables }}
"def remove_option ( self , section , option , remove_default = True ) : if super ( ) . has_option ( section , option ) : super ( ) . remove_option ( section , option ) if self . airflow_defaults . has_option ( section , option ) and remove_default : self . airflow_defaults . remove_option ( section , option )",Remove an option if it exists in config from a file or default config . If both of config have the same option this removes the option in both configs unless remove_default = False .
"def getsection ( self , section ) : if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : return None _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'AIRFLOW__{S}__' . format ( S = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , '' ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except ValueError : try : val = float ( val ) except ValueError : if val . lower ( ) in ( 't' , 'true' ) : val = True elif val . lower ( ) in ( 'f' , 'false' ) : val = False _section [ key ] = val return _section",Returns the section as a dict . Values are converted to int float bool as required .
"def as_dict ( self , display_source = False , display_sensitive = False , raw = False ) : cfg = { } configs = [ ( 'default' , self . airflow_defaults ) , ( 'airflow.cfg' , self ) , ] for ( source_name , config ) in configs : for section in config . sections ( ) : sect = cfg . setdefault ( section , OrderedDict ( ) ) for ( k , val ) in config . items ( section = section , raw = raw ) : if display_source : val = ( val , source_name ) sect [ k ] = val for ev in [ ev for ev in os . environ if ev . startswith ( 'AIRFLOW__' ) ] : try : _ , section , key = ev . split ( '__' ) opt = self . _get_env_var_option ( section , key ) except ValueError : continue if not display_sensitive and ev != 'AIRFLOW__CORE__UNIT_TEST_MODE' : opt = '< hidden >' elif raw : opt = opt . replace ( '%' , '%%' ) if display_source : opt = ( opt , 'env var' ) cfg . setdefault ( section . lower ( ) , OrderedDict ( ) ) . update ( { key . lower ( ) : opt } ) for ( section , key ) in self . as_command_stdout : opt = self . _get_cmd_option ( section , key ) if opt : if not display_sensitive : opt = '< hidden >' if display_source : opt = ( opt , 'cmd' ) elif raw : opt = opt . replace ( '%' , '%%' ) cfg . setdefault ( section , OrderedDict ( ) ) . update ( { key : opt } ) del cfg [ section ] [ key + '_cmd' ] return cfg",Returns the current configuration as an OrderedDict of OrderedDicts . : param display_source : If False the option value is returned . If True a tuple of ( option_value source ) is returned . Source is either airflow . cfg default env var or cmd . : type display_source : bool : param display_sensitive : If True the values of options set by env vars and bash commands will be displayed . If False those options are shown as < hidden > : type display_sensitive : bool : param raw : Should the values be output as interpolated values or the raw form that can be fed back in to ConfigParser : type raw : bool
"def allocate_ids ( self , partial_keys ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . allocateIds ( projectId = self . project_id , body = { 'keys' : partial_keys } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'keys' ]",Allocate IDs for incomplete keys .
"def begin_transaction ( self ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . beginTransaction ( projectId = self . project_id , body = { } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'transaction' ]",Begins a new transaction .
"def commit ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . commit ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp",Commit a transaction optionally creating deleting or modifying some entities .
"def lookup ( self , keys , read_consistency = None , transaction = None ) : conn = self . get_conn ( ) body = { 'keys' : keys } if read_consistency : body [ 'readConsistency' ] = read_consistency if transaction : body [ 'transaction' ] = transaction resp = ( conn . projects ( ) . lookup ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp",Lookup some entities by key .
"def rollback ( self , transaction ) : conn = self . get_conn ( ) conn . projects ( ) . rollback ( projectId = self . project_id , body = { 'transaction' : transaction } ) . execute ( num_retries = self . num_retries )",Roll back a transaction .
"def run_query ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . runQuery ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp [ 'batch' ]",Run a query for entities .
"def get_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . get ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp",Gets the latest state of a long - running operation .
"def delete_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp",Deletes the long - running operation .
"def poll_operation_until_done ( self , name , polling_interval_in_seconds ) : while True : result = self . get_operation ( name ) state = result [ 'metadata' ] [ 'common' ] [ 'state' ] if state == 'PROCESSING' : self . log . info ( 'Operation is processing. Re-polling state in {} seconds' . format ( polling_interval_in_seconds ) ) time . sleep ( polling_interval_in_seconds ) else : return result",Poll backup operation state until it s completed .
"def export_to_storage_bucket ( self , bucket , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) output_uri_prefix = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'outputUrlPrefix' : output_uri_prefix , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . export ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp",Export entities from Cloud Datastore to Cloud Storage for backup .
"def import_from_storage_bucket ( self , bucket , file , namespace = None , entity_filter = None , labels = None ) : admin_conn = self . get_conn ( ) input_url = 'gs://' + '/' . join ( filter ( None , [ bucket , namespace , file ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'inputUrl' : input_url , 'entityFilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . import_ ( projectId = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp",Import a backup from Cloud Storage to Cloud Datastore .
"def publish_to_target ( self , target_arn , message ) : conn = self . get_conn ( ) messages = { 'default' : message } return conn . publish ( TargetArn = target_arn , Message = json . dumps ( messages ) , MessageStructure = 'json' )",Publish a message to a topic or an endpoint .
"def get_hostname ( ) : try : callable_path = conf . get ( 'core' , 'hostname_callable' ) except AirflowConfigException : callable_path = None if not callable_path : return socket . getfqdn ( ) module_path , attr_name = callable_path . split ( ':' ) module = importlib . import_module ( module_path ) callable = getattr ( module , attr_name ) return callable ( )",Fetch the hostname using the callable from the config or using socket . getfqdn as a fallback .
def get_conn ( self ) : if not self . _conn : self . _conn = LanguageServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn,Retrieves connection to Cloud Natural Language service .
"def analyze_entities ( self , document , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . analyze_entities ( document = document , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata )",Finds named entities in the text along with entity types salience mentions for each entity and other properties .
"def annotate_text ( self , document , features , encoding_type = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . annotate_text ( document = document , features = features , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata , )",A convenience method that provides all the features that analyzeSentiment analyzeEntities and analyzeSyntax provide in one call .
"def classify_text ( self , document , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) return client . classify_text ( document = document , retry = retry , timeout = timeout , metadata = metadata )",Classifies a document into categories .
"def get_task ( dag_id , task_id ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = ""Dag id {} not found"" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'Task {} not found in dag {}' . format ( task_id , dag_id ) raise TaskNotFound ( error_message ) return dag . get_task ( task_id )",Return the task object identified by the given dag_id and task_id .
"def get_template_field ( env , fullname ) : modname , classname = fullname . rsplit ( ""."" , 1 ) try : with mock ( env . config . autodoc_mock_imports ) : mod = import_module ( modname ) except ImportError : raise RoleException ( ""Error loading %s module."" % ( modname , ) ) clazz = getattr ( mod , classname ) if not clazz : raise RoleException ( ""Error finding %s class in %s module."" % ( classname , modname ) ) template_fields = getattr ( clazz , ""template_fields"" ) if not template_fields : raise RoleException ( ""Could not find the template fields for %s class in %s module."" % ( classname , modname ) ) return list ( template_fields )",Gets template fields for specific operator class .
"def template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : text = utils . unescape ( text ) try : template_fields = get_template_field ( app . env , text ) except RoleException as e : msg = inliner . reporter . error ( ""invalid class name %s \n%s"" % ( text , e , ) , line = lineno ) prb = inliner . problematic ( rawtext , rawtext , msg ) return [ prb ] , [ msg ] node = nodes . inline ( rawtext = rawtext ) for i , field in enumerate ( template_fields ) : if i != 0 : node += nodes . Text ( "", "" ) node += nodes . literal ( field , """" , nodes . Text ( field ) ) return [ node ] , [ ]",A role that allows you to include a list of template fields in the middle of the text . This is especially useful when writing guides describing how to use the operator . The result is a list of fields where each field is shorted in the literal block .
"def dispose_orm ( ) : log . debug ( ""Disposing DB connection pool (PID %s)"" , os . getpid ( ) ) global engine global Session if Session : Session . remove ( ) Session = None if engine : engine . dispose ( ) engine = None",Properly close pooled database connections
"def prepare_classpath ( ) : if DAGS_FOLDER not in sys . path : sys . path . append ( DAGS_FOLDER ) config_path = os . path . join ( AIRFLOW_HOME , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if PLUGINS_FOLDER not in sys . path : sys . path . append ( PLUGINS_FOLDER )",Ensures that certain subfolders of AIRFLOW_HOME are on the classpath
"def _check_task_id ( self , context ) : ti = context [ 'ti' ] celery_result = ti . xcom_pull ( task_ids = self . target_task_id ) return celery_result . ready ( )",Gets the returned Celery result from the Airflow task ID provided to the sensor and returns True if the celery result has been finished execution .
"def detect_conf_var ( ) : ticket_cache = configuration . conf . get ( 'kerberos' , 'ccache' ) with open ( ticket_cache , 'rb' ) as f : return b'X-CACHECONF:' in f . read ( )",Return true if the ticket cache contains conf information as is found in ticket caches of Kerberos 1 . 8 . 1 or later . This is incompatible with the Sun Java Krb5LoginModule in Java6 so we need to take an action to work around it .
"def alchemy_to_dict ( obj ) : if not obj : return None d = { } for c in obj . __table__ . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d",Transforms a SQLAlchemy model instance into a dictionary
"def chunks ( items , chunk_size ) : if chunk_size <= 0 : raise ValueError ( 'Chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk_size ) : yield items [ i : i + chunk_size ]",Yield successive chunks of a given size from a list of items
"def reduce_in_chunks ( fn , iterable , initializer , chunk_size = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )",Reduce the given list of items by splitting it into chunks of the given size and passing each chunk through the reducer
"def chain ( * tasks ) : for up_task , down_task in zip ( tasks [ : - 1 ] , tasks [ 1 : ] ) : up_task . set_downstream ( down_task )",Given a number of tasks builds a dependency chain .
"def pprinttable ( rows ) : if not rows : return if hasattr ( rows [ 0 ] , '_fields' ) : headers = rows [ 0 ] . _fields else : headers = [ ""col{}"" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] lens = [ len ( s ) for s in headers ] for row in rows : for i in range ( len ( rows [ 0 ] ) ) : slenght = len ( ""{}"" . format ( row [ i ] ) ) if slenght > lens [ i ] : lens [ i ] = slenght formats = [ ] hformats = [ ] for i in range ( len ( rows [ 0 ] ) ) : if isinstance ( rows [ 0 ] [ i ] , int ) : formats . append ( ""%%%dd"" % lens [ i ] ) else : formats . append ( ""%%-%ds"" % lens [ i ] ) hformats . append ( ""%%-%ds"" % lens [ i ] ) pattern = "" | "" . join ( formats ) hpattern = "" | "" . join ( hformats ) separator = ""-+-"" . join ( [ '-' * n for n in lens ] ) s = """" s += separator + '\n' s += ( hpattern % tuple ( headers ) ) + '\n' s += separator + '\n' def f ( t ) : return ""{}"" . format ( t ) if isinstance ( t , basestring ) else t for line in rows : s += pattern % tuple ( f ( t ) for t in line ) + '\n' s += separator + '\n' return s",Returns a pretty ascii table from tuples
"def reap_process_group ( pid , log , sig = signal . SIGTERM , timeout = DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM ) : def on_terminate ( p ) : log . info ( ""Process %s (%s) terminated with exit code %s"" , p , p . pid , p . returncode ) if pid == os . getpid ( ) : raise RuntimeError ( ""I refuse to kill myself"" ) parent = psutil . Process ( pid ) children = parent . children ( recursive = True ) children . append ( parent ) try : pg = os . getpgid ( pid ) except OSError as err : if err . errno == errno . ESRCH : return raise log . info ( ""Sending %s to GPID %s"" , sig , pg ) os . killpg ( os . getpgid ( pid ) , sig ) gone , alive = psutil . wait_procs ( children , timeout = timeout , callback = on_terminate ) if alive : for p in alive : log . warn ( ""process %s (%s) did not respond to SIGTERM. Trying SIGKILL"" , p , pid ) os . killpg ( os . getpgid ( pid ) , signal . SIGKILL ) gone , alive = psutil . wait_procs ( alive , timeout = timeout , callback = on_terminate ) if alive : for p in alive : log . error ( ""Process %s (%s) could not be killed. Giving up."" , p , p . pid )",Tries really hard to terminate all children ( including grandchildren ) . Will send sig ( SIGTERM ) to the process group of pid . If any process is alive after timeout a SIGKILL will be send .
"def render_log_filename ( ti , try_number , filename_template ) : filename_template , filename_jinja_template = parse_template_string ( filename_template ) if filename_jinja_template : jinja_context = ti . get_template_context ( ) jinja_context [ 'try_number' ] = try_number return filename_jinja_template . render ( * * jinja_context ) return filename_template . format ( dag_id = ti . dag_id , task_id = ti . task_id , execution_date = ti . execution_date . isoformat ( ) , try_number = try_number )",Given task instance try_number filename_template return the rendered log filename
"def get_task_instance ( dag_id , task_id , execution_date ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = ""Dag id {} not found"" . format ( dag_id ) raise DagNotFound ( error_message ) dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'Task {} not found in dag {}' . format ( task_id , dag_id ) raise TaskNotFound ( error_message ) dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'Dag Run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise DagRunNotFound ( error_message ) task_instance = dagrun . get_task_instance ( task_id ) if not task_instance : error_message = ( 'Task {} instance for date {} not found' . format ( task_id , execution_date ) ) raise TaskInstanceNotFound ( error_message ) return task_instance",Return the task object identified by the given dag_id and task_id .
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import operators_modules for operators_module in operators_modules : sys . modules [ operators_module . __name__ ] = operators_module globals ( ) [ operators_module . _name ] = operators_module,Integrate plugins to the context
"def get_conn ( self ) : http_authorized = self . _authorize ( ) return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = False )",Returns a Google Cloud Dataproc service object .
"def wait ( self , operation ) : submitted = _DataProcOperation ( self . get_conn ( ) , operation , self . num_retries ) submitted . wait_for_done ( )",Awaits for Google Cloud Dataproc Operation to complete .
"def _deep_string_coerce ( content , json_path = 'json' ) : c = _deep_string_coerce if isinstance ( content , six . string_types ) : return content elif isinstance ( content , six . integer_types + ( float , ) ) : return str ( content ) elif isinstance ( content , ( list , tuple ) ) : return [ c ( e , '{0}[{1}]' . format ( json_path , i ) ) for i , e in enumerate ( content ) ] elif isinstance ( content , dict ) : return { k : c ( v , '{0}[{1}]' . format ( json_path , k ) ) for k , v in list ( content . items ( ) ) } else : param_type = type ( content ) msg = 'Type {0} used for parameter {1} is not a number or a string' . format ( param_type , json_path ) raise AirflowException ( msg )",Coerces content or all values of content if it is a dict to a string . The function will throw if content contains non - string or non - numeric types .
"def _handle_databricks_operator_execution ( operator , hook , log , context ) : if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_ID_KEY , value = operator . run_id ) log . info ( 'Run submitted with run_id: %s' , operator . run_id ) run_page_url = hook . get_run_page_url ( operator . run_id ) if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = XCOM_RUN_PAGE_URL_KEY , value = run_page_url ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) while True : run_state = hook . get_run_state ( operator . run_id ) if run_state . is_terminal : if run_state . is_successful : log . info ( '%s completed successfully.' , operator . task_id ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) return else : error_message = '{t} failed with terminal state: {s}' . format ( t = operator . task_id , s = run_state ) raise AirflowException ( error_message ) else : log . info ( '%s in run state: %s' , operator . task_id , run_state ) log . info ( 'View run status, Spark UI, and logs at %s' , run_page_url ) log . info ( 'Sleeping for %s seconds.' , operator . polling_period_seconds ) time . sleep ( operator . polling_period_seconds )",Handles the Airflow + Databricks lifecycle logic for a Databricks operator
"def run_cli ( self , pig , verbose = True ) : with TemporaryDirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir ) as f : f . write ( pig . encode ( 'utf-8' ) ) f . flush ( ) fname = f . name pig_bin = 'pig' cmd_extra = [ ] pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra if self . pig_properties : pig_properties_list = self . pig_properties . split ( ) pig_cmd . extend ( pig_properties_list ) if verbose : self . log . info ( ""%s"" , "" "" . join ( pig_cmd ) ) sp = subprocess . Popen ( pig_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) self . sp = sp stdout = '' for line in iter ( sp . stdout . readline , b'' ) : stdout += line . decode ( 'utf-8' ) if verbose : self . log . info ( line . strip ( ) ) sp . wait ( ) if sp . returncode : raise AirflowException ( stdout ) return stdout",Run an pig script using the pig cli
"def fetch_celery_task_state ( celery_task ) : try : with timeout ( seconds = 2 ) : res = ( celery_task [ 0 ] , celery_task [ 1 ] . state ) except Exception as e : exception_traceback = ""Celery Task ID: {}\n{}"" . format ( celery_task [ 0 ] , traceback . format_exc ( ) ) res = ExceptionWithTraceback ( e , exception_traceback ) return res",Fetch and return the state of the given celery task . The scope of this function is global so that it can be called by subprocesses in the pool .
"def _num_tasks_per_send_process ( self , to_send_count ) : return max ( 1 , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )",How many Celery tasks should each worker process send .
"def _num_tasks_per_fetch_process ( self ) : return max ( 1 , int ( math . ceil ( 1.0 * len ( self . tasks ) / self . _sync_parallelism ) ) )",How many Celery tasks should be sent to each worker process .
"def setdefault ( cls , key , default , deserialize_json = False ) : obj = Variable . get ( key , default_var = None , deserialize_json = deserialize_json ) if obj is None : if default is not None : Variable . set ( key , default , serialize_json = deserialize_json ) return default else : raise ValueError ( 'Default Value must be set' ) else : return obj",Like a Python builtin dict object setdefault returns the current value for a key and if it isn t there stores the default value and returns it .
"def get_conn ( self ) : authed_http = self . _authorize ( ) return build ( 'ml' , 'v1' , http = authed_http , cache_discovery = False )",Returns a Google MLEngine service object .
"def create_job ( self , project_id , job , use_existing_job_fn = None ) : request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) job_id = job [ 'jobId' ] try : request . execute ( ) except HttpError as e : if e . resp . status == 409 : if use_existing_job_fn is not None : existing_job = self . _get_job ( project_id , job_id ) if not use_existing_job_fn ( existing_job ) : self . log . error ( 'Job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) raise self . log . info ( 'Job with job_id %s already exist. Will waiting for it to finish' , job_id ) else : self . log . error ( 'Failed to create MLEngine job: {}' . format ( e ) ) raise return self . _wait_for_job_done ( project_id , job_id )",Launches a MLEngine job and wait for it to reach a terminal state .
"def _get_job ( self , project_id , job_id ) : job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) while True : try : return request . execute ( ) except HttpError as e : if e . resp . status == 429 : time . sleep ( 30 ) else : self . log . error ( 'Failed to get MLEngine job: {}' . format ( e ) ) raise",Gets a MLEngine job based on the job name .
"def _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : if interval <= 0 : raise ValueError ( ""Interval must be > 0"" ) while True : job = self . _get_job ( project_id , job_id ) if job [ 'state' ] in [ 'SUCCEEDED' , 'FAILED' , 'CANCELLED' ] : return job time . sleep ( interval )",Waits for the Job to reach a terminal state .
"def create_version ( self , project_id , model_name , version_spec ) : parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) response = create_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )",Creates the Version on Google Cloud ML Engine .
"def set_default_version ( self , project_id , model_name , version_name ) : full_version_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . setDefault ( name = full_version_name , body = { } ) try : response = request . execute ( ) self . log . info ( 'Successfully set version: %s to default' , response ) return response except HttpError as e : self . log . error ( 'Something went wrong: %s' , e ) raise",Sets a version to be the default . Blocks until finished .
"def list_versions ( self , project_id , model_name ) : result = [ ] full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageSize = 100 ) response = request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) while next_page_token is not None : next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pageToken = next_page_token , pageSize = 100 ) response = next_request . execute ( ) next_page_token = response . get ( 'nextPageToken' , None ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result",Lists all available versions of a model . Blocks until finished .
"def delete_version ( self , project_id , model_name , version_name ) : full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) response = delete_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , False ) , is_error_func = lambda resp : resp . get ( 'error' , None ) is not None )",Deletes the given version of a model . Blocks until finished .
"def create_model ( self , project_id , model ) : if not model [ 'name' ] : raise ValueError ( ""Model name must be provided and "" ""could not be an empty string"" ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )",Create a Model . Blocks until finished .
"def get_model ( self , project_id , model_name ) : if not model_name : raise ValueError ( ""Model name must be provided and "" ""it could not be an empty string"" ) full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) try : return request . execute ( ) except HttpError as e : if e . resp . status == 404 : self . log . error ( 'Model was not found: %s' , e ) return None raise",Gets a Model . Blocks until finished .
"def execute_work ( self , key , command ) : if key is None : return self . log . info ( ""%s running %s"" , self . __class__ . __name__ , command ) try : subprocess . check_call ( command , close_fds = True ) state = State . SUCCESS except subprocess . CalledProcessError as e : state = State . FAILED self . log . error ( ""Failed to execute task %s."" , str ( e ) ) self . result_queue . put ( ( key , state ) )",Executes command received and stores result state in queue . : param key : the key to identify the TI : type key : tuple ( dag_id task_id execution_date ) : param command : the command to execute : type command : str
"def write_batch_data ( self , items ) : dynamodb_conn = self . get_conn ( ) try : table = dynamodb_conn . Table ( self . table_name ) with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : for item in items : batch . put_item ( Item = item ) return True except Exception as general_error : raise AirflowException ( 'Failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) )",Write batch items to dynamodb table with provisioned throughout capacity .
def _integrate_plugins ( ) : from airflow . plugins_manager import executors_modules for executors_module in executors_modules : sys . modules [ executors_module . __name__ ] = executors_module globals ( ) [ executors_module . _name ] = executors_module,Integrate plugins to the context .
"def get_default_executor ( ) : global DEFAULT_EXECUTOR if DEFAULT_EXECUTOR is not None : return DEFAULT_EXECUTOR executor_name = configuration . conf . get ( 'core' , 'EXECUTOR' ) DEFAULT_EXECUTOR = _get_executor ( executor_name ) log = LoggingMixin ( ) . log log . info ( ""Using executor %s"" , executor_name ) return DEFAULT_EXECUTOR",Creates a new instance of the configured executor if none exists and returns it
"def _get_executor ( executor_name ) : if executor_name == Executors . LocalExecutor : return LocalExecutor ( ) elif executor_name == Executors . SequentialExecutor : return SequentialExecutor ( ) elif executor_name == Executors . CeleryExecutor : from airflow . executors . celery_executor import CeleryExecutor return CeleryExecutor ( ) elif executor_name == Executors . DaskExecutor : from airflow . executors . dask_executor import DaskExecutor return DaskExecutor ( ) elif executor_name == Executors . KubernetesExecutor : from airflow . contrib . executors . kubernetes_executor import KubernetesExecutor return KubernetesExecutor ( ) else : _integrate_plugins ( ) executor_path = executor_name . split ( '.' ) if len ( executor_path ) != 2 : raise AirflowException ( ""Executor {0} not supported: "" ""please specify in format plugin_module.executor"" . format ( executor_name ) ) if executor_path [ 0 ] in globals ( ) : return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) else : raise AirflowException ( ""Executor {0} not supported."" . format ( executor_name ) )",Creates a new instance of the named executor . In case the executor name is not know in airflow look for it in the plugins
"def on_error ( self , error , items ) : self . log . error ( 'Encountered Segment error: {segment_error} with ' 'items: {with_items}' . format ( segment_error = error , with_items = items ) ) raise AirflowException ( 'Segment error: {}' . format ( error ) )",Handles error callbacks when using Segment with segment_debug_mode set to True
"def run_pod ( self , pod , startup_timeout = 120 , get_logs = True ) : resp = self . run_pod_async ( pod ) curr_time = dt . now ( ) if resp . status . start_time is None : while self . pod_not_started ( pod ) : delta = dt . now ( ) - curr_time if delta . seconds >= startup_timeout : raise AirflowException ( ""Pod took too long to start"" ) time . sleep ( 1 ) self . log . debug ( 'Pod not yet started' ) return self . _monitor_pod ( pod , get_logs )",Launches the pod synchronously and waits for completion . Args : pod ( Pod ) : startup_timeout ( int ) : Timeout for startup of the pod ( if pod is pending for too long considers task a failure
"def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn",Returns a mssql connection object
"def execute ( self , context ) : self . _hook = SparkSubmitHook ( conf = self . _conf , conn_id = self . _conn_id , files = self . _files , py_files = self . _py_files , archives = self . _archives , driver_class_path = self . _driver_class_path , jars = self . _jars , java_class = self . _java_class , packages = self . _packages , exclude_packages = self . _exclude_packages , repositories = self . _repositories , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , application_args = self . _application_args , env_vars = self . _env_vars , verbose = self . _verbose , spark_binary = self . _spark_binary ) self . _hook . submit ( self . _application )",Call the SparkSubmitHook to run the provided spark job
"def trigger_dag ( dag_id ) : data = request . get_json ( force = True ) run_id = None if 'run_id' in data : run_id = data [ 'run_id' ] conf = None if 'conf' in data : conf = data [ 'conf' ] execution_date = None if 'execution_date' in data and data [ 'execution_date' ] is not None : execution_date = data [ 'execution_date' ] try : execution_date = timezone . parse ( execution_date ) except ValueError : error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response if getattr ( g , 'user' , None ) : _log . info ( ""User %s created %s"" , g . user , dr ) response = jsonify ( message = ""Created {}"" . format ( dr ) ) return response",Trigger a new dag run for a Dag with an execution date of now unless specified in the data .
"def delete_dag ( dag_id ) : try : count = delete . delete_dag ( dag_id ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = ""Removed {} record(s)"" . format ( count ) , count = count )",Delete all DB records related to the specified Dag .
"def dag_runs ( dag_id ) : try : state = request . args . get ( 'state' ) dagruns = get_dag_runs ( dag_id , state ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = 400 return response return jsonify ( dagruns )",Returns a list of Dag Runs for a specific DAG ID . : query param state : a query string parameter ?state = queued|running|success ... : param dag_id : String identifier of a DAG : return : List of DAG runs of a DAG with requested state or all runs if the state is not specified
"def get_dag_code ( dag_id ) : try : return get_code ( dag_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response",Return python code of a given dag_id .
"def task_info ( dag_id , task_id ) : try : info = get_task ( dag_id , task_id ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )",Returns a JSON with a task s public instance variables .
"def dag_paused ( dag_id , paused ) : DagModel = models . DagModel with create_session ( ) as session : orm_dag = ( session . query ( DagModel ) . filter ( DagModel . dag_id == dag_id ) . first ( ) ) if paused == 'true' : orm_dag . is_paused = True else : orm_dag . is_paused = False session . merge ( orm_dag ) session . commit ( ) return jsonify ( { 'response' : 'ok' } )",( Un ) pauses a dag
"def task_instance_info ( dag_id , execution_date , task_id ) : try : execution_date = timezone . parse ( execution_date ) except ValueError : error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : info = get_task_instance ( dag_id , task_id , execution_date ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )",Returns a JSON with a task instance s public instance variables . The format for the exec_date is expected to be YYYY - mm - DDTHH : MM : SS for example : 2016 - 11 - 16T11 : 34 : 15 . This will of course need to have been encoded for URL in the request .
"def dag_run_status ( dag_id , execution_date ) : try : execution_date = timezone . parse ( execution_date ) except ValueError : error_message = ( 'Given execution date, {}, could not be identified ' 'as a date. Example date format: 2015-11-16T14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : info = get_dag_run_state ( dag_id , execution_date ) except AirflowException as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( info )",Returns a JSON with a dag_run s public instance variables . The format for the exec_date is expected to be YYYY - mm - DDTHH : MM : SS for example : 2016 - 11 - 16T11 : 34 : 15 . This will of course need to have been encoded for URL in the request .
"def get_pools ( ) : try : pools = pool_api . get_pools ( ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( [ p . to_json ( ) for p in pools ] )",Get all pools .
"def create_pool ( ) : params = request . get_json ( force = True ) try : pool = pool_api . create_pool ( * * params ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )",Create a pool .
"def delete_pool ( name ) : try : pool = pool_api . delete_pool ( name = name ) except AirflowException as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )",Delete pool .
"def create_or_update ( self , resource_group , name , container_group ) : self . connection . container_groups . create_or_update ( resource_group , name , container_group )",Create a new container group
"def get_state_exitcode_details ( self , resource_group , name ) : current_state = self . _get_instance_view ( resource_group , name ) . current_state return ( current_state . state , current_state . exit_code , current_state . detail_status )",Get the state and exitcode of a container group
"def get_messages ( self , resource_group , name ) : instance_view = self . _get_instance_view ( resource_group , name ) return [ event . message for event in instance_view . events ]",Get the messages of a container group
"def get_logs ( self , resource_group , name , tail = 1000 ) : logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) return logs . content . splitlines ( True )",Get the tail from logs of a container group
"def delete ( self , resource_group , name ) : self . connection . container_groups . delete ( resource_group , name )",Delete a container group
"def exists ( self , resource_group , name ) : for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : if container . name == name : return True return False",Test if a container group exists
"def apply_defaults ( func ) : sig_cache = signature ( func ) non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . VAR_POSITIONAL , param . VAR_KEYWORD ) } @ wraps ( func ) def wrapper ( * args , * * kwargs ) : if len ( args ) > 1 : raise AirflowException ( ""Use keyword arguments when initializing operators"" ) dag_args = { } dag_params = { } dag = kwargs . get ( 'dag' , None ) or settings . CONTEXT_MANAGER_DAG if dag : dag_args = copy ( dag . default_args ) or { } dag_params = copy ( dag . params ) or { } params = { } if 'params' in kwargs : params = kwargs [ 'params' ] dag_params . update ( params ) default_args = { } if 'default_args' in kwargs : default_args = kwargs [ 'default_args' ] if 'params' in default_args : dag_params . update ( default_args [ 'params' ] ) del default_args [ 'params' ] dag_args . update ( default_args ) default_args = dag_args for arg in sig_cache . parameters : if arg not in kwargs and arg in default_args : kwargs [ arg ] = default_args [ arg ] missing_args = list ( non_optional_args - set ( kwargs ) ) if missing_args : msg = ""Argument {0} is required"" . format ( missing_args ) raise AirflowException ( msg ) kwargs [ 'params' ] = dag_params result = func ( * args , * * kwargs ) return result return wrapper",Function decorator that Looks for an argument named default_args and fills the unspecified arguments from it .
"def construct_ingest_query ( self , static_path , columns ) : num_shards = self . num_shards target_partition_size = self . target_partition_size if self . target_partition_size == - 1 : if self . num_shards == - 1 : target_partition_size = DEFAULT_TARGET_PARTITION_SIZE else : num_shards = - 1 metric_names = [ m [ 'fieldName' ] for m in self . metric_spec if m [ 'type' ] != 'count' ] dimensions = [ c for c in columns if c not in metric_names and c != self . ts_dim ] ingest_query_dict = { ""type"" : ""index_hadoop"" , ""spec"" : { ""dataSchema"" : { ""metricsSpec"" : self . metric_spec , ""granularitySpec"" : { ""queryGranularity"" : self . query_granularity , ""intervals"" : self . intervals , ""type"" : ""uniform"" , ""segmentGranularity"" : self . segment_granularity , } , ""parser"" : { ""type"" : ""string"" , ""parseSpec"" : { ""columns"" : columns , ""dimensionsSpec"" : { ""dimensionExclusions"" : [ ] , ""dimensions"" : dimensions , ""spatialDimensions"" : [ ] } , ""timestampSpec"" : { ""column"" : self . ts_dim , ""format"" : ""auto"" } , ""format"" : ""tsv"" } } , ""dataSource"" : self . druid_datasource } , ""tuningConfig"" : { ""type"" : ""hadoop"" , ""jobProperties"" : { ""mapreduce.job.user.classpath.first"" : ""false"" , ""mapreduce.map.output.compress"" : ""false"" , ""mapreduce.output.fileoutputformat.compress"" : ""false"" , } , ""partitionsSpec"" : { ""type"" : ""hashed"" , ""targetPartitionSize"" : target_partition_size , ""numShards"" : num_shards , } , } , ""ioConfig"" : { ""inputSpec"" : { ""paths"" : static_path , ""type"" : ""static"" } , ""type"" : ""hadoop"" } } } if self . job_properties : ingest_query_dict [ 'spec' ] [ 'tuningConfig' ] [ 'jobProperties' ] . update ( self . job_properties ) if self . hadoop_dependency_coordinates : ingest_query_dict [ 'hadoopDependencyCoordinates' ] = self . hadoop_dependency_coordinates return ingest_query_dict",Builds an ingest query for an HDFS TSV load .
"def execute ( self , context ) : self . log . info ( 'Transferring mail attachment %s from mail server via imap to s3 key %s...' , self . imap_attachment_name , self . s3_key ) with ImapHook ( imap_conn_id = self . imap_conn_id ) as imap_hook : imap_mail_attachments = imap_hook . retrieve_mail_attachments ( name = self . imap_attachment_name , mail_folder = self . imap_mail_folder , check_regex = self . imap_check_regex , latest_only = True ) s3_hook = S3Hook ( aws_conn_id = self . s3_conn_id ) s3_hook . load_bytes ( bytes_data = imap_mail_attachments [ 0 ] [ 1 ] , key = self . s3_key )",This function executes the transfer from the email server ( via imap ) into s3 .
"def poke ( self , context ) : self . log . info ( 'RedisPubSubSensor checking for message on channels: %s' , self . channels ) message = self . pubsub . get_message ( ) self . log . info ( 'Message %s from channel %s' , message , self . channels ) if message and message [ 'type' ] == 'message' : context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) self . pubsub . unsubscribe ( self . channels ) return True return False",Check for message on subscribed channels and write to xcom the message with key message
"def refresh_from_db ( self , session = None ) : DR = DagRun exec_date = func . cast ( self . execution_date , DateTime ) dr = session . query ( DR ) . filter ( DR . dag_id == self . dag_id , func . cast ( DR . execution_date , DateTime ) == exec_date , DR . run_id == self . run_id ) . one ( ) self . id = dr . id self . state = dr . state",Reloads the current dagrun from the database : param session : database session
"def find ( dag_id = None , run_id = None , execution_date = None , state = None , external_trigger = None , no_backfills = False , session = None ) : DR = DagRun qry = session . query ( DR ) if dag_id : qry = qry . filter ( DR . dag_id == dag_id ) if run_id : qry = qry . filter ( DR . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( DR . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( DR . execution_date == execution_date ) if state : qry = qry . filter ( DR . state == state ) if external_trigger is not None : qry = qry . filter ( DR . external_trigger == external_trigger ) if no_backfills : from airflow . jobs import BackfillJob qry = qry . filter ( DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) ) dr = qry . order_by ( DR . execution_date ) . all ( ) return dr",Returns a set of dag runs for the given search criteria .
"def get_task_instances ( self , state = None , session = None ) : from airflow . models . taskinstance import TaskInstance tis = session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . execution_date == self . execution_date , ) if state : if isinstance ( state , six . string_types ) : tis = tis . filter ( TaskInstance . state == state ) else : if None in state : tis = tis . filter ( or_ ( TaskInstance . state . in_ ( state ) , TaskInstance . state . is_ ( None ) ) ) else : tis = tis . filter ( TaskInstance . state . in_ ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( TaskInstance . task_id . in_ ( self . dag . task_ids ) ) return tis . all ( )",Returns the task instances for this dag run
"def get_task_instance ( self , task_id , session = None ) : from airflow . models . taskinstance import TaskInstance TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . execution_date == self . execution_date , TI . task_id == task_id ) . first ( ) return ti",Returns the task instance specified by task_id for this dag run
"def get_previous_dagrun ( self , session = None ) : return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date < self . execution_date ) . order_by ( DagRun . execution_date . desc ( ) ) . first ( )",The previous DagRun if there is one
"def get_previous_scheduled_dagrun ( self , session = None ) : dag = self . get_dag ( ) return session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == dag . previous_schedule ( self . execution_date ) ) . first ( )",The previous SCHEDULED DagRun if there is one
"def update_state ( self , session = None ) : dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) self . log . debug ( ""Updating state for %s considering %s task(s)"" , self , len ( tis ) ) for ti in list ( tis ) : if ti . state == State . REMOVED : tis . remove ( ti ) else : ti . task = dag . get_task ( ti . task_id ) start_dttm = timezone . utcnow ( ) unfinished_tasks = self . get_task_instances ( state = State . unfinished ( ) , session = session ) none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) none_task_concurrency = all ( t . task . task_concurrency is None for t in unfinished_tasks ) if unfinished_tasks and none_depends_on_past and none_task_concurrency : no_dependencies_met = True for ut in unfinished_tasks : old_state = ut . state deps_met = ut . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True , ignore_in_retry_period = True , ignore_in_reschedule_period = True ) , session = session ) if deps_met or old_state != ut . current_state ( session = session ) : no_dependencies_met = False break duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 Stats . timing ( ""dagrun.dependency-check.{}"" . format ( self . dag_id ) , duration ) root_ids = [ t . task_id for t in dag . roots ] roots = [ t for t in tis if t . task_id in root_ids ] if ( not unfinished_tasks and any ( r . state in ( State . FAILED , State . UPSTREAM_FAILED ) for r in roots ) ) : self . log . info ( 'Marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'task_failure' , session = session ) elif not unfinished_tasks and all ( r . state in ( State . SUCCESS , State . SKIPPED ) for r in roots ) : self . log . info ( 'Marking run %s successful' , self ) self . set_state ( State . SUCCESS ) dag . handle_callback ( self , success = True , reason = 'success' , session = session ) elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : self . log . info ( 'Deadlock; marking run %s failed' , self ) self . set_state ( State . FAILED ) dag . handle_callback ( self , success = False , reason = 'all_tasks_deadlocked' , session = session ) else : self . set_state ( State . RUNNING ) self . _emit_duration_stats_for_finished_state ( ) session . merge ( self ) session . commit ( ) return self . state",Determines the overall state of the DagRun based on the state of its TaskInstances .
"def verify_integrity ( self , session = None ) : from airflow . models . taskinstance import TaskInstance dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = None try : task = dag . get_task ( ti . task_id ) except AirflowException : if ti . state == State . REMOVED : pass elif self . state is not State . RUNNING and not dag . partial : self . log . warning ( ""Failed to get task '{}' for dag '{}'. "" ""Marking it as removed."" . format ( ti , dag ) ) Stats . incr ( ""task_removed_from_dag.{}"" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . REMOVED is_task_in_dag = task is not None should_restore_task = is_task_in_dag and ti . state == State . REMOVED if should_restore_task : self . log . info ( ""Restoring task '{}' which was previously "" ""removed from DAG '{}'"" . format ( ti , dag ) ) Stats . incr ( ""task_restored_to_dag.{}"" . format ( dag . dag_id ) , 1 , 1 ) ti . state = State . NONE for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : Stats . incr ( ""task_instance_created-{}"" . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = TaskInstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )",Verifies the DagRun by checking for removed tasks or tasks that are not in the database yet . It will set state to removed or add the task if required .
"def get_run ( session , dag_id , execution_date ) : qry = session . query ( DagRun ) . filter ( DagRun . dag_id == dag_id , DagRun . external_trigger == False , DagRun . execution_date == execution_date , ) return qry . first ( )",: param dag_id : DAG ID : type dag_id : unicode : param execution_date : execution date : type execution_date : datetime : return : DagRun corresponding to the given dag_id and execution date if one exists . None otherwise . : rtype : airflow . models . DagRun
"def jenkins_request_with_headers ( jenkins_server , req ) : try : response = jenkins_server . jenkins_request ( req ) response_body = response . content response_headers = response . headers if response_body is None : raise jenkins . EmptyResponseException ( ""Error communicating with server[%s]: "" ""empty response"" % jenkins_server . server ) return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } except HTTPError as e : if e . code in [ 401 , 403 , 500 ] : raise JenkinsException ( 'Error in request. ' + 'Possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) elif e . code == 404 : raise jenkins . NotFoundException ( 'Requested item could not be found' ) else : raise except socket . timeout as e : raise jenkins . TimeoutException ( 'Error in request: %s' % e ) except URLError as e : if str ( e . reason ) == ""timed out"" : raise jenkins . TimeoutException ( 'Error in request: %s' % e . reason ) raise JenkinsException ( 'Error in request: %s' % e . reason )",We need to get the headers in addition to the body answer to get the location from them This function uses jenkins_request method from python - jenkins library with just the return call changed
"def build_job ( self , jenkins_server ) : if self . parameters and isinstance ( self . parameters , six . string_types ) : import ast self . parameters = ast . literal_eval ( self . parameters ) if not self . parameters : self . parameters = None request = Request ( jenkins_server . build_job_url ( self . job_name , self . parameters , None ) ) return jenkins_request_with_headers ( jenkins_server , request )",This function makes an API call to Jenkins to trigger a build for job_name It returned a dict with 2 keys : body and headers . headers contains also a dict - like object which can be queried to get the location to poll in the queue .
"def poll_job_in_queue ( self , location , jenkins_server ) : try_count = 0 location = location + '/api/json' self . log . info ( 'Polling jenkins queue at the url %s' , location ) while try_count < self . max_try_before_job_appears : location_answer = jenkins_request_with_headers ( jenkins_server , Request ( location ) ) if location_answer is not None : json_response = json . loads ( location_answer [ 'body' ] ) if 'executable' in json_response : build_number = json_response [ 'executable' ] [ 'number' ] self . log . info ( 'Job executed on Jenkins side with the build number %s' , build_number ) return build_number try_count += 1 time . sleep ( self . sleep_time ) raise AirflowException ( ""The job hasn't been executed"" "" after polling the queue %d times"" , self . max_try_before_job_appears )",This method poll the jenkins queue until the job is executed . When we trigger a job through an API call the job is first put in the queue without having a build number assigned . Thus we have to wait the job exit the queue to know its build number . To do so we have to add / api / json ( or / api / xml ) to the location returned by the build_job call and poll this file . When a executable block appears in the json it means the job execution started and the field number then contains the build number .
"def context_to_airflow_vars ( context , in_env_var_format = False ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' task_instance = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_ID' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_TASK_ID' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_EXECUTION_DATE' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ AIRFLOW_VAR_NAME_FORMAT_MAPPING [ 'AIRFLOW_CONTEXT_DAG_RUN_ID' ] [ name_format ] ] = dag_run . run_id return params",Given a context this function provides a dictionary of values that can be used to externally reconstruct relations between dags dag_runs tasks and task_instances . Default to abc . def . ghi format and can be made to ABC_DEF_GHI format if in_env_var_format is set to True .
"def on_pre_execution ( * * kwargs ) : logging . debug ( ""Calling callbacks: %s"" , __pre_exec_callbacks ) for cb in __pre_exec_callbacks : try : cb ( * * kwargs ) except Exception : logging . exception ( 'Failed on pre-execution callback using %s' , cb )",Calls callbacks before execution . Note that any exception from callback will be logged but won t be propagated . : param kwargs : : return : None
"def on_post_execution ( * * kwargs ) : logging . debug ( ""Calling callbacks: %s"" , __post_exec_callbacks ) for cb in __post_exec_callbacks : try : cb ( * * kwargs ) except Exception : logging . exception ( 'Failed on post-execution callback using %s' , cb )",Calls callbacks after execution . As it s being called after execution it can capture status of execution duration etc . Note that any exception from callback will be logged but won t be propagated . : param kwargs : : return : None
"def conditionally_trigger ( context , dag_run_obj ) : c_p = context [ 'params' ] [ 'condition_param' ] print ( ""Controller DAG : conditionally_trigger = {}"" . format ( c_p ) ) if context [ 'params' ] [ 'condition_param' ] : dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag_run_obj . payload ) return dag_run_obj",This function decides whether or not to Trigger the remote DAG
"def send_metric ( self , metric_name , datapoint , tags = None , type_ = None , interval = None ) : response = api . Metric . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response ) return response",Sends a single datapoint metric to DataDog
"def query_metric ( self , query , from_seconds_ago , to_seconds_ago ) : now = int ( time . time ( ) ) response = api . Metric . query ( start = now - from_seconds_ago , end = now - to_seconds_ago , query = query ) self . validate_response ( response ) return response",Queries datadog for a specific metric potentially with some function applied to it and returns the results .
"def post_event ( self , title , text , aggregation_key = None , alert_type = None , date_happened = None , handle = None , priority = None , related_event_id = None , tags = None , device_name = None ) : response = api . Event . create ( title = title , text = text , aggregation_key = aggregation_key , alert_type = alert_type , date_happened = date_happened , handle = handle , priority = priority , related_event_id = related_event_id , tags = tags , host = self . host , device_name = device_name , source_type_name = self . source_type_name ) self . validate_response ( response ) return response",Posts an event to datadog ( processing finished potentially alerts other issues ) Think about this as a means to maintain persistence of alerts rather than alerting itself .
"def _get_token ( self , token , http_conn_id ) : if token : return token elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson return extra . get ( 'webhook_token' , '' ) else : raise AirflowException ( 'Cannot get token: No valid Slack ' 'webhook token nor conn_id supplied' )",Given either a manually set token or a conn_id return the webhook_token to use : param token : The manually provided token : type token : str : param http_conn_id : The conn_id provided : type http_conn_id : str : return : webhook_token ( str ) to use
def _build_slack_message ( self ) : cmd = { } if self . channel : cmd [ 'channel' ] = self . channel if self . username : cmd [ 'username' ] = self . username if self . icon_emoji : cmd [ 'icon_emoji' ] = self . icon_emoji if self . link_names : cmd [ 'link_names' ] = 1 if self . attachments : cmd [ 'attachments' ] = self . attachments cmd [ 'text' ] = self . message return json . dumps ( cmd ),Construct the Slack message . All relevant parameters are combined here to a valid Slack json message : return : Slack message ( str ) to send
"def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } slack_message = self . _build_slack_message ( ) self . run ( endpoint = self . webhook_token , data = slack_message , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )",Remote Popen ( actually execute the slack webhook call )
"def get_dag ( self , dag_id ) : from airflow . models . dag import DagModel root_dag_id = dag_id if dag_id in self . dags : dag = self . dags [ dag_id ] if dag . is_subdag : root_dag_id = dag . parent_dag . dag_id orm_dag = DagModel . get_current ( root_dag_id ) if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = False ) if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : return self . dags [ dag_id ] elif dag_id in self . dags : del self . dags [ dag_id ] return self . dags . get ( dag_id )",Gets the DAG out of the dictionary and refreshes it if expired
"def process_file ( self , filepath , only_if_updated = True , safe_mode = True ) : from airflow . models . dag import DAG found_dags = [ ] if filepath is None or not os . path . isfile ( filepath ) : return found_dags try : file_last_changed_on_disk = datetime . fromtimestamp ( os . path . getmtime ( filepath ) ) if only_if_updated and filepath in self . file_last_changed and file_last_changed_on_disk == self . file_last_changed [ filepath ] : return found_dags except Exception as e : self . log . exception ( e ) return found_dags mods = [ ] is_zipfile = zipfile . is_zipfile ( filepath ) if not is_zipfile : if safe_mode : with open ( filepath , 'rb' ) as f : content = f . read ( ) if not all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) : self . file_last_changed [ filepath ] = file_last_changed_on_disk if not self . has_logged : self . has_logged = True self . log . info ( ""File %s assumed to contain no DAGs. Skipping."" , filepath ) return found_dags self . log . debug ( ""Importing %s"" , filepath ) org_mod_name , _ = os . path . splitext ( os . path . split ( filepath ) [ - 1 ] ) mod_name = ( 'unusual_prefix_' + hashlib . sha1 ( filepath . encode ( 'utf-8' ) ) . hexdigest ( ) + '_' + org_mod_name ) if mod_name in sys . modules : del sys . modules [ mod_name ] with timeout ( configuration . conf . getint ( 'core' , ""DAGBAG_IMPORT_TIMEOUT"" ) ) : try : m = imp . load_source ( mod_name , filepath ) mods . append ( m ) except Exception as e : self . log . exception ( ""Failed to import: %s"" , filepath ) self . import_errors [ filepath ] = str ( e ) self . file_last_changed [ filepath ] = file_last_changed_on_disk else : zip_file = zipfile . ZipFile ( filepath ) for mod in zip_file . infolist ( ) : head , _ = os . path . split ( mod . filename ) mod_name , ext = os . path . splitext ( mod . filename ) if not head and ( ext == '.py' or ext == '.pyc' ) : if mod_name == '__init__' : self . log . warning ( ""Found __init__.%s at root of %s"" , ext , filepath ) if safe_mode : with zip_file . open ( mod . filename ) as zf : self . log . debug ( ""Reading %s from %s"" , mod . filename , filepath ) content = zf . read ( ) if not all ( [ s in content for s in ( b'DAG' , b'airflow' ) ] ) : self . file_last_changed [ filepath ] = ( file_last_changed_on_disk ) if not self . has_logged : self . has_logged = True self . log . info ( ""File %s assumed to contain no DAGs. Skipping."" , filepath ) if mod_name in sys . modules : del sys . modules [ mod_name ] try : sys . path . insert ( 0 , filepath ) m = importlib . import_module ( mod_name ) mods . append ( m ) except Exception as e : self . log . exception ( ""Failed to import: %s"" , filepath ) self . import_errors [ filepath ] = str ( e ) self . file_last_changed [ filepath ] = file_last_changed_on_disk for m in mods : for dag in list ( m . __dict__ . values ( ) ) : if isinstance ( dag , DAG ) : if not dag . full_filepath : dag . full_filepath = filepath if dag . fileloc != filepath and not is_zipfile : dag . fileloc = filepath try : dag . is_subdag = False self . bag_dag ( dag , parent_dag = dag , root_dag = dag ) if isinstance ( dag . _schedule_interval , six . string_types ) : croniter ( dag . _schedule_interval ) found_dags . append ( dag ) found_dags += dag . subdags except ( CroniterBadCronError , CroniterBadDateError , CroniterNotAlphaError ) as cron_e : self . log . exception ( ""Failed to bag_dag: %s"" , dag . full_filepath ) self . import_errors [ dag . full_filepath ] = ""Invalid Cron expression: "" + str ( cron_e ) self . file_last_changed [ dag . full_filepath ] = file_last_changed_on_disk except AirflowDagCycleException as cycle_exception : self . log . exception ( ""Failed to bag_dag: %s"" , dag . full_filepath ) self . import_errors [ dag . full_filepath ] = str ( cycle_exception ) self . file_last_changed [ dag . full_filepath ] = file_last_changed_on_disk self . file_last_changed [ filepath ] = file_last_changed_on_disk return found_dags",Given a path to a python module or zip file this method imports the module and look for dag objects within it .
"def kill_zombies ( self , zombies , session = None ) : from airflow . models . taskinstance import TaskInstance for zombie in zombies : if zombie . dag_id in self . dags : dag = self . dags [ zombie . dag_id ] if zombie . task_id in dag . task_ids : task = dag . get_task ( zombie . task_id ) ti = TaskInstance ( task , zombie . execution_date ) ti . start_date = zombie . start_date ti . end_date = zombie . end_date ti . try_number = zombie . try_number ti . state = zombie . state ti . test_mode = configuration . getboolean ( 'core' , 'unit_test_mode' ) ti . handle_failure ( ""{} detected as zombie"" . format ( ti ) , ti . test_mode , ti . get_template_context ( ) ) self . log . info ( 'Marked zombie job %s as %s' , ti , ti . state ) Stats . incr ( 'zombies_killed' ) session . commit ( )",Fail given zombie tasks which are tasks that haven t had a heartbeat for too long in the current DagBag .
"def bag_dag ( self , dag , parent_dag , root_dag ) : dag . test_cycle ( ) dag . resolve_template_files ( ) dag . last_loaded = timezone . utcnow ( ) for task in dag . tasks : settings . policy ( task ) subdags = dag . subdags try : for subdag in subdags : subdag . full_filepath = dag . full_filepath subdag . parent_dag = dag subdag . is_subdag = True self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) self . dags [ dag . dag_id ] = dag self . log . debug ( 'Loaded DAG %s' , dag ) except AirflowDagCycleException as cycle_exception : self . log . exception ( 'Exception bagging dag: %s' , dag . dag_id ) if dag == root_dag : for subdag in subdags : if subdag . dag_id in self . dags : del self . dags [ subdag . dag_id ] raise cycle_exception",Adds the DAG into the bag recurses into sub dags . Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags
"def collect_dags ( self , dag_folder = None , only_if_updated = True , include_examples = configuration . conf . getboolean ( 'core' , 'LOAD_EXAMPLES' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'DAG_DISCOVERY_SAFE_MODE' ) ) : start_dttm = timezone . utcnow ( ) dag_folder = dag_folder or self . dag_folder stats = [ ] FileLoadStat = namedtuple ( 'FileLoadStat' , ""file duration dag_num task_num dags"" ) dag_folder = correct_maybe_zipped ( dag_folder ) for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : try : ts = timezone . utcnow ( ) found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) td = timezone . utcnow ( ) - ts td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) stats . append ( FileLoadStat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) except Exception as e : self . log . exception ( e ) Stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) Stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) Stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = True )",Given a file path or a folder this method looks for python modules imports them and adds them to the dagbag collection .
"def dagbag_report ( self ) : report = textwrap . dedent ( """"""\n
        -------------------------------------------------------------------
        DagBag loading stats for {dag_folder}
        -------------------------------------------------------------------
        Number of DAGs: {dag_num}
        Total task number: {task_num}
        DagBag parsing time: {duration}
        {table}
        """""" ) stats = self . dagbag_stats return report . format ( dag_folder = self . dag_folder , duration = sum ( [ o . duration for o in stats ] ) , dag_num = sum ( [ o . dag_num for o in stats ] ) , task_num = sum ( [ o . task_num for o in stats ] ) , table = pprinttable ( stats ) , )",Prints a report around DagBag loading stats
"def execute ( self , context ) : self . _hook = SparkJDBCHook ( spark_app_name = self . _spark_app_name , spark_conn_id = self . _spark_conn_id , spark_conf = self . _spark_conf , spark_py_files = self . _spark_py_files , spark_files = self . _spark_files , spark_jars = self . _spark_jars , num_executors = self . _num_executors , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , verbose = self . _verbose , keytab = self . _keytab , principal = self . _principal , cmd_type = self . _cmd_type , jdbc_table = self . _jdbc_table , jdbc_conn_id = self . _jdbc_conn_id , jdbc_driver = self . _jdbc_driver , metastore_table = self . _metastore_table , jdbc_truncate = self . _jdbc_truncate , save_mode = self . _save_mode , save_format = self . _save_format , batch_size = self . _batch_size , fetch_size = self . _fetch_size , num_partitions = self . _num_partitions , partition_column = self . _partition_column , lower_bound = self . _lower_bound , upper_bound = self . _upper_bound , create_table_column_types = self . _create_table_column_types ) self . _hook . submit_jdbc_job ( )",Call the SparkSubmitHook to run the provided spark job
"def ds_add ( ds , days ) : ds = datetime . strptime ( ds , '%Y-%m-%d' ) if days : ds = ds + timedelta ( days ) return ds . isoformat ( ) [ : 10 ]",Add or subtract days from a YYYY - MM - DD
"def ds_format ( ds , input_format , output_format ) : return datetime . strptime ( ds , input_format ) . strftime ( output_format )",Takes an input string and outputs another string as specified in the output format
def _integrate_plugins ( ) : import sys from airflow . plugins_manager import macros_modules for macros_module in macros_modules : sys . modules [ macros_module . __name__ ] = macros_module globals ( ) [ macros_module . _name ] = macros_module,Integrate plugins to the context
"def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'Poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = False ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) return bool ( result )",poke matching files in a directory with self . regex
"def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = True ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) if self . be_empty : self . log . info ( 'Poking for filepath %s to a empty directory' , self . filepath ) return len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath else : self . log . info ( 'Poking for filepath %s to a non empty directory' , self . filepath ) result . pop ( 0 ) return bool ( result ) and result [ 0 ] [ 'file_type' ] == 'f'",poke for a non empty directory
"def clear_task_instances ( tis , session , activate_dag_runs = True , dag = None , ) : job_ids = [ ] for ti in tis : if ti . state == State . RUNNING : if ti . job_id : ti . state = State . SHUTDOWN job_ids . append ( ti . job_id ) else : task_id = ti . task_id if dag and dag . has_task ( task_id ) : task = dag . get_task ( task_id ) task_retries = task . retries ti . max_tries = ti . try_number + task_retries - 1 else : ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) ti . state = State . NONE session . merge ( ti ) if job_ids : from airflow . jobs import BaseJob as BJ for job in session . query ( BJ ) . filter ( BJ . id . in_ ( job_ids ) ) . all ( ) : job . state = State . SHUTDOWN if activate_dag_runs and tis : from airflow . models . dagrun import DagRun drs = session . query ( DagRun ) . filter ( DagRun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , DagRun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) for dr in drs : dr . state = State . RUNNING dr . start_date = timezone . utcnow ( )",Clears a set of task instances but makes sure the running ones get killed .
def try_number ( self ) : if self . state == State . RUNNING : return self . _try_number return self . _try_number + 1,Return the try number that this task number will be when it is actually run .
"def command ( self , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , local = False , pickle_id = None , raw = False , job_id = None , pool = None , cfg_path = None ) : return "" "" . join ( self . command_as_list ( mark_success = mark_success , ignore_all_deps = ignore_all_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps , ignore_ti_state = ignore_ti_state , local = local , pickle_id = pickle_id , raw = raw , job_id = job_id , pool = pool , cfg_path = cfg_path ) )",Returns a command that can be executed anywhere where airflow is installed . This command is part of the message sent to executors by the orchestrator .
"def command_as_list ( self , mark_success = False , ignore_all_deps = False , ignore_task_deps = False , ignore_depends_on_past = False , ignore_ti_state = False , local = False , pickle_id = None , raw = False , job_id = None , pool = None , cfg_path = None ) : dag = self . task . dag should_pass_filepath = not pickle_id and dag if should_pass_filepath and dag . full_filepath != dag . filepath : path = ""DAGS_FOLDER/{}"" . format ( dag . filepath ) elif should_pass_filepath and dag . full_filepath : path = dag . full_filepath else : path = None return TaskInstance . generate_command ( self . dag_id , self . task_id , self . execution_date , mark_success = mark_success , ignore_all_deps = ignore_all_deps , ignore_task_deps = ignore_task_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_ti_state = ignore_ti_state , local = local , pickle_id = pickle_id , file_path = path , raw = raw , job_id = job_id , pool = pool , cfg_path = cfg_path )",Returns a command that can be executed anywhere where airflow is installed . This command is part of the message sent to executors by the orchestrator .
"def generate_command ( dag_id , task_id , execution_date , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , local = False , pickle_id = None , file_path = None , raw = False , job_id = None , pool = None , cfg_path = None ) : iso = execution_date . isoformat ( ) cmd = [ ""airflow"" , ""run"" , str ( dag_id ) , str ( task_id ) , str ( iso ) ] cmd . extend ( [ ""--mark_success"" ] ) if mark_success else None cmd . extend ( [ ""--pickle"" , str ( pickle_id ) ] ) if pickle_id else None cmd . extend ( [ ""--job_id"" , str ( job_id ) ] ) if job_id else None cmd . extend ( [ ""-A"" ] ) if ignore_all_deps else None cmd . extend ( [ ""-i"" ] ) if ignore_task_deps else None cmd . extend ( [ ""-I"" ] ) if ignore_depends_on_past else None cmd . extend ( [ ""--force"" ] ) if ignore_ti_state else None cmd . extend ( [ ""--local"" ] ) if local else None cmd . extend ( [ ""--pool"" , pool ] ) if pool else None cmd . extend ( [ ""--raw"" ] ) if raw else None cmd . extend ( [ ""-sd"" , file_path ] ) if file_path else None cmd . extend ( [ ""--cfg_path"" , cfg_path ] ) if cfg_path else None return cmd",Generates the shell command required to execute this task instance .
"def current_state ( self , session = None ) : TI = TaskInstance ti = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date , ) . all ( ) if ti : state = ti [ 0 ] . state else : state = None return state",Get the very latest state from the database if a session is passed we use and looking up the state becomes part of the session otherwise a new session is used .
"def error ( self , session = None ) : self . log . error ( ""Recording the task instance as FAILED"" ) self . state = State . FAILED session . merge ( self ) session . commit ( )",Forces the task instance s state to FAILED in the database .
"def refresh_from_db ( self , session = None , lock_for_update = False ) : TI = TaskInstance qry = session . query ( TI ) . filter ( TI . dag_id == self . dag_id , TI . task_id == self . task_id , TI . execution_date == self . execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) if ti : self . state = ti . state self . start_date = ti . start_date self . end_date = ti . end_date self . try_number = ti . _try_number self . max_tries = ti . max_tries self . hostname = ti . hostname self . pid = ti . pid self . executor_config = ti . executor_config else : self . state = None",Refreshes the task instance from the database based on the primary key
"def clear_xcom_data ( self , session = None ) : session . query ( XCom ) . filter ( XCom . dag_id == self . dag_id , XCom . task_id == self . task_id , XCom . execution_date == self . execution_date ) . delete ( ) session . commit ( )",Clears all XCom data from the database for the task instance
"def key ( self ) : return self . dag_id , self . task_id , self . execution_date , self . try_number",Returns a tuple that identifies the task instance uniquely
"def are_dependents_done ( self , session = None ) : task = self . task if not task . downstream_task_ids : return True ti = session . query ( func . count ( TaskInstance . task_id ) ) . filter ( TaskInstance . dag_id == self . dag_id , TaskInstance . task_id . in_ ( task . downstream_task_ids ) , TaskInstance . execution_date == self . execution_date , TaskInstance . state == State . SUCCESS , ) count = ti [ 0 ] [ 0 ] return count == len ( task . downstream_task_ids )",Checks whether the dependents of this task instance have all succeeded . This is meant to be used by wait_for_downstream .
"def are_dependencies_met ( self , dep_context = None , session = None , verbose = False ) : dep_context = dep_context or DepContext ( ) failed = False verbose_aware_logger = self . log . info if verbose else self . log . debug for dep_status in self . get_failed_dep_statuses ( dep_context = dep_context , session = session ) : failed = True verbose_aware_logger ( ""Dependencies not met for %s, dependency '%s' FAILED: %s"" , self , dep_status . dep_name , dep_status . reason ) if failed : return False verbose_aware_logger ( ""Dependencies all met for %s"" , self ) return True",Returns whether or not all the conditions are met for this task instance to be run given the context for the dependencies ( e . g . a task instance being force run from the UI will ignore some dependencies ) .
"def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) hash = int ( hashlib . sha1 ( ""{}#{}#{}#{}"" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) modded_hash = min_backoff + hash % min_backoff delay_backoff_in_seconds = min ( modded_hash , timedelta . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay",Get datetime of the next retry if the task instance fails . For exponential backoff retry_delay is used as base and will be converted to seconds .
def ready_for_retry ( self ) : return ( self . state == State . UP_FOR_RETRY and self . next_retry_datetime ( ) < timezone . utcnow ( ) ),Checks on whether the task instance is in the right state and timeframe to be retried .
"def pool_full ( self , session ) : if not self . task . pool : return False pool = ( session . query ( Pool ) . filter ( Pool . pool == self . task . pool ) . first ( ) ) if not pool : return False open_slots = pool . open_slots ( session = session ) return open_slots <= 0",Returns a boolean as to whether the slot pool has room for this task to run
"def get_dagrun ( self , session ) : from airflow . models . dagrun import DagRun dr = session . query ( DagRun ) . filter ( DagRun . dag_id == self . dag_id , DagRun . execution_date == self . execution_date ) . first ( ) return dr",Returns the DagRun for this TaskInstance
"def _check_and_change_state_before_execution ( self , verbose = True , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , mark_success = False , test_mode = False , job_id = None , pool = None , session = None ) : task = self . task self . pool = pool or task . pool self . test_mode = test_mode self . refresh_from_db ( session = session , lock_for_update = True ) self . job_id = job_id self . hostname = get_hostname ( ) self . operator = task . __class__ . __name__ if not ignore_all_deps and not ignore_ti_state and self . state == State . SUCCESS : Stats . incr ( 'previously_succeeded' , 1 , 1 ) queue_dep_context = DepContext ( deps = QUEUE_DEPS , ignore_all_deps = ignore_all_deps , ignore_ti_state = ignore_ti_state , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps ) if not self . are_dependencies_met ( dep_context = queue_dep_context , session = session , verbose = True ) : session . commit ( ) return False hr = ""\n"" + ( ""-"" * 80 ) self . start_date = timezone . utcnow ( ) task_reschedules = TaskReschedule . find_for_task_instance ( self , session ) if task_reschedules : self . start_date = task_reschedules [ 0 ] . start_date dep_context = DepContext ( deps = RUN_DEPS - QUEUE_DEPS , ignore_all_deps = ignore_all_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps , ignore_ti_state = ignore_ti_state ) runnable = self . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) if not runnable and not mark_success : self . state = State . NONE self . log . warning ( hr ) self . log . warning ( ""FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt %s of "" ""%s. State set to NONE."" , self . try_number , self . max_tries + 1 ) self . log . warning ( hr ) self . queued_dttm = timezone . utcnow ( ) self . log . info ( ""Queuing into pool %s"" , self . pool ) session . merge ( self ) session . commit ( ) return False if self . state == State . RUNNING : self . log . warning ( ""Task Instance already running %s"" , self ) session . commit ( ) return False self . log . info ( hr ) self . log . info ( ""Starting attempt %s of %s"" , self . try_number , self . max_tries + 1 ) self . log . info ( hr ) self . _try_number += 1 if not test_mode : session . add ( Log ( State . RUNNING , self ) ) self . state = State . RUNNING self . pid = os . getpid ( ) self . end_date = None if not test_mode : session . merge ( self ) session . commit ( ) settings . engine . dispose ( ) if verbose : if mark_success : self . log . info ( ""Marking success for %s on %s"" , self . task , self . execution_date ) else : self . log . info ( ""Executing %s on %s"" , self . task , self . execution_date ) return True",Checks dependencies and then sets state to RUNNING if they are met . Returns True if and only if state is set to RUNNING which implies that task should be executed in preparation for _run_raw_task
"def _run_raw_task ( self , mark_success = False , test_mode = False , job_id = None , pool = None , session = None ) : task = self . task self . pool = pool or task . pool self . test_mode = test_mode self . refresh_from_db ( session = session ) self . job_id = job_id self . hostname = get_hostname ( ) self . operator = task . __class__ . __name__ context = { } actual_start_date = timezone . utcnow ( ) try : if not mark_success : context = self . get_template_context ( ) task_copy = copy . copy ( task ) self . task = task_copy def signal_handler ( signum , frame ) : self . log . error ( ""Received SIGTERM. Terminating subprocesses."" ) task_copy . on_kill ( ) raise AirflowException ( ""Task received SIGTERM signal"" ) signal . signal ( signal . SIGTERM , signal_handler ) self . clear_xcom_data ( ) start_time = time . time ( ) self . render_templates ( ) task_copy . pre_execute ( context = context ) result = None if task_copy . execution_timeout : try : with timeout ( int ( task_copy . execution_timeout . total_seconds ( ) ) ) : result = task_copy . execute ( context = context ) except AirflowTaskTimeout : task_copy . on_kill ( ) raise else : result = task_copy . execute ( context = context ) if task_copy . do_xcom_push and result is not None : self . xcom_push ( key = XCOM_RETURN_KEY , value = result ) task_copy . post_execute ( context = context , result = result ) end_time = time . time ( ) duration = end_time - start_time Stats . timing ( 'dag.{dag_id}.{task_id}.duration' . format ( dag_id = task_copy . dag_id , task_id = task_copy . task_id ) , duration ) Stats . incr ( 'operator_successes_{}' . format ( self . task . __class__ . __name__ ) , 1 , 1 ) Stats . incr ( 'ti_successes' ) self . refresh_from_db ( lock_for_update = True ) self . state = State . SUCCESS except AirflowSkipException : self . refresh_from_db ( lock_for_update = True ) self . state = State . SKIPPED except AirflowRescheduleException as reschedule_exception : self . refresh_from_db ( ) self . _handle_reschedule ( actual_start_date , reschedule_exception , test_mode , context ) return except AirflowException as e : self . refresh_from_db ( ) if self . state in { State . SUCCESS , State . FAILED } : return else : self . handle_failure ( e , test_mode , context ) raise except ( Exception , KeyboardInterrupt ) as e : self . handle_failure ( e , test_mode , context ) raise try : if task . on_success_callback : task . on_success_callback ( context ) except Exception as e3 : self . log . error ( ""Failed when executing success callback"" ) self . log . exception ( e3 ) self . end_date = timezone . utcnow ( ) self . set_duration ( ) if not test_mode : session . add ( Log ( self . state , self ) ) session . merge ( self ) session . commit ( )",Immediately runs the task ( without checking or changing db state before execution ) and then sets the appropriate final state after completion and runs any post - execute callbacks . Meant to be called only after another function changes the state to running .
"def xcom_push ( self , key , value , execution_date = None ) : if execution_date and execution_date < self . execution_date : raise ValueError ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) XCom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )",Make an XCom available for tasks to pull .
"def xcom_pull ( self , task_ids = None , dag_id = None , key = XCOM_RETURN_KEY , include_prior_dates = False ) : if dag_id is None : dag_id = self . dag_id pull_fn = functools . partial ( XCom . get_one , execution_date = self . execution_date , key = key , dag_id = dag_id , include_prior_dates = include_prior_dates ) if is_container ( task_ids ) : return tuple ( pull_fn ( task_id = t ) for t in task_ids ) else : return pull_fn ( task_id = task_ids )",Pull XComs that optionally meet certain criteria .
"def init_run_context ( self , raw = False ) : self . raw = raw self . _set_context ( self )",Sets the log context .
"def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb_write ( log , remote_loc , append = True ) if self . delete_local_copy : shutil . rmtree ( os . path . dirname ( local_loc ) ) self . closed = True",Close and upload local log file to remote storage Wasb .
"def _read ( self , ti , try_number , metadata = None ) : log_relative_path = self . _render_filename ( ti , try_number ) remote_loc = os . path . join ( self . remote_base , log_relative_path ) if self . wasb_log_exists ( remote_loc ) : remote_log = self . wasb_read ( remote_loc , return_error = True ) log = '*** Reading remote log from {}.\n{}\n' . format ( remote_loc , remote_log ) return log , { 'end_of_log' : True } else : return super ( ) . _read ( ti , try_number )",Read logs of given task instance and try_number from Wasb remote storage . If failed read the log from task instance host machine . : param ti : task instance object : param try_number : task instance try_number to read logs from : param metadata : log metadata can be used for steaming log reading and auto - tailing .
"def wasb_log_exists ( self , remote_log_location ) : try : return self . hook . check_for_blob ( self . wasb_container , remote_log_location ) except Exception : pass return False",Check if remote_log_location exists in remote storage : param remote_log_location : log s location in remote storage : return : True if location exists else False
"def wasb_read ( self , remote_log_location , return_error = False ) : try : return self . hook . read_file ( self . wasb_container , remote_log_location ) except AzureHttpError : msg = 'Could not read logs from {}' . format ( remote_log_location ) self . log . exception ( msg ) if return_error : return msg",Returns the log found at the remote_log_location . Returns if no logs are found or there is an error . : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param return_error : if True returns a string error message if an error occurs . Otherwise returns when an error occurs . : type return_error : bool
"def wasb_write ( self , log , remote_log_location , append = True ) : if append and self . wasb_log_exists ( remote_log_location ) : old_log = self . wasb_read ( remote_log_location ) log = '\n' . join ( [ old_log , log ] ) if old_log else log try : self . hook . load_string ( log , self . wasb_container , remote_log_location , ) except AzureHttpError : self . log . exception ( 'Could not write logs to %s' , remote_log_location )",Writes the log to the remote_log_location . Fails silently if no hook was created . : param log : the log to write to the remote_log_location : type log : str : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param append : if False any existing log file is overwritten . If True the new log is appended to any existing logs . : type append : bool
"def get_conn ( self ) : if not self . _conn : http_authorized = self . _authorize ( ) self . _conn = build ( 'compute' , self . api_version , http = http_authorized , cache_discovery = False ) return self . _conn",Retrieves connection to Google Compute Engine .
"def start_instance ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instances ( ) . start ( project = project_id , zone = zone , instance = resource_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except KeyError : raise AirflowException ( ""Wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )",Starts an existing instance defined by project_id zone and resource_id . Must be called with keyword arguments rather than positional .
"def set_machine_type ( self , zone , resource_id , body , project_id = None ) : response = self . _execute_set_machine_type ( zone , resource_id , body , project_id ) try : operation_name = response [ ""name"" ] except KeyError : raise AirflowException ( ""Wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )",Sets machine type of an instance defined by project_id zone and resource_id . Must be called with keyword arguments rather than positional .
"def get_instance_template ( self , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . get ( project = project_id , instanceTemplate = resource_id ) . execute ( num_retries = self . num_retries ) return response",Retrieves instance template by project_id and resource_id . Must be called with keyword arguments rather than positional .
"def insert_instance_template ( self , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceTemplates ( ) . insert ( project = project_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except KeyError : raise AirflowException ( ""Wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )",Inserts instance template using body specified Must be called with keyword arguments rather than positional .
"def get_instance_group_manager ( self , zone , resource_id , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . get ( project = project_id , zone = zone , instanceGroupManager = resource_id ) . execute ( num_retries = self . num_retries ) return response",Retrieves Instance Group Manager by project_id zone and resource_id . Must be called with keyword arguments rather than positional .
"def patch_instance_group_manager ( self , zone , resource_id , body , request_id = None , project_id = None ) : response = self . get_conn ( ) . instanceGroupManagers ( ) . patch ( project = project_id , zone = zone , instanceGroupManager = resource_id , body = body , requestId = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except KeyError : raise AirflowException ( ""Wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )",Patches Instance Group Manager with the specified body . Must be called with keyword arguments rather than positional .
"def _wait_for_operation_to_complete ( self , project_id , operation_name , zone = None ) : service = self . get_conn ( ) while True : if zone is None : operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) else : operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) if operation_response . get ( ""status"" ) == GceOperationStatus . DONE : error = operation_response . get ( ""error"" ) if error : code = operation_response . get ( ""httpErrorStatusCode"" ) msg = operation_response . get ( ""httpErrorMessage"" ) error_msg = str ( error . get ( ""errors"" ) ) [ 1 : - 1 ] raise AirflowException ( ""{} {}: "" . format ( code , msg ) + error_msg ) return time . sleep ( TIME_TO_SLEEP_IN_SECONDS )",Waits for the named operation to complete - checks status of the async call .
"def check_for_bucket ( self , bucket_name ) : try : self . get_conn ( ) . head_bucket ( Bucket = bucket_name ) return True except ClientError as e : self . log . info ( e . response [ ""Error"" ] [ ""Message"" ] ) return False",Check if bucket_name exists .
"def create_bucket ( self , bucket_name , region_name = None ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . meta . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( Bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( Bucket = bucket_name , CreateBucketConfiguration = { 'LocationConstraint' : region_name } )",Creates an Amazon S3 bucket .
"def check_for_prefix ( self , bucket_name , prefix , delimiter ) : prefix = prefix + delimiter if prefix [ - 1 ] != delimiter else prefix prefix_split = re . split ( r'(\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) previous_level = prefix_split [ 0 ] plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) return False if plist is None else prefix in plist",Checks that a prefix exists in a bucket
"def list_prefixes ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results = False prefixes = [ ] for page in response : if 'CommonPrefixes' in page : has_results = True for p in page [ 'CommonPrefixes' ] : prefixes . append ( p [ 'Prefix' ] ) if has_results : return prefixes",Lists prefixes in a bucket under prefix
"def list_keys ( self , bucket_name , prefix = '' , delimiter = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( Bucket = bucket_name , Prefix = prefix , Delimiter = delimiter , PaginationConfig = config ) has_results = False keys = [ ] for page in response : if 'Contents' in page : has_results = True for k in page [ 'Contents' ] : keys . append ( k [ 'Key' ] ) if has_results : return keys",Lists keys in a bucket under prefix and not containing delimiter
"def check_for_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( Bucket = bucket_name , Key = key ) return True except ClientError as e : self . log . info ( e . response [ ""Error"" ] [ ""Message"" ] ) return False",Checks if a key exists in a bucket
"def get_key ( self , key , bucket_name = None ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) obj = self . get_resource_type ( 's3' ) . Object ( bucket_name , key ) obj . load ( ) return obj",Returns a boto3 . s3 . Object
"def read_key ( self , key , bucket_name = None ) : obj = self . get_key ( key , bucket_name ) return obj . get ( ) [ 'Body' ] . read ( ) . decode ( 'utf-8' )",Reads a key from S3
"def select_key ( self , key , bucket_name = None , expression = 'SELECT * FROM S3Object' , expression_type = 'SQL' , input_serialization = None , output_serialization = None ) : if input_serialization is None : input_serialization = { 'CSV' : { } } if output_serialization is None : output_serialization = { 'CSV' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( Bucket = bucket_name , Key = key , Expression = expression , ExpressionType = expression_type , InputSerialization = input_serialization , OutputSerialization = output_serialization ) return '' . join ( event [ 'Records' ] [ 'Payload' ] . decode ( 'utf-8' ) for event in response [ 'Payload' ] if 'Records' in event )",Reads a key with S3 Select .
"def check_for_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : return self . get_wildcard_key ( wildcard_key = wildcard_key , bucket_name = bucket_name , delimiter = delimiter ) is not None",Checks that a key matching a wildcard expression exists in a bucket
"def get_wildcard_key ( self , wildcard_key , bucket_name = None , delimiter = '' ) : if not bucket_name : ( bucket_name , wildcard_key ) = self . parse_s3_url ( wildcard_key ) prefix = re . split ( r'[*]' , wildcard_key , 1 ) [ 0 ] klist = self . list_keys ( bucket_name , prefix = prefix , delimiter = delimiter ) if klist : key_matches = [ k for k in klist if fnmatch . fnmatch ( k , wildcard_key ) ] if key_matches : return self . get_key ( key_matches [ 0 ] , bucket_name )",Returns a boto3 . s3 . Object object matching the wildcard expression
"def load_file ( self , filename , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( ""The key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = ""AES256"" client = self . get_conn ( ) client . upload_file ( filename , bucket_name , key , ExtraArgs = extra_args )",Loads a local file to S3
"def load_string ( self , string_data , key , bucket_name = None , replace = False , encrypt = False , encoding = 'utf-8' ) : self . load_bytes ( string_data . encode ( encoding ) , key = key , bucket_name = bucket_name , replace = replace , encrypt = encrypt )",Loads a string to S3
"def load_bytes ( self , bytes_data , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( ""The key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = ""AES256"" filelike_buffer = BytesIO ( bytes_data ) client = self . get_conn ( ) client . upload_fileobj ( filelike_buffer , bucket_name , key , ExtraArgs = extra_args )",Loads bytes to S3
"def load_file_obj ( self , file_obj , key , bucket_name = None , replace = False , encrypt = False ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise ValueError ( ""The key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'ServerSideEncryption' ] = ""AES256"" client = self . get_conn ( ) client . upload_fileobj ( file_obj , bucket_name , key , ExtraArgs = extra_args )",Loads a file object to S3
"def copy_object ( self , source_bucket_key , dest_bucket_key , source_bucket_name = None , dest_bucket_name = None , source_version_id = None ) : if dest_bucket_name is None : dest_bucket_name , dest_bucket_key = self . parse_s3_url ( dest_bucket_key ) else : parsed_url = urlparse ( dest_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If dest_bucket_name is provided, ' + 'dest_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) if source_bucket_name is None : source_bucket_name , source_bucket_key = self . parse_s3_url ( source_bucket_key ) else : parsed_url = urlparse ( source_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise AirflowException ( 'If source_bucket_name is provided, ' + 'source_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) CopySource = { 'Bucket' : source_bucket_name , 'Key' : source_bucket_key , 'VersionId' : source_version_id } response = self . get_conn ( ) . copy_object ( Bucket = dest_bucket_name , Key = dest_bucket_key , CopySource = CopySource ) return response",Creates a copy of an object that is already stored in S3 .
"def delete_objects ( self , bucket , keys ) : if isinstance ( keys , list ) : keys = keys else : keys = [ keys ] delete_dict = { ""Objects"" : [ { ""Key"" : k } for k in keys ] } response = self . get_conn ( ) . delete_objects ( Bucket = bucket , Delete = delete_dict ) return response",: param bucket : Name of the bucket in which you are going to delete object ( s ) : type bucket : str : param keys : The key ( s ) to delete from S3 bucket .
def _query_cassandra ( self ) : self . hook = CassandraHook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute ( self . cql ) return cursor,Queries cassandra and returns a cursor to the results .
"def _write_local_data_files ( self , cursor ) : file_no = 0 tmp_file_handle = NamedTemporaryFile ( delete = True ) tmp_file_handles = { self . filename . format ( file_no ) : tmp_file_handle } for row in cursor : row_dict = self . generate_data_dict ( row . _fields , row ) s = json . dumps ( row_dict ) . encode ( 'utf-8' ) tmp_file_handle . write ( s ) tmp_file_handle . write ( b'\n' ) if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : file_no += 1 tmp_file_handle = NamedTemporaryFile ( delete = True ) tmp_file_handles [ self . filename . format ( file_no ) ] = tmp_file_handle return tmp_file_handles",Takes a cursor and writes results to a local file .
"def _write_local_schema_file ( self , cursor ) : schema = [ ] tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) for name , type in zip ( cursor . column_names , cursor . column_types ) : schema . append ( self . generate_schema_dict ( name , type ) ) json_serialized_schema = json . dumps ( schema ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( json_serialized_schema ) return { self . schema_filename : tmp_schema_file_handle }",Takes a cursor and writes the BigQuery schema for the results to a local file system .
"def convert_user_type ( cls , name , value ) : names = value . _fields values = [ cls . convert_value ( name , getattr ( value , name ) ) for name in names ] return cls . generate_data_dict ( names , values )",Converts a user type to RECORD that contains n fields where n is the number of attributes . Each element in the user type class will be converted to its corresponding data type in BQ .
"def convert_tuple_type ( cls , name , value ) : names = [ 'field_' + str ( i ) for i in range ( len ( value ) ) ] values = [ cls . convert_value ( name , value ) for name , value in zip ( names , value ) ] return cls . generate_data_dict ( names , values )",Converts a tuple to RECORD that contains n fields each will be converted to its corresponding data type in bq and will be named field_<index > where index is determined by the order of the tuple elements defined in cassandra .
"def convert_map_type ( cls , name , value ) : converted_map = [ ] for k , v in zip ( value . keys ( ) , value . values ( ) ) : converted_map . append ( { 'key' : cls . convert_value ( 'key' , k ) , 'value' : cls . convert_value ( 'value' , v ) } ) return converted_map",Converts a map to a repeated RECORD that contains two fields : key and value each will be converted to its corresponding data type in BQ .
"def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , sandbox_mode = False , * * kwargs ) : if files is None : files = [ ] mail = Mail ( ) from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'SENDGRID_MAIL_FROM' ) from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'SENDGRID_MAIL_SENDER' ) mail . from_email = Email ( from_email , from_name ) mail . subject = subject mail . mail_settings = MailSettings ( ) if sandbox_mode : mail . mail_settings . sandbox_mode = SandBoxMode ( enable = True ) personalization = Personalization ( ) to = get_email_address_list ( to ) for to_address in to : personalization . add_to ( Email ( to_address ) ) if cc : cc = get_email_address_list ( cc ) for cc_address in cc : personalization . add_cc ( Email ( cc_address ) ) if bcc : bcc = get_email_address_list ( bcc ) for bcc_address in bcc : personalization . add_bcc ( Email ( bcc_address ) ) pers_custom_args = kwargs . get ( 'personalization_custom_args' , None ) if isinstance ( pers_custom_args , dict ) : for key in pers_custom_args . keys ( ) : personalization . add_custom_arg ( CustomArg ( key , pers_custom_args [ key ] ) ) mail . add_personalization ( personalization ) mail . add_content ( Content ( 'text/html' , html_content ) ) categories = kwargs . get ( 'categories' , [ ] ) for cat in categories : mail . add_category ( Category ( cat ) ) for fname in files : basename = os . path . basename ( fname ) attachment = Attachment ( ) attachment . type = mimetypes . guess_type ( basename ) [ 0 ] attachment . filename = basename attachment . disposition = ""attachment"" attachment . content_id = '<{0}>' . format ( basename ) with open ( fname , ""rb"" ) as f : attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) mail . add_attachment ( attachment ) _post_sendgrid_mail ( mail . get ( ) )",Send an email with html content using sendgrid .
def get_conn ( self ) : if not self . _client : self . _client = SpeechClient ( credentials = self . _get_credentials ( ) ) return self . _client,Retrieves connection to Cloud Speech .
"def recognize_speech ( self , config , audio , retry = None , timeout = None ) : client = self . get_conn ( ) response = client . recognize ( config = config , audio = audio , retry = retry , timeout = timeout ) self . log . info ( ""Recognised speech: %s"" % response ) return response",Recognizes audio input
"def execute ( self , context ) : self . _hook = SparkSqlHook ( sql = self . _sql , conf = self . _conf , conn_id = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )",Call the SparkSqlHook to run the provided sql query
"def set_context ( self , ti ) : local_loc = self . _init_file ( ti ) self . handler = logging . FileHandler ( local_loc ) self . handler . setFormatter ( self . formatter ) self . handler . setLevel ( self . level )",Provide task_instance context to airflow task handler . : param ti : task instance object
"def _read ( self , ti , try_number , metadata = None ) : log_relative_path = self . _render_filename ( ti , try_number ) location = os . path . join ( self . local_base , log_relative_path ) log = """" if os . path . exists ( location ) : try : with open ( location ) as f : log += ""*** Reading local file: {}\n"" . format ( location ) log += """" . join ( f . readlines ( ) ) except Exception as e : log = ""*** Failed to load local log file: {}\n"" . format ( location ) log += ""*** {}\n"" . format ( str ( e ) ) else : url = os . path . join ( ""http://{ti.hostname}:{worker_log_server_port}/log"" , log_relative_path ) . format ( ti = ti , worker_log_server_port = conf . get ( 'celery' , 'WORKER_LOG_SERVER_PORT' ) ) log += ""*** Log file does not exist: {}\n"" . format ( location ) log += ""*** Fetching from: {}\n"" . format ( url ) try : timeout = None try : timeout = conf . getint ( 'webserver' , 'log_fetch_timeout_sec' ) except ( AirflowConfigException , ValueError ) : pass response = requests . get ( url , timeout = timeout ) response . raise_for_status ( ) log += '\n' + response . text except Exception as e : log += ""*** Failed to fetch log file from worker. {}\n"" . format ( str ( e ) ) return log , { 'end_of_log' : True }",Template method that contains custom logic of reading logs given the try_number . : param ti : task instance record : param try_number : current try_number to read log from : param metadata : log metadata can be used for steaming log reading and auto - tailing . : return : log message as a string and metadata .
"def read ( self , task_instance , try_number = None , metadata = None ) : if try_number is None : next_try = task_instance . next_try_number try_numbers = list ( range ( 1 , next_try ) ) elif try_number < 1 : logs = [ 'Error fetching the logs. Try number {} is invalid.' . format ( try_number ) , ] return logs else : try_numbers = [ try_number ] logs = [ '' ] * len ( try_numbers ) metadatas = [ { } ] * len ( try_numbers ) for i , try_number in enumerate ( try_numbers ) : log , metadata = self . _read ( task_instance , try_number , metadata ) logs [ i ] += log metadatas [ i ] = metadata return logs , metadatas",Read logs of given task instance from local machine . : param task_instance : task instance object : param try_number : task instance try_number to read logs from . If None it returns all logs separated by try_number : param metadata : log metadata can be used for steaming log reading and auto - tailing . : return : a list of logs
"def _init_file ( self , ti ) : relative_path = self . _render_filename ( ti , ti . try_number ) full_path = os . path . join ( self . local_base , relative_path ) directory = os . path . dirname ( full_path ) if not os . path . exists ( directory ) : mkdirs ( directory , 0o777 ) if not os . path . exists ( full_path ) : open ( full_path , ""a"" ) . close ( ) os . chmod ( full_path , 0o666 ) return full_path",Create log directory and give it correct permissions . : param ti : task instance object : return : relative log path of the given task instance
"def load_entrypoint_plugins ( entry_points , airflow_plugins ) : for entry_point in entry_points : log . debug ( 'Importing entry_point plugin %s' , entry_point . name ) plugin_obj = entry_point . load ( ) if is_valid_plugin ( plugin_obj , airflow_plugins ) : if callable ( getattr ( plugin_obj , 'on_load' , None ) ) : plugin_obj . on_load ( ) airflow_plugins . append ( plugin_obj ) return airflow_plugins",Load AirflowPlugin subclasses from the entrypoints provided . The entry_point group should be airflow . plugins .
"def is_valid_plugin ( plugin_obj , existing_plugins ) : if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , AirflowPlugin ) and ( plugin_obj is not AirflowPlugin ) ) : plugin_obj . validate ( ) return plugin_obj not in existing_plugins return False",Check whether a potential object is a subclass of the AirflowPlugin class .
"def skip ( self , dag_run , execution_date , tasks , session = None ) : if not tasks : return task_ids = [ d . task_id for d in tasks ] now = timezone . utcnow ( ) if dag_run : session . query ( TaskInstance ) . filter ( TaskInstance . dag_id == dag_run . dag_id , TaskInstance . execution_date == dag_run . execution_date , TaskInstance . task_id . in_ ( task_ids ) ) . update ( { TaskInstance . state : State . SKIPPED , TaskInstance . start_date : now , TaskInstance . end_date : now } , synchronize_session = False ) session . commit ( ) else : assert execution_date is not None , ""Execution date is None and no dag run"" self . log . warning ( ""No DAG RUN present this should not happen"" ) for task in tasks : ti = TaskInstance ( task , execution_date = execution_date ) ti . state = State . SKIPPED ti . start_date = now ti . end_date = now session . merge ( ti ) session . commit ( )",Sets tasks instances to skipped from the same dag run .
"def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson self . account_name = service_options . get ( 'account_name' ) adlCreds = lib . auth ( tenant_id = service_options . get ( 'tenant' ) , client_secret = conn . password , client_id = conn . login ) adlsFileSystemClient = core . AzureDLFileSystem ( adlCreds , store_name = self . account_name ) adlsFileSystemClient . connect ( ) return adlsFileSystemClient",Return a AzureDLFileSystem object .
"def check_for_file ( self , file_path ) : try : files = self . connection . glob ( file_path , details = False , invalidate_cache = True ) return len ( files ) == 1 except FileNotFoundError : return False",Check if a file exists on Azure Data Lake .
"def upload_file ( self , local_path , remote_path , nthreads = 64 , overwrite = True , buffersize = 4194304 , blocksize = 4194304 ) : multithread . ADLUploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize )",Upload a file to Azure Data Lake .
"def download_file ( self , local_path , remote_path , nthreads = 64 , overwrite = True , buffersize = 4194304 , blocksize = 4194304 ) : multithread . ADLDownloader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize )",Download a file from Azure Blob Storage .
"def list ( self , path ) : if ""*"" in path : return self . connection . glob ( path ) else : return self . connection . walk ( path )",List files in Azure Data Lake Storage
"def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . query_execution_context [ 'Database' ] = self . database self . result_configuration [ 'OutputLocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in AWSAthenaHook . FAILURE_STATES : raise Exception ( 'Final state of Athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in AWSAthenaHook . INTERMEDIATE_STATES : raise Exception ( 'Final state of Athena job is {}. ' 'Max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )",Run Presto Query on Athena
"def on_kill ( self ) : if self . query_execution_id : self . log . info ( ' Received a kill Signal. Time to Die') self . log . info ( 'Stopping Query with executionId - %s' , self . query_execution_id ) response = self . hook . stop_query ( self . query_execution_id ) http_status_code = None try : http_status_code = response [ 'ResponseMetadata' ] [ 'HTTPStatusCode' ] except Exception as ex : self . log . error ( 'Exception while cancelling query' , ex ) finally : if http_status_code is None or http_status_code != 200 : self . log . error ( 'Unable to request query cancel on athena. Exiting' ) else : self . log . info ( 'Polling Athena for query with id %s to reach final state' , self . query_execution_id ) self . hook . poll_query_status ( self . query_execution_id )",Cancel the submitted athena query
"def uncompress_file ( input_file_name , file_extension , dest_dir ) : if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise NotImplementedError ( ""Received {} format. Only gz and bz2 "" ""files can currently be uncompressed."" . format ( file_extension ) ) if file_extension . lower ( ) == '.gz' : fmodule = gzip . GzipFile elif file_extension . lower ( ) == '.bz2' : fmodule = bz2 . BZ2File with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , NamedTemporaryFile ( dir = dest_dir , mode = 'wb' , delete = False ) as f_uncompressed : shutil . copyfileobj ( f_compressed , f_uncompressed ) return f_uncompressed . name",Uncompress gz and bz2 files
def _query_mssql ( self ) : mssql = MsSqlHook ( mssql_conn_id = self . mssql_conn_id ) conn = mssql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor,Queries MSSQL and returns a cursor of results .
"def _write_local_data_files ( self , cursor ) : schema = list ( map ( lambda schema_tuple : schema_tuple [ 0 ] . replace ( ' ' , '_' ) , cursor . description ) ) file_no = 0 tmp_file_handle = NamedTemporaryFile ( delete = True ) tmp_file_handles = { self . filename . format ( file_no ) : tmp_file_handle } for row in cursor : row = map ( self . convert_types , row ) row_dict = dict ( zip ( schema , row ) ) s = json . dumps ( row_dict , sort_keys = True ) s = s . encode ( 'utf-8' ) tmp_file_handle . write ( s ) tmp_file_handle . write ( b'\n' ) if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : file_no += 1 tmp_file_handle = NamedTemporaryFile ( delete = True ) tmp_file_handles [ self . filename . format ( file_no ) ] = tmp_file_handle return tmp_file_handles",Takes a cursor and writes results to a local file .
"def _upload_to_gcs ( self , files_to_upload ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) for object_name , tmp_file_handle in files_to_upload . items ( ) : hook . upload ( self . bucket , object_name , tmp_file_handle . name , 'application/json' , ( self . gzip if object_name != self . schema_filename else False ) )",Upload all of the file splits ( and optionally the schema . json file ) to Google cloud storage .
"def convert_types ( cls , value ) : if isinstance ( value , decimal . Decimal ) : return float ( value ) else : return value",Takes a value from MSSQL and converts it to a value that s safe for JSON / Google Cloud Storage / BigQuery .
"def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : """"""
        An wrapper for cli functions. It assumes to have Namespace instance
        at 1st positional argument
        :param args: Positional argument. It assumes to have Namespace instance
        at 1st positional argument
        :param kwargs: A passthrough keyword argument
        """""" assert args assert isinstance ( args [ 0 ] , Namespace ) , ""1st positional argument should be argparse.Namespace instance, "" ""but {}"" . format ( args [ 0 ] ) metrics = _build_metrics ( f . __name__ , args [ 0 ] ) cli_action_loggers . on_pre_execution ( * * metrics ) try : return f ( * args , * * kwargs ) except Exception as e : metrics [ 'error' ] = e raise finally : metrics [ 'end_datetime' ] = datetime . utcnow ( ) cli_action_loggers . on_post_execution ( * * metrics ) return wrapper",Decorates function to execute function at the same time submitting action_logging but in CLI context . It will call action logger callbacks twice one for pre - execution and the other one for post - execution .
"def _build_metrics ( func_name , namespace ) : metrics = { 'sub_command' : func_name , 'start_datetime' : datetime . utcnow ( ) , 'full_command' : '{}' . format ( list ( sys . argv ) ) , 'user' : getpass . getuser ( ) } assert isinstance ( namespace , Namespace ) tmp_dic = vars ( namespace ) metrics [ 'dag_id' ] = tmp_dic . get ( 'dag_id' ) metrics [ 'task_id' ] = tmp_dic . get ( 'task_id' ) metrics [ 'execution_date' ] = tmp_dic . get ( 'execution_date' ) metrics [ 'host_name' ] = socket . gethostname ( ) extra = json . dumps ( dict ( ( k , metrics [ k ] ) for k in ( 'host_name' , 'full_command' ) ) ) log = Log ( event = 'cli_{}' . format ( func_name ) , task_instance = None , owner = metrics [ 'user' ] , extra = extra , task_id = metrics . get ( 'task_id' ) , dag_id = metrics . get ( 'dag_id' ) , execution_date = metrics . get ( 'execution_date' ) ) metrics [ 'log' ] = log return metrics",Builds metrics dict from function args It assumes that function arguments is from airflow . bin . cli module s function and has Namespace instance where it optionally contains dag_id task_id and execution_date .
"def _evaluate_trigger_rule ( self , ti , successes , skipped , failed , upstream_failed , done , flag_upstream_failed , session ) : TR = airflow . utils . trigger_rule . TriggerRule task = ti . task upstream = len ( task . upstream_task_ids ) tr = task . trigger_rule upstream_done = done >= upstream upstream_tasks_state = { ""total"" : upstream , ""successes"" : successes , ""skipped"" : skipped , ""failed"" : failed , ""upstream_failed"" : upstream_failed , ""done"" : done } if flag_upstream_failed : if tr == TR . ALL_SUCCESS : if upstream_failed or failed : ti . set_state ( State . UPSTREAM_FAILED , session ) elif skipped : ti . set_state ( State . SKIPPED , session ) elif tr == TR . ALL_FAILED : if successes or skipped : ti . set_state ( State . SKIPPED , session ) elif tr == TR . ONE_SUCCESS : if upstream_done and not successes : ti . set_state ( State . SKIPPED , session ) elif tr == TR . ONE_FAILED : if upstream_done and not ( failed or upstream_failed ) : ti . set_state ( State . SKIPPED , session ) elif tr == TR . NONE_FAILED : if upstream_failed or failed : ti . set_state ( State . UPSTREAM_FAILED , session ) elif skipped == upstream : ti . set_state ( State . SKIPPED , session ) elif tr == TR . NONE_SKIPPED : if skipped : ti . set_state ( State . SKIPPED , session ) if tr == TR . ONE_SUCCESS : if successes <= 0 : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires one upstream "" ""task success, but none were found. "" ""upstream_tasks_state={1}, upstream_task_ids={2}"" . format ( tr , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . ONE_FAILED : if not failed and not upstream_failed : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires one upstream "" ""task failure, but none were found. "" ""upstream_tasks_state={1}, upstream_task_ids={2}"" . format ( tr , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . ALL_SUCCESS : num_failures = upstream - successes if num_failures > 0 : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires all upstream "" ""tasks to have succeeded, but found {1} non-success(es). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_failures , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . ALL_FAILED : num_successes = upstream - failed - upstream_failed if num_successes > 0 : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires all upstream "" ""tasks to have failed, but found {1} non-failure(s). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_successes , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . ALL_DONE : if not upstream_done : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires all upstream "" ""tasks to have completed, but found {1} task(s) that "" ""weren't done. upstream_tasks_state={2}, "" ""upstream_task_ids={3}"" . format ( tr , upstream_done , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . NONE_FAILED : num_failures = upstream - successes - skipped if num_failures > 0 : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires all upstream "" ""tasks to have succeeded or been skipped, but found {1} non-success(es). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_failures , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == TR . NONE_SKIPPED : if skipped > 0 : yield self . _failing_status ( reason = ""Task's trigger rule '{0}' requires all upstream "" ""tasks to not have been skipped, but found {1} task(s) skipped. "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , skipped , upstream_tasks_state , task . upstream_task_ids ) ) else : yield self . _failing_status ( reason = ""No strategy to evaluate trigger rule '{0}'."" . format ( tr ) )",Yields a dependency status that indicate whether the given task instance s trigger rule was met .
"def _create_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( os . sep ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . debug ( ""Creating cgroup %s in %s"" , path_element , node . path ) node = node . create_cgroup ( path_element ) else : self . log . debug ( ""Not creating cgroup %s in %s since it already exists"" , path_element , node . path ) node = name_to_node [ path_element ] return node",Create the specified cgroup .
"def _delete_cgroup ( self , path ) : node = trees . Tree ( ) . root path_split = path . split ( ""/"" ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . warning ( ""Cgroup does not exist: %s"" , path ) return else : node = name_to_node [ path_element ] parent = node . parent self . log . debug ( ""Deleting cgroup %s/%s"" , parent , node . name ) parent . delete_cgroup ( node . name )",Delete the specified cgroup .
"def _get_cgroup_names ( ) : with open ( ""/proc/self/cgroup"" ) as f : lines = f . readlines ( ) d = { } for line in lines : line_split = line . rstrip ( ) . split ( "":"" ) subsystem = line_split [ 1 ] group_name = line_split [ 2 ] d [ subsystem ] = group_name return d",: return : a mapping between the subsystem name to the cgroup name : rtype : dict [ str str ]
def _parse_host ( host ) : urlparse_host = urlparse . urlparse ( host ) . hostname if urlparse_host : return urlparse_host else : return host,The purpose of this function is to be robust to improper connections settings provided by users specifically in the host field .
"def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'Using token auth.' ) auth = _TokenAuth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'Using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'GET' : request_func = requests . get elif method == 'POST' : request_func = requests . post else : raise AirflowException ( 'Unexpected HTTP Method: ' + method ) attempt_num = 1 while True : try : response = request_func ( url , json = json , auth = auth , headers = USER_AGENT_HEADER , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . RequestException as e : if not _retryable_error ( e ) : raise AirflowException ( 'Response: {0}, Status Code: {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise AirflowException ( ( 'API requests to Databricks failed {} times. ' + 'Giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )",Utility function to perform an API call with retries
"def get_conn ( self ) : if not self . conn : connection = self . get_connection ( self . conn_id ) extras = connection . extra_dejson self . conn = Salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url = connection . host , sandbox = extras . get ( 'sandbox' , False ) ) return self . conn",Sign into Salesforce only if we are not already signed in .
"def make_query ( self , query ) : conn = self . get_conn ( ) self . log . info ( ""Querying for all objects"" ) query_results = conn . query_all ( query ) self . log . info ( ""Received results: Total size: %s; Done: %s"" , query_results [ 'totalSize' ] , query_results [ 'done' ] ) return query_results",Make a query to Salesforce .
"def describe_object ( self , obj ) : conn = self . get_conn ( ) return conn . __getattr__ ( obj ) . describe ( )",Get the description of an object from Salesforce . This description is the object s schema and some extra metadata that Salesforce stores for each object .
"def get_available_fields ( self , obj ) : self . get_conn ( ) obj_description = self . describe_object ( obj ) return [ field [ 'name' ] for field in obj_description [ 'fields' ] ]",Get a list of all available fields for an object .
"def get_object_from_salesforce ( self , obj , fields ) : query = ""SELECT {} FROM {}"" . format ( "","" . join ( fields ) , obj ) self . log . info ( ""Making query to Salesforce: %s"" , query if len ( query ) < 30 else "" ... "" . join ( [ query [ : 15 ] , query [ - 15 : ] ] ) ) return self . make_query ( query )",Get all instances of the object from Salesforce . For each model only get the fields specified in fields .
"def _to_timestamp ( cls , column ) : try : column = pd . to_datetime ( column ) except ValueError : log = LoggingMixin ( ) . log log . warning ( ""Could not convert field to timestamps: %s"" , column . name ) return column converted = [ ] for value in column : try : converted . append ( value . timestamp ( ) ) except ( ValueError , AttributeError ) : converted . append ( pd . np . NaN ) return pd . Series ( converted , index = column . index )",Convert a column of a dataframe to UNIX timestamps if applicable
"def write_object_to_file ( self , query_results , filename , fmt = ""csv"" , coerce_to_timestamp = False , record_time_added = False ) : fmt = fmt . lower ( ) if fmt not in [ 'csv' , 'json' , 'ndjson' ] : raise ValueError ( ""Format value is not recognized: {}"" . format ( fmt ) ) df = pd . DataFrame . from_records ( query_results , exclude = [ ""attributes"" ] ) df . columns = [ column . lower ( ) for column in df . columns ] if coerce_to_timestamp and df . shape [ 0 ] > 0 : object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] self . log . info ( ""Coercing timestamps for: %s"" , object_name ) schema = self . describe_object ( object_name ) possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ ""date"" , ""datetime"" ] and field [ 'name' ] . lower ( ) in df . columns ] df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) if record_time_added : fetched_time = time . time ( ) df [ ""time_fetched_from_salesforce"" ] = fetched_time if fmt == ""csv"" : self . log . info ( ""Cleaning data and writing to CSV"" ) possible_strings = df . columns [ df . dtypes == ""object"" ] df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( ""\r\n"" , """" ) . str . replace ( ""\n"" , """" ) ) df . to_csv ( filename , index = False ) elif fmt == ""json"" : df . to_json ( filename , ""records"" , date_unit = ""s"" ) elif fmt == ""ndjson"" : df . to_json ( filename , ""records"" , lines = True , date_unit = ""s"" ) return df",Write query results to file .
"def _read ( self , ti , try_number , metadata = None ) : log_relative_path = self . _render_filename ( ti , try_number ) remote_loc = os . path . join ( self . remote_base , log_relative_path ) try : remote_log = self . gcs_read ( remote_loc ) log = '*** Reading remote log from {}.\n{}\n' . format ( remote_loc , remote_log ) return log , { 'end_of_log' : True } except Exception as e : log = '*** Unable to read remote log from {}\n*** {}\n\n' . format ( remote_loc , str ( e ) ) self . log . error ( log ) local_log , metadata = super ( ) . _read ( ti , try_number ) log += local_log return log , metadata",Read logs of given task instance and try_number from GCS . If failed read the log from task instance host machine . : param ti : task instance object : param try_number : task instance try_number to read logs from : param metadata : log metadata can be used for steaming log reading and auto - tailing .
"def gcs_read ( self , remote_log_location ) : bkt , blob = self . parse_gcs_url ( remote_log_location ) return self . hook . download ( bkt , blob ) . decode ( 'utf-8' )",Returns the log found at the remote_log_location . : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path )
"def gcs_write ( self , log , remote_log_location , append = True ) : if append : try : old_log = self . gcs_read ( remote_log_location ) log = '\n' . join ( [ old_log , log ] ) if old_log else log except Exception as e : if not hasattr ( e , 'resp' ) or e . resp . get ( 'status' ) != '404' : log = '*** Previous log discarded: {}\n\n' . format ( str ( e ) ) + log try : bkt , blob = self . parse_gcs_url ( remote_log_location ) from tempfile import NamedTemporaryFile with NamedTemporaryFile ( mode = 'w+' ) as tmpfile : tmpfile . write ( log ) tmpfile . flush ( ) self . hook . upload ( bkt , blob , tmpfile . name ) except Exception as e : self . log . error ( 'Could not write logs to %s: %s' , remote_log_location , e )",Writes the log to the remote_log_location . Fails silently if no hook was created . : param log : the log to write to the remote_log_location : type log : str : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param append : if False any existing log file is overwritten . If True the new log is appended to any existing logs . : type append : bool
"def parse_gcs_url ( gsurl ) : parsed_url = urlparse ( gsurl ) if not parsed_url . netloc : raise AirflowException ( 'Please provide a bucket name' ) else : bucket = parsed_url . netloc blob = parsed_url . path . strip ( '/' ) return bucket , blob",Given a Google Cloud Storage URL ( gs : // <bucket > / <blob > ) returns a tuple containing the corresponding bucket and blob .
"def get_conn ( self ) : if self . client is not None : return self . client options = self . extras if options . get ( 'ssl' , False ) : options . update ( { 'ssl_cert_reqs' : CERT_NONE } ) self . client = MongoClient ( self . uri , * * options ) return self . client",Fetches PyMongo Client
"def get_collection ( self , mongo_collection , mongo_db = None ) : mongo_db = mongo_db if mongo_db is not None else self . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )",Fetches a mongo collection object for querying .
"def aggregate ( self , mongo_collection , aggregate_query , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . aggregate ( aggregate_query , * * kwargs )",Runs an aggregation pipeline and returns the results https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . aggregate https : // api . mongodb . com / python / current / examples / aggregation . html
"def find ( self , mongo_collection , query , find_one = False , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if find_one : return collection . find_one ( query , * * kwargs ) else : return collection . find ( query , * * kwargs )",Runs a mongo find query and returns the results https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . find
"def insert_one ( self , mongo_collection , doc , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . insert_one ( doc , * * kwargs )",Inserts a single document into a mongo collection https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . insert_one
"def insert_many ( self , mongo_collection , docs , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . insert_many ( docs , * * kwargs )",Inserts many docs into a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . insert_many
"def update_one ( self , mongo_collection , filter_doc , update_doc , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . update_one ( filter_doc , update_doc , * * kwargs )",Updates a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . update_one
"def replace_one ( self , mongo_collection , doc , filter_doc = None , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_doc : filter_doc = { '_id' : doc [ '_id' ] } return collection . replace_one ( filter_doc , doc , * * kwargs )",Replaces a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . replace_one
"def replace_many ( self , mongo_collection , docs , filter_docs = None , mongo_db = None , upsert = False , collation = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_docs : filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] requests = [ ReplaceOne ( filter_docs [ i ] , docs [ i ] , upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] return collection . bulk_write ( requests , * * kwargs )",Replaces many documents in a mongo collection .
"def delete_one ( self , mongo_collection , filter_doc , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . delete_one ( filter_doc , * * kwargs )",Deletes a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . delete_one
"def delete_many ( self , mongo_collection , filter_doc , mongo_db = None , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . delete_many ( filter_doc , * * kwargs )",Deletes one or more documents in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . Collection . delete_many
"def has_mail_attachment ( self , name , mail_folder = 'INBOX' , check_regex = False ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = True ) return len ( mail_attachments ) > 0",Checks the mail folder for mails containing attachments with the given name .
"def retrieve_mail_attachments ( self , name , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) return mail_attachments",Retrieves mail s attachments in the mail folder by its name .
"def download_mail_attachments ( self , name , local_output_directory , mail_folder = 'INBOX' , check_regex = False , latest_only = False , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) self . _create_files ( mail_attachments , local_output_directory )",Downloads mail s attachments in the mail folder by its name to the local directory .
"def get_attachments_by_name ( self , name , check_regex , find_first = False ) : attachments = [ ] for part in self . mail . walk ( ) : mail_part = MailPart ( part ) if mail_part . is_attachment ( ) : found_attachment = mail_part . has_matching_name ( name ) if check_regex else mail_part . has_equal_name ( name ) if found_attachment : file_name , file_payload = mail_part . get_file ( ) self . log . info ( 'Found attachment: {}' . format ( file_name ) ) attachments . append ( ( file_name , file_payload ) ) if find_first : break return attachments",Gets all attachments by name for the mail .
"def get_file ( self ) : return self . part . get_filename ( ) , self . part . get_payload ( decode = True )",Gets the file including name and payload .
"def put_records ( self , records ) : firehose_conn = self . get_conn ( ) response = firehose_conn . put_record_batch ( DeliveryStreamName = self . delivery_stream , Records = records ) return response",Write batch records to Kinesis Firehose
"def _get_dep_statuses ( self , ti , session , dep_context ) : if dep_context . ignore_in_reschedule_period : yield self . _passing_status ( reason = ""The context specified that being in a reschedule period was "" ""permitted."" ) return if ti . state not in self . RESCHEDULEABLE_STATES : yield self . _passing_status ( reason = ""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state."" ) return task_reschedules = TaskReschedule . find_for_task_instance ( task_instance = ti ) if not task_reschedules : yield self . _passing_status ( reason = ""There is no reschedule request for this task instance."" ) return now = timezone . utcnow ( ) next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date if now >= next_reschedule_date : yield self . _passing_status ( reason = ""Task instance id ready for reschedule."" ) return yield self . _failing_status ( reason = ""Task is not ready for reschedule yet but will be rescheduled "" ""automatically. Current date is {0} and task will be rescheduled "" ""at {1}."" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) )",Determines whether a task is ready to be rescheduled . Only tasks in NONE state with at least one row in task_reschedule table are handled by this dependency class otherwise this dependency is considered as passed . This dependency fails if the latest reschedule request s reschedule date is still in future .
"def send_email ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'EMAIL_BACKEND' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = "", "" . join ( to ) return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , * * kwargs )",Send email using backend specified in EMAIL_BACKEND .
"def send_email_smtp ( to , subject , html_content , files = None , dryrun = False , cc = None , bcc = None , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : smtp_mail_from = configuration . conf . get ( 'smtp' , 'SMTP_MAIL_FROM' ) to = get_email_address_list ( to ) msg = MIMEMultipart ( mime_subtype ) msg [ 'Subject' ] = subject msg [ 'From' ] = smtp_mail_from msg [ 'To' ] = "", "" . join ( to ) recipients = to if cc : cc = get_email_address_list ( cc ) msg [ 'CC' ] = "", "" . join ( cc ) recipients = recipients + cc if bcc : bcc = get_email_address_list ( bcc ) recipients = recipients + bcc msg [ 'Date' ] = formatdate ( localtime = True ) mime_text = MIMEText ( html_content , 'html' , mime_charset ) msg . attach ( mime_text ) for fname in files or [ ] : basename = os . path . basename ( fname ) with open ( fname , ""rb"" ) as f : part = MIMEApplication ( f . read ( ) , Name = basename ) part [ 'Content-Disposition' ] = 'attachment; filename=""%s""' % basename part [ 'Content-ID' ] = '<%s>' % basename msg . attach ( part ) send_MIME_email ( smtp_mail_from , recipients , msg , dryrun )",Send an email with html content
"def process_result_value ( self , value , dialect ) : if value is not None : if value . tzinfo is None : value = value . replace ( tzinfo = utc ) else : value = value . astimezone ( utc ) return value",Processes DateTimes from the DB making sure it is always returning UTC . Not using timezone . convert_to_utc as that converts to configured TIMEZONE while the DB might be running with some other setting . We assume UTC datetimes in the database .
"def check_for_blob ( self , container_name , blob_name , * * kwargs ) : return self . connection . exists ( container_name , blob_name , * * kwargs )",Check if a blob exists on Azure Blob Storage .
"def check_for_prefix ( self , container_name , prefix , * * kwargs ) : matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , * * kwargs ) return len ( list ( matches ) ) > 0",Check if a prefix exists on Azure Blob storage .
"def load_file ( self , file_path , container_name , blob_name , * * kwargs ) : self . connection . create_blob_from_path ( container_name , blob_name , file_path , * * kwargs )",Upload a file to Azure Blob Storage .
"def load_string ( self , string_data , container_name , blob_name , * * kwargs ) : self . connection . create_blob_from_text ( container_name , blob_name , string_data , * * kwargs )",Upload a string to Azure Blob Storage .
"def get_file ( self , file_path , container_name , blob_name , * * kwargs ) : return self . connection . get_blob_to_path ( container_name , blob_name , file_path , * * kwargs )",Download a file from Azure Blob Storage .
"def read_file ( self , container_name , blob_name , * * kwargs ) : return self . connection . get_blob_to_text ( container_name , blob_name , * * kwargs ) . content",Read a file from Azure Blob Storage and return as a string .
"def delete_file ( self , container_name , blob_name , is_prefix = False , ignore_if_missing = False , * * kwargs ) : if is_prefix : blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , * * kwargs ) ] elif self . check_for_blob ( container_name , blob_name ) : blobs_to_delete = [ blob_name ] else : blobs_to_delete = [ ] if not ignore_if_missing and len ( blobs_to_delete ) == 0 : raise AirflowException ( 'Blob(s) not found: {}' . format ( blob_name ) ) for blob_uri in blobs_to_delete : self . log . info ( ""Deleting blob: "" + blob_uri ) self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , * * kwargs )",Delete a file from Azure Blob Storage .
"def mlsd ( conn , path = """" , facts = None ) : facts = facts or [ ] if facts : conn . sendcmd ( ""OPTS MLST "" + "";"" . join ( facts ) + "";"" ) if path : cmd = ""MLSD %s"" % path else : cmd = ""MLSD"" lines = [ ] conn . retrlines ( cmd , lines . append ) for line in lines : facts_found , _ , name = line . rstrip ( ftplib . CRLF ) . partition ( ' ' ) entry = { } for fact in facts_found [ : - 1 ] . split ( "";"" ) : key , _ , value = fact . partition ( ""="" ) entry [ key . lower ( ) ] = value yield ( name , entry )",BACKPORT FROM PYTHON3 FTPLIB .
"def get_conn ( self ) : if self . conn is None : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( ""passive"" , True ) self . conn = ftplib . FTP ( params . host , params . login , params . password ) self . conn . set_pasv ( pasv ) return self . conn",Returns a FTP connection object
"def describe_directory ( self , path ) : conn = self . get_conn ( ) conn . cwd ( path ) try : files = dict ( conn . mlsd ( ) ) except AttributeError : files = dict ( mlsd ( conn ) ) return files",Returns a dictionary of { filename : { attributes }} for all files on the remote system ( where the MLSD command is supported ) .
"def list_directory ( self , path , nlst = False ) : conn = self . get_conn ( ) conn . cwd ( path ) files = conn . nlst ( ) return files",Returns a list of files on the remote system .
"def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = None ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , 'wb' ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = None remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . retrbinary ( 'RETR %s' % remote_file_name , callback ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path ) if is_path and output_handle : output_handle . close ( )",Transfers the remote file to a local location .
"def store_file ( self , remote_full_path , local_full_path_or_buffer ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if is_path : input_handle = open ( local_full_path_or_buffer , 'rb' ) else : input_handle = local_full_path_or_buffer remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) conn . storbinary ( 'STOR %s' % remote_file_name , input_handle ) if is_path : input_handle . close ( )",Transfers a local file to the remote location .
"def rename ( self , from_name , to_name ) : conn = self . get_conn ( ) return conn . rename ( from_name , to_name )",Rename a file .
"def get_mod_time ( self , path ) : conn = self . get_conn ( ) ftp_mdtm = conn . sendcmd ( 'MDTM ' + path ) time_val = ftp_mdtm [ 4 : ] try : return datetime . datetime . strptime ( time_val , ""%Y%m%d%H%M%S.%f"" ) except ValueError : return datetime . datetime . strptime ( time_val , '%Y%m%d%H%M%S' )",Returns a datetime object representing the last time the file was modified
"def _create_dagruns ( dag , execution_dates , state , run_id_template ) : drs = DagRun . find ( dag_id = dag . dag_id , execution_date = execution_dates ) dates_to_create = list ( set ( execution_dates ) - set ( [ dr . execution_date for dr in drs ] ) ) for date in dates_to_create : dr = dag . create_dagrun ( run_id = run_id_template . format ( date . isoformat ( ) ) , execution_date = date , start_date = timezone . utcnow ( ) , external_trigger = False , state = state , ) drs . append ( dr ) return drs",Infers from the dates which dag runs need to be created and does so . : param dag : the dag to create dag runs for : param execution_dates : list of execution dates to evaluate : param state : the state to set the dag run to : param run_id_template : the template for run id to be with the execution date : return : newly created and existing dag runs for the execution dates supplied
"def set_state ( task , execution_date , upstream = False , downstream = False , future = False , past = False , state = State . SUCCESS , commit = False , session = None ) : assert timezone . is_localized ( execution_date ) assert task . dag is not None dag = task . dag latest_execution_date = dag . latest_execution_date assert latest_execution_date is not None end_date = latest_execution_date if future else execution_date if 'start_date' in dag . default_args : start_date = dag . default_args [ 'start_date' ] elif dag . start_date : start_date = dag . start_date else : start_date = execution_date start_date = execution_date if not past else start_date if dag . schedule_interval == '@once' : dates = [ start_date ] else : dates = dag . date_range ( start_date = start_date , end_date = end_date ) task_ids = [ task . task_id ] if downstream : relatives = task . get_flat_relatives ( upstream = False ) task_ids += [ t . task_id for t in relatives ] if upstream : relatives = task . get_flat_relatives ( upstream = True ) task_ids += [ t . task_id for t in relatives ] confirmed_dates = [ ] drs = DagRun . find ( dag_id = dag . dag_id , execution_date = dates ) for dr in drs : dr . dag = dag dr . verify_integrity ( ) confirmed_dates . append ( dr . execution_date ) dags = [ dag ] sub_dag_ids = [ ] while len ( dags ) > 0 : current_dag = dags . pop ( ) for task_id in task_ids : if not current_dag . has_task ( task_id ) : continue current_task = current_dag . get_task ( task_id ) if isinstance ( current_task , SubDagOperator ) : drs = _create_dagruns ( current_task . subdag , execution_dates = confirmed_dates , state = State . RUNNING , run_id_template = BackfillJob . ID_FORMAT_PREFIX ) for dr in drs : dr . dag = current_task . subdag dr . verify_integrity ( ) if commit : dr . state = state session . merge ( dr ) dags . append ( current_task . subdag ) sub_dag_ids . append ( current_task . subdag . dag_id ) TI = TaskInstance qry_dag = session . query ( TI ) . filter ( TI . dag_id == dag . dag_id , TI . execution_date . in_ ( confirmed_dates ) , TI . task_id . in_ ( task_ids ) ) . filter ( or_ ( TI . state . is_ ( None ) , TI . state != state ) ) if len ( sub_dag_ids ) > 0 : qry_sub_dag = session . query ( TI ) . filter ( TI . dag_id . in_ ( sub_dag_ids ) , TI . execution_date . in_ ( confirmed_dates ) ) . filter ( or_ ( TI . state . is_ ( None ) , TI . state != state ) ) if commit : tis_altered = qry_dag . with_for_update ( ) . all ( ) if len ( sub_dag_ids ) > 0 : tis_altered += qry_sub_dag . with_for_update ( ) . all ( ) for ti in tis_altered : ti . state = state else : tis_altered = qry_dag . all ( ) if len ( sub_dag_ids ) > 0 : tis_altered += qry_sub_dag . all ( ) return tis_altered",Set the state of a task instance and if needed its relatives . Can set state for future tasks ( calculated from execution_date ) and retroactively for past tasks . Will verify integrity of past dag runs in order to create tasks that did not exist . It will not create dag runs that are missing on the schedule ( but it will as for subdag dag runs if needed ) . : param task : the task from which to work . task . task . dag needs to be set : param execution_date : the execution date from which to start looking : param upstream : Mark all parents ( upstream tasks ) : param downstream : Mark all siblings ( downstream tasks ) of task_id including SubDags : param future : Mark all future tasks on the interval of the dag up until last execution date . : param past : Retroactively mark all tasks starting from start_date of the DAG : param state : State to which the tasks need to be set : param commit : Commit tasks to be altered to the database : param session : database session : return : list of tasks that have been created and updated
"def _set_dag_run_state ( dag_id , execution_date , state , session = None ) : DR = DagRun dr = session . query ( DR ) . filter ( DR . dag_id == dag_id , DR . execution_date == execution_date ) . one ( ) dr . state = state if state == State . RUNNING : dr . start_date = timezone . utcnow ( ) dr . end_date = None else : dr . end_date = timezone . utcnow ( ) session . merge ( dr )",Helper method that set dag run state in the DB . : param dag_id : dag_id of target dag run : param execution_date : the execution date from which to start looking : param state : target state : param session : database session
"def set_dag_run_state_to_success ( dag , execution_date , commit = False , session = None ) : res = [ ] if not dag or not execution_date : return res if commit : _set_dag_run_state ( dag . dag_id , execution_date , State . SUCCESS , session ) for task in dag . tasks : task . dag = dag new_state = set_state ( task = task , execution_date = execution_date , state = State . SUCCESS , commit = commit ) res . extend ( new_state ) return res",Set the dag run for a specific execution date and its task instances to success . : param dag : the DAG of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit DAG and tasks to be altered to the database : param session : database session : return : If commit is true list of tasks that have been updated otherwise list of tasks that will be updated : raises : AssertionError if dag or execution_date is invalid
"def set_dag_run_state_to_failed ( dag , execution_date , commit = False , session = None ) : res = [ ] if not dag or not execution_date : return res if commit : _set_dag_run_state ( dag . dag_id , execution_date , State . FAILED , session ) TI = TaskInstance task_ids = [ task . task_id for task in dag . tasks ] tis = session . query ( TI ) . filter ( TI . dag_id == dag . dag_id , TI . execution_date == execution_date , TI . task_id . in_ ( task_ids ) ) . filter ( TI . state == State . RUNNING ) task_ids_of_running_tis = [ ti . task_id for ti in tis ] for task in dag . tasks : if task . task_id not in task_ids_of_running_tis : continue task . dag = dag new_state = set_state ( task = task , execution_date = execution_date , state = State . FAILED , commit = commit ) res . extend ( new_state ) return res",Set the dag run for a specific execution date and its running task instances to failed . : param dag : the DAG of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit DAG and tasks to be altered to the database : param session : database session : return : If commit is true list of tasks that have been updated otherwise list of tasks that will be updated : raises : AssertionError if dag or execution_date is invalid
"def set_dag_run_state_to_running ( dag , execution_date , commit = False , session = None ) : res = [ ] if not dag or not execution_date : return res if commit : _set_dag_run_state ( dag . dag_id , execution_date , State . RUNNING , session ) return res",Set the dag run for a specific execution date to running . : param dag : the DAG of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit DAG and tasks to be altered to the database : param session : database session : return : If commit is true list of tasks that have been updated otherwise list of tasks that will be updated
"def git_version ( version ) : repo = None try : import git repo = git . Repo ( '.git' ) except ImportError : logger . warning ( 'gitpython not found: Cannot compute the git version.' ) return '' except Exception as e : logger . warning ( 'Cannot compute the git version. {}' . format ( e ) ) return '' if repo : sha = repo . head . commit . hexsha if repo . is_dirty ( ) : return '.dev0+{sha}.dirty' . format ( sha = sha ) return '.release:{version}+{sha}' . format ( version = version , sha = sha ) else : return 'no_git_version'",Return a version to identify the state of the underlying git repo . The version will indicate whether the head of the current git - backed working directory is tied to a release tag or not : it will indicate the former with a release : { version } prefix and the latter with a dev0 prefix . Following the prefix will be a sha of the current branch head . Finally a dirty suffix is appended to indicate that uncommitted changes are present .
"def execute ( self , context ) : self . hook = DiscordWebhookHook ( self . http_conn_id , self . webhook_endpoint , self . message , self . username , self . avatar_url , self . tts , self . proxy ) self . hook . execute ( )",Call the DiscordWebhookHook to post message
"def _validate_field ( self , validation_spec , dictionary_to_validate , parent = None , force_optional = False ) : field_name = validation_spec [ 'name' ] field_type = validation_spec . get ( 'type' ) optional = validation_spec . get ( 'optional' ) regexp = validation_spec . get ( 'regexp' ) allow_empty = validation_spec . get ( 'allow_empty' ) children_validation_specs = validation_spec . get ( 'fields' ) required_api_version = validation_spec . get ( 'api_version' ) custom_validation = validation_spec . get ( 'custom_validation' ) full_field_path = self . _get_field_name_with_parent ( field_name = field_name , parent = parent ) if required_api_version and required_api_version != self . _api_version : self . log . debug ( ""Skipping validation of the field '%s' for API version '%s' "" ""as it is only valid for API version '%s'"" , field_name , self . _api_version , required_api_version ) return False value = dictionary_to_validate . get ( field_name ) if ( optional or force_optional ) and value is None : self . log . debug ( ""The optional field '%s' is missing. That's perfectly OK."" , full_field_path ) return False self . _sanity_checks ( children_validation_specs = children_validation_specs , field_type = field_type , full_field_path = full_field_path , regexp = regexp , allow_empty = allow_empty , custom_validation = custom_validation , value = value ) if allow_empty is False : self . _validate_is_empty ( full_field_path , value ) if regexp : self . _validate_regexp ( full_field_path , regexp , value ) elif field_type == 'dict' : if not isinstance ( value , dict ) : raise GcpFieldValidationException ( ""The field '{}' should be of dictionary type according to the "" ""specification '{}' but it is '{}'"" . format ( full_field_path , validation_spec , value ) ) if children_validation_specs is None : self . log . debug ( ""The dict field '%s' has no nested fields defined in the "" ""specification '%s'. That's perfectly ok - it's content will "" ""not be validated."" , full_field_path , validation_spec ) else : self . _validate_dict ( children_validation_specs , full_field_path , value ) elif field_type == 'union' : if not children_validation_specs : raise GcpValidationSpecificationException ( ""The union field '%s' has no nested fields "" ""defined in specification '%s'. Unions should have at least one "" ""nested field defined."" , full_field_path , validation_spec ) self . _validate_union ( children_validation_specs , full_field_path , dictionary_to_validate ) elif field_type == 'list' : if not isinstance ( value , list ) : raise GcpFieldValidationException ( ""The field '{}' should be of list type according to the "" ""specification '{}' but it is '{}'"" . format ( full_field_path , validation_spec , value ) ) elif custom_validation : try : custom_validation ( value ) except Exception as e : raise GcpFieldValidationException ( ""Error while validating custom field '{}' specified by '{}': '{}'"" . format ( full_field_path , validation_spec , e ) ) elif field_type is None : self . log . debug ( ""The type of field '%s' is not specified in '%s'. "" ""Not validating its content."" , full_field_path , validation_spec ) else : raise GcpValidationSpecificationException ( ""The field '{}' is of type '{}' in specification '{}'."" ""This type is unknown to validation!"" . format ( full_field_path , field_type , validation_spec ) ) return True",Validates if field is OK .
"def validate ( self , body_to_validate ) : try : for validation_spec in self . _validation_specs : self . _validate_field ( validation_spec = validation_spec , dictionary_to_validate = body_to_validate ) except GcpFieldValidationException as e : raise GcpFieldValidationException ( ""There was an error when validating: body '{}': '{}'"" . format ( body_to_validate , e ) ) all_field_names = [ spec [ 'name' ] for spec in self . _validation_specs if spec . get ( 'type' ) != 'union' and spec . get ( 'api_version' ) != self . _api_version ] all_union_fields = [ spec for spec in self . _validation_specs if spec . get ( 'type' ) == 'union' ] for union_field in all_union_fields : all_field_names . extend ( [ nested_union_spec [ 'name' ] for nested_union_spec in union_field [ 'fields' ] if nested_union_spec . get ( 'type' ) != 'union' and nested_union_spec . get ( 'api_version' ) != self . _api_version ] ) for field_name in body_to_validate . keys ( ) : if field_name not in all_field_names : self . log . warning ( ""The field '%s' is in the body, but is not specified in the "" ""validation specification '%s'. "" ""This might be because you are using newer API version and "" ""new field names defined for that version. Then the warning "" ""can be safely ignored, or you might want to upgrade the operator"" ""to the version that supports the new API version."" , field_name , self . _validation_specs )",Validates if the body ( dictionary ) follows specification that the validator was instantiated with . Raises ValidationSpecificationException or ValidationFieldException in case of problems with specification or the body not conforming to the specification respectively .
"def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return FileService ( account_name = conn . login , account_key = conn . password , * * service_options )",Return the FileService object .
"def check_for_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , * * kwargs )",Check if a directory exists on Azure File Share .
"def check_for_file ( self , share_name , directory_name , file_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , file_name , * * kwargs )",Check if a file exists on Azure File Share .
"def list_directories_and_files ( self , share_name , directory_name = None , * * kwargs ) : return self . connection . list_directories_and_files ( share_name , directory_name , * * kwargs )",Return the list of directories and files stored on a Azure File Share .
"def create_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . create_directory ( share_name , directory_name , * * kwargs )",Create a new directory on a Azure File Share .
"def get_file ( self , file_path , share_name , directory_name , file_name , * * kwargs ) : self . connection . get_file_to_path ( share_name , directory_name , file_name , file_path , * * kwargs )",Download a file from Azure File Share .
"def get_file_to_stream ( self , stream , share_name , directory_name , file_name , * * kwargs ) : self . connection . get_file_to_stream ( share_name , directory_name , file_name , stream , * * kwargs )",Download a file from Azure File Share .
"def load_file ( self , file_path , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_path ( share_name , directory_name , file_name , file_path , * * kwargs )",Upload a file to Azure File Share .
"def load_string ( self , string_data , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_text ( share_name , directory_name , file_name , string_data , * * kwargs )",Upload a string to Azure File Share .
"def load_stream ( self , stream , share_name , directory_name , file_name , count , * * kwargs ) : self . connection . create_file_from_stream ( share_name , directory_name , file_name , stream , count , * * kwargs )",Upload a stream to Azure File Share .
"def set_context ( self , filename ) : local_loc = self . _init_file ( filename ) self . handler = logging . FileHandler ( local_loc ) self . handler . setFormatter ( self . formatter ) self . handler . setLevel ( self . level ) if self . _cur_date < datetime . today ( ) : self . _symlink_latest_log_directory ( ) self . _cur_date = datetime . today ( )",Provide filename context to airflow task handler . : param filename : filename in which the dag is located
"def _init_file ( self , filename ) : relative_path = self . _render_filename ( filename ) full_path = os . path . join ( self . _get_log_directory ( ) , relative_path ) directory = os . path . dirname ( full_path ) if not os . path . exists ( directory ) : try : os . makedirs ( directory ) except OSError : if not os . path . isdir ( directory ) : raise if not os . path . exists ( full_path ) : open ( full_path , ""a"" ) . close ( ) return full_path",Create log file and directory if required . : param filename : task instance object : return : relative log path of the given task instance
"def _parse_gcs_url ( gsurl ) : parsed_url = urlparse ( gsurl ) if not parsed_url . netloc : raise AirflowException ( 'Please provide a bucket name' ) else : bucket = parsed_url . netloc blob = parsed_url . path . lstrip ( '/' ) return bucket , blob",Given a Google Cloud Storage URL ( gs : // <bucket > / <blob > ) returns a tuple containing the corresponding bucket and blob .
def get_conn ( self ) : if not self . _conn : self . _conn = storage . Client ( credentials = self . _get_credentials ( ) ) return self . _conn,Returns a Google Cloud Storage service object .
"def copy ( self , source_bucket , source_object , destination_bucket = None , destination_object = None ) : destination_bucket = destination_bucket or source_bucket destination_object = destination_object or source_object if source_bucket == destination_bucket and source_object == destination_object : raise ValueError ( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % ( source_bucket , source_object ) ) if not source_bucket or not source_object : raise ValueError ( 'source_bucket and source_object cannot be empty.' ) client = self . get_conn ( ) source_bucket = client . get_bucket ( source_bucket ) source_object = source_bucket . blob ( source_object ) destination_bucket = client . get_bucket ( destination_bucket ) destination_object = source_bucket . copy_blob ( blob = source_object , destination_bucket = destination_bucket , new_name = destination_object ) self . log . info ( 'Object %s in bucket %s copied to object %s in bucket %s' , source_object . name , source_bucket . name , destination_object . name , destination_bucket . name )",Copies an object from a bucket to another with renaming if requested .
"def rewrite ( self , source_bucket , source_object , destination_bucket , destination_object = None ) : destination_object = destination_object or source_object if ( source_bucket == destination_bucket and source_object == destination_object ) : raise ValueError ( 'Either source/destination bucket or source/destination object ' 'must be different, not both the same: bucket=%s, object=%s' % ( source_bucket , source_object ) ) if not source_bucket or not source_object : raise ValueError ( 'source_bucket and source_object cannot be empty.' ) client = self . get_conn ( ) source_bucket = client . get_bucket ( bucket_name = source_bucket ) source_object = source_bucket . blob ( blob_name = source_object ) destination_bucket = client . get_bucket ( bucket_name = destination_bucket ) token , bytes_rewritten , total_bytes = destination_bucket . blob ( blob_name = destination_object ) . rewrite ( source = source_object ) self . log . info ( 'Total Bytes: %s | Bytes Written: %s' , total_bytes , bytes_rewritten ) while token is not None : token , bytes_rewritten , total_bytes = destination_bucket . blob ( blob_name = destination_object ) . rewrite ( source = source_object , token = token ) self . log . info ( 'Total Bytes: %s | Bytes Written: %s' , total_bytes , bytes_rewritten ) self . log . info ( 'Object %s in bucket %s copied to object %s in bucket %s' , source_object . name , source_bucket . name , destination_object , destination_bucket . name )",Has the same functionality as copy except that will work on files over 5 TB as well as when copying between locations and / or storage classes .
"def download ( self , bucket_name , object_name , filename = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) blob = bucket . blob ( blob_name = object_name ) if filename : blob . download_to_filename ( filename ) self . log . info ( 'File downloaded to %s' , filename ) return blob . download_as_string ( )",Get a file from Google Cloud Storage .
"def upload ( self , bucket_name , object_name , filename , mime_type = 'application/octet-stream' , gzip = False ) : if gzip : filename_gz = filename + '.gz' with open ( filename , 'rb' ) as f_in : with gz . open ( filename_gz , 'wb' ) as f_out : shutil . copyfileobj ( f_in , f_out ) filename = filename_gz client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . upload_from_filename ( filename = filename , content_type = mime_type ) if gzip : os . remove ( filename ) self . log . info ( 'File %s uploaded to %s in %s bucket' , filename , object_name , bucket_name )",Uploads a local file to Google Cloud Storage .
"def exists ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) return blob . exists ( )",Checks for the existence of a file in Google Cloud Storage .
"def is_updated_after ( self , bucket_name , object_name , ts ) : client = self . get_conn ( ) bucket = storage . Bucket ( client = client , name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_update_time = blob . updated if blob_update_time is not None : import dateutil . tz if not ts . tzinfo : ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) self . log . info ( ""Verify object date: %s > %s"" , blob_update_time , ts ) if blob_update_time > ts : return True return False",Checks if an blob_name is updated in Google Cloud Storage .
"def delete ( self , bucket_name , object_name ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . blob ( blob_name = object_name ) blob . delete ( ) self . log . info ( 'Blob %s deleted.' , object_name )",Deletes an object from the bucket .
"def list ( self , bucket_name , versions = None , max_results = None , prefix = None , delimiter = None ) : client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) ids = [ ] pageToken = None while True : blobs = bucket . list_blobs ( max_results = max_results , page_token = pageToken , prefix = prefix , delimiter = delimiter , versions = versions ) blob_names = [ ] for blob in blobs : blob_names . append ( blob . name ) prefixes = blobs . prefixes if prefixes : ids += list ( prefixes ) else : ids += blob_names pageToken = blobs . next_page_token if pageToken is None : break return ids",List all objects from the bucket with the give string prefix in name
"def get_size ( self , bucket_name , object_name ) : self . log . info ( 'Checking the file size of object: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_size = blob . size self . log . info ( 'The file size of %s is %s bytes.' , object_name , blob_size ) return blob_size",Gets the size of a file in Google Cloud Storage .
"def get_crc32c ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the crc32c checksum of ' 'object_name: %s in bucket_name: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_crc32c = blob . crc32c self . log . info ( 'The crc32c checksum of %s is %s' , object_name , blob_crc32c ) return blob_crc32c",Gets the CRC32c checksum of an object in Google Cloud Storage .
"def get_md5hash ( self , bucket_name , object_name ) : self . log . info ( 'Retrieving the MD5 hash of ' 'object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name = bucket_name ) blob = bucket . get_blob ( blob_name = object_name ) blob . reload ( ) blob_md5hash = blob . md5_hash self . log . info ( 'The md5Hash of %s is %s' , object_name , blob_md5hash ) return blob_md5hash",Gets the MD5 hash of an object in Google Cloud Storage .
"def create_bucket ( self , bucket_name , resource = None , storage_class = 'MULTI_REGIONAL' , location = 'US' , project_id = None , labels = None ) : self . log . info ( 'Creating Bucket: %s; Location: %s; Storage Class: %s' , bucket_name , location , storage_class ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket_resource = resource or { } for item in bucket_resource : if item != ""name"" : bucket . _patch_property ( name = item , value = resource [ item ] ) bucket . storage_class = storage_class bucket . labels = labels or { } bucket . create ( project = project_id , location = location ) return bucket . id",Creates a new bucket . Google Cloud Storage uses a flat namespace so you can t create a bucket with a name that is already in use .
"def insert_bucket_acl ( self , bucket_name , entity , role , user_project = None ) : self . log . info ( 'Creating a new ACL entry in bucket: %s' , bucket_name ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) bucket . acl . reload ( ) bucket . acl . entity_from_dict ( entity_dict = { ""entity"" : entity , ""role"" : role } ) if user_project : bucket . acl . user_project = user_project bucket . acl . save ( ) self . log . info ( 'A new ACL entry created in bucket: %s' , bucket_name )",Creates a new ACL entry on the specified bucket_name . See : https : // cloud . google . com / storage / docs / json_api / v1 / bucketAccessControls / insert
"def insert_object_acl ( self , bucket_name , object_name , entity , role , user_project = None ) : self . log . info ( 'Creating a new ACL entry for object: %s in bucket: %s' , object_name , bucket_name ) client = self . get_conn ( ) bucket = client . bucket ( bucket_name = bucket_name ) blob = bucket . blob ( object_name ) blob . acl . reload ( ) blob . acl . entity_from_dict ( entity_dict = { ""entity"" : entity , ""role"" : role } ) if user_project : blob . acl . user_project = user_project blob . acl . save ( ) self . log . info ( 'A new ACL entry created for object: %s in bucket: %s' , object_name , bucket_name )",Creates a new ACL entry on the specified object . See : https : // cloud . google . com / storage / docs / json_api / v1 / objectAccessControls / insert
"def compose ( self , bucket_name , source_objects , destination_object ) : if not source_objects or not len ( source_objects ) : raise ValueError ( 'source_objects cannot be empty.' ) if not bucket_name or not destination_object : raise ValueError ( 'bucket_name and destination_object cannot be empty.' ) self . log . info ( ""Composing %s to %s in the bucket %s"" , source_objects , destination_object , bucket_name ) client = self . get_conn ( ) bucket = client . get_bucket ( bucket_name ) destination_blob = bucket . blob ( destination_object ) destination_blob . compose ( sources = [ bucket . blob ( blob_name = source_object ) for source_object in source_objects ] ) self . log . info ( ""Completed successfully."" )",Composes a list of existing object into a new object in the same storage bucket_name
"def argmin ( arr , f ) : m = None i = None for idx , item in enumerate ( arr ) : if item is not None : if m is None or f ( item ) < m : m = f ( item ) i = idx return i",Return the index i in arr that minimizes f ( arr [ i ] )
"def secondary_training_status_changed ( current_job_description , prev_job_description ) : current_secondary_status_transitions = current_job_description . get ( 'SecondaryStatusTransitions' ) if current_secondary_status_transitions is None or len ( current_secondary_status_transitions ) == 0 : return False prev_job_secondary_status_transitions = prev_job_description . get ( 'SecondaryStatusTransitions' ) if prev_job_description is not None else None last_message = prev_job_secondary_status_transitions [ - 1 ] [ 'StatusMessage' ] if prev_job_secondary_status_transitions is not None and len ( prev_job_secondary_status_transitions ) > 0 else '' message = current_job_description [ 'SecondaryStatusTransitions' ] [ - 1 ] [ 'StatusMessage' ] return message != last_message",Returns true if training job s secondary status message has changed .
"def secondary_training_status_message ( job_description , prev_description ) : if job_description is None or job_description . get ( 'SecondaryStatusTransitions' ) is None or len ( job_description . get ( 'SecondaryStatusTransitions' ) ) == 0 : return '' prev_description_secondary_transitions = prev_description . get ( 'SecondaryStatusTransitions' ) if prev_description is not None else None prev_transitions_num = len ( prev_description [ 'SecondaryStatusTransitions' ] ) if prev_description_secondary_transitions is not None else 0 current_transitions = job_description [ 'SecondaryStatusTransitions' ] transitions_to_print = current_transitions [ - 1 : ] if len ( current_transitions ) == prev_transitions_num else current_transitions [ prev_transitions_num - len ( current_transitions ) : ] status_strs = [ ] for transition in transitions_to_print : message = transition [ 'StatusMessage' ] time_str = timezone . convert_to_utc ( job_description [ 'LastModifiedTime' ] ) . strftime ( '%Y-%m-%d %H:%M:%S' ) status_strs . append ( '{} {} - {}' . format ( time_str , transition [ 'Status' ] , message ) ) return '\n' . join ( status_strs )",Returns a string contains start time and the secondary training job status message .
"def tar_and_s3_upload ( self , path , key , bucket ) : with tempfile . TemporaryFile ( ) as temp_file : if os . path . isdir ( path ) : files = [ os . path . join ( path , name ) for name in os . listdir ( path ) ] else : files = [ path ] with tarfile . open ( mode = 'w:gz' , fileobj = temp_file ) as tar_file : for f in files : tar_file . add ( f , arcname = os . path . basename ( f ) ) temp_file . seek ( 0 ) self . s3_hook . load_file_obj ( temp_file , key , bucket , replace = True )",Tar the local file or directory and upload to s3
"def configure_s3_resources ( self , config ) : s3_operations = config . pop ( 'S3Operations' , None ) if s3_operations is not None : create_bucket_ops = s3_operations . get ( 'S3CreateBucket' , [ ] ) upload_ops = s3_operations . get ( 'S3Upload' , [ ] ) for op in create_bucket_ops : self . s3_hook . create_bucket ( bucket_name = op [ 'Bucket' ] ) for op in upload_ops : if op [ 'Tar' ] : self . tar_and_s3_upload ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] ) else : self . s3_hook . load_file ( op [ 'Path' ] , op [ 'Key' ] , op [ 'Bucket' ] )",Extract the S3 operations from the configuration and execute them .
"def check_s3_url ( self , s3url ) : bucket , key = S3Hook . parse_s3_url ( s3url ) if not self . s3_hook . check_for_bucket ( bucket_name = bucket ) : raise AirflowException ( ""The input S3 Bucket {} does not exist "" . format ( bucket ) ) if key and not self . s3_hook . check_for_key ( key = key , bucket_name = bucket ) and not self . s3_hook . check_for_prefix ( prefix = key , bucket_name = bucket , delimiter = '/' ) : raise AirflowException ( ""The input S3 Key "" ""or Prefix {} does not exist in the Bucket {}"" . format ( s3url , bucket ) ) return True",Check if an S3 URL exists
"def get_log_conn ( self ) : config = botocore . config . Config ( retries = { 'max_attempts' : 15 } ) return self . get_client_type ( 'logs' , config = config )",Establish an AWS connection for retrieving logs during training
"def create_training_job ( self , config , wait_for_completion = True , print_log = True , check_interval = 30 , max_ingestion_time = None ) : self . check_training_config ( config ) response = self . get_conn ( ) . create_training_job ( * * config ) if print_log : self . check_training_status_with_log ( config [ 'TrainingJobName' ] , self . non_terminal_states , self . failed_states , wait_for_completion , check_interval , max_ingestion_time ) elif wait_for_completion : describe_response = self . check_status ( config [ 'TrainingJobName' ] , 'TrainingJobStatus' , self . describe_training_job , check_interval , max_ingestion_time ) billable_time = ( describe_response [ 'TrainingEndTime' ] - describe_response [ 'TrainingStartTime' ] ) * describe_response [ 'ResourceConfig' ] [ 'InstanceCount' ] self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) ) return response",Create a training job
"def create_tuning_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_tuning_config ( config ) response = self . get_conn ( ) . create_hyper_parameter_tuning_job ( * * config ) if wait_for_completion : self . check_status ( config [ 'HyperParameterTuningJobName' ] , 'HyperParameterTuningJobStatus' , self . describe_tuning_job , check_interval , max_ingestion_time ) return response",Create a tuning job
"def create_transform_job ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : self . check_s3_url ( config [ 'TransformInput' ] [ 'DataSource' ] [ 'S3DataSource' ] [ 'S3Uri' ] ) response = self . get_conn ( ) . create_transform_job ( * * config ) if wait_for_completion : self . check_status ( config [ 'TransformJobName' ] , 'TransformJobStatus' , self . describe_transform_job , check_interval , max_ingestion_time ) return response",Create a transform job
"def create_endpoint ( self , config , wait_for_completion = True , check_interval = 30 , max_ingestion_time = None ) : response = self . get_conn ( ) . create_endpoint ( * * config ) if wait_for_completion : self . check_status ( config [ 'EndpointName' ] , 'EndpointStatus' , self . describe_endpoint , check_interval , max_ingestion_time , non_terminal_states = self . endpoint_non_terminal_states ) return response",Create an endpoint
"def describe_training_job_with_log ( self , job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) : log_group = '/aws/sagemaker/TrainingJobs' if len ( stream_names ) < instance_count : logs_conn = self . get_log_conn ( ) try : streams = logs_conn . describe_log_streams ( logGroupName = log_group , logStreamNamePrefix = job_name + '/' , orderBy = 'LogStreamName' , limit = instance_count ) stream_names = [ s [ 'logStreamName' ] for s in streams [ 'logStreams' ] ] positions . update ( [ ( s , Position ( timestamp = 0 , skip = 0 ) ) for s in stream_names if s not in positions ] ) except logs_conn . exceptions . ResourceNotFoundException : pass if len ( stream_names ) > 0 : for idx , event in self . multi_stream_iter ( log_group , stream_names , positions ) : self . log . info ( event [ 'message' ] ) ts , count = positions [ stream_names [ idx ] ] if event [ 'timestamp' ] == ts : positions [ stream_names [ idx ] ] = Position ( timestamp = ts , skip = count + 1 ) else : positions [ stream_names [ idx ] ] = Position ( timestamp = event [ 'timestamp' ] , skip = 1 ) if state == LogState . COMPLETE : return state , last_description , last_describe_job_call if state == LogState . JOB_COMPLETE : state = LogState . COMPLETE elif time . time ( ) - last_describe_job_call >= 30 : description = self . describe_training_job ( job_name ) last_describe_job_call = time . time ( ) if secondary_training_status_changed ( description , last_description ) : self . log . info ( secondary_training_status_message ( description , last_description ) ) last_description = description status = description [ 'TrainingJobStatus' ] if status not in self . non_terminal_states : state = LogState . JOB_COMPLETE return state , last_description , last_describe_job_call",Return the training job info associated with job_name and print CloudWatch logs
"def check_status ( self , job_name , key , describe_function , check_interval , max_ingestion_time , non_terminal_states = None ) : if not non_terminal_states : non_terminal_states = self . non_terminal_states sec = 0 running = True while running : time . sleep ( check_interval ) sec = sec + check_interval try : response = describe_function ( job_name ) status = response [ key ] self . log . info ( 'Job still running for %s seconds... ' 'current status is %s' % ( sec , status ) ) except KeyError : raise AirflowException ( 'Could not get status of the SageMaker job' ) except ClientError : raise AirflowException ( 'AWS request failed, check logs for more info' ) if status in non_terminal_states : running = True elif status in self . failed_states : raise AirflowException ( 'SageMaker job failed because %s' % response [ 'FailureReason' ] ) else : running = False if max_ingestion_time and sec > max_ingestion_time : raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) self . log . info ( 'SageMaker Job Compeleted' ) response = describe_function ( job_name ) return response",Check status of a SageMaker job
"def check_training_status_with_log ( self , job_name , non_terminal_states , failed_states , wait_for_completion , check_interval , max_ingestion_time ) : sec = 0 description = self . describe_training_job ( job_name ) self . log . info ( secondary_training_status_message ( description , None ) ) instance_count = description [ 'ResourceConfig' ] [ 'InstanceCount' ] status = description [ 'TrainingJobStatus' ] stream_names = [ ] positions = { } job_already_completed = status not in non_terminal_states state = LogState . TAILING if wait_for_completion and not job_already_completed else LogState . COMPLETE last_describe_job_call = time . time ( ) last_description = description while True : time . sleep ( check_interval ) sec = sec + check_interval state , last_description , last_describe_job_call = self . describe_training_job_with_log ( job_name , positions , stream_names , instance_count , state , last_description , last_describe_job_call ) if state == LogState . COMPLETE : break if max_ingestion_time and sec > max_ingestion_time : raise AirflowException ( 'SageMaker job took more than %s seconds' , max_ingestion_time ) if wait_for_completion : status = last_description [ 'TrainingJobStatus' ] if status in failed_states : reason = last_description . get ( 'FailureReason' , '(No reason provided)' ) raise AirflowException ( 'Error training {}: {} Reason: {}' . format ( job_name , status , reason ) ) billable_time = ( last_description [ 'TrainingEndTime' ] - last_description [ 'TrainingStartTime' ] ) * instance_count self . log . info ( 'Billable seconds:{}' . format ( int ( billable_time . total_seconds ( ) ) + 1 ) )",Display the logs for a given training job optionally tailing them until the job is complete .
"def execute ( self , context ) : bucket_helper = GoogleCloudBucketHelper ( self . gcp_conn_id , self . delegate_to ) self . py_file = bucket_helper . google_cloud_to_local ( self . py_file ) hook = DataFlowHook ( gcp_conn_id = self . gcp_conn_id , delegate_to = self . delegate_to , poll_sleep = self . poll_sleep ) dataflow_options = self . dataflow_default_options . copy ( ) dataflow_options . update ( self . options ) camel_to_snake = lambda name : re . sub ( r'[A-Z]' , lambda x : '_' + x . group ( 0 ) . lower ( ) , name ) formatted_options = { camel_to_snake ( key ) : dataflow_options [ key ] for key in dataflow_options } hook . start_python_dataflow ( self . job_name , formatted_options , self . py_file , self . py_options )",Execute the python dataflow job .
"def google_cloud_to_local ( self , file_name ) : if not file_name . startswith ( 'gs://' ) : return file_name path_components = file_name [ self . GCS_PREFIX_LENGTH : ] . split ( '/' ) if len ( path_components ) < 2 : raise Exception ( 'Invalid Google Cloud Storage (GCS) object path: {}' . format ( file_name ) ) bucket_id = path_components [ 0 ] object_id = '/' . join ( path_components [ 1 : ] ) local_file = '/tmp/dataflow{}-{}' . format ( str ( uuid . uuid4 ( ) ) [ : 8 ] , path_components [ - 1 ] ) self . _gcs_hook . download ( bucket_id , object_id , local_file ) if os . stat ( local_file ) . st_size > 0 : return local_file raise Exception ( 'Failed to download Google Cloud Storage (GCS) object: {}' . format ( file_name ) )",Checks whether the file specified by file_name is stored in Google Cloud Storage ( GCS ) if so downloads the file and saves it locally . The full path of the saved file will be returned . Otherwise the local file_name will be returned immediately .
"def run_migrations_offline ( ) : context . configure ( url = settings . SQL_ALCHEMY_CONN , target_metadata = target_metadata , literal_binds = True , compare_type = COMPARE_TYPE ) with context . begin_transaction ( ) : context . run_migrations ( )",Run migrations in offline mode .
"def run_migrations_online ( ) : connectable = settings . engine with connectable . connect ( ) as connection : context . configure ( connection = connection , transaction_per_migration = True , target_metadata = target_metadata , compare_type = COMPARE_TYPE , ) with context . begin_transaction ( ) : context . run_migrations ( )",Run migrations in online mode .
"def delete_instance ( self , instance_id , project_id = None ) : instance = self . get_instance ( instance_id = instance_id , project_id = project_id ) if instance : instance . delete ( ) else : self . log . info ( ""The instance '%s' does not exist in project '%s'. Exiting"" , instance_id , project_id )",Deletes the specified Cloud Bigtable instance . Raises google . api_core . exceptions . NotFound if the Cloud Bigtable instance does not exist .
"def create_instance ( self , instance_id , main_cluster_id , main_cluster_zone , project_id = None , replica_cluster_id = None , replica_cluster_zone = None , instance_display_name = None , instance_type = enums . Instance . Type . TYPE_UNSPECIFIED , instance_labels = None , cluster_nodes = None , cluster_storage_type = enums . StorageType . STORAGE_TYPE_UNSPECIFIED , timeout = None ) : cluster_storage_type = enums . StorageType ( cluster_storage_type ) instance_type = enums . Instance . Type ( instance_type ) instance = Instance ( instance_id , self . _get_client ( project_id = project_id ) , instance_display_name , instance_type , instance_labels , ) clusters = [ instance . cluster ( main_cluster_id , main_cluster_zone , cluster_nodes , cluster_storage_type ) ] if replica_cluster_id and replica_cluster_zone : clusters . append ( instance . cluster ( replica_cluster_id , replica_cluster_zone , cluster_nodes , cluster_storage_type ) ) operation = instance . create ( clusters = clusters ) operation . result ( timeout ) return instance",Creates new instance .
"def create_table ( instance , table_id , initial_split_keys = None , column_families = None ) : if column_families is None : column_families = { } if initial_split_keys is None : initial_split_keys = [ ] table = Table ( table_id , instance ) table . create ( initial_split_keys , column_families )",Creates the specified Cloud Bigtable table . Raises google . api_core . exceptions . AlreadyExists if the table exists .
"def delete_table ( self , instance_id , table_id , project_id = None ) : table = self . get_instance ( instance_id = instance_id , project_id = project_id ) . table ( table_id = table_id ) table . delete ( )",Deletes the specified table in Cloud Bigtable . Raises google . api_core . exceptions . NotFound if the table does not exist .
"def update_cluster ( instance , cluster_id , nodes ) : cluster = Cluster ( cluster_id , instance ) cluster . serve_nodes = nodes cluster . update ( )",Updates number of nodes in the specified Cloud Bigtable cluster . Raises google . api_core . exceptions . NotFound if the cluster does not exist .
"def _prepare_cli_cmd ( self ) : conn = self . conn hive_bin = 'hive' cmd_extra = [ ] if self . use_beeline : hive_bin = 'beeline' jdbc_url = ""jdbc:hive2://{host}:{port}/{schema}"" . format ( host = conn . host , port = conn . port , schema = conn . schema ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : template = conn . extra_dejson . get ( 'principal' , ""hive/_HOST@EXAMPLE.COM"" ) if ""_HOST"" in template : template = utils . replace_hostname_pattern ( utils . get_components ( template ) ) proxy_user = """" if conn . extra_dejson . get ( 'proxy_user' ) == ""login"" and conn . login : proxy_user = ""hive.server2.proxy.user={0}"" . format ( conn . login ) elif conn . extra_dejson . get ( 'proxy_user' ) == ""owner"" and self . run_as : proxy_user = ""hive.server2.proxy.user={0}"" . format ( self . run_as ) jdbc_url += "";principal={template};{proxy_user}"" . format ( template = template , proxy_user = proxy_user ) elif self . auth : jdbc_url += "";auth="" + self . auth jdbc_url = '""{}""' . format ( jdbc_url ) cmd_extra += [ '-u' , jdbc_url ] if conn . login : cmd_extra += [ '-n' , conn . login ] if conn . password : cmd_extra += [ '-p' , conn . password ] hive_params_list = self . hive_cli_params . split ( ) return [ hive_bin ] + cmd_extra + hive_params_list",This function creates the command list from available information
"def _prepare_hiveconf ( d ) : if not d : return [ ] return as_flattened_list ( zip ( [ ""-hiveconf"" ] * len ( d ) , [ ""{}={}"" . format ( k , v ) for k , v in d . items ( ) ] ) )",This function prepares a list of hiveconf params from a dictionary of key value pairs .
"def run_cli ( self , hql , schema = None , verbose = True , hive_conf = None ) : conn = self . conn schema = schema or conn . schema if schema : hql = ""USE {schema};\n{hql}"" . format ( schema = schema , hql = hql ) with TemporaryDirectory ( prefix = 'airflow_hiveop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir ) as f : hql = hql + '\n' f . write ( hql . encode ( 'UTF-8' ) ) f . flush ( ) hive_cmd = self . _prepare_cli_cmd ( ) env_context = get_context_from_env_var ( ) if hive_conf : env_context . update ( hive_conf ) hive_conf_params = self . _prepare_hiveconf ( env_context ) if self . mapred_queue : hive_conf_params . extend ( [ '-hiveconf' , 'mapreduce.job.queuename={}' . format ( self . mapred_queue ) , '-hiveconf' , 'mapred.job.queue.name={}' . format ( self . mapred_queue ) , '-hiveconf' , 'tez.job.queue.name={}' . format ( self . mapred_queue ) ] ) if self . mapred_queue_priority : hive_conf_params . extend ( [ '-hiveconf' , 'mapreduce.job.priority={}' . format ( self . mapred_queue_priority ) ] ) if self . mapred_job_name : hive_conf_params . extend ( [ '-hiveconf' , 'mapred.job.name={}' . format ( self . mapred_job_name ) ] ) hive_cmd . extend ( hive_conf_params ) hive_cmd . extend ( [ '-f' , f . name ] ) if verbose : self . log . info ( ""%s"" , "" "" . join ( hive_cmd ) ) sp = subprocess . Popen ( hive_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , cwd = tmp_dir , close_fds = True ) self . sp = sp stdout = '' while True : line = sp . stdout . readline ( ) if not line : break stdout += line . decode ( 'UTF-8' ) if verbose : self . log . info ( line . decode ( 'UTF-8' ) . strip ( ) ) sp . wait ( ) if sp . returncode : raise AirflowException ( stdout ) return stdout",Run an hql statement using the hive cli . If hive_conf is specified it should be a dict and the entries will be set as key / value pairs in HiveConf
"def load_df ( self , df , table , field_dict = None , delimiter = ',' , encoding = 'utf8' , pandas_kwargs = None , * * kwargs ) : def _infer_field_types_from_df ( df ) : DTYPE_KIND_HIVE_TYPE = { 'b' : 'BOOLEAN' , 'i' : 'BIGINT' , 'u' : 'BIGINT' , 'f' : 'DOUBLE' , 'c' : 'STRING' , 'M' : 'TIMESTAMP' , 'O' : 'STRING' , 'S' : 'STRING' , 'U' : 'STRING' , 'V' : 'STRING' } d = OrderedDict ( ) for col , dtype in df . dtypes . iteritems ( ) : d [ col ] = DTYPE_KIND_HIVE_TYPE [ dtype . kind ] return d if pandas_kwargs is None : pandas_kwargs = { } with TemporaryDirectory ( prefix = 'airflow_hiveop_' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir , mode = ""w"" ) as f : if field_dict is None : field_dict = _infer_field_types_from_df ( df ) df . to_csv ( path_or_buf = f , sep = delimiter , header = False , index = False , encoding = encoding , date_format = ""%Y-%m-%d %H:%M:%S"" , * * pandas_kwargs ) f . flush ( ) return self . load_file ( filepath = f . name , table = table , delimiter = delimiter , field_dict = field_dict , * * kwargs )",Loads a pandas DataFrame into hive .
"def load_file ( self , filepath , table , delimiter = "","" , field_dict = None , create = True , overwrite = True , partition = None , recreate = False , tblproperties = None ) : hql = '' if recreate : hql += ""DROP TABLE IF EXISTS {table};\n"" . format ( table = table ) if create or recreate : if field_dict is None : raise ValueError ( ""Must provide a field dict when creating a table"" ) fields = "",\n    "" . join ( [ k + ' ' + v for k , v in field_dict . items ( ) ] ) hql += ""CREATE TABLE IF NOT EXISTS {table} (\n{fields})\n"" . format ( table = table , fields = fields ) if partition : pfields = "",\n    "" . join ( [ p + "" STRING"" for p in partition ] ) hql += ""PARTITIONED BY ({pfields})\n"" . format ( pfields = pfields ) hql += ""ROW FORMAT DELIMITED\n"" hql += ""FIELDS TERMINATED BY '{delimiter}'\n"" . format ( delimiter = delimiter ) hql += ""STORED AS textfile\n"" if tblproperties is not None : tprops = "", "" . join ( [ ""'{0}'='{1}'"" . format ( k , v ) for k , v in tblproperties . items ( ) ] ) hql += ""TBLPROPERTIES({tprops})\n"" . format ( tprops = tprops ) hql += "";"" self . log . info ( hql ) self . run_cli ( hql ) hql = ""LOAD DATA LOCAL INPATH '{filepath}' "" . format ( filepath = filepath ) if overwrite : hql += ""OVERWRITE "" hql += ""INTO TABLE {table} "" . format ( table = table ) if partition : pvals = "", "" . join ( [ ""{0}='{1}'"" . format ( k , v ) for k , v in partition . items ( ) ] ) hql += ""PARTITION ({pvals})"" . format ( pvals = pvals ) hql += ';\n' self . log . info ( hql ) self . run_cli ( hql )",Loads a local file into Hive
"def get_metastore_client ( self ) : import hmsclient from thrift . transport import TSocket , TTransport from thrift . protocol import TBinaryProtocol ms = self . metastore_conn auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'NOSASL' ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = ms . extra_dejson . get ( 'authMechanism' , 'GSSAPI' ) kerberos_service_name = ms . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) socket = TSocket . TSocket ( ms . host , ms . port ) if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' and auth_mechanism == 'GSSAPI' : try : import saslwrapper as sasl except ImportError : import sasl def sasl_factory ( ) : sasl_client = sasl . Client ( ) sasl_client . setAttr ( ""host"" , ms . host ) sasl_client . setAttr ( ""service"" , kerberos_service_name ) sasl_client . init ( ) return sasl_client from thrift_sasl import TSaslClientTransport transport = TSaslClientTransport ( sasl_factory , ""GSSAPI"" , socket ) else : transport = TTransport . TBufferedTransport ( socket ) protocol = TBinaryProtocol . TBinaryProtocol ( transport ) return hmsclient . HMSClient ( iprot = protocol )",Returns a Hive thrift client .
"def check_for_partition ( self , schema , table , partition ) : with self . metastore as client : partitions = client . get_partitions_by_filter ( schema , table , partition , 1 ) if partitions : return True else : return False",Checks whether a partition exists
"def check_for_named_partition ( self , schema , table , partition_name ) : with self . metastore as client : return client . check_for_named_partition ( schema , table , partition_name )",Checks whether a partition with a given name exists
"def get_table ( self , table_name , db = 'default' ) : if db == 'default' and '.' in table_name : db , table_name = table_name . split ( '.' ) [ : 2 ] with self . metastore as client : return client . get_table ( dbname = db , tbl_name = table_name )",Get a metastore table object
"def get_tables ( self , db , pattern = '*' ) : with self . metastore as client : tables = client . get_tables ( db_name = db , pattern = pattern ) return client . get_table_objects_by_name ( db , tables )",Get a metastore table object
"def get_partitions ( self , schema , table_name , filter = None ) : with self . metastore as client : table = client . get_table ( dbname = schema , tbl_name = table_name ) if len ( table . partitionKeys ) == 0 : raise AirflowException ( ""The table isn't partitioned"" ) else : if filter : parts = client . get_partitions_by_filter ( db_name = schema , tbl_name = table_name , filter = filter , max_parts = HiveMetastoreHook . MAX_PART_COUNT ) else : parts = client . get_partitions ( db_name = schema , tbl_name = table_name , max_parts = HiveMetastoreHook . MAX_PART_COUNT ) pnames = [ p . name for p in table . partitionKeys ] return [ dict ( zip ( pnames , p . values ) ) for p in parts ]",Returns a list of all partitions in a table . Works only for tables with less than 32767 ( java short max val ) . For subpartitioned table the number might easily exceed this .
"def _get_max_partition_from_part_specs ( part_specs , partition_key , filter_map ) : if not part_specs : return None if partition_key not in part_specs [ 0 ] . keys ( ) : raise AirflowException ( ""Provided partition_key {} "" ""is not in part_specs."" . format ( partition_key ) ) if filter_map : is_subset = set ( filter_map . keys ( ) ) . issubset ( set ( part_specs [ 0 ] . keys ( ) ) ) if filter_map and not is_subset : raise AirflowException ( ""Keys in provided filter_map {} "" ""are not subset of part_spec keys: {}"" . format ( ', ' . join ( filter_map . keys ( ) ) , ', ' . join ( part_specs [ 0 ] . keys ( ) ) ) ) candidates = [ p_dict [ partition_key ] for p_dict in part_specs if filter_map is None or all ( item in p_dict . items ( ) for item in filter_map . items ( ) ) ] if not candidates : return None else : return max ( candidates ) . encode ( 'utf-8' )",Helper method to get max partition of partitions with partition_key from part specs . key : value pair in filter_map will be used to filter out partitions .
"def max_partition ( self , schema , table_name , field = None , filter_map = None ) : with self . metastore as client : table = client . get_table ( dbname = schema , tbl_name = table_name ) key_name_set = set ( key . name for key in table . partitionKeys ) if len ( table . partitionKeys ) == 1 : field = table . partitionKeys [ 0 ] . name elif not field : raise AirflowException ( ""Please specify the field you want the max "" ""value for."" ) elif field not in key_name_set : raise AirflowException ( ""Provided field is not a partition key."" ) if filter_map and not set ( filter_map . keys ( ) ) . issubset ( key_name_set ) : raise AirflowException ( ""Provided filter_map contains keys "" ""that are not partition key."" ) part_names = client . get_partition_names ( schema , table_name , max_parts = HiveMetastoreHook . MAX_PART_COUNT ) part_specs = [ client . partition_name_to_spec ( part_name ) for part_name in part_names ] return HiveMetastoreHook . _get_max_partition_from_part_specs ( part_specs , field , filter_map )",Returns the maximum value for all partitions with given field in a table . If only one partition key exist in the table the key will be used as field . filter_map should be a partition_key : partition_value map and will be used to filter out partitions .
"def table_exists ( self , table_name , db = 'default' ) : try : self . get_table ( table_name , db ) return True except Exception : return False",Check if table exists
"def get_conn ( self , schema = None ) : db = self . get_connection ( self . hiveserver2_conn_id ) auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'NONE' ) if auth_mechanism == 'NONE' and db . login is None : username = 'airflow' kerberos_service_name = None if configuration . conf . get ( 'core' , 'security' ) == 'kerberos' : auth_mechanism = db . extra_dejson . get ( 'authMechanism' , 'KERBEROS' ) kerberos_service_name = db . extra_dejson . get ( 'kerberos_service_name' , 'hive' ) if auth_mechanism == 'GSSAPI' : self . log . warning ( ""Detected deprecated 'GSSAPI' for authMechanism "" ""for %s. Please use 'KERBEROS' instead"" , self . hiveserver2_conn_id ) auth_mechanism = 'KERBEROS' from pyhive . hive import connect return connect ( host = db . host , port = db . port , auth = auth_mechanism , kerberos_service_name = kerberos_service_name , username = db . login or username , password = db . password , database = schema or db . schema or 'default' )",Returns a Hive connection object .
"def get_results ( self , hql , schema = 'default' , fetch_size = None , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) results = { 'data' : list ( results_iter ) , 'header' : header } return results",Get results of the provided hql in target schema .
"def to_csv ( self , hql , csv_filepath , schema = 'default' , delimiter = ',' , lineterminator = '\r\n' , output_header = True , fetch_size = 1000 , hive_conf = None ) : results_iter = self . _get_results ( hql , schema , fetch_size = fetch_size , hive_conf = hive_conf ) header = next ( results_iter ) message = None i = 0 with open ( csv_filepath , 'wb' ) as f : writer = csv . writer ( f , delimiter = delimiter , lineterminator = lineterminator , encoding = 'utf-8' ) try : if output_header : self . log . debug ( 'Cursor description is %s' , header ) writer . writerow ( [ c [ 0 ] for c in header ] ) for i , row in enumerate ( results_iter , 1 ) : writer . writerow ( row ) if i % fetch_size == 0 : self . log . info ( ""Written %s rows so far."" , i ) except ValueError as exception : message = str ( exception ) if message : os . remove ( csv_filepath ) raise ValueError ( message ) self . log . info ( ""Done. Loaded a total of %s rows."" , i )",Execute hql in target schema and write results to a csv file .
"def get_records ( self , hql , schema = 'default' , hive_conf = None ) : return self . get_results ( hql , schema = schema , hive_conf = hive_conf ) [ 'data' ]",Get a set of records from a Hive query .
"def get_pandas_df ( self , hql , schema = 'default' ) : import pandas as pd res = self . get_results ( hql , schema = schema ) df = pd . DataFrame ( res [ 'data' ] ) df . columns = [ c [ 0 ] for c in res [ 'header' ] ] return df",Get a pandas dataframe from a Hive query
def get_conn ( self ) : if not self . _client : self . _client = ProductSearchClient ( credentials = self . _get_credentials ( ) ) return self . _client,Retrieves connection to Cloud Vision .
"def create_product_set ( self , location , product_set , project_id = None , product_set_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) parent = ProductSearchClient . location_path ( project_id , location ) self . log . info ( 'Creating a new ProductSet under the parent: %s' , parent ) response = client . create_product_set ( parent = parent , product_set = product_set , product_set_id = product_set_id , retry = retry , timeout = timeout , metadata = metadata , ) self . log . info ( 'ProductSet created: %s' , response . name if response else '' ) self . log . debug ( 'ProductSet created:\n%s' , response ) if not product_set_id : product_set_id = self . _get_autogenerated_id ( response ) self . log . info ( 'Extracted autogenerated ProductSet ID from the response: %s' , product_set_id ) return product_set_id",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductSetCreateOperator
"def get_product_set ( self , location , product_set_id , project_id = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) name = ProductSearchClient . product_set_path ( project_id , location , product_set_id ) self . log . info ( 'Retrieving ProductSet: %s' , name ) response = client . get_product_set ( name = name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'ProductSet retrieved.' ) self . log . debug ( 'ProductSet retrieved:\n%s' , response ) return MessageToDict ( response )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductSetGetOperator
"def update_product_set ( self , product_set , location = None , product_set_id = None , update_mask = None , project_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) product_set = self . product_set_name_determiner . get_entity_with_name ( product_set , product_set_id , location , project_id ) self . log . info ( 'Updating ProductSet: %s' , product_set . name ) response = client . update_product_set ( product_set = product_set , update_mask = update_mask , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'ProductSet updated: %s' , response . name if response else '' ) self . log . debug ( 'ProductSet updated:\n%s' , response ) return MessageToDict ( response )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductSetUpdateOperator
"def delete_product_set ( self , location , product_set_id , project_id = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) name = ProductSearchClient . product_set_path ( project_id , location , product_set_id ) self . log . info ( 'Deleting ProductSet: %s' , name ) client . delete_product_set ( name = name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'ProductSet with the name [%s] deleted.' , name )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductSetDeleteOperator
"def create_product ( self , location , product , project_id = None , product_id = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) parent = ProductSearchClient . location_path ( project_id , location ) self . log . info ( 'Creating a new Product under the parent: %s' , parent ) response = client . create_product ( parent = parent , product = product , product_id = product_id , retry = retry , timeout = timeout , metadata = metadata , ) self . log . info ( 'Product created: %s' , response . name if response else '' ) self . log . debug ( 'Product created:\n%s' , response ) if not product_id : product_id = self . _get_autogenerated_id ( response ) self . log . info ( 'Extracted autogenerated Product ID from the response: %s' , product_id ) return product_id",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductCreateOperator
"def get_product ( self , location , product_id , project_id = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) name = ProductSearchClient . product_path ( project_id , location , product_id ) self . log . info ( 'Retrieving Product: %s' , name ) response = client . get_product ( name = name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'Product retrieved.' ) self . log . debug ( 'Product retrieved:\n%s' , response ) return MessageToDict ( response )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductGetOperator
"def update_product ( self , product , location = None , product_id = None , update_mask = None , project_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) product = self . product_name_determiner . get_entity_with_name ( product , product_id , location , project_id ) self . log . info ( 'Updating ProductSet: %s' , product . name ) response = client . update_product ( product = product , update_mask = update_mask , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'Product updated: %s' , response . name if response else '' ) self . log . debug ( 'Product updated:\n%s' , response ) return MessageToDict ( response )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductUpdateOperator
"def delete_product ( self , location , product_id , project_id = None , retry = None , timeout = None , metadata = None ) : client = self . get_conn ( ) name = ProductSearchClient . product_path ( project_id , location , product_id ) self . log . info ( 'Deleting ProductSet: %s' , name ) client . delete_product ( name = name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'Product with the name [%s] deleted:' , name )",For the documentation see : : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionProductDeleteOperator
"def create_reference_image ( self , location , product_id , reference_image , reference_image_id = None , project_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) self . log . info ( 'Creating ReferenceImage' ) parent = ProductSearchClient . product_path ( project = project_id , location = location , product = product_id ) response = client . create_reference_image ( parent = parent , reference_image = reference_image , reference_image_id = reference_image_id , retry = retry , timeout = timeout , metadata = metadata , ) self . log . info ( 'ReferenceImage created: %s' , response . name if response else '' ) self . log . debug ( 'ReferenceImage created:\n%s' , response ) if not reference_image_id : reference_image_id = self . _get_autogenerated_id ( response ) self . log . info ( 'Extracted autogenerated ReferenceImage ID from the response: %s' , reference_image_id ) return reference_image_id",For the documentation see : : py : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionReferenceImageCreateOperator
"def delete_reference_image ( self , location , product_id , reference_image_id , project_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) self . log . info ( 'Deleting ReferenceImage' ) name = ProductSearchClient . reference_image_path ( project = project_id , location = location , product = product_id , reference_image = reference_image_id ) response = client . delete_reference_image ( name = name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'ReferenceImage with the name [%s] deleted.' , name ) return MessageToDict ( response )",For the documentation see : : py : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionReferenceImageCreateOperator
"def add_product_to_product_set ( self , product_set_id , product_id , location = None , project_id = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) product_name = ProductSearchClient . product_path ( project_id , location , product_id ) product_set_name = ProductSearchClient . product_set_path ( project_id , location , product_set_id ) self . log . info ( 'Add Product[name=%s] to Product Set[name=%s]' , product_name , product_set_name ) client . add_product_to_product_set ( name = product_set_name , product = product_name , retry = retry , timeout = timeout , metadata = metadata ) self . log . info ( 'Product added to Product Set' )",For the documentation see : : py : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionAddProductToProductSetOperator
"def annotate_image ( self , request , retry = None , timeout = None ) : client = self . annotator_client self . log . info ( 'Annotating image' ) response = client . annotate_image ( request = request , retry = retry , timeout = timeout ) self . log . info ( 'Image annotated' ) return MessageToDict ( response )",For the documentation see : : py : class : ~airflow . contrib . operators . gcp_vision_image_annotator_operator . CloudVisionAnnotateImage
"def safe_search_detection ( self , image , max_results = None , retry = None , timeout = None , additional_properties = None ) : client = self . annotator_client self . log . info ( ""Detecting safe search"" ) if additional_properties is None : additional_properties = { } response = client . safe_search_detection ( image = image , max_results = max_results , retry = retry , timeout = timeout , * * additional_properties ) response = MessageToDict ( response ) self . _check_for_error ( response ) self . log . info ( ""Safe search detection finished"" ) return response",For the documentation see : : py : class : ~airflow . contrib . operators . gcp_vision_operator . CloudVisionDetectImageSafeSearchOperator
"def _get_endpoint ( self ) : conn = self . get_connection ( self . http_conn_id ) token = conn . password if not token : raise AirflowException ( 'Dingding token is requests but get nothing, ' 'check you conn_id configuration.' ) return 'robot/send?access_token={}' . format ( token )",Get Dingding endpoint for sending message .
"def _build_message ( self ) : if self . message_type in [ 'text' , 'markdown' ] : data = { 'msgtype' : self . message_type , self . message_type : { 'content' : self . message } if self . message_type == 'text' else self . message , 'at' : { 'atMobiles' : self . at_mobiles , 'isAtAll' : self . at_all } } else : data = { 'msgtype' : self . message_type , self . message_type : self . message } return json . dumps ( data )",Build different type of Dingding message As most commonly used type text message just need post message content rather than a dict like { content : message }
"def send ( self ) : support_type = [ 'text' , 'link' , 'markdown' , 'actionCard' , 'feedCard' ] if self . message_type not in support_type : raise ValueError ( 'DingdingWebhookHook only support {} ' 'so far, but receive {}' . format ( support_type , self . message_type ) ) data = self . _build_message ( ) self . log . info ( 'Sending Dingding type %s message %s' , self . message_type , data ) resp = self . run ( endpoint = self . _get_endpoint ( ) , data = data , headers = { 'Content-Type' : 'application/json' } ) if int ( resp . json ( ) . get ( 'errcode' ) ) != 0 : raise AirflowException ( 'Send Dingding message failed, receive error ' 'message %s' , resp . text ) self . log . info ( 'Success Send Dingding message' )",Send Dingding message
"def _read ( self , ti , try_number , metadata = None ) : if not metadata : metadata = { 'offset' : 0 } if 'offset' not in metadata : metadata [ 'offset' ] = 0 offset = metadata [ 'offset' ] log_id = self . _render_log_id ( ti , try_number ) logs = self . es_read ( log_id , offset ) next_offset = offset if not logs else logs [ - 1 ] . offset metadata [ 'offset' ] = next_offset metadata [ 'end_of_log' ] = False if not logs else logs [ - 1 ] . message == self . end_of_log_mark . strip ( ) cur_ts = pendulum . now ( ) if 'last_log_timestamp' in metadata : last_log_ts = timezone . parse ( metadata [ 'last_log_timestamp' ] ) if cur_ts . diff ( last_log_ts ) . in_minutes ( ) >= 5 : metadata [ 'end_of_log' ] = True if offset != next_offset or 'last_log_timestamp' not in metadata : metadata [ 'last_log_timestamp' ] = str ( cur_ts ) message = '\n' . join ( [ log . message for log in logs ] ) return message , metadata",Endpoint for streaming log . : param ti : task instance object : param try_number : try_number of the task instance : param metadata : log metadata can be used for steaming log reading and auto - tailing . : return : a list of log documents and metadata .
"def es_read ( self , log_id , offset ) : s = Search ( using = self . client ) . query ( 'match_phrase' , log_id = log_id ) . sort ( 'offset' ) s = s . filter ( 'range' , offset = { 'gt' : offset } ) logs = [ ] if s . count ( ) != 0 : try : logs = s [ self . MAX_LINE_PER_PAGE * self . PAGE : self . MAX_LINE_PER_PAGE ] . execute ( ) except Exception as e : self . log . exception ( 'Could not read log with log_id: %s, error: %s' , log_id , str ( e ) ) return logs",Returns the logs matching log_id in Elasticsearch and next offset . Returns if no log is found or there was an error . : param log_id : the log_id of the log to read . : type log_id : str : param offset : the offset start to read log from . : type offset : str
"def _bind_parameters ( operation , parameters ) : string_parameters = { } for ( name , value ) in iteritems ( parameters ) : if value is None : string_parameters [ name ] = 'NULL' elif isinstance ( value , basestring ) : string_parameters [ name ] = ""'"" + _escape ( value ) + ""'"" else : string_parameters [ name ] = str ( value ) return operation % string_parameters",Helper method that binds parameters to a SQL query .
"def _escape ( s ) : e = s e = e . replace ( '\\' , '\\\\' ) e = e . replace ( '\n' , '\\n' ) e = e . replace ( '\r' , '\\r' ) e = e . replace ( ""'"" , ""\\'"" ) e = e . replace ( '""' , '\\""' ) return e",Helper method that escapes parameters to a SQL query .
"def _bq_cast ( string_field , bq_type ) : if string_field is None : return None elif bq_type == 'INTEGER' : return int ( string_field ) elif bq_type == 'FLOAT' or bq_type == 'TIMESTAMP' : return float ( string_field ) elif bq_type == 'BOOLEAN' : if string_field not in [ 'true' , 'false' ] : raise ValueError ( ""{} must have value 'true' or 'false'"" . format ( string_field ) ) return string_field == 'true' else : return string_field",Helper method that casts a BigQuery row to the appropriate data types . This is useful because BigQuery returns all fields as strings .
"def _validate_value ( key , value , expected_type ) : if not isinstance ( value , expected_type ) : raise TypeError ( ""{} argument must have a type {} not {}"" . format ( key , expected_type , type ( value ) ) )",function to check expected type and raise error if type is not correct
"def get_conn ( self ) : service = self . get_service ( ) project = self . _get_field ( 'project' ) return BigQueryConnection ( service = service , project_id = project , use_legacy_sql = self . use_legacy_sql , location = self . location , num_retries = self . num_retries )",Returns a BigQuery PEP 249 connection object .
"def get_service ( self ) : http_authorized = self . _authorize ( ) return build ( 'bigquery' , 'v2' , http = http_authorized , cache_discovery = False )",Returns a BigQuery service object .
"def get_pandas_df ( self , sql , parameters = None , dialect = None ) : private_key = self . _get_field ( 'key_path' , None ) or self . _get_field ( 'keyfile_dict' , None ) if dialect is None : dialect = 'legacy' if self . use_legacy_sql else 'standard' return read_gbq ( sql , project_id = self . _get_field ( 'project' ) , dialect = dialect , verbose = False , private_key = private_key )",Returns a Pandas DataFrame for the results produced by a BigQuery query . The DbApiHook method must be overridden because Pandas doesn t support PEP 249 connections except for SQLite . See :
"def table_exists ( self , project_id , dataset_id , table_id ) : service = self . get_service ( ) try : service . tables ( ) . get ( projectId = project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) return True except HttpError as e : if e . resp [ 'status' ] == '404' : return False raise",Checks for the existence of a table in Google BigQuery .
"def create_empty_table ( self , project_id , dataset_id , table_id , schema_fields = None , time_partitioning = None , cluster_fields = None , labels = None , view = None , num_retries = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { 'tableReference' : { 'tableId' : table_id } } if schema_fields : table_resource [ 'schema' ] = { 'fields' : schema_fields } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if cluster_fields : table_resource [ 'clustering' ] = { 'fields' : cluster_fields } if labels : table_resource [ 'labels' ] = labels if view : table_resource [ 'view' ] = view num_retries = num_retries if num_retries else self . num_retries self . log . info ( 'Creating Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = num_retries ) self . log . info ( 'Table created successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Creates a new empty table in the dataset . To create a view which is defined by a SQL query parse a dictionary to view kwarg
"def create_external_table ( self , external_project_dataset_table , schema_fields , source_uris , source_format = 'CSV' , autodetect = False , compression = 'NONE' , ignore_unknown_values = False , max_bad_records = 0 , skip_leading_rows = 0 , field_delimiter = ',' , quote_character = None , allow_quoted_newlines = False , allow_jagged_rows = False , src_fmt_configs = None , labels = None ) : if src_fmt_configs is None : src_fmt_configs = { } project_id , dataset_id , external_table_id = _split_tablename ( table_input = external_project_dataset_table , default_project_id = self . project_id , var_name = 'external_project_dataset_table' ) source_format = source_format . upper ( ) allowed_formats = [ ""CSV"" , ""NEWLINE_DELIMITED_JSON"" , ""AVRO"" , ""GOOGLE_SHEETS"" , ""DATASTORE_BACKUP"" , ""PARQUET"" ] if source_format not in allowed_formats : raise ValueError ( ""{0} is not a valid source format. "" ""Please use one of the following types: {1}"" . format ( source_format , allowed_formats ) ) compression = compression . upper ( ) allowed_compressions = [ 'NONE' , 'GZIP' ] if compression not in allowed_compressions : raise ValueError ( ""{0} is not a valid compression format. "" ""Please use one of the following types: {1}"" . format ( compression , allowed_compressions ) ) table_resource = { 'externalDataConfiguration' : { 'autodetect' : autodetect , 'sourceFormat' : source_format , 'sourceUris' : source_uris , 'compression' : compression , 'ignoreUnknownValues' : ignore_unknown_values } , 'tableReference' : { 'projectId' : project_id , 'datasetId' : dataset_id , 'tableId' : external_table_id , } } if schema_fields : table_resource [ 'externalDataConfiguration' ] . update ( { 'schema' : { 'fields' : schema_fields } } ) self . log . info ( 'Creating external table: %s' , external_project_dataset_table ) if max_bad_records : table_resource [ 'externalDataConfiguration' ] [ 'maxBadRecords' ] = max_bad_records if 'skipLeadingRows' not in src_fmt_configs : src_fmt_configs [ 'skipLeadingRows' ] = skip_leading_rows if 'fieldDelimiter' not in src_fmt_configs : src_fmt_configs [ 'fieldDelimiter' ] = field_delimiter if 'quote_character' not in src_fmt_configs : src_fmt_configs [ 'quote' ] = quote_character if 'allowQuotedNewlines' not in src_fmt_configs : src_fmt_configs [ 'allowQuotedNewlines' ] = allow_quoted_newlines if 'allowJaggedRows' not in src_fmt_configs : src_fmt_configs [ 'allowJaggedRows' ] = allow_jagged_rows src_fmt_to_param_mapping = { 'CSV' : 'csvOptions' , 'GOOGLE_SHEETS' : 'googleSheetsOptions' } src_fmt_to_configs_mapping = { 'csvOptions' : [ 'allowJaggedRows' , 'allowQuotedNewlines' , 'fieldDelimiter' , 'skipLeadingRows' , 'quote' ] , 'googleSheetsOptions' : [ 'skipLeadingRows' ] } if source_format in src_fmt_to_param_mapping . keys ( ) : valid_configs = src_fmt_to_configs_mapping [ src_fmt_to_param_mapping [ source_format ] ] src_fmt_configs = { k : v for k , v in src_fmt_configs . items ( ) if k in valid_configs } table_resource [ 'externalDataConfiguration' ] [ src_fmt_to_param_mapping [ source_format ] ] = src_fmt_configs if labels : table_resource [ 'labels' ] = labels try : self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries ) self . log . info ( 'External table created successfully: %s' , external_project_dataset_table ) except HttpError as err : raise Exception ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Creates a new external table in the dataset with the data in Google Cloud Storage . See here :
"def patch_table ( self , dataset_id , table_id , project_id = None , description = None , expiration_time = None , external_data_configuration = None , friendly_name = None , labels = None , schema = None , time_partitioning = None , view = None , require_partition_filter = None ) : project_id = project_id if project_id is not None else self . project_id table_resource = { } if description is not None : table_resource [ 'description' ] = description if expiration_time is not None : table_resource [ 'expirationTime' ] = expiration_time if external_data_configuration : table_resource [ 'externalDataConfiguration' ] = external_data_configuration if friendly_name is not None : table_resource [ 'friendlyName' ] = friendly_name if labels : table_resource [ 'labels' ] = labels if schema : table_resource [ 'schema' ] = { 'fields' : schema } if time_partitioning : table_resource [ 'timePartitioning' ] = time_partitioning if view : table_resource [ 'view' ] = view if require_partition_filter is not None : table_resource [ 'requirePartitionFilter' ] = require_partition_filter self . log . info ( 'Patching Table %s:%s.%s' , project_id , dataset_id , table_id ) try : self . service . tables ( ) . patch ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Table patched successfully: %s:%s.%s' , project_id , dataset_id , table_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Patch information in an existing table . It only updates fileds that are provided in the request object .
"def run_query ( self , sql , destination_dataset_table = None , write_disposition = 'WRITE_EMPTY' , allow_large_results = False , flatten_results = None , udf_config = None , use_legacy_sql = None , maximum_billing_tier = None , maximum_bytes_billed = None , create_disposition = 'CREATE_IF_NEEDED' , query_params = None , labels = None , schema_update_options = ( ) , priority = 'INTERACTIVE' , time_partitioning = None , api_resource_configs = None , cluster_fields = None , location = None ) : if time_partitioning is None : time_partitioning = { } if location : self . location = location if not api_resource_configs : api_resource_configs = self . api_resource_configs else : _validate_value ( 'api_resource_configs' , api_resource_configs , dict ) configuration = deepcopy ( api_resource_configs ) if 'query' not in configuration : configuration [ 'query' ] = { } else : _validate_value ( ""api_resource_configs['query']"" , configuration [ 'query' ] , dict ) if sql is None and not configuration [ 'query' ] . get ( 'query' , None ) : raise TypeError ( '`BigQueryBaseCursor.run_query` ' 'missing 1 required positional argument: `sql`' ) allowed_schema_update_options = [ 'ALLOW_FIELD_ADDITION' , ""ALLOW_FIELD_RELAXATION"" ] if not set ( allowed_schema_update_options ) . issuperset ( set ( schema_update_options ) ) : raise ValueError ( ""{0} contains invalid schema update options. "" ""Please only use one or more of the following "" ""options: {1}"" . format ( schema_update_options , allowed_schema_update_options ) ) if schema_update_options : if write_disposition not in [ ""WRITE_APPEND"" , ""WRITE_TRUNCATE"" ] : raise ValueError ( ""schema_update_options is only "" ""allowed if write_disposition is "" ""'WRITE_APPEND' or 'WRITE_TRUNCATE'."" ) if destination_dataset_table : destination_project , destination_dataset , destination_table = _split_tablename ( table_input = destination_dataset_table , default_project_id = self . project_id ) destination_dataset_table = { 'projectId' : destination_project , 'datasetId' : destination_dataset , 'tableId' : destination_table , } if cluster_fields : cluster_fields = { 'fields' : cluster_fields } query_param_list = [ ( sql , 'query' , None , six . string_types ) , ( priority , 'priority' , 'INTERACTIVE' , six . string_types ) , ( use_legacy_sql , 'useLegacySql' , self . use_legacy_sql , bool ) , ( query_params , 'queryParameters' , None , list ) , ( udf_config , 'userDefinedFunctionResources' , None , list ) , ( maximum_billing_tier , 'maximumBillingTier' , None , int ) , ( maximum_bytes_billed , 'maximumBytesBilled' , None , float ) , ( time_partitioning , 'timePartitioning' , { } , dict ) , ( schema_update_options , 'schemaUpdateOptions' , None , tuple ) , ( destination_dataset_table , 'destinationTable' , None , dict ) , ( cluster_fields , 'clustering' , None , dict ) , ] for param_tuple in query_param_list : param , param_name , param_default , param_type = param_tuple if param_name not in configuration [ 'query' ] and param in [ None , { } , ( ) ] : if param_name == 'timePartitioning' : param_default = _cleanse_time_partitioning ( destination_dataset_table , time_partitioning ) param = param_default if param not in [ None , { } , ( ) ] : _api_resource_configs_duplication_check ( param_name , param , configuration [ 'query' ] ) configuration [ 'query' ] [ param_name ] = param _validate_value ( param_name , configuration [ 'query' ] [ param_name ] , param_type ) if param_name == 'schemaUpdateOptions' and param : self . log . info ( ""Adding experimental 'schemaUpdateOptions': "" ""%s"" , schema_update_options ) if param_name == 'destinationTable' : for key in [ 'projectId' , 'datasetId' , 'tableId' ] : if key not in configuration [ 'query' ] [ 'destinationTable' ] : raise ValueError ( ""Not correct 'destinationTable' in "" ""api_resource_configs. 'destinationTable' "" ""must be a dict with {'projectId':'', "" ""'datasetId':'', 'tableId':''}"" ) configuration [ 'query' ] . update ( { 'allowLargeResults' : allow_large_results , 'flattenResults' : flatten_results , 'writeDisposition' : write_disposition , 'createDisposition' : create_disposition , } ) if 'useLegacySql' in configuration [ 'query' ] and configuration [ 'query' ] [ 'useLegacySql' ] and 'queryParameters' in configuration [ 'query' ] : raise ValueError ( ""Query parameters are not allowed "" ""when using legacy SQL"" ) if labels : _api_resource_configs_duplication_check ( 'labels' , labels , configuration ) configuration [ 'labels' ] = labels return self . run_with_configuration ( configuration )",Executes a BigQuery SQL query . Optionally persists results in a BigQuery table . See here :
"def run_extract ( self , source_project_dataset_table , destination_cloud_storage_uris , compression = 'NONE' , export_format = 'CSV' , field_delimiter = ',' , print_header = True , labels = None ) : source_project , source_dataset , source_table = _split_tablename ( table_input = source_project_dataset_table , default_project_id = self . project_id , var_name = 'source_project_dataset_table' ) configuration = { 'extract' : { 'sourceTable' : { 'projectId' : source_project , 'datasetId' : source_dataset , 'tableId' : source_table , } , 'compression' : compression , 'destinationUris' : destination_cloud_storage_uris , 'destinationFormat' : export_format , } } if labels : configuration [ 'labels' ] = labels if export_format == 'CSV' : configuration [ 'extract' ] [ 'fieldDelimiter' ] = field_delimiter configuration [ 'extract' ] [ 'printHeader' ] = print_header return self . run_with_configuration ( configuration )",Executes a BigQuery extract command to copy data from BigQuery to Google Cloud Storage . See here :
"def run_copy ( self , source_project_dataset_tables , destination_project_dataset_table , write_disposition = 'WRITE_EMPTY' , create_disposition = 'CREATE_IF_NEEDED' , labels = None ) : source_project_dataset_tables = ( [ source_project_dataset_tables ] if not isinstance ( source_project_dataset_tables , list ) else source_project_dataset_tables ) source_project_dataset_tables_fixup = [ ] for source_project_dataset_table in source_project_dataset_tables : source_project , source_dataset , source_table = _split_tablename ( table_input = source_project_dataset_table , default_project_id = self . project_id , var_name = 'source_project_dataset_table' ) source_project_dataset_tables_fixup . append ( { 'projectId' : source_project , 'datasetId' : source_dataset , 'tableId' : source_table } ) destination_project , destination_dataset , destination_table = _split_tablename ( table_input = destination_project_dataset_table , default_project_id = self . project_id ) configuration = { 'copy' : { 'createDisposition' : create_disposition , 'writeDisposition' : write_disposition , 'sourceTables' : source_project_dataset_tables_fixup , 'destinationTable' : { 'projectId' : destination_project , 'datasetId' : destination_dataset , 'tableId' : destination_table } } } if labels : configuration [ 'labels' ] = labels return self . run_with_configuration ( configuration )",Executes a BigQuery copy command to copy data from one BigQuery table to another . See here :
"def run_load ( self , destination_project_dataset_table , source_uris , schema_fields = None , source_format = 'CSV' , create_disposition = 'CREATE_IF_NEEDED' , skip_leading_rows = 0 , write_disposition = 'WRITE_EMPTY' , field_delimiter = ',' , max_bad_records = 0 , quote_character = None , ignore_unknown_values = False , allow_quoted_newlines = False , allow_jagged_rows = False , schema_update_options = ( ) , src_fmt_configs = None , time_partitioning = None , cluster_fields = None , autodetect = False ) : if schema_fields is None and not autodetect : raise ValueError ( 'You must either pass a schema or autodetect=True.' ) if src_fmt_configs is None : src_fmt_configs = { } source_format = source_format . upper ( ) allowed_formats = [ ""CSV"" , ""NEWLINE_DELIMITED_JSON"" , ""AVRO"" , ""GOOGLE_SHEETS"" , ""DATASTORE_BACKUP"" , ""PARQUET"" ] if source_format not in allowed_formats : raise ValueError ( ""{0} is not a valid source format. "" ""Please use one of the following types: {1}"" . format ( source_format , allowed_formats ) ) allowed_schema_update_options = [ 'ALLOW_FIELD_ADDITION' , ""ALLOW_FIELD_RELAXATION"" ] if not set ( allowed_schema_update_options ) . issuperset ( set ( schema_update_options ) ) : raise ValueError ( ""{0} contains invalid schema update options."" ""Please only use one or more of the following options: {1}"" . format ( schema_update_options , allowed_schema_update_options ) ) destination_project , destination_dataset , destination_table = _split_tablename ( table_input = destination_project_dataset_table , default_project_id = self . project_id , var_name = 'destination_project_dataset_table' ) configuration = { 'load' : { 'autodetect' : autodetect , 'createDisposition' : create_disposition , 'destinationTable' : { 'projectId' : destination_project , 'datasetId' : destination_dataset , 'tableId' : destination_table , } , 'sourceFormat' : source_format , 'sourceUris' : source_uris , 'writeDisposition' : write_disposition , 'ignoreUnknownValues' : ignore_unknown_values } } time_partitioning = _cleanse_time_partitioning ( destination_project_dataset_table , time_partitioning ) if time_partitioning : configuration [ 'load' ] . update ( { 'timePartitioning' : time_partitioning } ) if cluster_fields : configuration [ 'load' ] . update ( { 'clustering' : { 'fields' : cluster_fields } } ) if schema_fields : configuration [ 'load' ] [ 'schema' ] = { 'fields' : schema_fields } if schema_update_options : if write_disposition not in [ ""WRITE_APPEND"" , ""WRITE_TRUNCATE"" ] : raise ValueError ( ""schema_update_options is only "" ""allowed if write_disposition is "" ""'WRITE_APPEND' or 'WRITE_TRUNCATE'."" ) else : self . log . info ( ""Adding experimental 'schemaUpdateOptions': %s"" , schema_update_options ) configuration [ 'load' ] [ 'schemaUpdateOptions' ] = schema_update_options if max_bad_records : configuration [ 'load' ] [ 'maxBadRecords' ] = max_bad_records if 'skipLeadingRows' not in src_fmt_configs : src_fmt_configs [ 'skipLeadingRows' ] = skip_leading_rows if 'fieldDelimiter' not in src_fmt_configs : src_fmt_configs [ 'fieldDelimiter' ] = field_delimiter if 'ignoreUnknownValues' not in src_fmt_configs : src_fmt_configs [ 'ignoreUnknownValues' ] = ignore_unknown_values if quote_character is not None : src_fmt_configs [ 'quote' ] = quote_character if allow_quoted_newlines : src_fmt_configs [ 'allowQuotedNewlines' ] = allow_quoted_newlines src_fmt_to_configs_mapping = { 'CSV' : [ 'allowJaggedRows' , 'allowQuotedNewlines' , 'autodetect' , 'fieldDelimiter' , 'skipLeadingRows' , 'ignoreUnknownValues' , 'nullMarker' , 'quote' ] , 'DATASTORE_BACKUP' : [ 'projectionFields' ] , 'NEWLINE_DELIMITED_JSON' : [ 'autodetect' , 'ignoreUnknownValues' ] , 'PARQUET' : [ 'autodetect' , 'ignoreUnknownValues' ] , 'AVRO' : [ 'useAvroLogicalTypes' ] , } valid_configs = src_fmt_to_configs_mapping [ source_format ] src_fmt_configs = { k : v for k , v in src_fmt_configs . items ( ) if k in valid_configs } configuration [ 'load' ] . update ( src_fmt_configs ) if allow_jagged_rows : configuration [ 'load' ] [ 'allowJaggedRows' ] = allow_jagged_rows return self . run_with_configuration ( configuration )",Executes a BigQuery load command to load data from Google Cloud Storage to BigQuery . See here :
"def run_with_configuration ( self , configuration ) : jobs = self . service . jobs ( ) job_data = { 'configuration' : configuration } query_reply = jobs . insert ( projectId = self . project_id , body = job_data ) . execute ( num_retries = self . num_retries ) self . running_job_id = query_reply [ 'jobReference' ] [ 'jobId' ] if 'location' in query_reply [ 'jobReference' ] : location = query_reply [ 'jobReference' ] [ 'location' ] else : location = self . location keep_polling_job = True while keep_polling_job : try : if location : job = jobs . get ( projectId = self . project_id , jobId = self . running_job_id , location = location ) . execute ( num_retries = self . num_retries ) else : job = jobs . get ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) if job [ 'status' ] [ 'state' ] == 'DONE' : keep_polling_job = False if 'errorResult' in job [ 'status' ] : raise Exception ( 'BigQuery job failed. Final error was: {}. The job was: {}' . format ( job [ 'status' ] [ 'errorResult' ] , job ) ) else : self . log . info ( 'Waiting for job to complete : %s, %s' , self . project_id , self . running_job_id ) time . sleep ( 5 ) except HttpError as err : if err . resp . status in [ 500 , 503 ] : self . log . info ( '%s: Retryable error, waiting for job to complete: %s' , err . resp . status , self . running_job_id ) time . sleep ( 5 ) else : raise Exception ( 'BigQuery job status check failed. Final error was: {}' . format ( err . resp . status ) ) return self . running_job_id",Executes a BigQuery SQL query . See here :
"def cancel_query ( self ) : jobs = self . service . jobs ( ) if ( self . running_job_id and not self . poll_job_complete ( self . running_job_id ) ) : self . log . info ( 'Attempting to cancel job : %s, %s' , self . project_id , self . running_job_id ) if self . location : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id , location = self . location ) . execute ( num_retries = self . num_retries ) else : jobs . cancel ( projectId = self . project_id , jobId = self . running_job_id ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'No running BigQuery jobs to cancel.' ) return max_polling_attempts = 12 polling_attempts = 0 job_complete = False while polling_attempts < max_polling_attempts and not job_complete : polling_attempts = polling_attempts + 1 job_complete = self . poll_job_complete ( self . running_job_id ) if job_complete : self . log . info ( 'Job successfully canceled: %s, %s' , self . project_id , self . running_job_id ) elif polling_attempts == max_polling_attempts : self . log . info ( ""Stopping polling due to timeout. Job with id %s "" ""has not completed cancel and may or may not finish."" , self . running_job_id ) else : self . log . info ( 'Waiting for canceled job with id %s to finish.' , self . running_job_id ) time . sleep ( 5 )",Cancel all started queries that have not yet completed
"def get_schema ( self , dataset_id , table_id ) : tables_resource = self . service . tables ( ) . get ( projectId = self . project_id , datasetId = dataset_id , tableId = table_id ) . execute ( num_retries = self . num_retries ) return tables_resource [ 'schema' ]",Get the schema for a given datset . table . see https : // cloud . google . com / bigquery / docs / reference / v2 / tables#resource
"def get_tabledata ( self , dataset_id , table_id , max_results = None , selected_fields = None , page_token = None , start_index = None ) : optional_params = { } if max_results : optional_params [ 'maxResults' ] = max_results if selected_fields : optional_params [ 'selectedFields' ] = selected_fields if page_token : optional_params [ 'pageToken' ] = page_token if start_index : optional_params [ 'startIndex' ] = start_index return ( self . service . tabledata ( ) . list ( projectId = self . project_id , datasetId = dataset_id , tableId = table_id , * * optional_params ) . execute ( num_retries = self . num_retries ) )",Get the data of a given dataset . table and optionally with selected columns . see https : // cloud . google . com / bigquery / docs / reference / v2 / tabledata / list
"def run_table_delete ( self , deletion_dataset_table , ignore_if_missing = False ) : deletion_project , deletion_dataset , deletion_table = _split_tablename ( table_input = deletion_dataset_table , default_project_id = self . project_id ) try : self . service . tables ( ) . delete ( projectId = deletion_project , datasetId = deletion_dataset , tableId = deletion_table ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Deleted table %s:%s.%s.' , deletion_project , deletion_dataset , deletion_table ) except HttpError : if not ignore_if_missing : raise Exception ( 'Table deletion failed. Table does not exist.' ) else : self . log . info ( 'Table does not exist. Skipping.' )",Delete an existing table from the dataset ; If the table does not exist return an error unless ignore_if_missing is set to True .
"def run_table_upsert ( self , dataset_id , table_resource , project_id = None ) : table_id = table_resource [ 'tableReference' ] [ 'tableId' ] project_id = project_id if project_id is not None else self . project_id tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) while True : for table in tables_list_resp . get ( 'tables' , [ ] ) : if table [ 'tableReference' ] [ 'tableId' ] == table_id : self . log . info ( 'Table %s:%s.%s exists, updating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . update ( projectId = project_id , datasetId = dataset_id , tableId = table_id , body = table_resource ) . execute ( num_retries = self . num_retries ) if 'nextPageToken' in tables_list_resp : tables_list_resp = self . service . tables ( ) . list ( projectId = project_id , datasetId = dataset_id , pageToken = tables_list_resp [ 'nextPageToken' ] ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'Table %s:%s.%s does not exist. creating.' , project_id , dataset_id , table_id ) return self . service . tables ( ) . insert ( projectId = project_id , datasetId = dataset_id , body = table_resource ) . execute ( num_retries = self . num_retries )",creates a new empty table in the dataset ; If the table already exists update the existing table . Since BigQuery does not natively allow table upserts this is not an atomic operation .
"def run_grant_dataset_view_access ( self , source_dataset , view_dataset , view_table , source_project = None , view_project = None ) : source_project = source_project if source_project else self . project_id view_project = view_project if view_project else self . project_id source_dataset_resource = self . service . datasets ( ) . get ( projectId = source_project , datasetId = source_dataset ) . execute ( num_retries = self . num_retries ) access = source_dataset_resource [ 'access' ] if 'access' in source_dataset_resource else [ ] view_access = { 'view' : { 'projectId' : view_project , 'datasetId' : view_dataset , 'tableId' : view_table } } if view_access not in access : self . log . info ( 'Granting table %s:%s.%s authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) access . append ( view_access ) return self . service . datasets ( ) . patch ( projectId = source_project , datasetId = source_dataset , body = { 'access' : access } ) . execute ( num_retries = self . num_retries ) else : self . log . info ( 'Table %s:%s.%s already has authorized view access to %s:%s dataset.' , view_project , view_dataset , view_table , source_project , source_dataset ) return source_dataset_resource",Grant authorized view access of a dataset to a view table . If this view has already been granted access to the dataset do nothing . This method is not atomic . Running it may clobber a simultaneous update .
"def create_empty_dataset ( self , dataset_id = """" , project_id = """" , dataset_reference = None ) : if dataset_reference : _validate_value ( 'dataset_reference' , dataset_reference , dict ) else : dataset_reference = { } if ""datasetReference"" not in dataset_reference : dataset_reference [ ""datasetReference"" ] = { } if not dataset_reference [ ""datasetReference"" ] . get ( ""datasetId"" ) and not dataset_id : raise ValueError ( ""{} not provided datasetId. Impossible to create dataset"" ) dataset_required_params = [ ( dataset_id , ""datasetId"" , """" ) , ( project_id , ""projectId"" , self . project_id ) ] for param_tuple in dataset_required_params : param , param_name , param_default = param_tuple if param_name not in dataset_reference [ 'datasetReference' ] : if param_default and not param : self . log . info ( ""%s was not specified. Will be used default value %s."" , param_name , param_default ) param = param_default dataset_reference [ 'datasetReference' ] . update ( { param_name : param } ) elif param : _api_resource_configs_duplication_check ( param_name , param , dataset_reference [ 'datasetReference' ] , 'dataset_reference' ) dataset_id = dataset_reference . get ( ""datasetReference"" ) . get ( ""datasetId"" ) dataset_project_id = dataset_reference . get ( ""datasetReference"" ) . get ( ""projectId"" ) self . log . info ( 'Creating Dataset: %s in project: %s ' , dataset_id , dataset_project_id ) try : self . service . datasets ( ) . insert ( projectId = dataset_project_id , body = dataset_reference ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Dataset created successfully: In project %s ' 'Dataset %s' , dataset_project_id , dataset_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Create a new empty dataset : https : // cloud . google . com / bigquery / docs / reference / rest / v2 / datasets / insert
"def delete_dataset ( self , project_id , dataset_id ) : project_id = project_id if project_id is not None else self . project_id self . log . info ( 'Deleting from project: %s  Dataset:%s' , project_id , dataset_id ) try : self . service . datasets ( ) . delete ( projectId = project_id , datasetId = dataset_id ) . execute ( num_retries = self . num_retries ) self . log . info ( 'Dataset deleted successfully: In project %s ' 'Dataset %s' , project_id , dataset_id ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Delete a dataset of Big query in your project . : param project_id : The name of the project where we have the dataset . : type project_id : str : param dataset_id : The dataset to be delete . : type dataset_id : str : return :
"def get_dataset ( self , dataset_id , project_id = None ) : if not dataset_id or not isinstance ( dataset_id , str ) : raise ValueError ( ""dataset_id argument must be provided and has "" ""a type 'str'. You provided: {}"" . format ( dataset_id ) ) dataset_project_id = project_id if project_id else self . project_id try : dataset_resource = self . service . datasets ( ) . get ( datasetId = dataset_id , projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) self . log . info ( ""Dataset Resource: %s"" , dataset_resource ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return dataset_resource",Method returns dataset_resource if dataset exist and raised 404 error if dataset does not exist
"def get_datasets_list ( self , project_id = None ) : dataset_project_id = project_id if project_id else self . project_id try : datasets_list = self . service . datasets ( ) . list ( projectId = dataset_project_id ) . execute ( num_retries = self . num_retries ) [ 'datasets' ] self . log . info ( ""Datasets List: %s"" , datasets_list ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) ) return datasets_list",Method returns full list of BigQuery datasets in the current project
"def insert_all ( self , project_id , dataset_id , table_id , rows , ignore_unknown_values = False , skip_invalid_rows = False , fail_on_error = False ) : dataset_project_id = project_id if project_id else self . project_id body = { ""rows"" : rows , ""ignoreUnknownValues"" : ignore_unknown_values , ""kind"" : ""bigquery#tableDataInsertAllRequest"" , ""skipInvalidRows"" : skip_invalid_rows , } try : self . log . info ( 'Inserting %s row(s) into Table %s:%s.%s' , len ( rows ) , dataset_project_id , dataset_id , table_id ) resp = self . service . tabledata ( ) . insertAll ( projectId = dataset_project_id , datasetId = dataset_id , tableId = table_id , body = body ) . execute ( num_retries = self . num_retries ) if 'insertErrors' not in resp : self . log . info ( 'All row(s) inserted successfully: %s:%s.%s' , dataset_project_id , dataset_id , table_id ) else : error_msg = '{} insert error(s) occurred: {}:{}.{}. Details: {}' . format ( len ( resp [ 'insertErrors' ] ) , dataset_project_id , dataset_id , table_id , resp [ 'insertErrors' ] ) if fail_on_error : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( error_msg ) ) self . log . info ( error_msg ) except HttpError as err : raise AirflowException ( 'BigQuery job failed. Error was: {}' . format ( err . content ) )",Method to stream data into BigQuery one record at a time without needing to run a load job
"def execute ( self , operation , parameters = None ) : sql = _bind_parameters ( operation , parameters ) if parameters else operation self . job_id = self . run_query ( sql )",Executes a BigQuery query and returns the job ID .
"def executemany ( self , operation , seq_of_parameters ) : for parameters in seq_of_parameters : self . execute ( operation , parameters )",Execute a BigQuery query multiple times with different parameters .
"def next ( self ) : if not self . job_id : return None if len ( self . buffer ) == 0 : if self . all_pages_loaded : return None query_results = ( self . service . jobs ( ) . getQueryResults ( projectId = self . project_id , jobId = self . job_id , pageToken = self . page_token ) . execute ( num_retries = self . num_retries ) ) if 'rows' in query_results and query_results [ 'rows' ] : self . page_token = query_results . get ( 'pageToken' ) fields = query_results [ 'schema' ] [ 'fields' ] col_types = [ field [ 'type' ] for field in fields ] rows = query_results [ 'rows' ] for dict_row in rows : typed_row = ( [ _bq_cast ( vs [ 'v' ] , col_types [ idx ] ) for idx , vs in enumerate ( dict_row [ 'f' ] ) ] ) self . buffer . append ( typed_row ) if not self . page_token : self . all_pages_loaded = True else : self . page_token = None self . job_id = None self . page_token = None return None return self . buffer . pop ( 0 )",Helper method for fetchone which returns the next row from a buffer . If the buffer is empty attempts to paginate through the result set for the next page and load it into the buffer .
"def fetchmany ( self , size = None ) : if size is None : size = self . arraysize result = [ ] for _ in range ( size ) : one = self . fetchone ( ) if one is None : break else : result . append ( one ) return result",Fetch the next set of rows of a query result returning a sequence of sequences ( e . g . a list of tuples ) . An empty sequence is returned when no more rows are available . The number of rows to fetch per call is specified by the parameter . If it is not given the cursor s arraysize determines the number of rows to be fetched . The method should try to fetch as many rows as indicated by the size parameter . If this is not possible due to the specified number of rows not being available fewer rows may be returned . An : py : class : ~pyhive . exc . Error ( or subclass ) exception is raised if the previous call to : py : meth : execute did not produce any result set or no call was issued yet .
def fetchall ( self ) : result = [ ] while True : one = self . fetchone ( ) if one is None : break else : result . append ( one ) return result,Fetch all ( remaining ) rows of a query result returning them as a sequence of sequences ( e . g . a list of tuples ) .
"def configure_manifest_files ( app ) : def parse_manifest_json ( ) : try : global manifest manifest_file = os . path . join ( os . path . dirname ( __file__ ) , 'static/dist/manifest.json' ) with open ( manifest_file , 'r' ) as f : manifest . update ( json . load ( f ) ) for k in manifest . keys ( ) : manifest [ k ] = os . path . join ( ""dist"" , manifest [ k ] ) except Exception : print ( ""Please make sure to build the frontend in "" ""static/ directory and restart the server"" ) pass def get_asset_url ( filename ) : if app . debug : parse_manifest_json ( ) return url_for ( 'static' , filename = manifest . get ( filename , '' ) ) parse_manifest_json ( ) @ app . context_processor def get_url_for_asset ( ) : """"""
        Template tag to return the asset URL.
        WebPack renders the assets after minification and modification
        under the static/dist folder.
        This template tag reads the asset name in manifest.json and returns
        the appropriate file.
        """""" return dict ( url_for_asset = get_asset_url )",Loads the manifest file and register the url_for_asset_ template tag . : param app : : return :
"def _query_postgres ( self ) : postgres = PostgresHook ( postgres_conn_id = self . postgres_conn_id ) conn = postgres . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql , self . parameters ) return cursor",Queries Postgres and returns a cursor to the results .
"def _write_local_data_files ( self , cursor ) : schema = list ( map ( lambda schema_tuple : schema_tuple [ 0 ] , cursor . description ) ) tmp_file_handles = { } row_no = 0 def _create_new_file ( ) : handle = NamedTemporaryFile ( delete = True ) filename = self . filename . format ( len ( tmp_file_handles ) ) tmp_file_handles [ filename ] = handle return handle if cursor . rowcount > 0 : tmp_file_handle = _create_new_file ( ) for row in cursor : row = map ( self . convert_types , row ) row_dict = dict ( zip ( schema , row ) ) s = json . dumps ( row_dict , sort_keys = True ) . encode ( 'utf-8' ) tmp_file_handle . write ( s ) tmp_file_handle . write ( b'\n' ) if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : tmp_file_handle = _create_new_file ( ) row_no += 1 self . log . info ( 'Received %s rows over %s files' , row_no , len ( tmp_file_handles ) ) return tmp_file_handles",Takes a cursor and writes results to a local file .
"def _write_local_schema_file ( self , cursor ) : schema = [ ] for field in cursor . description : field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) field_mode = 'REPEATED' if field [ 1 ] in ( 1009 , 1005 , 1007 , 1016 ) else 'NULLABLE' schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema ) tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) s = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( s ) return { self . schema_filename : tmp_schema_file_handle }",Takes a cursor and writes the BigQuery schema for the results to a local file system .
"def convert_types ( cls , value ) : if type ( value ) in ( datetime . datetime , datetime . date ) : return time . mktime ( value . timetuple ( ) ) elif type ( value ) == datetime . time : formated_time = time . strptime ( str ( value ) , ""%H:%M:%S"" ) return datetime . timedelta ( hours = formated_time . tm_hour , minutes = formated_time . tm_min , seconds = formated_time . tm_sec ) . seconds elif isinstance ( value , Decimal ) : return float ( value ) else : return value",Takes a value from Postgres and converts it to a value that s safe for JSON / Google Cloud Storage / BigQuery . Dates are converted to UTC seconds . Decimals are converted to floats . Times are converted to seconds .
"def _make_intermediate_dirs ( sftp_client , remote_directory ) : if remote_directory == '/' : sftp_client . chdir ( '/' ) return if remote_directory == '' : return try : sftp_client . chdir ( remote_directory ) except IOError : dirname , basename = os . path . split ( remote_directory . rstrip ( '/' ) ) _make_intermediate_dirs ( sftp_client , dirname ) sftp_client . mkdir ( basename ) sftp_client . chdir ( basename ) return",Create all the intermediate directories in a remote host
"def create_queue ( self , queue_name , attributes = None ) : return self . get_conn ( ) . create_queue ( QueueName = queue_name , Attributes = attributes or { } )",Create queue using connection object
"def send_message ( self , queue_url , message_body , delay_seconds = 0 , message_attributes = None ) : return self . get_conn ( ) . send_message ( QueueUrl = queue_url , MessageBody = message_body , DelaySeconds = delay_seconds , MessageAttributes = message_attributes or { } )",Send message to the queue
def _integrate_plugins ( ) : from airflow . plugins_manager import hooks_modules for hooks_module in hooks_modules : sys . modules [ hooks_module . __name__ ] = hooks_module globals ( ) [ hooks_module . _name ] = hooks_module,Integrate plugins to the context
"def run_command ( self , run_with = None , join_args = False ) : run_with = run_with or [ ] cmd = [ "" "" . join ( self . _command ) ] if join_args else self . _command full_cmd = run_with + cmd self . log . info ( 'Running: %s' , full_cmd ) proc = subprocess . Popen ( full_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , universal_newlines = True , close_fds = True , env = os . environ . copy ( ) , preexec_fn = os . setsid ) log_reader = threading . Thread ( target = self . _read_task_logs , args = ( proc . stdout , ) , ) log_reader . daemon = True log_reader . start ( ) return proc",Run the task command .
"def on_finish ( self ) : if self . _cfg_path and os . path . isfile ( self . _cfg_path ) : if self . run_as_user : subprocess . call ( [ 'sudo' , 'rm' , self . _cfg_path ] , close_fds = True ) else : os . remove ( self . _cfg_path )",A callback that should be called when this is done running .
"def _main ( ) : usage = ""usage: nvd3.py [options]"" parser = OptionParser ( usage = usage , version = ( ""python-nvd3 - Charts generator with "" ""nvd3.js and d3.js"" ) ) parser . add_option ( ""-q"" , ""--quiet"" , action = ""store_false"" , dest = ""verbose"" , default = True , help = ""don't print messages to stdout"" ) ( options , args ) = parser . parse_args ( )",Parse options and process commands
"def add_serie ( self , y , x , name = None , extra = None , * * kwargs ) : if not name : name = ""Serie %d"" % ( self . serie_no ) if 'shape' in kwargs or 'size' in kwargs : csize = kwargs . get ( 'size' , 1 ) cshape = kwargs . get ( 'shape' , 'circle' ) serie = [ { 'x' : x [ i ] , 'y' : j , 'shape' : cshape , 'size' : csize [ i ] if isinstance ( csize , list ) else csize } for i , j in enumerate ( y ) ] else : if self . model == 'pieChart' : serie = [ { 'label' : x [ i ] , 'value' : y } for i , y in enumerate ( y ) ] else : serie = [ { 'x' : x [ i ] , 'y' : y } for i , y in enumerate ( y ) ] data_keyvalue = { 'values' : serie , 'key' : name } if 'type' in kwargs and kwargs [ 'type' ] : data_keyvalue [ 'type' ] = kwargs [ 'type' ] if 'yaxis' in kwargs and kwargs [ 'yaxis' ] : data_keyvalue [ 'yAxis' ] = kwargs [ 'yaxis' ] else : if self . model != 'pieChart' : data_keyvalue [ 'yAxis' ] = '1' if 'bar' in kwargs and kwargs [ 'bar' ] : data_keyvalue [ 'bar' ] = 'true' if 'disabled' in kwargs and kwargs [ 'disabled' ] : data_keyvalue [ 'disabled' ] = 'true' if 'color' in kwargs and kwargs [ 'color' ] : data_keyvalue [ 'color' ] = kwargs [ 'color' ] if extra : if self . model == 'pieChart' : if 'color_list' in extra and extra [ 'color_list' ] : self . color_list = extra [ 'color_list' ] if extra . get ( 'date_format' ) : self . charttooltip_dateformat = extra [ 'date_format' ] if extra . get ( 'tooltip' ) : self . custom_tooltip_flag = True if self . model != 'pieChart' : _start = extra [ 'tooltip' ] [ 'y_start' ] _end = extra [ 'tooltip' ] [ 'y_end' ] _start = ( ""'"" + str ( _start ) + ""' + "" ) if _start else '' _end = ( "" + '"" + str ( _end ) + ""'"" ) if _end else '' if self . model == 'linePlusBarChart' : if self . tooltip_condition_string : self . tooltip_condition_string += stab ( 5 ) self . tooltip_condition_string += stab ( 0 ) + ""if(key.indexOf('"" + name + ""') > -1 ){\n"" + stab ( 6 ) + ""var y = "" + _start + "" String(graph.point.y) "" + _end + "";\n"" + stab ( 5 ) + ""}\n"" elif self . model == 'cumulativeLineChart' : self . tooltip_condition_string += stab ( 0 ) + ""if(key == '"" + name + ""'){\n"" + stab ( 6 ) + ""var y = "" + _start + "" String(e) "" + _end + "";\n"" + stab ( 5 ) + ""}\n"" else : self . tooltip_condition_string += stab ( 5 ) + ""if(key == '"" + name + ""'){\n"" + stab ( 6 ) + ""var y = "" + _start + "" String(graph.point.y) "" + _end + "";\n"" + stab ( 5 ) + ""}\n"" if self . model == 'pieChart' : _start = extra [ 'tooltip' ] [ 'y_start' ] _end = extra [ 'tooltip' ] [ 'y_end' ] _start = ( ""'"" + str ( _start ) + ""' + "" ) if _start else '' _end = ( "" + '"" + str ( _end ) + ""'"" ) if _end else '' self . tooltip_condition_string += ""var y = "" + _start + "" String(y) "" + _end + "";\n"" self . serie_no += 1 self . series . append ( data_keyvalue )",add serie - Series are list of data that will be plotted y { 1 2 3 4 5 } / x { 1 2 3 4 5 }
def buildcontent ( self ) : self . buildcontainer ( ) self . buildjschart ( ) self . htmlcontent = self . template_content_nvd3 . render ( chart = self ),Build HTML content only no header or body tags . To be useful this will usually require the attribute juqery_on_ready to be set which will wrap the js in $ ( function () { <regular_js > } ; )
def buildhtml ( self ) : self . buildcontent ( ) self . content = self . htmlcontent self . htmlcontent = self . template_page_nvd3 . render ( chart = self ),Build the HTML page Create the htmlheader with css / js Create html page Add Js code for nvd3
def buildhtmlheader ( self ) : self . htmlheader = '' global _js_initialized if '_js_initialized' not in globals ( ) or not _js_initialized : for css in self . header_css : self . htmlheader += css for js in self . header_js : self . htmlheader += js,generate HTML header content
"def buildcontainer ( self ) : if self . container : return if self . width : if self . width [ - 1 ] != '%' : self . style += 'width:%spx;' % self . width else : self . style += 'width:%s;' % self . width if self . height : if self . height [ - 1 ] != '%' : self . style += 'height:%spx;' % self . height else : self . style += 'height:%s;' % self . height if self . style : self . style = 'style=""%s""' % self . style self . container = self . containerheader + '<div id=""%s""><svg %s></svg></div>\n' % ( self . name , self . style )",generate HTML div
def buildjschart ( self ) : self . jschart = '' if self . tooltip_condition_string == '' : self . tooltip_condition_string = 'var y = String(graph.point.y);\n' self . series_js = json . dumps ( self . series ),generate javascript code for the chart
"def create_x_axis ( self , name , label = None , format = None , date = False , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : if format == 'AM_PM' : axis [ 'tickFormat' ] = ""function(d) { return get_am_pm(parseInt(d)); }"" else : axis [ 'tickFormat' ] = ""d3.format(',%s')"" % format if label : axis [ 'axisLabel' ] = ""'"" + label + ""'"" if date : self . dateformat = format axis [ 'tickFormat' ] = ( ""function(d) { return d3.time.format('%s')"" ""(new Date(parseInt(d))) }\n"" """" % self . dateformat ) if name [ 0 ] == 'x' : self . x_axis_date = True self . axislist [ name ] = axis if name == ""xAxis"" and self . focus_enable : self . axislist [ 'x2Axis' ] = axis",Create X - axis
"def create_y_axis ( self , name , label = None , format = None , custom_format = False ) : axis = { } if custom_format and format : axis [ 'tickFormat' ] = format elif format : axis [ 'tickFormat' ] = ""d3.format(',%s')"" % format if label : axis [ 'axisLabel' ] = ""'"" + label + ""'"" self . axislist [ name ] = axis",Create Y - axis
def buildcontent ( self ) : self . buildcontainer ( ) self . buildjschart ( ) self . htmlcontent = self . template_chart_nvd3 . render ( chart = self ),Build HTML content only no header or body tags . To be useful this will usually require the attribute juqery_on_ready to be set which will wrap the js in $ ( function () { <regular_js > } ; )
def get_conn ( self ) : conn = self . get_connection ( self . sqlite_conn_id ) conn = sqlite3 . connect ( conn . host ) return conn,Returns a sqlite connection object
"def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : with create_session ( ) as session : if g . user . is_anonymous : user = 'anonymous' else : user = g . user . username log = Log ( event = f . __name__ , task_instance = None , owner = user , extra = str ( list ( request . args . items ( ) ) ) , task_id = request . args . get ( 'task_id' ) , dag_id = request . args . get ( 'dag_id' ) ) if 'execution_date' in request . args : log . execution_date = pendulum . parse ( request . args . get ( 'execution_date' ) ) session . add ( log ) return f ( * args , * * kwargs ) return wrapper",Decorator to log user actions
"def gzipped ( f ) : @ functools . wraps ( f ) def view_func ( * args , * * kwargs ) : @ after_this_request def zipper ( response ) : accept_encoding = request . headers . get ( 'Accept-Encoding' , '' ) if 'gzip' not in accept_encoding . lower ( ) : return response response . direct_passthrough = False if ( response . status_code < 200 or response . status_code >= 300 or 'Content-Encoding' in response . headers ) : return response gzip_buffer = IO ( ) gzip_file = gzip . GzipFile ( mode = 'wb' , fileobj = gzip_buffer ) gzip_file . write ( response . data ) gzip_file . close ( ) response . data = gzip_buffer . getvalue ( ) response . headers [ 'Content-Encoding' ] = 'gzip' response . headers [ 'Vary' ] = 'Accept-Encoding' response . headers [ 'Content-Length' ] = len ( response . data ) return response return f ( * args , * * kwargs ) return view_func",Decorator to make a view compressed
"def has_dag_access ( * * dag_kwargs ) : def decorator ( f ) : @ functools . wraps ( f ) def wrapper ( self , * args , * * kwargs ) : has_access = self . appbuilder . sm . has_access dag_id = request . args . get ( 'dag_id' ) can_dag_edit = dag_kwargs . get ( 'can_dag_edit' , False ) if ( has_access ( 'can_dag_edit' , 'all_dags' ) or has_access ( 'can_dag_edit' , dag_id ) or ( not can_dag_edit and ( has_access ( 'can_dag_read' , 'all_dags' ) or has_access ( 'can_dag_read' , dag_id ) ) ) ) : return f ( self , * args , * * kwargs ) else : flash ( ""Access is Denied"" , ""danger"" ) return redirect ( url_for ( self . appbuilder . sm . auth_view . __class__ . __name__ + "".login"" ) ) return wrapper return decorator",Decorator to check whether the user has read / write permission on the dag .
"def get_last_dagrun ( dag_id , session , include_externally_triggered = False ) : DR = DagRun query = session . query ( DR ) . filter ( DR . dag_id == dag_id ) if not include_externally_triggered : query = query . filter ( DR . external_trigger == False ) query = query . order_by ( DR . execution_date . desc ( ) ) return query . first ( )",Returns the last dag run for a dag None if there was none . Last dag run can be any type of run eg . scheduled or backfilled . Overridden DagRuns are ignored .
"def create_dagrun ( self , run_id , state , execution_date , start_date = None , external_trigger = False , conf = None , session = None ) : return self . get_dag ( ) . create_dagrun ( run_id = run_id , state = state , execution_date = execution_date , start_date = start_date , external_trigger = external_trigger , conf = conf , session = session )",Creates a dag run from this dag including the tasks associated with this dag . Returns the dag run .
"def execute ( self , context ) : hook = SQSHook ( aws_conn_id = self . aws_conn_id ) result = hook . send_message ( queue_url = self . sqs_queue , message_body = self . message_content , delay_seconds = self . delay_seconds , message_attributes = self . message_attributes ) self . log . info ( 'result is send_message is %s' , result ) return result",Publish the message to SQS queue
"def generate_pages ( current_page , num_of_pages , search = None , showPaused = None , window = 7 ) : void_link = 'javascript:void(0)' first_node = Markup ( """"""<li class=""paginate_button {disabled}"" id=""dags_first"">
    <a href=""{href_link}"" aria-controls=""dags"" data-dt-idx=""0"" tabindex=""0"">&laquo;</a>
</li>"""""" ) previous_node = Markup ( """"""<li class=""paginate_button previous {disabled}"" id=""dags_previous"">
    <a href=""{href_link}"" aria-controls=""dags"" data-dt-idx=""0"" tabindex=""0"">&lt;</a>
</li>"""""" ) next_node = Markup ( """"""<li class=""paginate_button next {disabled}"" id=""dags_next"">
    <a href=""{href_link}"" aria-controls=""dags"" data-dt-idx=""3"" tabindex=""0"">&gt;</a>
</li>"""""" ) last_node = Markup ( """"""<li class=""paginate_button {disabled}"" id=""dags_last"">
    <a href=""{href_link}"" aria-controls=""dags"" data-dt-idx=""3"" tabindex=""0"">&raquo;</a>
</li>"""""" ) page_node = Markup ( """"""<li class=""paginate_button {is_active}"">
    <a href=""{href_link}"" aria-controls=""dags"" data-dt-idx=""2"" tabindex=""0"">{page_num}</a>
</li>"""""" ) output = [ Markup ( '<ul class=""pagination"" style=""margin-top:0px;"">' ) ] is_disabled = 'disabled' if current_page <= 0 else '' output . append ( first_node . format ( href_link = ""?{}"" . format ( get_params ( page = 0 , search = search , showPaused = showPaused ) ) , disabled = is_disabled ) ) page_link = void_link if current_page > 0 : page_link = '?{}' . format ( get_params ( page = ( current_page - 1 ) , search = search , showPaused = showPaused ) ) output . append ( previous_node . format ( href_link = page_link , disabled = is_disabled ) ) mid = int ( window / 2 ) last_page = num_of_pages - 1 if current_page <= mid or num_of_pages < window : pages = [ i for i in range ( 0 , min ( num_of_pages , window ) ) ] elif mid < current_page < last_page - mid : pages = [ i for i in range ( current_page - mid , current_page + mid + 1 ) ] else : pages = [ i for i in range ( num_of_pages - window , last_page + 1 ) ] def is_current ( current , page ) : return page == current for page in pages : vals = { 'is_active' : 'active' if is_current ( current_page , page ) else '' , 'href_link' : void_link if is_current ( current_page , page ) else '?{}' . format ( get_params ( page = page , search = search , showPaused = showPaused ) ) , 'page_num' : page + 1 } output . append ( page_node . format ( * * vals ) ) is_disabled = 'disabled' if current_page >= num_of_pages - 1 else '' page_link = ( void_link if current_page >= num_of_pages - 1 else '?{}' . format ( get_params ( page = current_page + 1 , search = search , showPaused = showPaused ) ) ) output . append ( next_node . format ( href_link = page_link , disabled = is_disabled ) ) output . append ( last_node . format ( href_link = ""?{}"" . format ( get_params ( page = last_page , search = search , showPaused = showPaused ) ) , disabled = is_disabled ) ) output . append ( Markup ( '</ul>' ) ) return Markup ( '\n' . join ( output ) )",Generates the HTML for a paging component using a similar logic to the paging auto - generated by Flask managed views . The paging component defines a number of pages visible in the pager ( window ) and once the user goes to a page beyond the largest visible it would scroll to the right the page numbers and keeps the current one in the middle of the pager component . When in the last pages the pages won t scroll and just keep moving until the last page . Pager also contains <first previous ... next last > pages . This component takes into account custom parameters such as search and showPaused which could be added to the pages link in order to maintain the state between client and server . It also allows to make a bookmark on a specific paging state . : param current_page : the current page number 0 - indexed : param num_of_pages : the total number of pages : param search : the search query string if any : param showPaused : false if paused dags will be hidden otherwise true to show them : param window : the number of pages to be shown in the paging component ( 7 default ) : return : the HTML string of the paging component
"def json_response ( obj ) : return Response ( response = json . dumps ( obj , indent = 4 , cls = AirflowJsonEncoder ) , status = 200 , mimetype = ""application/json"" )",returns a json response from a json serializable python object
"def open_maybe_zipped ( f , mode = 'r' ) : _ , archive , filename = ZIP_REGEX . search ( f ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return zipfile . ZipFile ( archive , mode = mode ) . open ( filename ) else : return io . open ( f , mode = mode )",Opens the given file . If the path contains a folder with a . zip suffix then the folder is treated as a zip archive opening the file inside the archive .
"def make_cache_key ( * args , * * kwargs ) : path = request . path args = str ( hash ( frozenset ( request . args . items ( ) ) ) ) return ( path + args ) . encode ( 'ascii' , 'ignore' )",Used by cache to get a unique key per URL
def get_conn ( self ) : if not self . _conn : self . _conn = VideoIntelligenceServiceClient ( credentials = self . _get_credentials ( ) ) return self . _conn,Returns Gcp Video Intelligence Service client
"def annotate_video ( self , input_uri = None , input_content = None , features = None , video_context = None , output_uri = None , location = None , retry = None , timeout = None , metadata = None , ) : client = self . get_conn ( ) return client . annotate_video ( input_uri = input_uri , input_content = input_content , features = features , video_context = video_context , output_uri = output_uri , location_id = location , retry = retry , timeout = timeout , metadata = metadata , )",Performs video annotation .
"def _get_api_key ( self ) : conn = self . get_connection ( self . http_conn_id ) api_key = conn . password if not api_key : raise AirflowException ( 'Opsgenie API Key is required for this hook, ' 'please check your conn_id configuration.' ) return api_key",Get Opsgenie api_key for creating alert
"def get_conn ( self , headers = None ) : conn = self . get_connection ( self . http_conn_id ) self . base_url = conn . host if conn . host else 'https://api.opsgenie.com' session = requests . Session ( ) if headers : session . headers . update ( headers ) return session",Overwrite HttpHook get_conn because this hook just needs base_url and headers and does not need generic params
"def execute ( self , payload = { } ) : api_key = self . _get_api_key ( ) return self . run ( endpoint = 'v2/alerts' , data = json . dumps ( payload ) , headers = { 'Content-Type' : 'application/json' , 'Authorization' : 'GenieKey %s' % api_key } )",Execute the Opsgenie Alert call
"def poke ( self , context ) : bash_command = self . bash_command self . log . info ( ""Tmp dir root location: \n %s"" , gettempdir ( ) ) with TemporaryDirectory ( prefix = 'airflowtmp' ) as tmp_dir : with NamedTemporaryFile ( dir = tmp_dir , prefix = self . task_id ) as f : f . write ( bytes ( bash_command , 'utf_8' ) ) f . flush ( ) fname = f . name script_location = tmp_dir + ""/"" + fname self . log . info ( ""Temporary script location: %s"" , script_location ) self . log . info ( ""Running command: %s"" , bash_command ) sp = Popen ( [ 'bash' , fname ] , stdout = PIPE , stderr = STDOUT , close_fds = True , cwd = tmp_dir , env = self . env , preexec_fn = os . setsid ) self . sp = sp self . log . info ( ""Output:"" ) line = '' for line in iter ( sp . stdout . readline , b'' ) : line = line . decode ( self . output_encoding ) . strip ( ) self . log . info ( line ) sp . wait ( ) self . log . info ( ""Command exited with return code %s"" , sp . returncode ) return not sp . returncode",Execute the bash command in a temporary directory which will be cleaned afterwards
"def _build_opsgenie_payload ( self ) : payload = { } for key in [ ""message"" , ""alias"" , ""description"" , ""responders"" , ""visibleTo"" , ""actions"" , ""tags"" , ""details"" , ""entity"" , ""source"" , ""priority"" , ""user"" , ""note"" ] : val = getattr ( self , key ) if val : payload [ key ] = val return payload",Construct the Opsgenie JSON payload . All relevant parameters are combined here to a valid Opsgenie JSON payload .
"def execute ( self , context ) : self . hook = OpsgenieAlertHook ( self . opsgenie_conn_id ) self . hook . execute ( self . _build_opsgenie_payload ( ) )",Call the OpsgenieAlertHook to post message
def get_conn ( self ) : if not self . conn : self . conn = self . get_client_type ( 'athena' ) return self . conn,check if aws conn exists already or create one and return it
"def run_query ( self , query , query_context , result_configuration , client_request_token = None ) : response = self . conn . start_query_execution ( QueryString = query , ClientRequestToken = client_request_token , QueryExecutionContext = query_context , ResultConfiguration = result_configuration ) query_execution_id = response [ 'QueryExecutionId' ] return query_execution_id",Run Presto query on athena with provided config and return submitted query_execution_id
"def check_query_status ( self , query_execution_id ) : response = self . conn . get_query_execution ( QueryExecutionId = query_execution_id ) state = None try : state = response [ 'QueryExecution' ] [ 'Status' ] [ 'State' ] except Exception as ex : self . log . error ( 'Exception while getting query state' , ex ) finally : return state",Fetch the status of submitted athena query . Returns None or one of valid query states .
"def get_query_results ( self , query_execution_id ) : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . error ( 'Invalid Query state' ) return None elif query_state in self . INTERMEDIATE_STATES or query_state in self . FAILURE_STATES : self . log . error ( 'Query is in {state} state. Cannot fetch results' . format ( state = query_state ) ) return None return self . conn . get_query_results ( QueryExecutionId = query_execution_id )",Fetch submitted athena query results . returns none if query is in intermediate state or failed / cancelled state else dict of query output
"def poll_query_status ( self , query_execution_id , max_tries = None ) : try_number = 1 final_query_state = None while True : query_state = self . check_query_status ( query_execution_id ) if query_state is None : self . log . info ( 'Trial {try_number}: Invalid query state. Retrying again' . format ( try_number = try_number ) ) elif query_state in self . INTERMEDIATE_STATES : self . log . info ( 'Trial {try_number}: Query is still in an intermediate state - {state}' . format ( try_number = try_number , state = query_state ) ) else : self . log . info ( 'Trial {try_number}: Query execution completed. Final state is {state}' . format ( try_number = try_number , state = query_state ) ) final_query_state = query_state break if max_tries and try_number >= max_tries : final_query_state = query_state break try_number += 1 sleep ( self . sleep_time ) return final_query_state",Poll the status of submitted athena query until query state reaches final state . Returns one of the final states
"def get_conn ( self ) : if self . conn is None : cnopts = pysftp . CnOpts ( ) if self . no_host_key_check : cnopts . hostkeys = None cnopts . compression = self . compress conn_params = { 'host' : self . remote_host , 'port' : self . port , 'username' : self . username , 'cnopts' : cnopts } if self . password and self . password . strip ( ) : conn_params [ 'password' ] = self . password if self . key_file : conn_params [ 'private_key' ] = self . key_file if self . private_key_pass : conn_params [ 'private_key_pass' ] = self . private_key_pass self . conn = pysftp . Connection ( * * conn_params ) return self . conn",Returns an SFTP connection object
"def describe_directory ( self , path ) : conn = self . get_conn ( ) flist = conn . listdir_attr ( path ) files = { } for f in flist : modify = datetime . datetime . fromtimestamp ( f . st_mtime ) . strftime ( '%Y%m%d%H%M%S' ) files [ f . filename ] = { 'size' : f . st_size , 'type' : 'dir' if stat . S_ISDIR ( f . st_mode ) else 'file' , 'modify' : modify } return files",Returns a dictionary of { filename : { attributes }} for all files on the remote system ( where the MLSD command is supported ) . : param path : full path to the remote directory : type path : str
"def list_directory ( self , path ) : conn = self . get_conn ( ) files = conn . listdir ( path ) return files",Returns a list of files on the remote system . : param path : full path to the remote directory to list : type path : str
"def create_directory ( self , path , mode = 777 ) : conn = self . get_conn ( ) conn . mkdir ( path , mode )",Creates a directory on the remote system . : param path : full path to the remote directory to create : type path : str : param mode : int representation of octal mode for directory
"def retrieve_file ( self , remote_full_path , local_full_path ) : conn = self . get_conn ( ) self . log . info ( 'Retrieving file from FTP: %s' , remote_full_path ) conn . get ( remote_full_path , local_full_path ) self . log . info ( 'Finished retrieving file from FTP: %s' , remote_full_path )",Transfers the remote file to a local location . If local_full_path is a string path the file will be put at that location : param remote_full_path : full path to the remote file : type remote_full_path : str : param local_full_path : full path to the local file : type local_full_path : str
"def store_file ( self , remote_full_path , local_full_path ) : conn = self . get_conn ( ) conn . put ( local_full_path , remote_full_path )",Transfers a local file to the remote location . If local_full_path_or_buffer is a string path the file will be read from that location : param remote_full_path : full path to the remote file : type remote_full_path : str : param local_full_path : full path to the local file : type local_full_path : str
"def __handle_rate_limit_exception ( self , rate_limit_exception ) : retry_after = int ( rate_limit_exception . response . headers . get ( 'Retry-After' , 60 ) ) self . log . info ( ""Hit Zendesk API rate limit. Pausing for %s seconds"" , retry_after ) time . sleep ( retry_after )",Sleep for the time specified in the exception . If not specified wait for 60 seconds .
"def call ( self , path , query = None , get_all_pages = True , side_loading = False ) : zendesk = self . get_conn ( ) first_request_successful = False while not first_request_successful : try : results = zendesk . call ( path , query ) first_request_successful = True except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) keys = [ path . split ( ""/"" ) [ - 1 ] . split ( "".json"" ) [ 0 ] ] next_page = results [ 'next_page' ] if side_loading : keys += query [ 'include' ] . split ( ',' ) results = { key : results [ key ] for key in keys } if get_all_pages : while next_page is not None : try : next_url = next_page . split ( self . __url ) [ 1 ] self . log . info ( ""Calling %s"" , next_url ) more_res = zendesk . call ( next_url ) for key in results : results [ key ] . extend ( more_res [ key ] ) if next_page == more_res [ 'next_page' ] : break else : next_page = more_res [ 'next_page' ] except RateLimitError as rle : self . __handle_rate_limit_exception ( rle ) except ZendeskError as ze : if b""Use a start_time older than 5 minutes"" in ze . msg : break else : raise ze return results",Call Zendesk API and return results
"def get_partitions ( self , database_name , table_name , expression = '' , page_size = None , max_items = None ) : config = { 'PageSize' : page_size , 'MaxItems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'get_partitions' ) response = paginator . paginate ( DatabaseName = database_name , TableName = table_name , Expression = expression , PaginationConfig = config ) partitions = set ( ) for page in response : for p in page [ 'Partitions' ] : partitions . add ( tuple ( p [ 'Values' ] ) ) return partitions",Retrieves the partition values for a table .
"def check_for_partition ( self , database_name , table_name , expression ) : partitions = self . get_partitions ( database_name , table_name , expression , max_items = 1 ) if partitions : return True else : return False",Checks whether a partition exists
"def get_table ( self , database_name , table_name ) : result = self . get_conn ( ) . get_table ( DatabaseName = database_name , Name = table_name ) return result [ 'Table' ]",Get the information of the table
"def get_table_location ( self , database_name , table_name ) : table = self . get_table ( database_name , table_name ) return table [ 'StorageDescriptor' ] [ 'Location' ]",Get the physical location of the table
"def cluster_status ( self , cluster_identifier ) : conn = self . get_conn ( ) try : response = conn . describe_clusters ( ClusterIdentifier = cluster_identifier ) [ 'Clusters' ] return response [ 0 ] [ 'ClusterStatus' ] if response else None except conn . exceptions . ClusterNotFoundFault : return 'cluster_not_found'",Return status of a cluster
"def delete_cluster ( self , cluster_identifier , skip_final_cluster_snapshot = True , final_cluster_snapshot_identifier = '' ) : response = self . get_conn ( ) . delete_cluster ( ClusterIdentifier = cluster_identifier , SkipFinalClusterSnapshot = skip_final_cluster_snapshot , FinalClusterSnapshotIdentifier = final_cluster_snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None",Delete a cluster and optionally create a snapshot
"def describe_cluster_snapshots ( self , cluster_identifier ) : response = self . get_conn ( ) . describe_cluster_snapshots ( ClusterIdentifier = cluster_identifier ) if 'Snapshots' not in response : return None snapshots = response [ 'Snapshots' ] snapshots = filter ( lambda x : x [ 'Status' ] , snapshots ) snapshots . sort ( key = lambda x : x [ 'SnapshotCreateTime' ] , reverse = True ) return snapshots",Gets a list of snapshots for a cluster
"def restore_from_cluster_snapshot ( self , cluster_identifier , snapshot_identifier ) : response = self . get_conn ( ) . restore_from_cluster_snapshot ( ClusterIdentifier = cluster_identifier , SnapshotIdentifier = snapshot_identifier ) return response [ 'Cluster' ] if response [ 'Cluster' ] else None",Restores a cluster from its snapshot
"def create_cluster_snapshot ( self , snapshot_identifier , cluster_identifier ) : response = self . get_conn ( ) . create_cluster_snapshot ( SnapshotIdentifier = snapshot_identifier , ClusterIdentifier = cluster_identifier , ) return response [ 'Snapshot' ] if response [ 'Snapshot' ] else None",Creates a snapshot of a cluster
"def execute ( self , * * kwargs ) : if not self . api_params : self . construct_api_call_params ( ) slack = SlackHook ( token = self . token , slack_conn_id = self . slack_conn_id ) slack . call ( self . method , self . api_params )",SlackAPIOperator calls will not fail even if the call is not unsuccessful . It should not prevent a DAG from completing in success
"def add_volume ( self , volume ) : self . _add_volume ( name = volume . name , configs = volume . configs )",Args : volume ( Volume ) :
"def add_mount ( self , volume_mount ) : self . _add_mount ( name = volume_mount . name , mount_path = volume_mount . mount_path , sub_path = volume_mount . sub_path , read_only = volume_mount . read_only )",Args : volume_mount ( VolumeMount ) :
"def create_job_flow ( self , job_flow_overrides ) : if not self . emr_conn_id : raise AirflowException ( 'emr_conn_id must be present to use create_job_flow' ) emr_conn = self . get_connection ( self . emr_conn_id ) config = emr_conn . extra_dejson . copy ( ) config . update ( job_flow_overrides ) response = self . get_conn ( ) . run_job_flow ( * * config ) return response",Creates a job flow using the config from the EMR connection . Keys of the json extra hash may have the arguments of the boto3 run_job_flow method . Overrides for this config may be passed as the job_flow_overrides .
"def filter_for_filesize ( result , size = None ) : if size : log = LoggingMixin ( ) . log log . debug ( 'Filtering for file size >= %s in files: %s' , size , map ( lambda x : x [ 'path' ] , result ) ) size *= settings . MEGABYTE result = [ x for x in result if x [ 'length' ] >= size ] log . debug ( 'HdfsSensor.poke: after size filter result is %s' , result ) return result",Will test the filepath result and test if its size is at least self . filesize
"def filter_for_ignored_ext ( result , ignored_ext , ignore_copying ) : if ignore_copying : log = LoggingMixin ( ) . log regex_builder = r""^.*\.(%s$)$"" % '$|' . join ( ignored_ext ) ignored_extensions_regex = re . compile ( regex_builder ) log . debug ( 'Filtering result for ignored extensions: %s in files %s' , ignored_extensions_regex . pattern , map ( lambda x : x [ 'path' ] , result ) ) result = [ x for x in result if not ignored_extensions_regex . match ( x [ 'path' ] ) ] log . debug ( 'HdfsSensor.poke: after ext filter result is %s' , result ) return result",Will filter if instructed to do so the result to remove matching criteria
"def execute ( self , context ) : s3_conn = S3Hook ( self . s3_conn_id ) if self . is_pipeline : results = MongoHook ( self . mongo_conn_id ) . aggregate ( mongo_collection = self . mongo_collection , aggregate_query = self . mongo_query , mongo_db = self . mongo_db ) else : results = MongoHook ( self . mongo_conn_id ) . find ( mongo_collection = self . mongo_collection , query = self . mongo_query , mongo_db = self . mongo_db ) docs_str = self . _stringify ( self . transform ( results ) ) s3_conn . load_string ( string_data = docs_str , key = self . s3_key , bucket_name = self . s3_bucket , replace = self . replace ) return True",Executed by task_instance at runtime
"def _stringify ( iterable , joinable = '\n' ) : return joinable . join ( [ json . dumps ( doc , default = json_util . default ) for doc in iterable ] )",Takes an iterable ( pymongo Cursor or Array ) containing dictionaries and returns a stringified version using python join
"def get_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( ""Pool name shouldn't be empty"" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( ""Pool '%s' doesn't exist"" % name ) return pool",Get pool by a given name .
"def create_pool ( name , slots , description , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( ""Pool name shouldn't be empty"" ) try : slots = int ( slots ) except ValueError : raise AirflowBadRequest ( ""Bad value for `slots`: %s"" % slots ) session . expire_on_commit = False pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : pool = Pool ( pool = name , slots = slots , description = description ) session . add ( pool ) else : pool . slots = slots pool . description = description session . commit ( ) return pool",Create a pool with a given parameters .
"def delete_pool ( name , session = None ) : if not ( name and name . strip ( ) ) : raise AirflowBadRequest ( ""Pool name shouldn't be empty"" ) pool = session . query ( Pool ) . filter_by ( pool = name ) . first ( ) if pool is None : raise PoolNotFound ( ""Pool '%s' doesn't exist"" % name ) session . delete ( pool ) session . commit ( ) return pool",Delete pool by a given name .
"def _dict_to_proto ( py_dict , proto ) : dict_json_str = json . dumps ( py_dict ) return json_format . Parse ( dict_json_str , proto )",Converts a python dictionary to the proto supplied
"def wait_for_operation ( self , operation , project_id = None ) : self . log . info ( ""Waiting for OPERATION_NAME %s"" , operation . name ) time . sleep ( OPERATIONAL_POLL_INTERVAL ) while operation . status != Operation . Status . DONE : if operation . status == Operation . Status . RUNNING or operation . status == Operation . Status . PENDING : time . sleep ( OPERATIONAL_POLL_INTERVAL ) else : raise exceptions . GoogleCloudError ( ""Operation has failed with status: %s"" % operation . status ) operation = self . get_operation ( operation . name , project_id = project_id or self . project_id ) return operation",Given an operation continuously fetches the status from Google Cloud until either completion or an error occurring
"def get_operation ( self , operation_name , project_id = None ) : return self . get_client ( ) . get_operation ( project_id = project_id or self . project_id , zone = self . location , operation_id = operation_name )",Fetches the operation from Google Cloud
"def _append_label ( cluster_proto , key , val ) : val = val . replace ( '.' , '-' ) . replace ( '+' , '-' ) cluster_proto . resource_labels . update ( { key : val } ) return cluster_proto",Append labels to provided Cluster Protobuf
"def delete_cluster ( self , name , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : self . log . info ( ""Deleting (project_id=%s, zone=%s, cluster_id=%s)"" , self . project_id , self . location , name ) try : op = self . get_client ( ) . delete_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster_id = name , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . self_link except NotFound as error : self . log . info ( 'Assuming Success: %s' , error . message )",Deletes the cluster including the Kubernetes endpoint and all worker nodes . Firewalls and routes that were configured during cluster creation are also deleted . Other Google Compute Engine resources that might be in use by the cluster ( e . g . load balancer resources ) will not be deleted if they werent present at the initial create time .
"def create_cluster ( self , cluster , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : if isinstance ( cluster , dict ) : cluster_proto = Cluster ( ) cluster = self . _dict_to_proto ( py_dict = cluster , proto = cluster_proto ) elif not isinstance ( cluster , Cluster ) : raise AirflowException ( ""cluster is not instance of Cluster proto or python dict"" ) self . _append_label ( cluster , 'airflow-version' , 'v' + version . version ) self . log . info ( ""Creating (project_id=%s, zone=%s, cluster_name=%s)"" , self . project_id , self . location , cluster . name ) try : op = self . get_client ( ) . create_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster = cluster , retry = retry , timeout = timeout ) op = self . wait_for_operation ( op ) return op . target_link except AlreadyExists as error : self . log . info ( 'Assuming Success: %s' , error . message ) return self . get_cluster ( name = cluster . name ) . self_link",Creates a cluster consisting of the specified number and type of Google Compute Engine instances .
"def get_cluster ( self , name , project_id = None , retry = DEFAULT , timeout = DEFAULT ) : self . log . info ( ""Fetching cluster (project_id=%s, zone=%s, cluster_name=%s)"" , project_id or self . project_id , self . location , name ) return self . get_client ( ) . get_cluster ( project_id = project_id or self . project_id , zone = self . location , cluster_id = name , retry = retry , timeout = timeout ) . self_link",Gets details of specified cluster
"def _get_webhook_endpoint ( self , http_conn_id , webhook_endpoint ) : if webhook_endpoint : endpoint = webhook_endpoint elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson endpoint = extra . get ( 'webhook_endpoint' , '' ) else : raise AirflowException ( 'Cannot get webhook endpoint: No valid Discord ' 'webhook endpoint or http_conn_id supplied.' ) if not re . match ( '^webhooks/[0-9]+/[a-zA-Z0-9_-]+$' , endpoint ) : raise AirflowException ( 'Expected Discord webhook endpoint in the form ' 'of ""webhooks/{webhook.id}/{webhook.token}"".' ) return endpoint",Given a Discord http_conn_id return the default webhook endpoint or override if a webhook_endpoint is manually supplied .
def _build_discord_payload ( self ) : payload = { } if self . username : payload [ 'username' ] = self . username if self . avatar_url : payload [ 'avatar_url' ] = self . avatar_url payload [ 'tts' ] = self . tts if len ( self . message ) <= 2000 : payload [ 'content' ] = self . message else : raise AirflowException ( 'Discord message length must be 2000 or fewer ' 'characters.' ) return json . dumps ( payload ),Construct the Discord JSON payload . All relevant parameters are combined here to a valid Discord JSON payload .
"def execute ( self ) : proxies = { } if self . proxy : proxies = { 'https' : self . proxy } discord_payload = self . _build_discord_payload ( ) self . run ( endpoint = self . webhook_endpoint , data = discord_payload , headers = { 'Content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )",Execute the Discord webhook call
"def encrypt ( self , key_name , plaintext , authenticated_data = None ) : keys = self . get_conn ( ) . projects ( ) . locations ( ) . keyRings ( ) . cryptoKeys ( ) body = { 'plaintext' : _b64encode ( plaintext ) } if authenticated_data : body [ 'additionalAuthenticatedData' ] = _b64encode ( authenticated_data ) request = keys . encrypt ( name = key_name , body = body ) response = request . execute ( num_retries = self . num_retries ) ciphertext = response [ 'ciphertext' ] return ciphertext",Encrypts a plaintext message using Google Cloud KMS .
"def Popen ( self , cmd , * * kwargs ) : masked_cmd = ' ' . join ( self . cmd_mask_password ( cmd ) ) self . log . info ( ""Executing command: {}"" . format ( masked_cmd ) ) self . sp = subprocess . Popen ( cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , * * kwargs ) for line in iter ( self . sp . stdout ) : self . log . info ( line . strip ( ) ) self . sp . wait ( ) self . log . info ( ""Command exited with return code %s"" , self . sp . returncode ) if self . sp . returncode : raise AirflowException ( ""Sqoop command failed: {}"" . format ( masked_cmd ) )",Remote Popen
"def import_table ( self , table , target_dir = None , append = False , file_type = ""text"" , columns = None , split_by = None , where = None , direct = False , driver = None , extra_import_options = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ ""--table"" , table ] if columns : cmd += [ ""--columns"" , columns ] if where : cmd += [ ""--where"" , where ] self . Popen ( cmd )",Imports table from remote location to target dir . Arguments are copies of direct sqoop command line arguments
"def import_query ( self , query , target_dir , append = False , file_type = ""text"" , split_by = None , direct = None , driver = None , extra_import_options = None ) : cmd = self . _import_cmd ( target_dir , append , file_type , split_by , direct , driver , extra_import_options ) cmd += [ ""--query"" , query ] self . Popen ( cmd )",Imports a specific query from the rdbms to hdfs
"def export_table ( self , table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch , relaxed_isolation , extra_export_options = None ) : cmd = self . _export_cmd ( table , export_dir , input_null_string , input_null_non_string , staging_table , clear_staging_table , enclosed_by , escaped_by , input_fields_terminated_by , input_lines_terminated_by , input_optionally_enclosed_by , batch , relaxed_isolation , extra_export_options ) self . Popen ( cmd )",Exports Hive table to remote location . Arguments are copies of direct sqoop command line Arguments
def get_conn ( self ) : if not self . _client : self . _client = TextToSpeechClient ( credentials = self . _get_credentials ( ) ) return self . _client,Retrieves connection to Cloud Text to Speech .
"def synthesize_speech ( self , input_data , voice , audio_config , retry = None , timeout = None ) : client = self . get_conn ( ) self . log . info ( ""Synthesizing input: %s"" % input_data ) return client . synthesize_speech ( input_ = input_data , voice = voice , audio_config = audio_config , retry = retry , timeout = timeout )",Synthesizes text input
"def close ( self ) : if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . s3_write ( log , remote_loc ) self . closed = True",Close and upload local log file to remote storage S3 .
"def s3_read ( self , remote_log_location , return_error = False ) : try : return self . hook . read_key ( remote_log_location ) except Exception : msg = 'Could not read logs from {}' . format ( remote_log_location ) self . log . exception ( msg ) if return_error : return msg",Returns the log found at the remote_log_location . Returns if no logs are found or there is an error . : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param return_error : if True returns a string error message if an error occurs . Otherwise returns when an error occurs . : type return_error : bool
"def s3_write ( self , log , remote_log_location , append = True ) : if append and self . s3_log_exists ( remote_log_location ) : old_log = self . s3_read ( remote_log_location ) log = '\n' . join ( [ old_log , log ] ) if old_log else log try : self . hook . load_string ( log , key = remote_log_location , replace = True , encrypt = configuration . conf . getboolean ( 'core' , 'ENCRYPT_S3_LOGS' ) , ) except Exception : self . log . exception ( 'Could not write logs to %s' , remote_log_location )",Writes the log to the remote_log_location . Fails silently if no hook was created . : param log : the log to write to the remote_log_location : type log : str : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param append : if False any existing log file is overwritten . If True the new log is appended to any existing logs . : type append : bool
"def _get_init_containers ( self ) : if self . kube_config . dags_volume_claim or self . kube_config . dags_volume_host or self . kube_config . dags_in_image : return [ ] init_environment = [ { 'name' : 'GIT_SYNC_REPO' , 'value' : self . kube_config . git_repo } , { 'name' : 'GIT_SYNC_BRANCH' , 'value' : self . kube_config . git_branch } , { 'name' : 'GIT_SYNC_ROOT' , 'value' : self . kube_config . git_sync_root } , { 'name' : 'GIT_SYNC_DEST' , 'value' : self . kube_config . git_sync_dest } , { 'name' : 'GIT_SYNC_DEPTH' , 'value' : '1' } , { 'name' : 'GIT_SYNC_ONE_TIME' , 'value' : 'true' } ] if self . kube_config . git_user : init_environment . append ( { 'name' : 'GIT_SYNC_USERNAME' , 'value' : self . kube_config . git_user } ) if self . kube_config . git_password : init_environment . append ( { 'name' : 'GIT_SYNC_PASSWORD' , 'value' : self . kube_config . git_password } ) volume_mounts = [ { 'mountPath' : self . kube_config . git_sync_root , 'name' : self . dags_volume_name , 'readOnly' : False } ] if self . kube_config . git_ssh_key_secret_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_secret_volume_name , 'mountPath' : '/etc/git-secret/ssh' , 'subPath' : 'ssh' } ) init_environment . extend ( [ { 'name' : 'GIT_SSH_KEY_FILE' , 'value' : '/etc/git-secret/ssh' } , { 'name' : 'GIT_SYNC_SSH' , 'value' : 'true' } ] ) if self . kube_config . git_ssh_known_hosts_configmap_name : volume_mounts . append ( { 'name' : self . git_sync_ssh_known_hosts_volume_name , 'mountPath' : '/etc/git-secret/known_hosts' , 'subPath' : 'known_hosts' } ) init_environment . extend ( [ { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'true' } , { 'name' : 'GIT_SSH_KNOWN_HOSTS_FILE' , 'value' : '/etc/git-secret/known_hosts' } ] ) else : init_environment . append ( { 'name' : 'GIT_KNOWN_HOSTS' , 'value' : 'false' } ) return [ { 'name' : self . kube_config . git_sync_init_container_name , 'image' : self . kube_config . git_sync_container , 'securityContext' : { 'runAsUser' : 65533 } , 'env' : init_environment , 'volumeMounts' : volume_mounts } ]",When using git to retrieve the DAGs use the GitSync Init Container
"def _get_environment ( self ) : env = { } for env_var_name , env_var_val in six . iteritems ( self . kube_config . kube_env_vars ) : env [ env_var_name ] = env_var_val env [ ""AIRFLOW__CORE__EXECUTOR"" ] = ""LocalExecutor"" if self . kube_config . airflow_configmap : env [ 'AIRFLOW_HOME' ] = self . worker_airflow_home env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = self . worker_airflow_dags if ( not self . kube_config . airflow_configmap and 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' not in self . kube_config . kube_secrets ) : env [ 'AIRFLOW__CORE__SQL_ALCHEMY_CONN' ] = conf . get ( ""core"" , ""SQL_ALCHEMY_CONN"" ) if self . kube_config . git_dags_folder_mount_point : dag_volume_mount_path = os . path . join ( self . kube_config . git_dags_folder_mount_point , self . kube_config . git_sync_dest , self . kube_config . git_subpath ) env [ 'AIRFLOW__CORE__DAGS_FOLDER' ] = dag_volume_mount_path return env",Defines any necessary environment variables for the pod executor
"def _get_secrets ( self ) : worker_secrets = [ ] for env_var_name , obj_key_pair in six . iteritems ( self . kube_config . kube_secrets ) : k8s_secret_obj , k8s_secret_key = obj_key_pair . split ( '=' ) worker_secrets . append ( Secret ( 'env' , env_var_name , k8s_secret_obj , k8s_secret_key ) ) if self . kube_config . env_from_secret_ref : for secret_ref in self . kube_config . env_from_secret_ref . split ( ',' ) : worker_secrets . append ( Secret ( 'env' , None , secret_ref ) ) return worker_secrets",Defines any necessary secrets for the pod executor
def _get_security_context ( self ) : security_context = { } if self . kube_config . worker_run_as_user : security_context [ 'runAsUser' ] = self . kube_config . worker_run_as_user if self . kube_config . worker_fs_group : security_context [ 'fsGroup' ] = self . kube_config . worker_fs_group if self . kube_config . git_ssh_key_secret_name and security_context . get ( 'fsGroup' ) is None : security_context [ 'fsGroup' ] = 65533 return security_context,Defines the security context
"def kill ( self , ti ) : if self . cmd is None : if not ti and not self . task_instance : raise Exception ( ""Unable to cancel Qubole Command, context is unavailable!"" ) elif not ti : ti = self . task_instance cmd_id = ti . xcom_pull ( key = ""qbol_cmd_id"" , task_ids = ti . task_id ) self . cmd = self . cls . find ( cmd_id ) if self . cls and self . cmd : self . log . info ( 'Sending KILL signal to Qubole Command Id: %s' , self . cmd . id ) self . cmd . cancel ( )",Kill ( cancel ) a Qubole command : param ti : Task Instance of the dag used to determine the Quboles command id : return : response from Qubole
"def get_results ( self , ti = None , fp = None , inline = True , delim = None , fetch = True ) : if fp is None : iso = datetime . datetime . utcnow ( ) . isoformat ( ) logpath = os . path . expanduser ( configuration . conf . get ( 'core' , 'BASE_LOG_FOLDER' ) ) resultpath = logpath + '/' + self . dag_id + '/' + self . task_id + '/results' configuration . mkdir_p ( resultpath ) fp = open ( resultpath + '/' + iso , 'wb' ) if self . cmd is None : cmd_id = ti . xcom_pull ( key = ""qbol_cmd_id"" , task_ids = self . task_id ) self . cmd = self . cls . find ( cmd_id ) self . cmd . get_results ( fp , inline , delim , fetch ) fp . flush ( ) fp . close ( ) return fp . name",Get results ( or just s3 locations ) of a command from Qubole and save into a file : param ti : Task Instance of the dag used to determine the Quboles command id : param fp : Optional file pointer will create one and return if None passed : param inline : True to download actual results False to get s3 locations only : param delim : Replaces the CTL - A chars with the given delim defaults to : param fetch : when inline is True get results directly from s3 ( if large ) : return : file location containing actual results or s3 locations of results
"def get_log ( self , ti ) : if self . cmd is None : cmd_id = ti . xcom_pull ( key = ""qbol_cmd_id"" , task_ids = self . task_id ) Command . get_log_id ( self . cls , cmd_id )",Get Logs of a command from Qubole : param ti : Task Instance of the dag used to determine the Quboles command id : return : command log as text
"def get_jobs_id ( self , ti ) : if self . cmd is None : cmd_id = ti . xcom_pull ( key = ""qbol_cmd_id"" , task_ids = self . task_id ) Command . get_jobs_id ( self . cls , cmd_id )",Get jobs associated with a Qubole commands : param ti : Task Instance of the dag used to determine the Quboles command id : return : Job information associated with command
"def get_extra_links ( self , operator , dttm ) : conn = BaseHook . get_connection ( operator . kwargs [ 'qubole_conn_id' ] ) if conn and conn . host : host = re . sub ( r'api$' , 'v2/analyze?command_id=' , conn . host ) else : host = 'https://api.qubole.com/v2/analyze?command_id=' ti = TaskInstance ( task = operator , execution_date = dttm ) qds_command_id = ti . xcom_pull ( task_ids = operator . task_id , key = 'qbol_cmd_id' ) url = host + str ( qds_command_id ) if qds_command_id else '' return url",Get link to qubole command result page .
"def heartbeat ( self ) : try : with create_session ( ) as session : job = session . query ( BaseJob ) . filter_by ( id = self . id ) . one ( ) make_transient ( job ) session . commit ( ) if job . state == State . SHUTDOWN : self . kill ( ) is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) if not is_unit_test : sleep_for = 0 if job . latest_heartbeat : seconds_remaining = self . heartrate - ( timezone . utcnow ( ) - job . latest_heartbeat ) . total_seconds ( ) sleep_for = max ( 0 , seconds_remaining ) sleep ( sleep_for ) with create_session ( ) as session : job = session . query ( BaseJob ) . filter ( BaseJob . id == self . id ) . first ( ) job . latest_heartbeat = timezone . utcnow ( ) session . merge ( job ) session . commit ( ) self . heartbeat_callback ( session = session ) self . log . debug ( '[heartbeat]' ) except OperationalError as e : self . log . error ( ""Scheduler heartbeat got an exception: %s"" , str ( e ) )",Heartbeats update the job s entry in the database with a timestamp for the latest_heartbeat and allows for the job to be killed externally . This allows at the system level to monitor what is actually active .
"def reset_state_for_orphaned_tasks ( self , filter_by_dag_run = None , session = None ) : queued_tis = self . executor . queued_tasks running_tis = self . executor . running resettable_states = [ State . SCHEDULED , State . QUEUED ] TI = models . TaskInstance DR = models . DagRun if filter_by_dag_run is None : resettable_tis = ( session . query ( TI ) . join ( DR , and_ ( TI . dag_id == DR . dag_id , TI . execution_date == DR . execution_date ) ) . filter ( DR . state == State . RUNNING , DR . run_id . notlike ( BackfillJob . ID_PREFIX + '%' ) , TI . state . in_ ( resettable_states ) ) ) . all ( ) else : resettable_tis = filter_by_dag_run . get_task_instances ( state = resettable_states , session = session ) tis_to_reset = [ ] for ti in resettable_tis : if ti . key not in queued_tis and ti . key not in running_tis : tis_to_reset . append ( ti ) if len ( tis_to_reset ) == 0 : return [ ] def query ( result , items ) : filter_for_tis = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in items ] ) reset_tis = ( session . query ( TI ) . filter ( or_ ( * filter_for_tis ) , TI . state . in_ ( resettable_states ) ) . with_for_update ( ) . all ( ) ) for ti in reset_tis : ti . state = State . NONE session . merge ( ti ) return result + reset_tis reset_tis = helpers . reduce_in_chunks ( query , tis_to_reset , [ ] , self . max_tis_per_query ) task_instance_str = '\n\t' . join ( [ repr ( x ) for x in reset_tis ] ) session . commit ( ) self . log . info ( ""Reset the following %s TaskInstances:\n\t%s"" , len ( reset_tis ) , task_instance_str ) return reset_tis",This function checks if there are any tasks in the dagrun ( or all ) that have a scheduled state but are not known by the executor . If it finds those it will reset the state to None so they will get picked up again . The batch option is for performance reasons as the queries are made in sequence .
"def _launch_process ( result_queue , file_path , pickle_dags , dag_id_white_list , thread_name , zombies ) : def helper ( ) : log = logging . getLogger ( ""airflow.processor"" ) stdout = StreamLogWriter ( log , logging . INFO ) stderr = StreamLogWriter ( log , logging . WARN ) set_context ( log , file_path ) try : sys . stdout = stdout sys . stderr = stderr settings . configure_orm ( ) threading . current_thread ( ) . name = thread_name start_time = time . time ( ) log . info ( ""Started process (PID=%s) to work on %s"" , os . getpid ( ) , file_path ) scheduler_job = SchedulerJob ( dag_ids = dag_id_white_list , log = log ) result = scheduler_job . process_file ( file_path , zombies , pickle_dags ) result_queue . put ( result ) end_time = time . time ( ) log . info ( ""Processing %s took %.3f seconds"" , file_path , end_time - start_time ) except Exception : log . exception ( ""Got an exception! Propagating..."" ) raise finally : sys . stdout = sys . __stdout__ sys . stderr = sys . __stderr__ settings . dispose_orm ( ) p = multiprocessing . Process ( target = helper , args = ( ) , name = ""{}-Process"" . format ( thread_name ) ) p . start ( ) return p",Launch a process to process the given file .
"def start ( self ) : self . _process = DagFileProcessor . _launch_process ( self . _result_queue , self . file_path , self . _pickle_dags , self . _dag_id_white_list , ""DagFileProcessor{}"" . format ( self . _instance_id ) , self . _zombies ) self . _start_time = timezone . utcnow ( )",Launch the process and start processing the DAG .
"def terminate ( self , sigkill = False ) : if self . _process is None : raise AirflowException ( ""Tried to call stop before starting!"" ) self . _result_queue = None self . _process . terminate ( ) self . _process . join ( 5 ) if sigkill and self . _process . is_alive ( ) : self . log . warning ( ""Killing PID %s"" , self . _process . pid ) os . kill ( self . _process . pid , signal . SIGKILL )",Terminate ( and then kill ) the process launched to process the file .
"def done ( self ) : if self . _process is None : raise AirflowException ( ""Tried to see if it's done before starting!"" ) if self . _done : return True if self . _result_queue and not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . _done = True self . log . debug ( ""Waiting for %s"" , self . _process ) self . _process . join ( ) return True if self . _result_queue and not self . _process . is_alive ( ) : self . _done = True if not self . _result_queue . empty ( ) : self . _result = self . _result_queue . get_nowait ( ) self . log . debug ( ""Waiting for %s"" , self . _process ) self . _process . join ( ) return True return False",Check if the process launched to process this file is done .
"def _exit_gracefully ( self , signum , frame ) : self . log . info ( ""Exiting gracefully upon receiving signal %s"" , signum ) if self . processor_agent : self . processor_agent . end ( ) sys . exit ( os . EX_OK )",Helper method to clean up processor_agent to avoid leaving orphan processes .
"def manage_slas ( self , dag , session = None ) : if not any ( [ isinstance ( ti . sla , timedelta ) for ti in dag . tasks ] ) : self . log . info ( ""Skipping SLA check for %s because no tasks in DAG have SLAs"" , dag ) return TI = models . TaskInstance sq = ( session . query ( TI . task_id , func . max ( TI . execution_date ) . label ( 'max_ti' ) ) . with_hint ( TI , 'USE INDEX (PRIMARY)' , dialect_name = 'mysql' ) . filter ( TI . dag_id == dag . dag_id ) . filter ( or_ ( TI . state == State . SUCCESS , TI . state == State . SKIPPED ) ) . filter ( TI . task_id . in_ ( dag . task_ids ) ) . group_by ( TI . task_id ) . subquery ( 'sq' ) ) max_tis = session . query ( TI ) . filter ( TI . dag_id == dag . dag_id , TI . task_id == sq . c . task_id , TI . execution_date == sq . c . max_ti , ) . all ( ) ts = timezone . utcnow ( ) for ti in max_tis : task = dag . get_task ( ti . task_id ) dttm = ti . execution_date if isinstance ( task . sla , timedelta ) : dttm = dag . following_schedule ( dttm ) while dttm < timezone . utcnow ( ) : following_schedule = dag . following_schedule ( dttm ) if following_schedule + task . sla < timezone . utcnow ( ) : session . merge ( SlaMiss ( task_id = ti . task_id , dag_id = ti . dag_id , execution_date = dttm , timestamp = ts ) ) dttm = dag . following_schedule ( dttm ) session . commit ( ) slas = ( session . query ( SlaMiss ) . filter ( SlaMiss . notification_sent == False , SlaMiss . dag_id == dag . dag_id ) . all ( ) ) if slas : sla_dates = [ sla . execution_date for sla in slas ] qry = ( session . query ( TI ) . filter ( TI . state != State . SUCCESS , TI . execution_date . in_ ( sla_dates ) , TI . dag_id == dag . dag_id ) . all ( ) ) blocking_tis = [ ] for ti in qry : if ti . task_id in dag . task_ids : ti . task = dag . get_task ( ti . task_id ) blocking_tis . append ( ti ) else : session . delete ( ti ) session . commit ( ) task_list = ""\n"" . join ( [ sla . task_id + ' on ' + sla . execution_date . isoformat ( ) for sla in slas ] ) blocking_task_list = ""\n"" . join ( [ ti . task_id + ' on ' + ti . execution_date . isoformat ( ) for ti in blocking_tis ] ) email_sent = False notification_sent = False if dag . sla_miss_callback : self . log . info ( ' --------------> ABOUT TO CALL SLA MISS CALL BACK ' ) try : dag . sla_miss_callback ( dag , task_list , blocking_task_list , slas , blocking_tis ) notification_sent = True except Exception : self . log . exception ( ""Could not call sla_miss_callback for DAG %s"" , dag . dag_id ) email_content = """"""\
            Here's a list of tasks that missed their SLAs:
            <pre><code>{task_list}\n<code></pre>
            Blocking tasks:
            <pre><code>{blocking_task_list}\n{bug}<code></pre>
            """""" . format ( task_list = task_list , blocking_task_list = blocking_task_list , bug = asciiart . bug ) emails = set ( ) for task in dag . tasks : if task . email : if isinstance ( task . email , basestring ) : emails |= set ( get_email_address_list ( task . email ) ) elif isinstance ( task . email , ( list , tuple ) ) : emails |= set ( task . email ) if emails : try : send_email ( emails , ""[airflow] SLA miss on DAG="" + dag . dag_id , email_content ) email_sent = True notification_sent = True except Exception : self . log . exception ( ""Could not send SLA Miss email notification for"" "" DAG %s"" , dag . dag_id ) if notification_sent : for sla in slas : if email_sent : sla . email_sent = True sla . notification_sent = True session . merge ( sla ) session . commit ( )",Finding all tasks that have SLAs defined and sending alert emails where needed . New SLA misses are also recorded in the database .
"def update_import_errors ( session , dagbag ) : for dagbag_file in dagbag . file_last_changed : session . query ( errors . ImportError ) . filter ( errors . ImportError . filename == dagbag_file ) . delete ( ) for filename , stacktrace in six . iteritems ( dagbag . import_errors ) : session . add ( errors . ImportError ( filename = filename , stacktrace = stacktrace ) ) session . commit ( )",For the DAGs in the given DagBag record any associated import errors and clears errors for files that no longer have them . These are usually displayed through the Airflow UI so that users know that there are issues parsing DAGs .
"def create_dag_run ( self , dag , session = None ) : if dag . schedule_interval and conf . getboolean ( 'scheduler' , 'USE_JOB_SCHEDULE' ) : active_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , external_trigger = False , session = session ) if len ( active_runs ) >= dag . max_active_runs and not dag . dagrun_timeout : return timedout_runs = 0 for dr in active_runs : if ( dr . start_date and dag . dagrun_timeout and dr . start_date < timezone . utcnow ( ) - dag . dagrun_timeout ) : dr . state = State . FAILED dr . end_date = timezone . utcnow ( ) dag . handle_callback ( dr , success = False , reason = 'dagrun_timeout' , session = session ) timedout_runs += 1 session . commit ( ) if len ( active_runs ) - timedout_runs >= dag . max_active_runs : return qry = ( session . query ( func . max ( DagRun . execution_date ) ) . filter_by ( dag_id = dag . dag_id ) . filter ( or_ ( DagRun . external_trigger == False , DagRun . run_id . like ( DagRun . ID_PREFIX + '%' ) ) ) ) last_scheduled_run = qry . scalar ( ) if dag . schedule_interval == '@once' and last_scheduled_run : return None if not ( dag . catchup or dag . schedule_interval == '@once' ) : now = timezone . utcnow ( ) next_start = dag . following_schedule ( now ) last_start = dag . previous_schedule ( now ) if next_start <= now : new_start = last_start else : new_start = dag . previous_schedule ( last_start ) if dag . start_date : if new_start >= dag . start_date : dag . start_date = new_start else : dag . start_date = new_start next_run_date = None if not last_scheduled_run : task_start_dates = [ t . start_date for t in dag . tasks ] if task_start_dates : next_run_date = dag . normalize_schedule ( min ( task_start_dates ) ) self . log . debug ( ""Next run date based on tasks %s"" , next_run_date ) else : next_run_date = dag . following_schedule ( last_scheduled_run ) last_run = dag . get_last_dagrun ( session = session ) if last_run and next_run_date : while next_run_date <= last_run . execution_date : next_run_date = dag . following_schedule ( next_run_date ) if dag . start_date : next_run_date = ( dag . start_date if not next_run_date else max ( next_run_date , dag . start_date ) ) if next_run_date == dag . start_date : next_run_date = dag . normalize_schedule ( dag . start_date ) self . log . debug ( ""Dag start date: %s. Next run date: %s"" , dag . start_date , next_run_date ) if not next_run_date or next_run_date > timezone . utcnow ( ) : return if dag . schedule_interval == '@once' : period_end = next_run_date elif next_run_date : period_end = dag . following_schedule ( next_run_date ) if next_run_date and dag . end_date and next_run_date > dag . end_date : return min_task_end_date = [ ] task_end_dates = [ t . end_date for t in dag . tasks if t . end_date ] if task_end_dates : min_task_end_date = min ( task_end_dates ) if next_run_date and min_task_end_date and next_run_date > min_task_end_date : return if next_run_date and period_end and period_end <= timezone . utcnow ( ) : next_run = dag . create_dagrun ( run_id = DagRun . ID_PREFIX + next_run_date . isoformat ( ) , execution_date = next_run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False ) return next_run",This method checks whether a new DagRun needs to be created for a DAG based on scheduling interval . Returns DagRun if one is scheduled . Otherwise returns None .
"def _process_task_instances ( self , dag , queue , session = None ) : dag_runs = DagRun . find ( dag_id = dag . dag_id , state = State . RUNNING , session = session ) active_dag_runs = [ ] for run in dag_runs : self . log . info ( ""Examining DAG run %s"" , run ) if run . execution_date > timezone . utcnow ( ) : self . log . error ( ""Execution date is in future: %s"" , run . execution_date ) continue if len ( active_dag_runs ) >= dag . max_active_runs : self . log . info ( ""Number of active dag runs reached max_active_run."" ) break if run . is_backfill : continue run . dag = dag run . verify_integrity ( session = session ) run . update_state ( session = session ) if run . state == State . RUNNING : make_transient ( run ) active_dag_runs . append ( run ) for run in active_dag_runs : self . log . debug ( ""Examining active DAG run: %s"" , run ) tis = run . get_task_instances ( state = ( State . NONE , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) ) for ti in tis : task = dag . get_task ( ti . task_id ) ti . task = task if ti . are_dependencies_met ( dep_context = DepContext ( flag_upstream_failed = True ) , session = session ) : self . log . debug ( 'Queuing task: %s' , ti ) queue . append ( ti . key )",This method schedules the tasks for a single DAG by looking at the active DAG runs and adding task instances that should run to the queue .
"def _change_state_for_tis_without_dagrun ( self , simple_dag_bag , old_states , new_state , session = None ) : tis_changed = 0 query = session . query ( models . TaskInstance ) . outerjoin ( models . DagRun , and_ ( models . TaskInstance . dag_id == models . DagRun . dag_id , models . TaskInstance . execution_date == models . DagRun . execution_date ) ) . filter ( models . TaskInstance . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . filter ( models . TaskInstance . state . in_ ( old_states ) ) . filter ( or_ ( models . DagRun . state != State . RUNNING , models . DagRun . state . is_ ( None ) ) ) if self . using_sqlite : tis_to_change = query . with_for_update ( ) . all ( ) for ti in tis_to_change : ti . set_state ( new_state , session = session ) tis_changed += 1 else : subq = query . subquery ( ) tis_changed = session . query ( models . TaskInstance ) . filter ( and_ ( models . TaskInstance . dag_id == subq . c . dag_id , models . TaskInstance . task_id == subq . c . task_id , models . TaskInstance . execution_date == subq . c . execution_date ) ) . update ( { models . TaskInstance . state : new_state } , synchronize_session = False ) session . commit ( ) if tis_changed > 0 : self . log . warning ( ""Set %s task instances to state=%s as their associated DagRun was not in RUNNING state"" , tis_changed , new_state )",For all DAG IDs in the SimpleDagBag look for task instances in the old_states and set them to new_state if the corresponding DagRun does not exist or exists but is not in the running state . This normally should not happen but it can if the state of DagRuns are changed manually .
"def __get_concurrency_maps ( self , states , session = None ) : TI = models . TaskInstance ti_concurrency_query = ( session . query ( TI . task_id , TI . dag_id , func . count ( '*' ) ) . filter ( TI . state . in_ ( states ) ) . group_by ( TI . task_id , TI . dag_id ) ) . all ( ) dag_map = defaultdict ( int ) task_map = defaultdict ( int ) for result in ti_concurrency_query : task_id , dag_id , count = result dag_map [ dag_id ] += count task_map [ ( dag_id , task_id ) ] = count return dag_map , task_map",Get the concurrency maps .
"def _find_executable_task_instances ( self , simple_dag_bag , states , session = None ) : executable_tis = [ ] TI = models . TaskInstance DR = models . DagRun DM = models . DagModel ti_query = ( session . query ( TI ) . filter ( TI . dag_id . in_ ( simple_dag_bag . dag_ids ) ) . outerjoin ( DR , and_ ( DR . dag_id == TI . dag_id , DR . execution_date == TI . execution_date ) ) . filter ( or_ ( DR . run_id == None , not_ ( DR . run_id . like ( BackfillJob . ID_PREFIX + '%' ) ) ) ) . outerjoin ( DM , DM . dag_id == TI . dag_id ) . filter ( or_ ( DM . dag_id == None , not_ ( DM . is_paused ) ) ) ) if None in states : ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( states ) ) ) else : ti_query = ti_query . filter ( TI . state . in_ ( states ) ) task_instances_to_examine = ti_query . all ( ) if len ( task_instances_to_examine ) == 0 : self . log . debug ( ""No tasks to consider for execution."" ) return executable_tis task_instance_str = ""\n\t"" . join ( [ repr ( x ) for x in task_instances_to_examine ] ) self . log . info ( ""%s tasks up for execution:\n\t%s"" , len ( task_instances_to_examine ) , task_instance_str ) pools = { p . pool : p for p in session . query ( models . Pool ) . all ( ) } pool_to_task_instances = defaultdict ( list ) for task_instance in task_instances_to_examine : pool_to_task_instances [ task_instance . pool ] . append ( task_instance ) states_to_count_as_running = [ State . RUNNING , State . QUEUED ] dag_concurrency_map , task_concurrency_map = self . __get_concurrency_maps ( states = states_to_count_as_running , session = session ) for pool , task_instances in pool_to_task_instances . items ( ) : pool_name = pool if not pool : open_slots = models . Pool . default_pool_open_slots ( ) pool_name = models . Pool . default_pool_name else : if pool not in pools : self . log . warning ( ""Tasks using non-existent pool '%s' will not be scheduled"" , pool ) open_slots = 0 else : open_slots = pools [ pool ] . open_slots ( session = session ) num_ready = len ( task_instances ) self . log . info ( ""Figuring out tasks to run in Pool(name=%s) with %s open slots "" ""and %s task instances ready to be queued"" , pool , open_slots , num_ready ) priority_sorted_task_instances = sorted ( task_instances , key = lambda ti : ( - ti . priority_weight , ti . execution_date ) ) num_starving_tasks = 0 for current_index , task_instance in enumerate ( priority_sorted_task_instances ) : if open_slots <= 0 : self . log . info ( ""Not scheduling since there are %s open slots in pool %s"" , open_slots , pool ) num_starving_tasks = len ( priority_sorted_task_instances ) - current_index break dag_id = task_instance . dag_id simple_dag = simple_dag_bag . get_dag ( dag_id ) current_dag_concurrency = dag_concurrency_map [ dag_id ] dag_concurrency_limit = simple_dag_bag . get_dag ( dag_id ) . concurrency self . log . info ( ""DAG %s has %s/%s running and queued tasks"" , dag_id , current_dag_concurrency , dag_concurrency_limit ) if current_dag_concurrency >= dag_concurrency_limit : self . log . info ( ""Not executing %s since the number of tasks running or queued "" ""from DAG %s is >= to the DAG's task concurrency limit of %s"" , task_instance , dag_id , dag_concurrency_limit ) continue task_concurrency_limit = simple_dag . get_task_special_arg ( task_instance . task_id , 'task_concurrency' ) if task_concurrency_limit is not None : current_task_concurrency = task_concurrency_map [ ( task_instance . dag_id , task_instance . task_id ) ] if current_task_concurrency >= task_concurrency_limit : self . log . info ( ""Not executing %s since the task concurrency for"" "" this task has been reached."" , task_instance ) continue if self . executor . has_task ( task_instance ) : self . log . debug ( ""Not handling task %s as the executor reports it is running"" , task_instance . key ) continue executable_tis . append ( task_instance ) open_slots -= 1 dag_concurrency_map [ dag_id ] += 1 task_concurrency_map [ ( task_instance . dag_id , task_instance . task_id ) ] += 1 Stats . gauge ( 'pool.starving_tasks.{pool_name}' . format ( pool_name = pool_name ) , num_starving_tasks ) task_instance_str = ""\n\t"" . join ( [ repr ( x ) for x in executable_tis ] ) self . log . info ( ""Setting the following tasks to queued state:\n\t%s"" , task_instance_str ) for ti in executable_tis : copy_dag_id = ti . dag_id copy_execution_date = ti . execution_date copy_task_id = ti . task_id make_transient ( ti ) ti . dag_id = copy_dag_id ti . execution_date = copy_execution_date ti . task_id = copy_task_id return executable_tis",Finds TIs that are ready for execution with respect to pool limits dag concurrency executor state and priority .
"def _change_state_for_executable_task_instances ( self , task_instances , acceptable_states , session = None ) : if len ( task_instances ) == 0 : session . commit ( ) return [ ] TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == ti . dag_id , TI . task_id == ti . task_id , TI . execution_date == ti . execution_date ) for ti in task_instances ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) if None in acceptable_states : ti_query = ti_query . filter ( or_ ( TI . state == None , TI . state . in_ ( acceptable_states ) ) ) else : ti_query = ti_query . filter ( TI . state . in_ ( acceptable_states ) ) tis_to_set_to_queued = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_queued ) == 0 : self . log . info ( ""No tasks were able to have their state changed to queued."" ) session . commit ( ) return [ ] for task_instance in tis_to_set_to_queued : task_instance . state = State . QUEUED task_instance . queued_dttm = ( timezone . utcnow ( ) if not task_instance . queued_dttm else task_instance . queued_dttm ) session . merge ( task_instance ) simple_task_instances = [ SimpleTaskInstance ( ti ) for ti in tis_to_set_to_queued ] task_instance_str = ""\n\t"" . join ( [ repr ( x ) for x in tis_to_set_to_queued ] ) session . commit ( ) self . log . info ( ""Setting the following %s tasks to queued state:\n\t%s"" , len ( tis_to_set_to_queued ) , task_instance_str ) return simple_task_instances",Changes the state of task instances in the list with one of the given states to QUEUED atomically and returns the TIs changed in SimpleTaskInstance format .
"def _enqueue_task_instances_with_queued_state ( self , simple_dag_bag , simple_task_instances ) : TI = models . TaskInstance for simple_task_instance in simple_task_instances : simple_dag = simple_dag_bag . get_dag ( simple_task_instance . dag_id ) command = TI . generate_command ( simple_task_instance . dag_id , simple_task_instance . task_id , simple_task_instance . execution_date , local = True , mark_success = False , ignore_all_deps = False , ignore_depends_on_past = False , ignore_task_deps = False , ignore_ti_state = False , pool = simple_task_instance . pool , file_path = simple_dag . full_filepath , pickle_id = simple_dag . pickle_id ) priority = simple_task_instance . priority_weight queue = simple_task_instance . queue self . log . info ( ""Sending %s to executor with priority %s and queue %s"" , simple_task_instance . key , priority , queue ) self . executor . queue_command ( simple_task_instance , command , priority = priority , queue = queue )",Takes task_instances which should have been set to queued and enqueues them with the executor .
"def _execute_task_instances ( self , simple_dag_bag , states , session = None ) : executable_tis = self . _find_executable_task_instances ( simple_dag_bag , states , session = session ) def query ( result , items ) : simple_tis_with_state_changed = self . _change_state_for_executable_task_instances ( items , states , session = session ) self . _enqueue_task_instances_with_queued_state ( simple_dag_bag , simple_tis_with_state_changed ) session . commit ( ) return result + len ( simple_tis_with_state_changed ) return helpers . reduce_in_chunks ( query , executable_tis , 0 , self . max_tis_per_query )",Attempts to execute TaskInstances that should be executed by the scheduler .
"def _change_state_for_tasks_failed_to_execute ( self , session ) : if self . executor . queued_tasks : TI = models . TaskInstance filter_for_ti_state_change = ( [ and_ ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date , TI . _try_number == try_number - 1 , TI . state == State . QUEUED ) for dag_id , task_id , execution_date , try_number in self . executor . queued_tasks . keys ( ) ] ) ti_query = ( session . query ( TI ) . filter ( or_ ( * filter_for_ti_state_change ) ) ) tis_to_set_to_scheduled = ( ti_query . with_for_update ( ) . all ( ) ) if len ( tis_to_set_to_scheduled ) == 0 : session . commit ( ) return for task_instance in tis_to_set_to_scheduled : task_instance . state = State . SCHEDULED task_instance_str = ""\n\t"" . join ( [ repr ( x ) for x in tis_to_set_to_scheduled ] ) session . commit ( ) self . log . info ( ""Set the following tasks to scheduled state:\n\t%s"" , task_instance_str )",If there are tasks left over in the executor we set them back to SCHEDULED to avoid creating hanging tasks .
"def _process_dags ( self , dagbag , dags , tis_out ) : for dag in dags : dag = dagbag . get_dag ( dag . dag_id ) if not dag : self . log . error ( ""DAG ID %s was not found in the DagBag"" , dag . dag_id ) continue if dag . is_paused : self . log . info ( ""Not processing DAG %s since it's paused"" , dag . dag_id ) continue self . log . info ( ""Processing %s"" , dag . dag_id ) dag_run = self . create_dag_run ( dag ) if dag_run : expected_start_date = dag . following_schedule ( dag_run . execution_date ) if expected_start_date : schedule_delay = dag_run . start_date - expected_start_date Stats . timing ( 'dagrun.schedule_delay.{dag_id}' . format ( dag_id = dag . dag_id ) , schedule_delay ) self . log . info ( ""Created %s"" , dag_run ) self . _process_task_instances ( dag , tis_out ) self . manage_slas ( dag )",Iterates over the dags and processes them . Processing includes :
"def _process_executor_events ( self , simple_dag_bag , session = None ) : TI = models . TaskInstance for key , state in list ( self . executor . get_event_buffer ( simple_dag_bag . dag_ids ) . items ( ) ) : dag_id , task_id , execution_date , try_number = key self . log . info ( ""Executor reports execution of %s.%s execution_date=%s "" ""exited with status %s for try_number %s"" , dag_id , task_id , execution_date , state , try_number ) if state == State . FAILED or state == State . SUCCESS : qry = session . query ( TI ) . filter ( TI . dag_id == dag_id , TI . task_id == task_id , TI . execution_date == execution_date ) ti = qry . first ( ) if not ti : self . log . warning ( ""TaskInstance %s went missing from the database"" , ti ) continue if ti . try_number == try_number and ti . state == State . QUEUED : msg = ( ""Executor reports task instance {} finished ({}) "" ""although the task says its {}. Was the task "" ""killed externally?"" . format ( ti , state , ti . state ) ) self . log . error ( msg ) try : simple_dag = simple_dag_bag . get_dag ( dag_id ) dagbag = models . DagBag ( simple_dag . full_filepath ) dag = dagbag . get_dag ( dag_id ) ti . task = dag . get_task ( task_id ) ti . handle_failure ( msg ) except Exception : self . log . error ( ""Cannot load the dag bag to handle failure for %s"" "". Setting task to FAILED without callbacks or "" ""retries. Do you have enough resources?"" , ti ) ti . state = State . FAILED session . merge ( ti ) session . commit ( )",Respond to executor events .
"def _execute_helper ( self ) : self . executor . start ( ) self . log . info ( ""Resetting orphaned tasks for active dag runs"" ) self . reset_state_for_orphaned_tasks ( ) self . processor_agent . start ( ) execute_start_time = timezone . utcnow ( ) last_self_heartbeat_time = timezone . utcnow ( ) while True : self . log . debug ( ""Starting Loop..."" ) loop_start_time = time . time ( ) if self . using_sqlite : self . processor_agent . heartbeat ( ) self . log . debug ( ""Waiting for processors to finish since we're using sqlite"" ) self . processor_agent . wait_until_finished ( ) self . log . debug ( ""Harvesting DAG parsing results"" ) simple_dags = self . processor_agent . harvest_simple_dags ( ) self . log . debug ( ""Harvested {} SimpleDAGs"" . format ( len ( simple_dags ) ) ) simple_dag_bag = SimpleDagBag ( simple_dags ) if len ( simple_dags ) > 0 : try : simple_dag_bag = SimpleDagBag ( simple_dags ) self . _change_state_for_tis_without_dagrun ( simple_dag_bag , [ State . UP_FOR_RETRY ] , State . FAILED ) self . _change_state_for_tis_without_dagrun ( simple_dag_bag , [ State . QUEUED , State . SCHEDULED , State . UP_FOR_RESCHEDULE ] , State . NONE ) self . _execute_task_instances ( simple_dag_bag , ( State . SCHEDULED , ) ) except Exception as e : self . log . error ( ""Error queuing tasks"" ) self . log . exception ( e ) continue self . log . debug ( ""Heartbeating the executor"" ) self . executor . heartbeat ( ) self . _change_state_for_tasks_failed_to_execute ( ) self . _process_executor_events ( simple_dag_bag ) time_since_last_heartbeat = ( timezone . utcnow ( ) - last_self_heartbeat_time ) . total_seconds ( ) if time_since_last_heartbeat > self . heartrate : self . log . debug ( ""Heartbeating the scheduler"" ) self . heartbeat ( ) last_self_heartbeat_time = timezone . utcnow ( ) is_unit_test = conf . getboolean ( 'core' , 'unit_test_mode' ) loop_end_time = time . time ( ) loop_duration = loop_end_time - loop_start_time self . log . debug ( ""Ran scheduling loop in %.2f seconds"" , loop_duration ) if not is_unit_test : self . log . debug ( ""Sleeping for %.2f seconds"" , self . _processor_poll_interval ) time . sleep ( self . _processor_poll_interval ) if self . processor_agent . done : self . _last_loop = True if self . _last_loop : self . log . info ( ""Exiting scheduler loop as all files"" "" have been processed {} times"" . format ( self . num_runs ) ) break if loop_duration < 1 and not is_unit_test : sleep_length = 1 - loop_duration self . log . debug ( ""Sleeping for {0:.2f} seconds to prevent excessive logging"" . format ( sleep_length ) ) sleep ( sleep_length ) self . processor_agent . terminate ( ) if self . processor_agent . all_files_processed : self . log . info ( ""Deactivating DAGs that haven't been touched since %s"" , execute_start_time . isoformat ( ) ) models . DAG . deactivate_stale_dags ( execute_start_time ) self . executor . end ( ) settings . Session . remove ( )",The actual scheduler loop . The main steps in the loop are : # . Harvest DAG parsing results through DagFileProcessorAgent # . Find and queue executable tasks # . Change task instance state in DB # . Queue tasks in executor # . Heartbeat executor # . Execute queued tasks in executor asynchronously # . Sync on the states of running tasks
"def process_file ( self , file_path , zombies , pickle_dags = False , session = None ) : self . log . info ( ""Processing file %s for tasks to queue"" , file_path ) simple_dags = [ ] try : dagbag = models . DagBag ( file_path , include_examples = False ) except Exception : self . log . exception ( ""Failed at reloading the DAG file %s"" , file_path ) Stats . incr ( 'dag_file_refresh_error' , 1 , 1 ) return [ ] if len ( dagbag . dags ) > 0 : self . log . info ( ""DAG(s) %s retrieved from %s"" , dagbag . dags . keys ( ) , file_path ) else : self . log . warning ( ""No viable dags retrieved from %s"" , file_path ) self . update_import_errors ( session , dagbag ) return [ ] for dag in dagbag . dags . values ( ) : dag . sync_to_db ( ) paused_dag_ids = [ dag . dag_id for dag in dagbag . dags . values ( ) if dag . is_paused ] for dag_id in dagbag . dags : if dag_id not in paused_dag_ids : dag = dagbag . get_dag ( dag_id ) pickle_id = None if pickle_dags : pickle_id = dag . pickle ( session ) . id simple_dags . append ( SimpleDag ( dag , pickle_id = pickle_id ) ) if len ( self . dag_ids ) > 0 : dags = [ dag for dag in dagbag . dags . values ( ) if dag . dag_id in self . dag_ids and dag . dag_id not in paused_dag_ids ] else : dags = [ dag for dag in dagbag . dags . values ( ) if not dag . parent_dag and dag . dag_id not in paused_dag_ids ] ti_keys_to_schedule = [ ] self . _process_dags ( dagbag , dags , ti_keys_to_schedule ) for ti_key in ti_keys_to_schedule : dag = dagbag . dags [ ti_key [ 0 ] ] task = dag . get_task ( ti_key [ 1 ] ) ti = models . TaskInstance ( task , ti_key [ 2 ] ) ti . refresh_from_db ( session = session , lock_for_update = True ) dep_context = DepContext ( deps = QUEUE_DEPS , ignore_task_deps = True ) if ti . are_dependencies_met ( dep_context = dep_context , session = session , verbose = True ) : ti . state = State . SCHEDULED self . log . info ( ""Creating / updating %s in ORM"" , ti ) session . merge ( ti ) session . commit ( ) try : self . update_import_errors ( session , dagbag ) except Exception : self . log . exception ( ""Error logging import errors!"" ) try : dagbag . kill_zombies ( zombies ) except Exception : self . log . exception ( ""Error killing zombies!"" ) return simple_dags",Process a Python file containing Airflow DAGs .
"def _update_counters ( self , ti_status ) : for key , ti in list ( ti_status . running . items ( ) ) : ti . refresh_from_db ( ) if ti . state == State . SUCCESS : ti_status . succeeded . add ( key ) self . log . debug ( ""Task instance %s succeeded. Don't rerun."" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . SKIPPED : ti_status . skipped . add ( key ) self . log . debug ( ""Task instance %s skipped. Don't rerun."" , ti ) ti_status . running . pop ( key ) continue elif ti . state == State . FAILED : self . log . error ( ""Task instance %s failed"" , ti ) ti_status . failed . add ( key ) ti_status . running . pop ( key ) continue elif ti . state == State . UP_FOR_RETRY : self . log . warning ( ""Task instance %s is up for retry"" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . UP_FOR_RESCHEDULE : self . log . warning ( ""Task instance %s is up for reschedule"" , ti ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti elif ti . state == State . NONE : self . log . warning ( ""FIXME: task instance %s state was set to none externally or "" ""reaching concurrency limits. Re-adding task to queue."" , ti ) ti . set_state ( State . SCHEDULED ) ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti",Updates the counters per state of the tasks that were running . Can re - add to tasks to run in case required .
"def _manage_executor_state ( self , running ) : executor = self . executor for key , state in list ( executor . get_event_buffer ( ) . items ( ) ) : if key not in running : self . log . warning ( ""%s state %s not in running=%s"" , key , state , running . values ( ) ) continue ti = running [ key ] ti . refresh_from_db ( ) self . log . debug ( ""Executor state: %s task %s"" , state , ti ) if state == State . FAILED or state == State . SUCCESS : if ti . state == State . RUNNING or ti . state == State . QUEUED : msg = ( ""Executor reports task instance {} finished ({}) "" ""although the task says its {}. Was the task "" ""killed externally?"" . format ( ti , state , ti . state ) ) self . log . error ( msg ) ti . handle_failure ( msg )",Checks if the executor agrees with the state of task instances that are running
"def _get_dag_run ( self , run_date , session = None ) : run_id = BackfillJob . ID_FORMAT_PREFIX . format ( run_date . isoformat ( ) ) respect_dag_max_active_limit = ( True if ( self . dag . schedule_interval and not self . dag . is_subdag ) else False ) current_active_dag_count = self . dag . get_num_active_runs ( external_trigger = False ) run = DagRun . find ( dag_id = self . dag . dag_id , execution_date = run_date , session = session ) if run is not None and len ( run ) > 0 : run = run [ 0 ] if run . state == State . RUNNING : respect_dag_max_active_limit = False else : run = None if ( respect_dag_max_active_limit and current_active_dag_count >= self . dag . max_active_runs ) : return None run = run or self . dag . create_dagrun ( run_id = run_id , execution_date = run_date , start_date = timezone . utcnow ( ) , state = State . RUNNING , external_trigger = False , session = session , conf = self . conf , ) run . dag = self . dag run . state = State . RUNNING run . run_id = run_id run . verify_integrity ( session = session ) return run",Returns a dag run for the given run date which will be matched to an existing dag run if available or create a new dag run otherwise . If the max_active_runs limit is reached this function will return None .
"def _task_instances_for_dag_run ( self , dag_run , session = None ) : tasks_to_run = { } if dag_run is None : return tasks_to_run self . reset_state_for_orphaned_tasks ( filter_by_dag_run = dag_run , session = session ) dag_run . refresh_from_db ( ) make_transient ( dag_run ) for ti in dag_run . get_task_instances ( ) : if ti . state == State . NONE : ti . set_state ( State . SCHEDULED , session = session ) if ti . state != State . REMOVED : tasks_to_run [ ti . key ] = ti return tasks_to_run",Returns a map of task instance key to task instance object for the tasks to run in the given dag run .
"def _process_backfill_task_instances ( self , ti_status , executor , pickle_id , start_date = None , session = None ) : executed_run_dates = [ ] while ( ( len ( ti_status . to_run ) > 0 or len ( ti_status . running ) > 0 ) and len ( ti_status . deadlocked ) == 0 ) : self . log . debug ( ""*** Clearing out not_ready list ***"" ) ti_status . not_ready . clear ( ) @ provide_session def _per_task_process ( task , key , ti , session = None ) : ti . refresh_from_db ( ) task = self . dag . get_task ( ti . task_id ) ti . task = task ignore_depends_on_past = ( self . ignore_first_depends_on_past and ti . execution_date == ( start_date or ti . start_date ) ) self . log . debug ( ""Task instance to run %s state %s"" , ti , ti . state ) if ti . state == State . SUCCESS : ti_status . succeeded . add ( key ) self . log . debug ( ""Task instance %s succeeded. Don't rerun."" , ti ) ti_status . to_run . pop ( key ) if key in ti_status . running : ti_status . running . pop ( key ) return elif ti . state == State . SKIPPED : ti_status . skipped . add ( key ) self . log . debug ( ""Task instance %s skipped. Don't rerun."" , ti ) ti_status . to_run . pop ( key ) if key in ti_status . running : ti_status . running . pop ( key ) return elif ti . state == State . NONE : self . log . warning ( ""FIXME: task instance {} state was set to None "" ""externally. This should not happen"" ) ti . set_state ( State . SCHEDULED , session = session ) if self . rerun_failed_tasks : if ti . state in ( State . FAILED , State . UPSTREAM_FAILED ) : self . log . error ( ""Task instance {ti} "" ""with state {state}"" . format ( ti = ti , state = ti . state ) ) if key in ti_status . running : ti_status . running . pop ( key ) ti . set_state ( State . SCHEDULED , session = session ) else : if ti . state in ( State . FAILED , State . UPSTREAM_FAILED ) : self . log . error ( ""Task instance {ti} "" ""with {state} state"" . format ( ti = ti , state = ti . state ) ) ti_status . failed . add ( key ) ti_status . to_run . pop ( key ) if key in ti_status . running : ti_status . running . pop ( key ) return backfill_context = DepContext ( deps = RUN_DEPS , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = self . ignore_task_deps , flag_upstream_failed = True ) if ti . are_dependencies_met ( dep_context = backfill_context , session = session , verbose = self . verbose ) : ti . refresh_from_db ( lock_for_update = True , session = session ) if ti . state in ( State . SCHEDULED , State . UP_FOR_RETRY , State . UP_FOR_RESCHEDULE ) : if executor . has_task ( ti ) : self . log . debug ( ""Task Instance %s already in executor "" ""waiting for queue to clear"" , ti ) else : self . log . debug ( 'Sending %s to executor' , ti ) ti . state = State . QUEUED ti . queued_dttm = timezone . utcnow ( ) if not ti . queued_dttm else ti . queued_dttm session . merge ( ti ) cfg_path = None if executor . __class__ in ( executors . LocalExecutor , executors . SequentialExecutor ) : cfg_path = tmp_configuration_copy ( ) executor . queue_task_instance ( ti , mark_success = self . mark_success , pickle_id = pickle_id , ignore_task_deps = self . ignore_task_deps , ignore_depends_on_past = ignore_depends_on_past , pool = self . pool , cfg_path = cfg_path ) ti_status . running [ key ] = ti ti_status . to_run . pop ( key ) session . commit ( ) return if ti . state == State . UPSTREAM_FAILED : self . log . error ( ""Task instance %s upstream failed"" , ti ) ti_status . failed . add ( key ) ti_status . to_run . pop ( key ) if key in ti_status . running : ti_status . running . pop ( key ) return if ti . state == State . UP_FOR_RETRY : self . log . debug ( ""Task instance %s retry period not "" ""expired yet"" , ti ) if key in ti_status . running : ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti return if ti . state == State . UP_FOR_RESCHEDULE : self . log . debug ( ""Task instance %s reschedule period not "" ""expired yet"" , ti ) if key in ti_status . running : ti_status . running . pop ( key ) ti_status . to_run [ key ] = ti return self . log . debug ( 'Adding %s to not_ready' , ti ) ti_status . not_ready . add ( key ) non_pool_slots = conf . getint ( 'core' , 'non_pooled_backfill_task_slot_count' ) try : for task in self . dag . topological_sort ( ) : for key , ti in list ( ti_status . to_run . items ( ) ) : if task . task_id != ti . task_id : continue if task . pool : pool = session . query ( models . Pool ) . filter ( models . Pool . pool == task . pool ) . first ( ) if not pool : raise PoolNotFound ( 'Unknown pool: {}' . format ( task . pool ) ) open_slots = pool . open_slots ( session = session ) if open_slots <= 0 : raise NoAvailablePoolSlot ( ""Not scheduling since there are "" ""%s open slots in pool %s"" . format ( open_slots , task . pool ) ) else : if non_pool_slots <= 0 : raise NoAvailablePoolSlot ( ""Not scheduling since there are no "" ""non_pooled_backfill_task_slot_count."" ) non_pool_slots -= 1 num_running_tasks = DAG . get_num_task_instances ( self . dag_id , states = ( State . QUEUED , State . RUNNING ) ) if num_running_tasks >= self . dag . concurrency : raise DagConcurrencyLimitReached ( ""Not scheduling since concurrency limit "" ""is reached."" ) _per_task_process ( task , key , ti ) except ( NoAvailablePoolSlot , DagConcurrencyLimitReached ) as e : self . log . debug ( e ) self . heartbeat ( ) executor . heartbeat ( ) if ( ti_status . not_ready and ti_status . not_ready == set ( ti_status . to_run ) and len ( ti_status . running ) == 0 ) : self . log . warning ( ""Deadlock discovered for ti_status.to_run=%s"" , ti_status . to_run . values ( ) ) ti_status . deadlocked . update ( ti_status . to_run . values ( ) ) ti_status . to_run . clear ( ) self . _manage_executor_state ( ti_status . running ) self . _update_counters ( ti_status = ti_status ) _dag_runs = ti_status . active_runs [ : ] for run in _dag_runs : run . update_state ( session = session ) if run . state in State . finished ( ) : ti_status . finished_runs += 1 ti_status . active_runs . remove ( run ) executed_run_dates . append ( run . execution_date ) self . _log_progress ( ti_status ) return executed_run_dates",Process a set of task instances from a set of dag runs . Special handling is done to account for different task instance states that could be present when running them in a backfill process .
"def _execute_for_run_dates ( self , run_dates , ti_status , executor , pickle_id , start_date , session = None ) : for next_run_date in run_dates : dag_run = self . _get_dag_run ( next_run_date , session = session ) tis_map = self . _task_instances_for_dag_run ( dag_run , session = session ) if dag_run is None : continue ti_status . active_runs . append ( dag_run ) ti_status . to_run . update ( tis_map or { } ) processed_dag_run_dates = self . _process_backfill_task_instances ( ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) ti_status . executed_dag_run_dates . update ( processed_dag_run_dates )",Computes the dag runs and their respective task instances for the given run dates and executes the task instances . Returns a list of execution dates of the dag runs that were executed .
"def _set_unfinished_dag_runs_to_failed ( self , dag_runs , session = None ) : for dag_run in dag_runs : dag_run . update_state ( ) if dag_run . state not in State . finished ( ) : dag_run . set_state ( State . FAILED ) session . merge ( dag_run )",Go through the dag_runs and update the state based on the task_instance state . Then set DAG runs that are not finished to failed .
"def _execute ( self , session = None ) : ti_status = BackfillJob . _DagRunTaskStatus ( ) start_date = self . bf_start_date run_dates = self . dag . get_run_dates ( start_date = start_date , end_date = self . bf_end_date ) if self . run_backwards : tasks_that_depend_on_past = [ t . task_id for t in self . dag . task_dict . values ( ) if t . depends_on_past ] if tasks_that_depend_on_past : raise AirflowException ( 'You cannot backfill backwards because one or more tasks depend_on_past: {}' . format ( "","" . join ( tasks_that_depend_on_past ) ) ) run_dates = run_dates [ : : - 1 ] if len ( run_dates ) == 0 : self . log . info ( ""No run dates were found for the given dates and dag interval."" ) return pickle_id = None if not self . donot_pickle and self . executor . __class__ not in ( executors . LocalExecutor , executors . SequentialExecutor ) : pickle = DagPickle ( self . dag ) session . add ( pickle ) session . commit ( ) pickle_id = pickle . id executor = self . executor executor . start ( ) ti_status . total_runs = len ( run_dates ) try : remaining_dates = ti_status . total_runs while remaining_dates > 0 : dates_to_process = [ run_date for run_date in run_dates if run_date not in ti_status . executed_dag_run_dates ] self . _execute_for_run_dates ( run_dates = dates_to_process , ti_status = ti_status , executor = executor , pickle_id = pickle_id , start_date = start_date , session = session ) remaining_dates = ( ti_status . total_runs - len ( ti_status . executed_dag_run_dates ) ) err = self . _collect_errors ( ti_status = ti_status , session = session ) if err : raise AirflowException ( err ) if remaining_dates > 0 : self . log . info ( ""max_active_runs limit for dag %s has been reached "" "" - waiting for other dag runs to finish"" , self . dag_id ) time . sleep ( self . delay_on_limit_secs ) except ( KeyboardInterrupt , SystemExit ) : self . log . warning ( ""Backfill terminated by user."" ) self . _set_unfinished_dag_runs_to_failed ( ti_status . active_runs ) finally : session . commit ( ) executor . end ( ) self . log . info ( ""Backfill done. Exiting."" )",Initializes all components required to run a dag for a specified date range and calls helper method to execute the tasks .
"def heartbeat_callback ( self , session = None ) : if self . terminating : self . task_runner . terminate ( ) return self . task_instance . refresh_from_db ( ) ti = self . task_instance fqdn = get_hostname ( ) same_hostname = fqdn == ti . hostname same_process = ti . pid == os . getpid ( ) if ti . state == State . RUNNING : if not same_hostname : self . log . warning ( ""The recorded hostname %s "" ""does not match this instance's hostname "" ""%s"" , ti . hostname , fqdn ) raise AirflowException ( ""Hostname of job runner does not match"" ) elif not same_process : current_pid = os . getpid ( ) self . log . warning ( ""Recorded pid %s does not match "" ""the current pid %s"" , ti . pid , current_pid ) raise AirflowException ( ""PID of job runner does not match"" ) elif ( self . task_runner . return_code ( ) is None and hasattr ( self . task_runner , 'process' ) ) : self . log . warning ( ""State of this instance has been externally set to %s. "" ""Taking the poison pill."" , ti . state ) self . task_runner . terminate ( ) self . terminating = True",Self destruct task if state has been moved away from running externally
"def _get_client ( self , project_id ) : if not self . _client : self . _client = Client ( project = project_id , credentials = self . _get_credentials ( ) ) return self . _client",Provides a client for interacting with the Cloud Spanner API .
"def get_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : return None return instance",Gets information about a particular instance .
"def _apply_to_instance ( self , project_id , instance_id , configuration_name , node_count , display_name , func ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id , configuration_name = configuration_name , node_count = node_count , display_name = display_name ) try : operation = func ( instance ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result )",Invokes a method on a given instance by applying a specified Callable .
"def create_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . create ( ) )",Creates a new Cloud Spanner instance .
"def update_instance ( self , instance_id , configuration_name , node_count , display_name , project_id = None ) : return self . _apply_to_instance ( project_id , instance_id , configuration_name , node_count , display_name , lambda x : x . update ( ) )",Updates an existing Cloud Spanner instance .
"def delete_instance ( self , instance_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id ) try : instance . delete ( ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e",Deletes an existing Cloud Spanner instance .
"def get_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( ""The instance {} does not exist in project {} !"" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : return None else : return database",Retrieves a database in Cloud Spanner . If the database does not exist in the specified instance it returns None .
"def create_database ( self , instance_id , database_id , ddl_statements , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( ""The instance {} does not exist in project {} !"" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id , ddl_statements = ddl_statements ) try : operation = database . create ( ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return",Creates a new database in Cloud Spanner .
"def update_database ( self , instance_id , database_id , ddl_statements , project_id = None , operation_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( ""The instance {} does not exist in project {} !"" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) try : operation = database . update_ddl ( ddl_statements = ddl_statements , operation_id = operation_id ) if operation : result = operation . result ( ) self . log . info ( result ) return except AlreadyExists as e : if e . code == 409 and operation_id in e . message : self . log . info ( ""Replayed update_ddl message - the operation id %s "" ""was already done before."" , operation_id ) return except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e",Updates DDL of a database in Cloud Spanner .
"def delete_database ( self , instance_id , database_id , project_id = None ) : instance = self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) if not instance . exists ( ) : raise AirflowException ( ""The instance {} does not exist in project {} !"" . format ( instance_id , project_id ) ) database = instance . database ( database_id = database_id ) if not database . exists ( ) : self . log . info ( ""The database {} is already deleted from instance {}. "" ""Exiting."" . format ( database_id , instance_id ) ) return try : operation = database . drop ( ) except GoogleAPICallError as e : self . log . error ( 'An error occurred: %s. Exiting.' , e . message ) raise e if operation : result = operation . result ( ) self . log . info ( result ) return",Drops a database in Cloud Spanner .
"def execute_dml ( self , instance_id , database_id , queries , project_id = None ) : self . _get_client ( project_id = project_id ) . instance ( instance_id = instance_id ) . database ( database_id = database_id ) . run_in_transaction ( lambda transaction : self . _execute_sql_in_transaction ( transaction , queries ) )",Executes an arbitrary DML query ( INSERT UPDATE DELETE ) .
"def poke ( self , context ) : self . log . info ( 'Poking for %s' , self . attachment_name ) with ImapHook ( imap_conn_id = self . conn_id ) as imap_hook : return imap_hook . has_mail_attachment ( name = self . attachment_name , mail_folder = self . mail_folder , check_regex = self . check_regex )",Pokes for a mail attachment on the mail server .
"def prepare_additional_parameters ( additional_properties , language_hints , web_detection_params ) : if language_hints is None and web_detection_params is None : return additional_properties if additional_properties is None : return { } merged_additional_parameters = deepcopy ( additional_properties ) if 'image_context' not in merged_additional_parameters : merged_additional_parameters [ 'image_context' ] = { } merged_additional_parameters [ 'image_context' ] [ 'language_hints' ] = merged_additional_parameters [ 'image_context' ] . get ( 'language_hints' , language_hints ) merged_additional_parameters [ 'image_context' ] [ 'web_detection_params' ] = merged_additional_parameters [ 'image_context' ] . get ( 'web_detection_params' , web_detection_params ) return merged_additional_parameters",Creates additional_properties parameter based on language_hints web_detection_params and additional_properties parameters specified by the user
def get_conn ( self ) : if self . session and not self . session . is_shutdown : return self . session self . session = self . cluster . connect ( self . keyspace ) return self . session,Returns a cassandra Session object
"def table_exists ( self , table ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) cluster_metadata = self . get_conn ( ) . cluster . metadata return ( keyspace in cluster_metadata . keyspaces and table in cluster_metadata . keyspaces [ keyspace ] . tables )",Checks if a table exists in Cassandra
"def record_exists ( self , table , keys ) : keyspace = self . keyspace if '.' in table : keyspace , table = table . split ( '.' , 1 ) ks = "" AND "" . join ( ""{}=%({})s"" . format ( key , key ) for key in keys . keys ( ) ) cql = ""SELECT * FROM {keyspace}.{table} WHERE {keys}"" . format ( keyspace = keyspace , table = table , keys = ks ) try : rs = self . get_conn ( ) . execute ( cql , keys ) return rs . one ( ) is not None except Exception : return False",Checks if a record exists in Cassandra
"def _build_spark_submit_command ( self , application ) : connection_cmd = self . _get_spark_binary_path ( ) connection_cmd += [ ""--master"" , self . _connection [ 'master' ] ] if self . _conf : for key in self . _conf : connection_cmd += [ ""--conf"" , ""{}={}"" . format ( key , str ( self . _conf [ key ] ) ) ] if self . _env_vars and ( self . _is_kubernetes or self . _is_yarn ) : if self . _is_yarn : tmpl = ""spark.yarn.appMasterEnv.{}={}"" else : tmpl = ""spark.kubernetes.driverEnv.{}={}"" for key in self . _env_vars : connection_cmd += [ ""--conf"" , tmpl . format ( key , str ( self . _env_vars [ key ] ) ) ] elif self . _env_vars and self . _connection [ 'deploy_mode' ] != ""cluster"" : self . _env = self . _env_vars elif self . _env_vars and self . _connection [ 'deploy_mode' ] == ""cluster"" : raise AirflowException ( ""SparkSubmitHook env_vars is not supported in standalone-cluster mode."" ) if self . _is_kubernetes : connection_cmd += [ ""--conf"" , ""spark.kubernetes.namespace={}"" . format ( self . _connection [ 'namespace' ] ) ] if self . _files : connection_cmd += [ ""--files"" , self . _files ] if self . _py_files : connection_cmd += [ ""--py-files"" , self . _py_files ] if self . _archives : connection_cmd += [ ""--archives"" , self . _archives ] if self . _driver_class_path : connection_cmd += [ ""--driver-class-path"" , self . _driver_class_path ] if self . _jars : connection_cmd += [ ""--jars"" , self . _jars ] if self . _packages : connection_cmd += [ ""--packages"" , self . _packages ] if self . _exclude_packages : connection_cmd += [ ""--exclude-packages"" , self . _exclude_packages ] if self . _repositories : connection_cmd += [ ""--repositories"" , self . _repositories ] if self . _num_executors : connection_cmd += [ ""--num-executors"" , str ( self . _num_executors ) ] if self . _total_executor_cores : connection_cmd += [ ""--total-executor-cores"" , str ( self . _total_executor_cores ) ] if self . _executor_cores : connection_cmd += [ ""--executor-cores"" , str ( self . _executor_cores ) ] if self . _executor_memory : connection_cmd += [ ""--executor-memory"" , self . _executor_memory ] if self . _driver_memory : connection_cmd += [ ""--driver-memory"" , self . _driver_memory ] if self . _keytab : connection_cmd += [ ""--keytab"" , self . _keytab ] if self . _principal : connection_cmd += [ ""--principal"" , self . _principal ] if self . _name : connection_cmd += [ ""--name"" , self . _name ] if self . _java_class : connection_cmd += [ ""--class"" , self . _java_class ] if self . _verbose : connection_cmd += [ ""--verbose"" ] if self . _connection [ 'queue' ] : connection_cmd += [ ""--queue"" , self . _connection [ 'queue' ] ] if self . _connection [ 'deploy_mode' ] : connection_cmd += [ ""--deploy-mode"" , self . _connection [ 'deploy_mode' ] ] connection_cmd += [ application ] if self . _application_args : connection_cmd += self . _application_args self . log . info ( ""Spark-Submit cmd: %s"" , connection_cmd ) return connection_cmd",Construct the spark - submit command to execute . : param application : command to append to the spark - submit command : type application : str : return : full command to be executed
"def _build_track_driver_status_command ( self ) : connection_cmd = self . _get_spark_binary_path ( ) connection_cmd += [ ""--master"" , self . _connection [ 'master' ] ] if self . _driver_id : connection_cmd += [ ""--status"" , self . _driver_id ] else : raise AirflowException ( ""Invalid status: attempted to poll driver "" + ""status but no driver id is known. Giving up."" ) self . log . debug ( ""Poll driver status cmd: %s"" , connection_cmd ) return connection_cmd",Construct the command to poll the driver status .
"def submit ( self , application = """" , * * kwargs ) : spark_submit_cmd = self . _build_spark_submit_command ( application ) if hasattr ( self , '_env' ) : env = os . environ . copy ( ) env . update ( self . _env ) kwargs [ ""env"" ] = env self . _submit_sp = subprocess . Popen ( spark_submit_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = True , * * kwargs ) self . _process_spark_submit_log ( iter ( self . _submit_sp . stdout . readline , '' ) ) returncode = self . _submit_sp . wait ( ) if returncode or ( self . _is_kubernetes and self . _spark_exit_code != 0 ) : raise AirflowException ( ""Cannot execute: {}. Error code is: {}."" . format ( spark_submit_cmd , returncode ) ) self . log . debug ( ""Should track driver: {}"" . format ( self . _should_track_driver_status ) ) if self . _should_track_driver_status : if self . _driver_id is None : raise AirflowException ( ""No driver id is known: something went wrong when executing "" + ""the spark submit command"" ) self . _driver_status = ""SUBMITTED"" self . _start_driver_status_tracking ( ) if self . _driver_status != ""FINISHED"" : raise AirflowException ( ""ERROR : Driver {} badly exited with status {}"" . format ( self . _driver_id , self . _driver_status ) )",Remote Popen to execute the spark - submit job
"def _process_spark_submit_log ( self , itr ) : for line in itr : line = line . strip ( ) if self . _is_yarn and self . _connection [ 'deploy_mode' ] == 'cluster' : match = re . search ( '(application[0-9_]+)' , line ) if match : self . _yarn_application_id = match . groups ( ) [ 0 ] self . log . info ( ""Identified spark driver id: %s"" , self . _yarn_application_id ) elif self . _is_kubernetes : match = re . search ( r'\s*pod name: ((.+?)-([a-z0-9]+)-driver)' , line ) if match : self . _kubernetes_driver_pod = match . groups ( ) [ 0 ] self . log . info ( ""Identified spark driver pod: %s"" , self . _kubernetes_driver_pod ) match_exit_code = re . search ( r'\s*Exit code: (\d+)' , line ) if match_exit_code : self . _spark_exit_code = int ( match_exit_code . groups ( ) [ 0 ] ) elif self . _should_track_driver_status and not self . _driver_id : match_driver_id = re . search ( r'(driver-[0-9\-]+)' , line ) if match_driver_id : self . _driver_id = match_driver_id . groups ( ) [ 0 ] self . log . info ( ""identified spark driver id: {}"" . format ( self . _driver_id ) ) else : self . log . info ( line ) self . log . debug ( ""spark submit log: {}"" . format ( line ) )",Processes the log files and extracts useful information out of it .
"def _process_spark_status_log ( self , itr ) : for line in itr : line = line . strip ( ) if ""driverState"" in line : self . _driver_status = line . split ( ' : ' ) [ 1 ] . replace ( ',' , '' ) . replace ( '\""' , '' ) . strip ( ) self . log . debug ( ""spark driver status log: {}"" . format ( line ) )",parses the logs of the spark driver status query process
"def _start_driver_status_tracking ( self ) : missed_job_status_reports = 0 max_missed_job_status_reports = 10 while self . _driver_status not in [ ""FINISHED"" , ""UNKNOWN"" , ""KILLED"" , ""FAILED"" , ""ERROR"" ] : time . sleep ( 1 ) self . log . debug ( ""polling status of spark driver with id {}"" . format ( self . _driver_id ) ) poll_drive_status_cmd = self . _build_track_driver_status_command ( ) status_process = subprocess . Popen ( poll_drive_status_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , bufsize = - 1 , universal_newlines = True ) self . _process_spark_status_log ( iter ( status_process . stdout . readline , '' ) ) returncode = status_process . wait ( ) if returncode : if missed_job_status_reports < max_missed_job_status_reports : missed_job_status_reports = missed_job_status_reports + 1 else : raise AirflowException ( ""Failed to poll for the driver status {} times: returncode = {}"" . format ( max_missed_job_status_reports , returncode ) )",Polls the driver based on self . _driver_id to get the status . Finish successfully when the status is FINISHED . Finish failed when the status is ERROR / UNKNOWN / KILLED / FAILED .
"def _build_spark_driver_kill_command ( self ) : if self . _connection [ 'spark_home' ] : connection_cmd = [ os . path . join ( self . _connection [ 'spark_home' ] , 'bin' , self . _connection [ 'spark_binary' ] ) ] else : connection_cmd = [ self . _connection [ 'spark_binary' ] ] connection_cmd += [ ""--master"" , self . _connection [ 'master' ] ] connection_cmd += [ ""--kill"" , self . _driver_id ] self . log . debug ( ""Spark-Kill cmd: %s"" , connection_cmd ) return connection_cmd",Construct the spark - submit command to kill a driver . : return : full command to kill a driver
"def get_task_runner ( local_task_job ) : if _TASK_RUNNER == ""StandardTaskRunner"" : return StandardTaskRunner ( local_task_job ) elif _TASK_RUNNER == ""CgroupTaskRunner"" : from airflow . contrib . task_runner . cgroup_task_runner import CgroupTaskRunner return CgroupTaskRunner ( local_task_job ) else : raise AirflowException ( ""Unknown task runner type {}"" . format ( _TASK_RUNNER ) )",Get the task runner that can be used to run the given job .
"def _wait_for_task_ended ( self ) : try : waiter = self . client . get_waiter ( 'job_execution_complete' ) waiter . config . max_attempts = sys . maxsize waiter . wait ( jobs = [ self . jobId ] ) except ValueError : retry = True retries = 0 while retries < self . max_retries and retry : self . log . info ( 'AWS Batch retry in the next %s seconds' , retries ) response = self . client . describe_jobs ( jobs = [ self . jobId ] ) if response [ 'jobs' ] [ - 1 ] [ 'status' ] in [ 'SUCCEEDED' , 'FAILED' ] : retry = False sleep ( 1 + pow ( retries * 0.1 , 2 ) ) retries += 1",Try to use a waiter from the below pull request
def _query_mysql ( self ) : mysql = MySqlHook ( mysql_conn_id = self . mysql_conn_id ) conn = mysql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor,Queries mysql and returns a cursor to the results .
"def _write_local_data_files ( self , cursor ) : schema = list ( map ( lambda schema_tuple : schema_tuple [ 0 ] , cursor . description ) ) col_type_dict = self . _get_col_type_dict ( ) file_no = 0 tmp_file_handle = NamedTemporaryFile ( delete = True ) if self . export_format == 'csv' : file_mime_type = 'text/csv' else : file_mime_type = 'application/json' files_to_upload = [ { 'file_name' : self . filename . format ( file_no ) , 'file_handle' : tmp_file_handle , 'file_mime_type' : file_mime_type } ] if self . export_format == 'csv' : csv_writer = self . _configure_csv_file ( tmp_file_handle , schema ) for row in cursor : row = self . _convert_types ( schema , col_type_dict , row ) if self . export_format == 'csv' : csv_writer . writerow ( row ) else : row_dict = dict ( zip ( schema , row ) ) s = json . dumps ( row_dict , sort_keys = True ) . encode ( 'utf-8' ) tmp_file_handle . write ( s ) tmp_file_handle . write ( b'\n' ) if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : file_no += 1 tmp_file_handle = NamedTemporaryFile ( delete = True ) files_to_upload . append ( { 'file_name' : self . filename . format ( file_no ) , 'file_handle' : tmp_file_handle , 'file_mime_type' : file_mime_type } ) if self . export_format == 'csv' : csv_writer = self . _configure_csv_file ( tmp_file_handle , schema ) return files_to_upload",Takes a cursor and writes results to a local file .
"def _configure_csv_file ( self , file_handle , schema ) : csv_writer = csv . writer ( file_handle , encoding = 'utf-8' , delimiter = self . field_delimiter ) csv_writer . writerow ( schema ) return csv_writer",Configure a csv writer with the file_handle and write schema as headers for the new file .
"def _write_local_schema_file ( self , cursor ) : schema_str = None schema_file_mime_type = 'application/json' tmp_schema_file_handle = NamedTemporaryFile ( delete = True ) if self . schema is not None and isinstance ( self . schema , string_types ) : schema_str = self . schema . encode ( 'utf-8' ) elif self . schema is not None and isinstance ( self . schema , list ) : schema_str = json . dumps ( self . schema ) . encode ( 'utf-8' ) else : schema = [ ] for field in cursor . description : field_name = field [ 0 ] field_type = self . type_map ( field [ 1 ] ) if field [ 6 ] or field_type == 'TIMESTAMP' : field_mode = 'NULLABLE' else : field_mode = 'REQUIRED' schema . append ( { 'name' : field_name , 'type' : field_type , 'mode' : field_mode , } ) schema_str = json . dumps ( schema , sort_keys = True ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( schema_str ) self . log . info ( 'Using schema for %s: %s' , self . schema_filename , schema_str ) schema_file_to_upload = { 'file_name' : self . schema_filename , 'file_handle' : tmp_schema_file_handle , 'file_mime_type' : schema_file_mime_type } return schema_file_to_upload",Takes a cursor and writes the BigQuery schema in . json format for the results to a local file system .
"def _upload_to_gcs ( self , files_to_upload ) : hook = GoogleCloudStorageHook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) for tmp_file in files_to_upload : hook . upload ( self . bucket , tmp_file . get ( 'file_name' ) , tmp_file . get ( 'file_handle' ) . name , mime_type = tmp_file . get ( 'file_mime_type' ) )",Upload all of the file splits ( and optionally the schema . json file ) to Google cloud storage .
"def _convert_types ( schema , col_type_dict , row ) : converted_row = [ ] for col_name , col_val in zip ( schema , row ) : if type ( col_val ) in ( datetime , date ) : col_val = time . mktime ( col_val . timetuple ( ) ) elif isinstance ( col_val , Decimal ) : col_val = float ( col_val ) elif col_type_dict . get ( col_name ) == ""BYTES"" : col_val = base64 . standard_b64encode ( col_val ) . decode ( 'ascii' ) else : col_val = col_val converted_row . append ( col_val ) return converted_row",Takes a value from MySQLdb and converts it to a value that s safe for JSON / Google cloud storage / BigQuery . Dates are converted to UTC seconds . Decimals are converted to floats . Binary type fields are encoded with base64 as imported BYTES data must be base64 - encoded according to Bigquery SQL date type documentation : https : // cloud . google . com / bigquery / data - types
"def _get_col_type_dict ( self ) : schema = [ ] if isinstance ( self . schema , string_types ) : schema = json . loads ( self . schema ) elif isinstance ( self . schema , list ) : schema = self . schema elif self . schema is not None : self . log . warn ( 'Using default schema due to unexpected type.' 'Should be a string or list.' ) col_type_dict = { } try : col_type_dict = { col [ 'name' ] : col [ 'type' ] for col in schema } except KeyError : self . log . warn ( 'Using default schema due to missing name or type. Please ' 'refer to: https://cloud.google.com/bigquery/docs/schemas' '#specifying_a_json_schema_file' ) return col_type_dict",Return a dict of column name and column type based on self . schema if not None .
"def type_map ( cls , mysql_type ) : d = { FIELD_TYPE . INT24 : 'INTEGER' , FIELD_TYPE . TINY : 'INTEGER' , FIELD_TYPE . BIT : 'INTEGER' , FIELD_TYPE . DATETIME : 'TIMESTAMP' , FIELD_TYPE . DATE : 'TIMESTAMP' , FIELD_TYPE . DECIMAL : 'FLOAT' , FIELD_TYPE . NEWDECIMAL : 'FLOAT' , FIELD_TYPE . DOUBLE : 'FLOAT' , FIELD_TYPE . FLOAT : 'FLOAT' , FIELD_TYPE . LONG : 'INTEGER' , FIELD_TYPE . LONGLONG : 'INTEGER' , FIELD_TYPE . SHORT : 'INTEGER' , FIELD_TYPE . TIMESTAMP : 'TIMESTAMP' , FIELD_TYPE . YEAR : 'INTEGER' , } return d [ mysql_type ] if mysql_type in d else 'STRING'",Helper function that maps from MySQL fields to BigQuery fields . Used when a schema_filename is set .
"def authenticate ( session , username , password ) : if not username or not password : raise AuthenticationError ( ) user = session . query ( PasswordUser ) . filter ( PasswordUser . username == username ) . first ( ) if not user : raise AuthenticationError ( ) if not user . authenticate ( password ) : raise AuthenticationError ( ) log . info ( ""User %s successfully authenticated"" , username ) return user",Authenticate a PasswordUser with the specified username / password .
"def execute ( self , context ) : self . hook = SqoopHook ( conn_id = self . conn_id , verbose = self . verbose , num_mappers = self . num_mappers , hcatalog_database = self . hcatalog_database , hcatalog_table = self . hcatalog_table , properties = self . properties ) if self . cmd_type == 'export' : self . hook . export_table ( table = self . table , export_dir = self . export_dir , input_null_string = self . input_null_string , input_null_non_string = self . input_null_non_string , staging_table = self . staging_table , clear_staging_table = self . clear_staging_table , enclosed_by = self . enclosed_by , escaped_by = self . escaped_by , input_fields_terminated_by = self . input_fields_terminated_by , input_lines_terminated_by = self . input_lines_terminated_by , input_optionally_enclosed_by = self . input_optionally_enclosed_by , batch = self . batch , relaxed_isolation = self . relaxed_isolation , extra_export_options = self . extra_export_options ) elif self . cmd_type == 'import' : if self . create_hcatalog_table : self . extra_import_options [ 'create-hcatalog-table' ] = '' if self . table and self . query : raise AirflowException ( 'Cannot specify query and table together. Need to specify either or.' ) if self . table : self . hook . import_table ( table = self . table , target_dir = self . target_dir , append = self . append , file_type = self . file_type , columns = self . columns , split_by = self . split_by , where = self . where , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) elif self . query : self . hook . import_query ( query = self . query , target_dir = self . target_dir , append = self . append , file_type = self . file_type , split_by = self . split_by , direct = self . direct , driver = self . driver , extra_import_options = self . extra_import_options ) else : raise AirflowException ( ""Provide query or table parameter to import using Sqoop"" ) else : raise AirflowException ( ""cmd_type should be 'import' or 'export'"" )",Execute sqoop job
"def apply_lineage ( func ) : backend = _get_backend ( ) @ wraps ( func ) def wrapper ( self , context , * args , * * kwargs ) : self . log . debug ( ""Backend: %s, Lineage called with inlets: %s, outlets: %s"" , backend , self . inlets , self . outlets ) ret_val = func ( self , context , * args , * * kwargs ) outlets = [ x . as_dict ( ) for x in self . outlets ] inlets = [ x . as_dict ( ) for x in self . inlets ] if len ( self . outlets ) > 0 : self . xcom_push ( context , key = PIPELINE_OUTLETS , value = outlets , execution_date = context [ 'ti' ] . execution_date ) if len ( self . inlets ) > 0 : self . xcom_push ( context , key = PIPELINE_INLETS , value = inlets , execution_date = context [ 'ti' ] . execution_date ) if backend : backend . send_lineage ( operator = self , inlets = self . inlets , outlets = self . outlets , context = context ) return ret_val return wrapper",Saves the lineage to XCom and if configured to do so sends it to the backend .
"def prepare_lineage ( func ) : @ wraps ( func ) def wrapper ( self , context , * args , * * kwargs ) : self . log . debug ( ""Preparing lineage inlets and outlets"" ) task_ids = set ( self . _inlets [ 'task_ids' ] ) . intersection ( self . get_flat_relative_ids ( upstream = True ) ) if task_ids : inlets = self . xcom_pull ( context , task_ids = task_ids , dag_id = self . dag_id , key = PIPELINE_OUTLETS ) inlets = [ item for sublist in inlets if sublist for item in sublist ] inlets = [ DataSet . map_type ( i [ 'typeName' ] ) ( data = i [ 'attributes' ] ) for i in inlets ] self . inlets . extend ( inlets ) if self . _inlets [ 'auto' ] : task_ids = set ( self . _inlets [ 'task_ids' ] ) . symmetric_difference ( self . upstream_task_ids ) inlets = self . xcom_pull ( context , task_ids = task_ids , dag_id = self . dag_id , key = PIPELINE_OUTLETS ) inlets = [ item for sublist in inlets if sublist for item in sublist ] inlets = [ DataSet . map_type ( i [ 'typeName' ] ) ( data = i [ 'attributes' ] ) for i in inlets ] self . inlets . extend ( inlets ) if len ( self . _inlets [ 'datasets' ] ) > 0 : self . inlets . extend ( self . _inlets [ 'datasets' ] ) if len ( self . _outlets [ 'datasets' ] ) > 0 : self . outlets . extend ( self . _outlets [ 'datasets' ] ) self . log . debug ( ""inlets: %s, outlets: %s"" , self . inlets , self . outlets ) for dataset in chain ( self . inlets , self . outlets ) : dataset . set_context ( context ) return func ( self , context , * args , * * kwargs ) return wrapper",Prepares the lineage inlets and outlets . Inlets can be :
"def extra_dejson ( self ) : obj = { } if self . extra : try : obj = json . loads ( self . extra ) except Exception as e : self . log . exception ( e ) self . log . error ( ""Failed parsing the json for conn_id %s"" , self . conn_id ) return obj",Returns the extra property by deserializing json .
"def date_range ( start_date , end_date = None , num = None , delta = None ) : if not delta : return [ ] if end_date and start_date > end_date : raise Exception ( ""Wait. start_date needs to be before end_date"" ) if end_date and num : raise Exception ( ""Wait. Either specify end_date OR num"" ) if not end_date and not num : end_date = timezone . utcnow ( ) delta_iscron = False tz = start_date . tzinfo if isinstance ( delta , six . string_types ) : delta_iscron = True start_date = timezone . make_naive ( start_date , tz ) cron = croniter ( delta , start_date ) elif isinstance ( delta , timedelta ) : delta = abs ( delta ) dates = [ ] if end_date : if timezone . is_naive ( start_date ) : end_date = timezone . make_naive ( end_date , tz ) while start_date <= end_date : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date ) if delta_iscron : start_date = cron . get_next ( datetime ) else : start_date += delta else : for _ in range ( abs ( num ) ) : if timezone . is_naive ( start_date ) : dates . append ( timezone . make_aware ( start_date , tz ) ) else : dates . append ( start_date ) if delta_iscron : if num > 0 : start_date = cron . get_next ( datetime ) else : start_date = cron . get_prev ( datetime ) else : if num > 0 : start_date += delta else : start_date -= delta return sorted ( dates )",Get a set of dates as a list based on a start end and delta delta can be something that can be added to datetime . datetime or a cron expression as a str
"def round_time ( dt , delta , start_date = timezone . make_aware ( datetime . min ) ) : if isinstance ( delta , six . string_types ) : tz = start_date . tzinfo start_date = timezone . make_naive ( start_date , tz ) cron = croniter ( delta , start_date ) prev = cron . get_prev ( datetime ) if prev == start_date : return timezone . make_aware ( start_date , tz ) else : return timezone . make_aware ( prev , tz ) dt -= timedelta ( microseconds = dt . microsecond ) upper = 1 while start_date + upper * delta < dt : upper *= 2 lower = upper // 2 while True : if start_date + ( lower + 1 ) * delta >= dt : if ( start_date + ( lower + 1 ) * delta ) - dt <= dt - ( start_date + lower * delta ) : return start_date + ( lower + 1 ) * delta else : return start_date + lower * delta candidate = lower + ( upper - lower ) // 2 if start_date + candidate * delta >= dt : upper = candidate else : lower = candidate",Returns the datetime of the form start_date + i * delta which is closest to dt for any non - negative integer i . Note that delta may be a datetime . timedelta or a dateutil . relativedelta >>> round_time ( datetime ( 2015 1 1 6 ) timedelta ( days = 1 )) datetime . datetime ( 2015 1 1 0 0 ) >>> round_time ( datetime ( 2015 1 2 ) relativedelta ( months = 1 )) datetime . datetime ( 2015 1 1 0 0 ) >>> round_time ( datetime ( 2015 9 16 0 0 ) timedelta ( 1 ) datetime ( 2015 9 14 0 0 )) datetime . datetime ( 2015 9 16 0 0 ) >>> round_time ( datetime ( 2015 9 15 0 0 ) timedelta ( 1 ) datetime ( 2015 9 14 0 0 )) datetime . datetime ( 2015 9 15 0 0 ) >>> round_time ( datetime ( 2015 9 14 0 0 ) timedelta ( 1 ) datetime ( 2015 9 14 0 0 )) datetime . datetime ( 2015 9 14 0 0 ) >>> round_time ( datetime ( 2015 9 13 0 0 ) timedelta ( 1 ) datetime ( 2015 9 14 0 0 )) datetime . datetime ( 2015 9 14 0 0 )
def infer_time_unit ( time_seconds_arr ) : if len ( time_seconds_arr ) == 0 : return 'hours' max_time_seconds = max ( time_seconds_arr ) if max_time_seconds <= 60 * 2 : return 'seconds' elif max_time_seconds <= 60 * 60 * 2 : return 'minutes' elif max_time_seconds <= 24 * 60 * 60 * 2 : return 'hours' else : return 'days',Determine the most appropriate time unit for an array of time durations specified in seconds . e . g . 5400 seconds = > minutes 36000 seconds = > hours
"def scale_time_units ( time_seconds_arr , unit ) : if unit == 'minutes' : return list ( map ( lambda x : x * 1.0 / 60 , time_seconds_arr ) ) elif unit == 'hours' : return list ( map ( lambda x : x * 1.0 / ( 60 * 60 ) , time_seconds_arr ) ) elif unit == 'days' : return list ( map ( lambda x : x * 1.0 / ( 24 * 60 * 60 ) , time_seconds_arr ) ) return time_seconds_arr",Convert an array of time durations in seconds to the specified time unit .
"def days_ago ( n , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) : today = timezone . utcnow ( ) . replace ( hour = hour , minute = minute , second = second , microsecond = microsecond ) return today - timedelta ( days = n )",Get a datetime object representing n days ago . By default the time is set to midnight .
"def get_dag_runs ( dag_id , state = None ) : dagbag = DagBag ( ) if dag_id not in dagbag . dags : error_message = ""Dag id {} not found"" . format ( dag_id ) raise AirflowException ( error_message ) dag_runs = list ( ) state = state . lower ( ) if state else None for run in DagRun . find ( dag_id = dag_id , state = state ) : dag_runs . append ( { 'id' : run . id , 'run_id' : run . run_id , 'state' : run . state , 'dag_id' : run . dag_id , 'execution_date' : run . execution_date . isoformat ( ) , 'start_date' : ( ( run . start_date or '' ) and run . start_date . isoformat ( ) ) , 'dag_run_url' : url_for ( 'Airflow.graph' , dag_id = run . dag_id , execution_date = run . execution_date ) } ) return dag_runs",Returns a list of Dag Runs for a specific DAG ID . : param dag_id : String identifier of a DAG : param state : queued|running|success ... : return : List of DAG runs of a DAG with requested state or all runs if the state is not specified
"def init_role ( self , role_name , role_vms , role_perms ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] role = self . find_role ( role_name ) if not role : role = self . add_role ( role_name ) if len ( role . permissions ) == 0 : self . log . info ( 'Initializing permissions for role:%s in the database.' , role_name ) role_pvms = set ( ) for pvm in pvms : if pvm . view_menu . name in role_vms and pvm . permission . name in role_perms : role_pvms . add ( pvm ) role . permissions = list ( role_pvms ) self . get_session . merge ( role ) self . get_session . commit ( ) else : self . log . debug ( 'Existing permissions for the role:%s ' 'within the database will persist.' , role_name )",Initialize the role with the permissions and related view - menus .
"def delete_role ( self , role_name ) : session = self . get_session role = session . query ( sqla_models . Role ) . filter ( sqla_models . Role . name == role_name ) . first ( ) if role : self . log . info ( ""Deleting role '%s'"" , role_name ) session . delete ( role ) session . commit ( ) else : raise AirflowException ( ""Role named '{}' does not exist"" . format ( role_name ) )",Delete the given Role
"def get_user_roles ( self , user = None ) : if user is None : user = g . user if user . is_anonymous : public_role = appbuilder . config . get ( 'AUTH_ROLE_PUBLIC' ) return [ appbuilder . security_manager . find_role ( public_role ) ] if public_role else [ ] return user . roles",Get all the roles associated with the user .
"def get_all_permissions_views ( self ) : perms_views = set ( ) for role in self . get_user_roles ( ) : perms_views . update ( { ( perm_view . permission . name , perm_view . view_menu . name ) for perm_view in role . permissions } ) return perms_views",Returns a set of tuples with the perm name and view menu name
"def get_accessible_dag_ids ( self , username = None ) : if not username : username = g . user if username . is_anonymous or 'Public' in username . roles : return set ( ) roles = { role . name for role in username . roles } if { 'Admin' , 'Viewer' , 'User' , 'Op' } & roles : return self . DAG_VMS user_perms_views = self . get_all_permissions_views ( ) return set ( [ view for perm , view in user_perms_views if perm in self . DAG_PERMS ] )",Return a set of dags that user has access to ( either read or write ) .
"def has_access ( self , permission , view_name , user = None ) : if not user : user = g . user if user . is_anonymous : return self . is_item_public ( permission , view_name ) return self . _has_view_access ( user , permission , view_name )",Verify whether a given user could perform certain permission ( e . g can_read can_write ) on the given dag_id .
"def _has_role ( self , role_name_or_list ) : if not isinstance ( role_name_or_list , list ) : role_name_or_list = [ role_name_or_list ] return any ( [ r . name in role_name_or_list for r in self . get_user_roles ( ) ] )",Whether the user has this role name
"def _has_perm ( self , permission_name , view_menu_name ) : if hasattr ( self , 'perms' ) : if ( permission_name , view_menu_name ) in self . perms : return True self . _get_and_cache_perms ( ) return ( permission_name , view_menu_name ) in self . perms",Whether the user has this perm
"def clean_perms ( self ) : self . log . debug ( 'Cleaning faulty perms' ) sesh = self . get_session pvms = ( sesh . query ( sqla_models . PermissionView ) . filter ( or_ ( sqla_models . PermissionView . permission == None , sqla_models . PermissionView . view_menu == None , ) ) ) deleted_count = pvms . delete ( ) sesh . commit ( ) if deleted_count : self . log . info ( 'Deleted %s faulty permissions' , deleted_count )",FAB leaves faulty permissions that need to be cleaned up
"def _merge_perm ( self , permission_name , view_menu_name ) : permission = self . find_permission ( permission_name ) view_menu = self . find_view_menu ( view_menu_name ) pv = None if permission and view_menu : pv = self . get_session . query ( self . permissionview_model ) . filter_by ( permission = permission , view_menu = view_menu ) . first ( ) if not pv and permission_name and view_menu_name : self . add_permission_view_menu ( permission_name , view_menu_name )",Add the new permission view_menu to ab_permission_view_role if not exists . It will add the related entry to ab_permission and ab_view_menu two meta tables as well .
"def create_custom_dag_permission_view ( self , session = None ) : self . log . debug ( 'Fetching a set of all permission, view_menu from FAB meta-table' ) def merge_pv ( perm , view_menu ) : """"""Create permission view menu only if it doesn't exist"""""" if view_menu and perm and ( view_menu , perm ) not in all_pvs : self . _merge_perm ( perm , view_menu ) all_pvs = set ( ) for pv in self . get_session . query ( self . permissionview_model ) . all ( ) : if pv . permission and pv . view_menu : all_pvs . add ( ( pv . permission . name , pv . view_menu . name ) ) all_dags_models = session . query ( models . DagModel ) . filter ( or_ ( models . DagModel . is_active , models . DagModel . is_paused ) ) . filter ( ~ models . DagModel . is_subdag ) . all ( ) for dag in all_dags_models : for perm in self . DAG_PERMS : merge_pv ( perm , dag . dag_id ) all_roles = self . get_all_roles ( ) user_role = self . find_role ( 'User' ) dag_role = [ role for role in all_roles if role . name not in EXISTING_ROLES ] update_perm_views = [ ] dag_vm = self . find_view_menu ( 'all_dags' ) ab_perm_view_role = sqla_models . assoc_permissionview_role perm_view = self . permissionview_model view_menu = self . viewmenu_model all_perm_view_by_user = session . query ( ab_perm_view_role ) . join ( perm_view , perm_view . id == ab_perm_view_role . columns . permission_view_id ) . filter ( ab_perm_view_role . columns . role_id == user_role . id ) . join ( view_menu ) . filter ( perm_view . view_menu_id != dag_vm . id ) all_perm_views = set ( [ role . permission_view_id for role in all_perm_view_by_user ] ) for role in dag_role : existing_perm_view_by_user = self . get_session . query ( ab_perm_view_role ) . filter ( ab_perm_view_role . columns . role_id == role . id ) existing_perms_views = set ( [ pv . permission_view_id for pv in existing_perm_view_by_user ] ) missing_perm_views = all_perm_views - existing_perms_views for perm_view_id in missing_perm_views : update_perm_views . append ( { 'permission_view_id' : perm_view_id , 'role_id' : role . id } ) if update_perm_views : self . get_session . execute ( ab_perm_view_role . insert ( ) , update_perm_views ) self . get_session . commit ( )",Workflow : 1 . Fetch all the existing ( permissions view - menu ) from Airflow DB . 2 . Fetch all the existing dag models that are either active or paused . Exclude the subdags . 3 . Create both read and write permission view - menus relation for every dags from step 2 4 . Find out all the dag specific roles ( excluded pubic admin viewer op user ) 5 . Get all the permission - vm owned by the user role . 6 . Grant all the user role s permission - vm except the all - dag view - menus to the dag roles . 7 . Commit the updated permission - vm - role into db
def update_admin_perm_view ( self ) : pvms = self . get_session . query ( sqla_models . PermissionView ) . all ( ) pvms = [ p for p in pvms if p . permission and p . view_menu ] admin = self . find_role ( 'Admin' ) admin . permissions = list ( set ( admin . permissions ) | set ( pvms ) ) self . get_session . commit ( ),Admin should have all the permission - views . Add the missing ones to the table for admin .
"def sync_roles ( self ) : self . log . debug ( 'Start syncing user roles.' ) self . create_perm_vm_for_all_dag ( ) for config in self . ROLE_CONFIGS : role = config [ 'role' ] vms = config [ 'vms' ] perms = config [ 'perms' ] self . init_role ( role , vms , perms ) self . create_custom_dag_permission_view ( ) self . update_admin_perm_view ( ) self . clean_perms ( )",1 . Init the default role ( Admin Viewer User Op public ) with related permissions . 2 . Init the custom role ( dag - user ) with related permissions .
"def sync_perm_for_dag ( self , dag_id , access_control = None ) : for dag_perm in self . DAG_PERMS : perm_on_dag = self . find_permission_view_menu ( dag_perm , dag_id ) if perm_on_dag is None : self . add_permission_view_menu ( dag_perm , dag_id ) if access_control : self . _sync_dag_view_permissions ( dag_id , access_control )",Sync permissions for given dag id . The dag id surely exists in our dag bag as only / refresh button or cli . sync_perm will call this function
"def _sync_dag_view_permissions ( self , dag_id , access_control ) : def _get_or_create_dag_permission ( perm_name ) : dag_perm = self . find_permission_view_menu ( perm_name , dag_id ) if not dag_perm : self . log . info ( ""Creating new permission '%s' on view '%s'"" , perm_name , dag_id ) dag_perm = self . add_permission_view_menu ( perm_name , dag_id ) return dag_perm def _revoke_stale_permissions ( dag_view ) : existing_dag_perms = self . find_permissions_view_menu ( dag_view ) for perm in existing_dag_perms : non_admin_roles = [ role for role in perm . role if role . name != 'Admin' ] for role in non_admin_roles : target_perms_for_role = access_control . get ( role . name , { } ) if perm . permission . name not in target_perms_for_role : self . log . info ( ""Revoking '%s' on DAG '%s' for role '%s'"" , perm . permission , dag_id , role . name ) self . del_permission_role ( role , perm ) dag_view = self . find_view_menu ( dag_id ) if dag_view : _revoke_stale_permissions ( dag_view ) for rolename , perms in access_control . items ( ) : role = self . find_role ( rolename ) if not role : raise AirflowException ( ""The access_control mapping for DAG '{}' includes a role "" ""named '{}', but that role does not exist"" . format ( dag_id , rolename ) ) perms = set ( perms ) invalid_perms = perms - self . DAG_PERMS if invalid_perms : raise AirflowException ( ""The access_control map for DAG '{}' includes the following "" ""invalid permissions: {}; The set of valid permissions "" ""is: {}"" . format ( dag_id , ( perms - self . DAG_PERMS ) , self . DAG_PERMS ) ) for perm_name in perms : dag_perm = _get_or_create_dag_permission ( perm_name ) self . add_permission_role ( role , dag_perm )",Set the access policy on the given DAG s ViewModel .
"def create_perm_vm_for_all_dag ( self ) : for dag_vm in self . DAG_VMS : for perm in self . DAG_PERMS : self . _merge_perm ( permission_name = perm , view_menu_name = dag_vm )",Create perm - vm if not exist and insert into FAB security model for all - dags .
"def get_fernet ( ) : global _fernet log = LoggingMixin ( ) . log if _fernet : return _fernet try : from cryptography . fernet import Fernet , MultiFernet , InvalidToken global InvalidFernetToken InvalidFernetToken = InvalidToken except BuiltinImportError : log . warning ( ""cryptography not found - values will not be stored encrypted."" ) _fernet = NullFernet ( ) return _fernet try : fernet_key = configuration . conf . get ( 'core' , 'FERNET_KEY' ) if not fernet_key : log . warning ( ""empty cryptography key - values will not be stored encrypted."" ) _fernet = NullFernet ( ) else : _fernet = MultiFernet ( [ Fernet ( fernet_part . encode ( 'utf-8' ) ) for fernet_part in fernet_key . split ( ',' ) ] ) _fernet . is_encrypted = True except ( ValueError , TypeError ) as ve : raise AirflowException ( ""Could not create Fernet object: {}"" . format ( ve ) ) return _fernet",Deferred load of Fernet key .
"def poke ( self , context ) : if '.' in self . table_name : self . database_name , self . table_name = self . table_name . split ( '.' ) self . log . info ( 'Poking for table %s. %s, expression %s' , self . database_name , self . table_name , self . expression ) return self . get_hook ( ) . check_for_partition ( self . database_name , self . table_name , self . expression )",Checks for existence of the partition in the AWS Glue Catalog table
"def get_hook ( self ) : if not hasattr ( self , 'hook' ) : from airflow . contrib . hooks . aws_glue_catalog_hook import AwsGlueCatalogHook self . hook = AwsGlueCatalogHook ( aws_conn_id = self . aws_conn_id , region_name = self . region_name ) return self . hook",Gets the AwsGlueCatalogHook
"def poke ( self , context ) : sqs_hook = SQSHook ( aws_conn_id = self . aws_conn_id ) sqs_conn = sqs_hook . get_conn ( ) self . log . info ( 'SQSSensor checking for message on queue: %s' , self . sqs_queue ) messages = sqs_conn . receive_message ( QueueUrl = self . sqs_queue , MaxNumberOfMessages = self . max_messages , WaitTimeSeconds = self . wait_time_seconds ) self . log . info ( ""reveived message %s"" , str ( messages ) ) if 'Messages' in messages and len ( messages [ 'Messages' ] ) > 0 : entries = [ { 'Id' : message [ 'MessageId' ] , 'ReceiptHandle' : message [ 'ReceiptHandle' ] } for message in messages [ 'Messages' ] ] result = sqs_conn . delete_message_batch ( QueueUrl = self . sqs_queue , Entries = entries ) if 'Successful' in result : context [ 'ti' ] . xcom_push ( key = 'messages' , value = messages ) return True else : raise AirflowException ( 'Delete SQS Messages failed ' + str ( result ) + ' for messages ' + str ( messages ) ) return False",Check for message on subscribed queue and write to xcom the message with key messages
"def tmp_configuration_copy ( chmod = 0o600 ) : cfg_dict = conf . as_dict ( display_sensitive = True , raw = True ) temp_fd , cfg_path = mkstemp ( ) with os . fdopen ( temp_fd , 'w' ) as temp_file : if chmod is not None : os . fchmod ( temp_fd , chmod ) json . dump ( cfg_dict , temp_file ) return cfg_path",Returns a path for a temporary file including a full copy of the configuration settings . : return : a path to a temporary file
"def get_conn ( self ) : effective_user = self . proxy_user autoconfig = self . autoconfig use_sasl = configuration . conf . get ( 'core' , 'security' ) == 'kerberos' try : connections = self . get_connections ( self . hdfs_conn_id ) if not effective_user : effective_user = connections [ 0 ] . login if not autoconfig : autoconfig = connections [ 0 ] . extra_dejson . get ( 'autoconfig' , False ) hdfs_namenode_principal = connections [ 0 ] . extra_dejson . get ( 'hdfs_namenode_principal' ) except AirflowException : if not autoconfig : raise if autoconfig : client = AutoConfigClient ( effective_user = effective_user , use_sasl = use_sasl ) elif len ( connections ) == 1 : client = Client ( connections [ 0 ] . host , connections [ 0 ] . port , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) elif len ( connections ) > 1 : nn = [ Namenode ( conn . host , conn . port ) for conn in connections ] client = HAClient ( nn , effective_user = effective_user , use_sasl = use_sasl , hdfs_namenode_principal = hdfs_namenode_principal ) else : raise HDFSHookException ( ""conn_id doesn't exist in the repository "" ""and autoconfig is not specified"" ) return client",Returns a snakebite HDFSClient object .
"def get_conn ( self ) : connections = self . get_connections ( self . webhdfs_conn_id ) for connection in connections : try : self . log . debug ( 'Trying namenode %s' , connection . host ) client = self . _get_client ( connection ) client . status ( '/' ) self . log . debug ( 'Using namenode %s for hook' , connection . host ) return client except HdfsError as hdfs_error : self . log . debug ( 'Read operation on namenode %s failed with error: %s' , connection . host , hdfs_error ) hosts = [ connection . host for connection in connections ] error_message = 'Read operations failed on the namenodes below:\n{hosts}' . format ( hosts = '\n' . join ( hosts ) ) raise AirflowWebHDFSHookException ( error_message )",Establishes a connection depending on the security mode set via config or environment variable .
"def check_for_path ( self , hdfs_path ) : conn = self . get_conn ( ) status = conn . status ( hdfs_path , strict = False ) return bool ( status )",Check for the existence of a path in HDFS by querying FileStatus .
"def load_file ( self , source , destination , overwrite = True , parallelism = 1 , * * kwargs ) : conn = self . get_conn ( ) conn . upload ( hdfs_path = destination , local_path = source , overwrite = overwrite , n_threads = parallelism , * * kwargs ) self . log . debug ( ""Uploaded file %s to %s"" , source , destination )",r Uploads a file to HDFS .
"def get_conn ( self ) : conn = self . get_connection ( self . pinot_broker_conn_id ) pinot_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/pql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'Get the connection to pinot ' 'broker on {host}' . format ( host = conn . host ) ) return pinot_broker_conn",Establish a connection to pinot broker through pinot dbqpi .
"def get_uri ( self ) : conn = self . get_connection ( getattr ( self , self . conn_name_attr ) ) host = conn . host if conn . port is not None : host += ':{port}' . format ( port = conn . port ) conn_type = 'http' if not conn . conn_type else conn . conn_type endpoint = conn . extra_dejson . get ( 'endpoint' , 'pql' ) return '{conn_type}://{host}/{endpoint}' . format ( conn_type = conn_type , host = host , endpoint = endpoint )",Get the connection uri for pinot broker .
"def get_records ( self , sql ) : with self . get_conn ( ) as cur : cur . execute ( sql ) return cur . fetchall ( )",Executes the sql and returns a set of records .
"def get_first ( self , sql ) : with self . get_conn ( ) as cur : cur . execute ( sql ) return cur . fetchone ( )",Executes the sql and returns the first resulting row .
"def smart_truncate ( string , max_length = 0 , word_boundary = False , separator = ' ' , save_order = False ) : string = string . strip ( separator ) if not max_length : return string if len ( string ) < max_length : return string if not word_boundary : return string [ : max_length ] . strip ( separator ) if separator not in string : return string [ : max_length ] truncated = '' for word in string . split ( separator ) : if word : next_len = len ( truncated ) + len ( word ) if next_len < max_length : truncated += '{0}{1}' . format ( word , separator ) elif next_len == max_length : truncated += '{0}' . format ( word ) break else : if save_order : break if not truncated : truncated = string [ : max_length ] return truncated . strip ( separator )",Truncate a string . : param string ( str ) : string for modification : param max_length ( int ) : output string length : param word_boundary ( bool ) : : param save_order ( bool ) : if True then word order of output string is like input string : param separator ( str ) : separator between words : return :
"def slugify ( text , entities = True , decimal = True , hexadecimal = True , max_length = 0 , word_boundary = False , separator = DEFAULT_SEPARATOR , save_order = False , stopwords = ( ) , regex_pattern = None , lowercase = True , replacements = ( ) ) : if replacements : for old , new in replacements : text = text . replace ( old , new ) if not isinstance ( text , _unicode_type ) : text = _unicode ( text , 'utf-8' , 'ignore' ) text = QUOTE_PATTERN . sub ( DEFAULT_SEPARATOR , text ) text = unidecode . unidecode ( text ) if not isinstance ( text , _unicode_type ) : text = _unicode ( text , 'utf-8' , 'ignore' ) if entities : text = CHAR_ENTITY_PATTERN . sub ( lambda m : unichr ( name2codepoint [ m . group ( 1 ) ] ) , text ) if decimal : try : text = DECIMAL_PATTERN . sub ( lambda m : unichr ( int ( m . group ( 1 ) ) ) , text ) except Exception : pass if hexadecimal : try : text = HEX_PATTERN . sub ( lambda m : unichr ( int ( m . group ( 1 ) , 16 ) ) , text ) except Exception : pass text = unicodedata . normalize ( 'NFKD' , text ) if lowercase : text = text . lower ( ) text = QUOTE_PATTERN . sub ( '' , text ) text = NUMBERS_PATTERN . sub ( '' , text ) if lowercase : pattern = regex_pattern or ALLOWED_CHARS_PATTERN else : pattern = regex_pattern or ALLOWED_CHARS_PATTERN_WITH_UPPERCASE text = re . sub ( pattern , DEFAULT_SEPARATOR , text ) text = DUPLICATE_DASH_PATTERN . sub ( DEFAULT_SEPARATOR , text ) . strip ( DEFAULT_SEPARATOR ) if stopwords : if lowercase : stopwords_lower = [ s . lower ( ) for s in stopwords ] words = [ w for w in text . split ( DEFAULT_SEPARATOR ) if w not in stopwords_lower ] else : words = [ w for w in text . split ( DEFAULT_SEPARATOR ) if w not in stopwords ] text = DEFAULT_SEPARATOR . join ( words ) if replacements : for old , new in replacements : text = text . replace ( old , new ) if max_length > 0 : text = smart_truncate ( text , max_length , word_boundary , DEFAULT_SEPARATOR , save_order ) if separator != DEFAULT_SEPARATOR : text = text . replace ( DEFAULT_SEPARATOR , separator ) return text",Make a slug from the given text . : param text ( str ) : initial text : param entities ( bool ) : : param decimal ( bool ) : : param hexadecimal ( bool ) : : param max_length ( int ) : output string length : param word_boundary ( bool ) : : param save_order ( bool ) : if parameter is True and max_length > 0 return whole words in the initial order : param separator ( str ) : separator between words : param stopwords ( iterable ) : words to discount : param regex_pattern ( str ) : regex pattern for allowed characters : param lowercase ( bool ) : activate case sensitivity by setting it to False : param replacements ( iterable ) : list of replacement rules e . g . [[ | or ] [ % percent ]] : return ( str ) :
"def set ( cls , key , value , execution_date , task_id , dag_id , session = None ) : session . expunge_all ( ) enable_pickling = configuration . getboolean ( 'core' , 'enable_xcom_pickling' ) if enable_pickling : value = pickle . dumps ( value ) else : try : value = json . dumps ( value ) . encode ( 'UTF-8' ) except ValueError : log = LoggingMixin ( ) . log log . error ( ""Could not serialize the XCOM value into JSON. "" ""If you are using pickles instead of JSON "" ""for XCOM, then you need to enable pickle "" ""support for XCOM in your airflow config."" ) raise session . query ( cls ) . filter ( cls . key == key , cls . execution_date == execution_date , cls . task_id == task_id , cls . dag_id == dag_id ) . delete ( ) session . commit ( ) session . add ( XCom ( key = key , value = value , execution_date = execution_date , task_id = task_id , dag_id = dag_id ) ) session . commit ( )",Store an XCom value . TODO : pickling has been deprecated and JSON is preferred . pickling will be removed in Airflow 2 . 0 .
"def get_one ( cls , execution_date , key = None , task_id = None , dag_id = None , include_prior_dates = False , session = None ) : filters = [ ] if key : filters . append ( cls . key == key ) if task_id : filters . append ( cls . task_id == task_id ) if dag_id : filters . append ( cls . dag_id == dag_id ) if include_prior_dates : filters . append ( cls . execution_date <= execution_date ) else : filters . append ( cls . execution_date == execution_date ) query = ( session . query ( cls . value ) . filter ( and_ ( * filters ) ) . order_by ( cls . execution_date . desc ( ) , cls . timestamp . desc ( ) ) ) result = query . first ( ) if result : enable_pickling = configuration . getboolean ( 'core' , 'enable_xcom_pickling' ) if enable_pickling : return pickle . loads ( result . value ) else : try : return json . loads ( result . value . decode ( 'UTF-8' ) ) except ValueError : log = LoggingMixin ( ) . log log . error ( ""Could not deserialize the XCOM value from JSON. "" ""If you are using pickles instead of JSON "" ""for XCOM, then you need to enable pickle "" ""support for XCOM in your airflow config."" ) raise",Retrieve an XCom value optionally meeting certain criteria . TODO : pickling has been deprecated and JSON is preferred . pickling will be removed in Airflow 2 . 0 .
"def get_many ( cls , execution_date , key = None , task_ids = None , dag_ids = None , include_prior_dates = False , limit = 100 , session = None ) : filters = [ ] if key : filters . append ( cls . key == key ) if task_ids : filters . append ( cls . task_id . in_ ( as_tuple ( task_ids ) ) ) if dag_ids : filters . append ( cls . dag_id . in_ ( as_tuple ( dag_ids ) ) ) if include_prior_dates : filters . append ( cls . execution_date <= execution_date ) else : filters . append ( cls . execution_date == execution_date ) query = ( session . query ( cls ) . filter ( and_ ( * filters ) ) . order_by ( cls . execution_date . desc ( ) , cls . timestamp . desc ( ) ) . limit ( limit ) ) results = query . all ( ) return results",Retrieve an XCom value optionally meeting certain criteria TODO : pickling has been deprecated and JSON is preferred . pickling will be removed in Airflow 2 . 0 .
"def _convert_date_to_dict ( field_date ) : return { DAY : field_date . day , MONTH : field_date . month , YEAR : field_date . year }",Convert native python datetime . date object to a format supported by the API
"def _convert_time_to_dict ( time ) : return { HOURS : time . hour , MINUTES : time . minute , SECONDS : time . second }",Convert native python datetime . time object to a format supported by the API
"def get_conn ( self ) : conn = self . get_connection ( self . redis_conn_id ) self . host = conn . host self . port = conn . port self . password = None if str ( conn . password ) . lower ( ) in [ 'none' , 'false' , '' ] else conn . password self . db = conn . extra_dejson . get ( 'db' , None ) if not self . redis : self . log . debug ( 'Initializing redis object for conn_id ""%s"" on %s:%s:%s' , self . redis_conn_id , self . host , self . port , self . db ) self . redis = Redis ( host = self . host , port = self . port , password = self . password , db = self . db ) return self . redis",Returns a Redis connection .
"def get_conn ( self ) : conn = self . get_connection ( self . oracle_conn_id ) conn_config = { 'user' : conn . login , 'password' : conn . password } dsn = conn . extra_dejson . get ( 'dsn' , None ) sid = conn . extra_dejson . get ( 'sid' , None ) mod = conn . extra_dejson . get ( 'module' , None ) service_name = conn . extra_dejson . get ( 'service_name' , None ) port = conn . port if conn . port else 1521 if dsn and sid and not service_name : conn_config [ 'dsn' ] = cx_Oracle . makedsn ( dsn , port , sid ) elif dsn and service_name and not sid : conn_config [ 'dsn' ] = cx_Oracle . makedsn ( dsn , port , service_name = service_name ) else : conn_config [ 'dsn' ] = conn . host if 'encoding' in conn . extra_dejson : conn_config [ 'encoding' ] = conn . extra_dejson . get ( 'encoding' ) if 'nencoding' not in conn . extra_dejson : conn_config [ 'nencoding' ] = conn . extra_dejson . get ( 'encoding' ) if 'nencoding' in conn . extra_dejson : conn_config [ 'nencoding' ] = conn . extra_dejson . get ( 'nencoding' ) if 'threaded' in conn . extra_dejson : conn_config [ 'threaded' ] = conn . extra_dejson . get ( 'threaded' ) if 'events' in conn . extra_dejson : conn_config [ 'events' ] = conn . extra_dejson . get ( 'events' ) mode = conn . extra_dejson . get ( 'mode' , '' ) . lower ( ) if mode == 'sysdba' : conn_config [ 'mode' ] = cx_Oracle . SYSDBA elif mode == 'sysasm' : conn_config [ 'mode' ] = cx_Oracle . SYSASM elif mode == 'sysoper' : conn_config [ 'mode' ] = cx_Oracle . SYSOPER elif mode == 'sysbkp' : conn_config [ 'mode' ] = cx_Oracle . SYSBKP elif mode == 'sysdgd' : conn_config [ 'mode' ] = cx_Oracle . SYSDGD elif mode == 'syskmt' : conn_config [ 'mode' ] = cx_Oracle . SYSKMT elif mode == 'sysrac' : conn_config [ 'mode' ] = cx_Oracle . SYSRAC purity = conn . extra_dejson . get ( 'purity' , '' ) . lower ( ) if purity == 'new' : conn_config [ 'purity' ] = cx_Oracle . ATTR_PURITY_NEW elif purity == 'self' : conn_config [ 'purity' ] = cx_Oracle . ATTR_PURITY_SELF elif purity == 'default' : conn_config [ 'purity' ] = cx_Oracle . ATTR_PURITY_DEFAULT conn = cx_Oracle . connect ( * * conn_config ) if mod is not None : conn . module = mod return conn",Returns a oracle connection object Optional parameters for using a custom DSN connection ( instead of using a server alias from tnsnames . ora ) The dsn ( data source name ) is the TNS entry ( from the Oracle names server or tnsnames . ora file ) or is a string like the one returned from makedsn () .
"def insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 ) : if target_fields : target_fields = ', ' . join ( target_fields ) target_fields = '({})' . format ( target_fields ) else : target_fields = '' conn = self . get_conn ( ) cur = conn . cursor ( ) if self . supports_autocommit : cur . execute ( 'SET autocommit = 0' ) conn . commit ( ) i = 0 for row in rows : i += 1 lst = [ ] for cell in row : if isinstance ( cell , basestring ) : lst . append ( ""'"" + str ( cell ) . replace ( ""'"" , ""''"" ) + ""'"" ) elif cell is None : lst . append ( 'NULL' ) elif type ( cell ) == float and numpy . isnan ( cell ) : lst . append ( 'NULL' ) elif isinstance ( cell , numpy . datetime64 ) : lst . append ( ""'"" + str ( cell ) + ""'"" ) elif isinstance ( cell , datetime ) : lst . append ( ""to_date('"" + cell . strftime ( '%Y-%m-%d %H:%M:%S' ) + ""','YYYY-MM-DD HH24:MI:SS')"" ) else : lst . append ( str ( cell ) ) values = tuple ( lst ) sql = 'INSERT /*+ APPEND */ ' 'INTO {0} {1} VALUES ({2})' . format ( table , target_fields , ',' . join ( values ) ) cur . execute ( sql ) if i % commit_every == 0 : conn . commit ( ) self . log . info ( 'Loaded %s into %s rows so far' , i , table ) conn . commit ( ) cur . close ( ) conn . close ( ) self . log . info ( 'Done loading. Loaded a total of %s rows' , i )",A generic way to insert a set of tuples into a table the whole set of inserts is treated as one transaction Changes from standard DbApiHook implementation :
"def bulk_insert_rows ( self , table , rows , target_fields = None , commit_every = 5000 ) : if not rows : raise ValueError ( ""parameter rows could not be None or empty iterable"" ) conn = self . get_conn ( ) cursor = conn . cursor ( ) values_base = target_fields if target_fields else rows [ 0 ] prepared_stm = 'insert into {tablename} {columns} values ({values})' . format ( tablename = table , columns = '({})' . format ( ', ' . join ( target_fields ) ) if target_fields else '' , values = ', ' . join ( ':%s' % i for i in range ( 1 , len ( values_base ) + 1 ) ) , ) row_count = 0 row_chunk = [ ] for row in rows : row_chunk . append ( row ) row_count += 1 if row_count % commit_every == 0 : cursor . prepare ( prepared_stm ) cursor . executemany ( None , row_chunk ) conn . commit ( ) self . log . info ( '[%s] inserted %s rows' , table , row_count ) row_chunk = [ ] cursor . prepare ( prepared_stm ) cursor . executemany ( None , row_chunk ) conn . commit ( ) self . log . info ( '[%s] inserted %s rows' , table , row_count ) cursor . close ( ) conn . close ( )",A performant bulk insert for cx_Oracle that uses prepared statements via executemany () . For best performance pass in rows as an iterator .
"def get_conn ( self ) : db = self . get_connection ( getattr ( self , self . conn_name_attr ) ) return self . connector . connect ( host = db . host , port = db . port , username = db . login , schema = db . schema )",Returns a connection object
"def get_pandas_df ( self , sql , parameters = None ) : import pandas . io . sql as psql with closing ( self . get_conn ( ) ) as conn : return psql . read_sql ( sql , con = conn , params = parameters )",Executes the sql and returns a pandas dataframe
"def get_records ( self , sql , parameters = None ) : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : if parameters is not None : cur . execute ( sql , parameters ) else : cur . execute ( sql ) return cur . fetchall ( )",Executes the sql and returns a set of records .
"def get_first ( self , sql , parameters = None ) : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : if parameters is not None : cur . execute ( sql , parameters ) else : cur . execute ( sql ) return cur . fetchone ( )",Executes the sql and returns the first resulting row .
"def run ( self , sql , autocommit = False , parameters = None ) : if isinstance ( sql , basestring ) : sql = [ sql ] with closing ( self . get_conn ( ) ) as conn : if self . supports_autocommit : self . set_autocommit ( conn , autocommit ) with closing ( conn . cursor ( ) ) as cur : for s in sql : if parameters is not None : self . log . info ( ""{} with parameters {}"" . format ( s , parameters ) ) cur . execute ( s , parameters ) else : self . log . info ( s ) cur . execute ( s ) if not self . get_autocommit ( conn ) : conn . commit ( )",Runs a command or a list of commands . Pass a list of sql statements to the sql parameter to get them to execute sequentially
"def set_autocommit ( self , conn , autocommit ) : if not self . supports_autocommit and autocommit : self . log . warn ( ( ""%s connection doesn't support "" ""autocommit but autocommit activated."" ) , getattr ( self , self . conn_name_attr ) ) conn . autocommit = autocommit",Sets the autocommit flag on the connection
"def insert_rows ( self , table , rows , target_fields = None , commit_every = 1000 , replace = False ) : if target_fields : target_fields = "", "" . join ( target_fields ) target_fields = ""({})"" . format ( target_fields ) else : target_fields = '' i = 0 with closing ( self . get_conn ( ) ) as conn : if self . supports_autocommit : self . set_autocommit ( conn , False ) conn . commit ( ) with closing ( conn . cursor ( ) ) as cur : for i , row in enumerate ( rows , 1 ) : lst = [ ] for cell in row : lst . append ( self . _serialize_cell ( cell , conn ) ) values = tuple ( lst ) placeholders = [ ""%s"" , ] * len ( values ) if not replace : sql = ""INSERT INTO "" else : sql = ""REPLACE INTO "" sql += ""{0} {1} VALUES ({2})"" . format ( table , target_fields , "","" . join ( placeholders ) ) cur . execute ( sql , values ) if commit_every and i % commit_every == 0 : conn . commit ( ) self . log . info ( ""Loaded %s into %s rows so far"" , i , table ) conn . commit ( ) self . log . info ( ""Done loading. Loaded a total of %s rows"" , i )",A generic way to insert a set of tuples into a table a new transaction is created every commit_every rows
"def _serialize_cell ( cell , conn = None ) : if cell is None : return None if isinstance ( cell , datetime ) : return cell . isoformat ( ) return str ( cell )",Returns the SQL literal of the cell as a string .
"def health ( self , session = None ) : BJ = jobs . BaseJob payload = { } scheduler_health_check_threshold = timedelta ( seconds = conf . getint ( 'scheduler' , 'scheduler_health_check_threshold' ) ) latest_scheduler_heartbeat = None payload [ 'metadatabase' ] = { 'status' : 'healthy' } try : latest_scheduler_heartbeat = session . query ( func . max ( BJ . latest_heartbeat ) ) . filter ( BJ . state == 'running' , BJ . job_type == 'SchedulerJob' ) . scalar ( ) except Exception : payload [ 'metadatabase' ] [ 'status' ] = 'unhealthy' if not latest_scheduler_heartbeat : scheduler_status = 'unhealthy' else : if timezone . utcnow ( ) - latest_scheduler_heartbeat <= scheduler_health_check_threshold : scheduler_status = 'healthy' else : scheduler_status = 'unhealthy' payload [ 'scheduler' ] = { 'status' : scheduler_status , 'latest_scheduler_heartbeat' : str ( latest_scheduler_heartbeat ) } return wwwutils . json_response ( payload )",An endpoint helping check the health status of the Airflow instance including metadatabase and scheduler .
"def extra_links ( self ) : dag_id = request . args . get ( 'dag_id' ) task_id = request . args . get ( 'task_id' ) execution_date = request . args . get ( 'execution_date' ) link_name = request . args . get ( 'link_name' ) dttm = airflow . utils . timezone . parse ( execution_date ) dag = dagbag . get_dag ( dag_id ) if not dag or task_id not in dag . task_ids : response = jsonify ( { 'url' : None , 'error' : ""can't find dag {dag} or task_id {task_id}"" . format ( dag = dag , task_id = task_id ) } ) response . status_code = 404 return response task = dag . get_task ( task_id ) try : url = task . get_extra_links ( dttm , link_name ) except ValueError as err : response = jsonify ( { 'url' : None , 'error' : str ( err ) } ) response . status_code = 404 return response if url : response = jsonify ( { 'error' : None , 'url' : url } ) response . status_code = 200 return response else : response = jsonify ( { 'url' : None , 'error' : 'No URL found for {dest}' . format ( dest = link_name ) } ) response . status_code = 404 return response",A restful endpoint that returns external links for a given Operator
"def get_query ( self ) : return ( super ( ) . get_query ( ) . filter ( or_ ( models . DagModel . is_active , models . DagModel . is_paused ) ) . filter ( ~ models . DagModel . is_subdag ) )",Default filters for model
def get_count_query ( self ) : return ( super ( ) . get_count_query ( ) . filter ( models . DagModel . is_active ) . filter ( ~ models . DagModel . is_subdag ) ),Default filters for model
"def get_conn ( self ) : conn = self . get_connection ( self . cloudant_conn_id ) self . _validate_connection ( conn ) cloudant_session = cloudant ( user = conn . login , passwd = conn . password , account = conn . host ) return cloudant_session",Opens a connection to the cloudant service and closes it automatically if used as context manager .
"def execute ( self , context ) : self . hook = SlackWebhookHook ( self . http_conn_id , self . webhook_token , self . message , self . attachments , self . channel , self . username , self . icon_emoji , self . link_names , self . proxy ) self . hook . execute ( )",Call the SlackWebhookHook to post the provided Slack message
"def _get_credentials ( self ) : key_path = self . _get_field ( 'key_path' , False ) keyfile_dict = self . _get_field ( 'keyfile_dict' , False ) scope = self . _get_field ( 'scope' , None ) if scope : scopes = [ s . strip ( ) for s in scope . split ( ',' ) ] else : scopes = _DEFAULT_SCOPES if not key_path and not keyfile_dict : self . log . info ( 'Getting connection using `google.auth.default()` ' 'since no key file is defined for hook.' ) credentials , _ = google . auth . default ( scopes = scopes ) elif key_path : if key_path . endswith ( '.json' ) : self . log . debug ( 'Getting connection using JSON key file %s' % key_path ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_file ( key_path , scopes = scopes ) ) elif key_path . endswith ( '.p12' ) : raise AirflowException ( 'Legacy P12 key file are not supported, ' 'use a JSON key file.' ) else : raise AirflowException ( 'Unrecognised extension for key file.' ) else : try : keyfile_dict = json . loads ( keyfile_dict ) keyfile_dict [ 'private_key' ] = keyfile_dict [ 'private_key' ] . replace ( '\\n' , '\n' ) credentials = ( google . oauth2 . service_account . Credentials . from_service_account_info ( keyfile_dict , scopes = scopes ) ) except json . decoder . JSONDecodeError : raise AirflowException ( 'Invalid key JSON.' ) return credentials . with_subject ( self . delegate_to ) if self . delegate_to else credentials",Returns the Credentials object for Google API
"def _authorize ( self ) : credentials = self . _get_credentials ( ) http = httplib2 . Http ( ) authed_http = google_auth_httplib2 . AuthorizedHttp ( credentials , http = http ) return authed_http",Returns an authorized HTTP object to be used to build a Google cloud service hook connection .
"def _get_field ( self , f , default = None ) : long_f = 'extra__google_cloud_platform__{}' . format ( f ) if hasattr ( self , 'extras' ) and long_f in self . extras : return self . extras [ long_f ] else : return default",Fetches a field from extras and returns it . This is some Airflow magic . The google_cloud_platform hook type adds custom UI elements to the hook page which allow admins to specify service_account key_path etc . They get formatted as shown below .
"def catch_http_exception ( func ) : @ functools . wraps ( func ) def wrapper_decorator ( self , * args , * * kwargs ) : try : return func ( self , * args , * * kwargs ) except GoogleAPICallError as e : if isinstance ( e , AlreadyExists ) : raise e else : self . log . error ( 'The request failed:\n%s' , str ( e ) ) raise AirflowException ( e ) except RetryError as e : self . log . error ( 'The request failed due to a retryable error and retry attempts failed.' ) raise AirflowException ( e ) except ValueError as e : self . log . error ( 'The request failed, the parameters are invalid.' ) raise AirflowException ( e ) except HttpError as e : self . log . error ( 'The request failed:\n%s' , str ( e ) ) raise AirflowException ( e ) return wrapper_decorator",Function decorator that intercepts HTTP Errors and raises AirflowException with more informative message .
"def fallback_to_default_project_id ( func ) : @ functools . wraps ( func ) def inner_wrapper ( self , * args , * * kwargs ) : if len ( args ) > 0 : raise AirflowException ( ""You must use keyword arguments in this methods rather than"" "" positional"" ) if 'project_id' in kwargs : kwargs [ 'project_id' ] = self . _get_project_id ( kwargs [ 'project_id' ] ) else : kwargs [ 'project_id' ] = self . _get_project_id ( None ) if not kwargs [ 'project_id' ] : raise AirflowException ( ""The project id must be passed either as "" ""keyword project_id parameter or as project_id extra "" ""in GCP connection definition. Both are not set!"" ) return func ( self , * args , * * kwargs ) return inner_wrapper",Decorator that provides fallback for Google Cloud Platform project id . If the project is None it will be replaced with the project_id from the service account the Hook is authenticated with . Project id can be specified either via project_id kwarg or via first parameter in positional args .
"def unfinished ( cls ) : return [ cls . NONE , cls . SCHEDULED , cls . QUEUED , cls . RUNNING , cls . SHUTDOWN , cls . UP_FOR_RETRY , cls . UP_FOR_RESCHEDULE ]",A list of states indicating that a task either has not completed a run or has not even started .
"def delete_dag ( dag_id , keep_records_in_log = True , session = None ) : DM = models . DagModel dag = session . query ( DM ) . filter ( DM . dag_id == dag_id ) . first ( ) if dag is None : raise DagNotFound ( ""Dag id {} not found"" . format ( dag_id ) ) if dag . fileloc and os . path . exists ( dag . fileloc ) : raise DagFileExists ( ""Dag id {} is still in DagBag. "" ""Remove the DAG file first: {}"" . format ( dag_id , dag . fileloc ) ) count = 0 for m in models . base . Base . _decl_class_registry . values ( ) : if hasattr ( m , ""dag_id"" ) : if keep_records_in_log and m . __name__ == 'Log' : continue cond = or_ ( m . dag_id == dag_id , m . dag_id . like ( dag_id + "".%"" ) ) count += session . query ( m ) . filter ( cond ) . delete ( synchronize_session = 'fetch' ) if dag . is_subdag : p , c = dag_id . rsplit ( ""."" , 1 ) for m in models . DagRun , TaskFail , models . TaskInstance : count += session . query ( m ) . filter ( m . dag_id == p , m . task_id == c ) . delete ( ) return count",: param dag_id : the dag_id of the DAG to delete : type dag_id : str : param keep_records_in_log : whether keep records of the given dag_id in the Log table in the backend database ( for reasons like auditing ) . The default value is True . : type keep_records_in_log : bool
"def _prepare_command ( self , cmd ) : connection_cmd = [ ""spark-sql"" ] if self . _conf : for conf_el in self . _conf . split ( "","" ) : connection_cmd += [ ""--conf"" , conf_el ] if self . _total_executor_cores : connection_cmd += [ ""--total-executor-cores"" , str ( self . _total_executor_cores ) ] if self . _executor_cores : connection_cmd += [ ""--executor-cores"" , str ( self . _executor_cores ) ] if self . _executor_memory : connection_cmd += [ ""--executor-memory"" , self . _executor_memory ] if self . _keytab : connection_cmd += [ ""--keytab"" , self . _keytab ] if self . _principal : connection_cmd += [ ""--principal"" , self . _principal ] if self . _num_executors : connection_cmd += [ ""--num-executors"" , str ( self . _num_executors ) ] if self . _sql : sql = self . _sql . strip ( ) if sql . endswith ( "".sql"" ) or sql . endswith ( "".hql"" ) : connection_cmd += [ ""-f"" , sql ] else : connection_cmd += [ ""-e"" , sql ] if self . _master : connection_cmd += [ ""--master"" , self . _master ] if self . _name : connection_cmd += [ ""--name"" , self . _name ] if self . _verbose : connection_cmd += [ ""--verbose"" ] if self . _yarn_queue : connection_cmd += [ ""--queue"" , self . _yarn_queue ] connection_cmd += cmd self . log . debug ( ""Spark-Sql cmd: %s"" , connection_cmd ) return connection_cmd",Construct the spark - sql command to execute . Verbose output is enabled as default .
"def run_query ( self , cmd = """" , * * kwargs ) : spark_sql_cmd = self . _prepare_command ( cmd ) self . _sp = subprocess . Popen ( spark_sql_cmd , stdout = subprocess . PIPE , stderr = subprocess . STDOUT , * * kwargs ) for line in iter ( self . _sp . stdout . readline , '' ) : self . log . info ( line ) returncode = self . _sp . wait ( ) if returncode : raise AirflowException ( ""Cannot execute {} on {}. Process exit code: {}."" . format ( cmd , self . _conn . host , returncode ) )",Remote Popen ( actually execute the Spark - sql query )
"def vgg11_bn ( pretrained = False , * * kwargs ) : if pretrained : kwargs [ 'init_weights' ] = False model = VGG ( make_layers ( cfg [ 'A' ] , batch_norm = True ) , * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'vgg11_bn' ] ) ) return model",VGG 11 - layer model ( configuration A ) with batch normalization
"def vgg13 ( pretrained = False , * * kwargs ) : if pretrained : kwargs [ 'init_weights' ] = False model = VGG ( make_layers ( cfg [ 'B' ] ) , * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'vgg13' ] ) ) return model",VGG 13 - layer model ( configuration B )
"def alexnet ( pretrained = False , * * kwargs ) : model = AlexNet ( * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'alexnet' ] ) ) return model",r AlexNet model architecture from the One weird trick ... <https : // arxiv . org / abs / 1404 . 5997 > _ paper .
"def densenet121 ( pretrained = False , * * kwargs ) : model = DenseNet ( num_init_features = 64 , growth_rate = 32 , block_config = ( 6 , 12 , 24 , 16 ) , * * kwargs ) if pretrained : _load_state_dict ( model , model_urls [ 'densenet121' ] ) return model",r Densenet - 121 model from Densely Connected Convolutional Networks <https : // arxiv . org / pdf / 1608 . 06993 . pdf > _
"def to_tensor ( pic ) : if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : raise TypeError ( 'pic should be PIL Image or ndarray. Got {}' . format ( type ( pic ) ) ) if isinstance ( pic , np . ndarray ) : if pic . ndim == 2 : pic = pic [ : , : , None ] img = torch . from_numpy ( pic . transpose ( ( 2 , 0 , 1 ) ) ) if isinstance ( img , torch . ByteTensor ) : return img . float ( ) . div ( 255 ) else : return img if accimage is not None and isinstance ( pic , accimage . Image ) : nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) pic . copyto ( nppic ) return torch . from_numpy ( nppic ) if pic . mode == 'I' : img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) elif pic . mode == 'I;16' : img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) elif pic . mode == 'F' : img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) elif pic . mode == '1' : img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) else : img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) if pic . mode == 'YCbCr' : nchannel = 3 elif pic . mode == 'I;16' : nchannel = 1 else : nchannel = len ( pic . mode ) img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2 ) . contiguous ( ) if isinstance ( img , torch . ByteTensor ) : return img . float ( ) . div ( 255 ) else : return img",Convert a PIL Image or numpy . ndarray to tensor .
"def to_pil_image ( pic , mode = None ) : if not ( isinstance ( pic , torch . Tensor ) or isinstance ( pic , np . ndarray ) ) : raise TypeError ( 'pic should be Tensor or ndarray. Got {}.' . format ( type ( pic ) ) ) elif isinstance ( pic , torch . Tensor ) : if pic . ndimension ( ) not in { 2 , 3 } : raise ValueError ( 'pic should be 2/3 dimensional. Got {} dimensions.' . format ( pic . ndimension ( ) ) ) elif pic . ndimension ( ) == 2 : pic = pic . unsqueeze ( 0 ) elif isinstance ( pic , np . ndarray ) : if pic . ndim not in { 2 , 3 } : raise ValueError ( 'pic should be 2/3 dimensional. Got {} dimensions.' . format ( pic . ndim ) ) elif pic . ndim == 2 : pic = np . expand_dims ( pic , 2 ) npimg = pic if isinstance ( pic , torch . FloatTensor ) : pic = pic . mul ( 255 ) . byte ( ) if isinstance ( pic , torch . Tensor ) : npimg = np . transpose ( pic . numpy ( ) , ( 1 , 2 , 0 ) ) if not isinstance ( npimg , np . ndarray ) : raise TypeError ( 'Input pic must be a torch.Tensor or NumPy ndarray, ' + 'not {}' . format ( type ( npimg ) ) ) if npimg . shape [ 2 ] == 1 : expected_mode = None npimg = npimg [ : , : , 0 ] if npimg . dtype == np . uint8 : expected_mode = 'L' elif npimg . dtype == np . int16 : expected_mode = 'I;16' elif npimg . dtype == np . int32 : expected_mode = 'I' elif npimg . dtype == np . float32 : expected_mode = 'F' if mode is not None and mode != expected_mode : raise ValueError ( ""Incorrect mode ({}) supplied for input type {}. Should be {}"" . format ( mode , np . dtype , expected_mode ) ) mode = expected_mode elif npimg . shape [ 2 ] == 2 : permitted_2_channel_modes = [ 'LA' ] if mode is not None and mode not in permitted_2_channel_modes : raise ValueError ( ""Only modes {} are supported for 2D inputs"" . format ( permitted_2_channel_modes ) ) if mode is None and npimg . dtype == np . uint8 : mode = 'LA' elif npimg . shape [ 2 ] == 4 : permitted_4_channel_modes = [ 'RGBA' , 'CMYK' , 'RGBX' ] if mode is not None and mode not in permitted_4_channel_modes : raise ValueError ( ""Only modes {} are supported for 4D inputs"" . format ( permitted_4_channel_modes ) ) if mode is None and npimg . dtype == np . uint8 : mode = 'RGBA' else : permitted_3_channel_modes = [ 'RGB' , 'YCbCr' , 'HSV' ] if mode is not None and mode not in permitted_3_channel_modes : raise ValueError ( ""Only modes {} are supported for 3D inputs"" . format ( permitted_3_channel_modes ) ) if mode is None and npimg . dtype == np . uint8 : mode = 'RGB' if mode is None : raise TypeError ( 'Input type {} is not supported' . format ( npimg . dtype ) ) return Image . fromarray ( npimg , mode = mode )",Convert a tensor or an ndarray to PIL Image .
"def normalize ( tensor , mean , std , inplace = False ) : if not _is_tensor_image ( tensor ) : raise TypeError ( 'tensor is not a torch image.' ) if not inplace : tensor = tensor . clone ( ) mean = torch . as_tensor ( mean , dtype = torch . float32 , device = tensor . device ) std = torch . as_tensor ( std , dtype = torch . float32 , device = tensor . device ) tensor . sub_ ( mean [ : , None , None ] ) . div_ ( std [ : , None , None ] ) return tensor",Normalize a tensor image with mean and standard deviation .
"def resize ( img , size , interpolation = Image . BILINEAR ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and len ( size ) == 2 ) ) : raise TypeError ( 'Got inappropriate size arg: {}' . format ( size ) ) if isinstance ( size , int ) : w , h = img . size if ( w <= h and w == size ) or ( h <= w and h == size ) : return img if w < h : ow = size oh = int ( size * h / w ) return img . resize ( ( ow , oh ) , interpolation ) else : oh = size ow = int ( size * w / h ) return img . resize ( ( ow , oh ) , interpolation ) else : return img . resize ( size [ : : - 1 ] , interpolation )",r Resize the input PIL Image to the given size .
"def pad ( img , padding , fill = 0 , padding_mode = 'constant' ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) if not isinstance ( padding , ( numbers . Number , tuple ) ) : raise TypeError ( 'Got inappropriate padding arg' ) if not isinstance ( fill , ( numbers . Number , str , tuple ) ) : raise TypeError ( 'Got inappropriate fill arg' ) if not isinstance ( padding_mode , str ) : raise TypeError ( 'Got inappropriate padding_mode arg' ) if isinstance ( padding , Sequence ) and len ( padding ) not in [ 2 , 4 ] : raise ValueError ( ""Padding must be an int or a 2, or 4 element tuple, not a "" + ""{} element tuple"" . format ( len ( padding ) ) ) assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] , 'Padding mode should be either constant, edge, reflect or symmetric' if padding_mode == 'constant' : if img . mode == 'P' : palette = img . getpalette ( ) image = ImageOps . expand ( img , border = padding , fill = fill ) image . putpalette ( palette ) return image return ImageOps . expand ( img , border = padding , fill = fill ) else : if isinstance ( padding , int ) : pad_left = pad_right = pad_top = pad_bottom = padding if isinstance ( padding , Sequence ) and len ( padding ) == 2 : pad_left = pad_right = padding [ 0 ] pad_top = pad_bottom = padding [ 1 ] if isinstance ( padding , Sequence ) and len ( padding ) == 4 : pad_left = padding [ 0 ] pad_top = padding [ 1 ] pad_right = padding [ 2 ] pad_bottom = padding [ 3 ] if img . mode == 'P' : palette = img . getpalette ( ) img = np . asarray ( img ) img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) img = Image . fromarray ( img ) img . putpalette ( palette ) return img img = np . asarray ( img ) if len ( img . shape ) == 3 : img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) , ( 0 , 0 ) ) , padding_mode ) if len ( img . shape ) == 2 : img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) return Image . fromarray ( img )",r Pad the given PIL Image on all sides with specified padding mode and fill value .
"def crop ( img , i , j , h , w ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . crop ( ( j , i , j + w , i + h ) )",Crop the given PIL Image .
"def resized_crop ( img , i , j , h , w , size , interpolation = Image . BILINEAR ) : assert _is_pil_image ( img ) , 'img should be PIL Image' img = crop ( img , i , j , h , w ) img = resize ( img , size , interpolation ) return img",Crop the given PIL Image and resize it to desired size .
def hflip ( img ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . transpose ( Image . FLIP_LEFT_RIGHT ),Horizontally flip the given PIL Image .
"def _get_perspective_coeffs ( startpoints , endpoints ) : matrix = [ ] for p1 , p2 in zip ( endpoints , startpoints ) : matrix . append ( [ p1 [ 0 ] , p1 [ 1 ] , 1 , 0 , 0 , 0 , - p2 [ 0 ] * p1 [ 0 ] , - p2 [ 0 ] * p1 [ 1 ] ] ) matrix . append ( [ 0 , 0 , 0 , p1 [ 0 ] , p1 [ 1 ] , 1 , - p2 [ 1 ] * p1 [ 0 ] , - p2 [ 1 ] * p1 [ 1 ] ] ) A = torch . tensor ( matrix , dtype = torch . float ) B = torch . tensor ( startpoints , dtype = torch . float ) . view ( 8 ) res = torch . gels ( B , A ) [ 0 ] return res . squeeze_ ( 1 ) . tolist ( )",Helper function to get the coefficients ( a b c d e f g h ) for the perspective transforms .
"def perspective ( img , startpoints , endpoints , interpolation = Image . BICUBIC ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) coeffs = _get_perspective_coeffs ( startpoints , endpoints ) return img . transform ( img . size , Image . PERSPECTIVE , coeffs , interpolation )",Perform perspective transform of the given PIL Image .
def vflip ( img ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . transpose ( Image . FLIP_TOP_BOTTOM ),Vertically flip the given PIL Image .
"def five_crop ( img , size ) : if isinstance ( size , numbers . Number ) : size = ( int ( size ) , int ( size ) ) else : assert len ( size ) == 2 , ""Please provide only two dimensions (h, w) for size."" w , h = img . size crop_h , crop_w = size if crop_w > w or crop_h > h : raise ValueError ( ""Requested crop size {} is bigger than input size {}"" . format ( size , ( h , w ) ) ) tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) center = center_crop ( img , ( crop_h , crop_w ) ) return ( tl , tr , bl , br , center )",Crop the given PIL Image into four corners and the central crop .
"def ten_crop ( img , size , vertical_flip = False ) : if isinstance ( size , numbers . Number ) : size = ( int ( size ) , int ( size ) ) else : assert len ( size ) == 2 , ""Please provide only two dimensions (h, w) for size."" first_five = five_crop ( img , size ) if vertical_flip : img = vflip ( img ) else : img = hflip ( img ) second_five = five_crop ( img , size ) return first_five + second_five",r Crop the given PIL Image into four corners and the central crop plus the flipped version of these ( horizontal flipping is used by default ) .
"def adjust_brightness ( img , brightness_factor ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) enhancer = ImageEnhance . Brightness ( img ) img = enhancer . enhance ( brightness_factor ) return img",Adjust brightness of an Image .
"def adjust_contrast ( img , contrast_factor ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) enhancer = ImageEnhance . Contrast ( img ) img = enhancer . enhance ( contrast_factor ) return img",Adjust contrast of an Image .
"def adjust_saturation ( img , saturation_factor ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) enhancer = ImageEnhance . Color ( img ) img = enhancer . enhance ( saturation_factor ) return img",Adjust color saturation of an image .
"def adjust_hue ( img , hue_factor ) : if not ( - 0.5 <= hue_factor <= 0.5 ) : raise ValueError ( 'hue_factor is not in [-0.5, 0.5].' . format ( hue_factor ) ) if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) input_mode = img . mode if input_mode in { 'L' , '1' , 'I' , 'F' } : return img h , s , v = img . convert ( 'HSV' ) . split ( ) np_h = np . array ( h , dtype = np . uint8 ) with np . errstate ( over = 'ignore' ) : np_h += np . uint8 ( hue_factor * 255 ) h = Image . fromarray ( np_h , 'L' ) img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) return img",Adjust hue of an image .
"def adjust_gamma ( img , gamma , gain = 1 ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) if gamma < 0 : raise ValueError ( 'Gamma should be a non-negative real number' ) input_mode = img . mode img = img . convert ( 'RGB' ) gamma_map = [ 255 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256 ) ] * 3 img = img . point ( gamma_map ) img = img . convert ( input_mode ) return img",r Perform gamma correction on an image .
"def rotate ( img , angle , resample = False , expand = False , center = None ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) return img . rotate ( angle , resample , expand , center )",Rotate the image by angle .
"def affine ( img , angle , translate , scale , shear , resample = 0 , fillcolor = None ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) assert isinstance ( translate , ( tuple , list ) ) and len ( translate ) == 2 , ""Argument translate should be a list or tuple of length 2"" assert scale > 0.0 , ""Argument scale should be positive"" output_size = img . size center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) kwargs = { ""fillcolor"" : fillcolor } if PILLOW_VERSION [ 0 ] == '5' else { } return img . transform ( output_size , Image . AFFINE , matrix , resample , * * kwargs )",Apply affine transformation on the image keeping image center invariant
"def to_grayscale ( img , num_output_channels = 1 ) : if not _is_pil_image ( img ) : raise TypeError ( 'img should be PIL Image. Got {}' . format ( type ( img ) ) ) if num_output_channels == 1 : img = img . convert ( 'L' ) elif num_output_channels == 3 : img = img . convert ( 'L' ) np_img = np . array ( img , dtype = np . uint8 ) np_img = np . dstack ( [ np_img , np_img , np_img ] ) img = Image . fromarray ( np_img , 'RGB' ) else : raise ValueError ( 'num_output_channels should be either 1 or 3' ) return img",Convert image to grayscale version of image .
"def make_grid ( tensor , nrow = 8 , padding = 2 , normalize = False , range = None , scale_each = False , pad_value = 0 ) : if not ( torch . is_tensor ( tensor ) or ( isinstance ( tensor , list ) and all ( torch . is_tensor ( t ) for t in tensor ) ) ) : raise TypeError ( 'tensor or list of tensors expected, got {}' . format ( type ( tensor ) ) ) if isinstance ( tensor , list ) : tensor = torch . stack ( tensor , dim = 0 ) if tensor . dim ( ) == 2 : tensor = tensor . unsqueeze ( 0 ) if tensor . dim ( ) == 3 : if tensor . size ( 0 ) == 1 : tensor = torch . cat ( ( tensor , tensor , tensor ) , 0 ) tensor = tensor . unsqueeze ( 0 ) if tensor . dim ( ) == 4 and tensor . size ( 1 ) == 1 : tensor = torch . cat ( ( tensor , tensor , tensor ) , 1 ) if normalize is True : tensor = tensor . clone ( ) if range is not None : assert isinstance ( range , tuple ) , ""range has to be a tuple (min, max) if specified. min and max are numbers"" def norm_ip ( img , min , max ) : img . clamp_ ( min = min , max = max ) img . add_ ( - min ) . div_ ( max - min + 1e-5 ) def norm_range ( t , range ) : if range is not None : norm_ip ( t , range [ 0 ] , range [ 1 ] ) else : norm_ip ( t , float ( t . min ( ) ) , float ( t . max ( ) ) ) if scale_each is True : for t in tensor : norm_range ( t , range ) else : norm_range ( tensor , range ) if tensor . size ( 0 ) == 1 : return tensor . squeeze ( ) nmaps = tensor . size ( 0 ) xmaps = min ( nrow , nmaps ) ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) height , width = int ( tensor . size ( 2 ) + padding ) , int ( tensor . size ( 3 ) + padding ) grid = tensor . new_full ( ( 3 , height * ymaps + padding , width * xmaps + padding ) , pad_value ) k = 0 for y in irange ( ymaps ) : for x in irange ( xmaps ) : if k >= nmaps : break grid . narrow ( 1 , y * height + padding , height - padding ) . narrow ( 2 , x * width + padding , width - padding ) . copy_ ( tensor [ k ] ) k = k + 1 return grid",Make a grid of images .
"def save_image ( tensor , filename , nrow = 8 , padding = 2 , normalize = False , range = None , scale_each = False , pad_value = 0 ) : from PIL import Image grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value , normalize = normalize , range = range , scale_each = scale_each ) ndarr = grid . mul_ ( 255 ) . add_ ( 0.5 ) . clamp_ ( 0 , 255 ) . permute ( 1 , 2 , 0 ) . to ( 'cpu' , torch . uint8 ) . numpy ( ) im = Image . fromarray ( ndarr ) im . save ( filename )",Save a given Tensor into an image file .
"def _find_classes ( self , dir ) : if sys . version_info >= ( 3 , 5 ) : classes = [ d . name for d in os . scandir ( dir ) if d . is_dir ( ) ] else : classes = [ d for d in os . listdir ( dir ) if os . path . isdir ( os . path . join ( dir , d ) ) ] classes . sort ( ) class_to_idx = { classes [ i ] : i for i in range ( len ( classes ) ) } return classes , class_to_idx",Finds the class folders in a dataset .
"def read_image_file ( data_dir , image_ext , n ) : def PIL2array ( _img ) : """"""Convert PIL image type to numpy 2D array
        """""" return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) def find_files ( _data_dir , _image_ext ) : """"""Return a list with the file names of the images containing the patches
        """""" files = [ ] for file_dir in os . listdir ( _data_dir ) : if file_dir . endswith ( _image_ext ) : files . append ( os . path . join ( _data_dir , file_dir ) ) return sorted ( files ) patches = [ ] list_files = find_files ( data_dir , image_ext ) for fpath in list_files : img = Image . open ( fpath ) for y in range ( 0 , 1024 , 64 ) : for x in range ( 0 , 1024 , 64 ) : patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) patches . append ( PIL2array ( patch ) ) return torch . ByteTensor ( np . array ( patches [ : n ] ) )",Return a Tensor containing the patches
"def read_info_file ( data_dir , info_file ) : labels = [ ] with open ( os . path . join ( data_dir , info_file ) , 'r' ) as f : labels = [ int ( line . split ( ) [ 0 ] ) for line in f ] return torch . LongTensor ( labels )",Return a Tensor containing the list of labels Read the file and keep only the ID of the 3D point .
"def read_matches_files ( data_dir , matches_file ) : matches = [ ] with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : for line in f : line_split = line . split ( ) matches . append ( [ int ( line_split [ 0 ] ) , int ( line_split [ 3 ] ) , int ( line_split [ 1 ] == line_split [ 4 ] ) ] ) return torch . LongTensor ( matches )",Return a Tensor containing the ground truth matches Read the file and keep only 3D point ID . Matches are represented with a 1 non matches with a 0 .
"def conv1x1 ( in_planes , out_planes , stride = 1 ) : return nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , bias = False )",1x1 convolution
"def accuracy ( output , target , topk = ( 1 , ) ) : with torch . no_grad ( ) : maxk = max ( topk ) batch_size = target . size ( 0 ) _ , pred = output . topk ( maxk , 1 , True , True ) pred = pred . t ( ) correct = pred . eq ( target [ None ] ) res = [ ] for k in topk : correct_k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) res . append ( correct_k * ( 100.0 / batch_size ) ) return res",Computes the accuracy over the k top predictions for the specified values of k
"def setup_for_distributed ( is_master ) : import builtins as __builtin__ builtin_print = __builtin__ . print def print ( * args , * * kwargs ) : force = kwargs . pop ( 'force' , False ) if is_master or force : builtin_print ( * args , * * kwargs ) __builtin__ . print = print",This function disables printing when not in master process
"def synchronize_between_processes ( self ) : if not is_dist_avail_and_initialized ( ) : return t = torch . tensor ( [ self . count , self . total ] , dtype = torch . float64 , device = 'cuda' ) dist . barrier ( ) dist . all_reduce ( t ) t = t . tolist ( ) self . count = int ( t [ 0 ] ) self . total = t [ 1 ]",Warning : does not synchronize the deque!
"def squeezenet1_1 ( pretrained = False , * * kwargs ) : model = SqueezeNet ( version = 1.1 , * * kwargs ) if pretrained : model . load_state_dict ( model_zoo . load_url ( model_urls [ 'squeezenet1_1' ] ) ) return model",r SqueezeNet 1 . 1 model from the official SqueezeNet repo <https : // github . com / DeepScale / SqueezeNet / tree / master / SqueezeNet_v1 . 1 > _ . SqueezeNet 1 . 1 has 2 . 4x less computation and slightly fewer parameters than SqueezeNet 1 . 0 without sacrificing accuracy .
def makedir_exist_ok ( dirpath ) : try : os . makedirs ( dirpath ) except OSError as e : if e . errno == errno . EEXIST : pass else : raise,Python2 support for os . makedirs ( .. exist_ok = True )
"def download_url ( url , root , filename = None , md5 = None ) : from six . moves import urllib root = os . path . expanduser ( root ) if not filename : filename = os . path . basename ( url ) fpath = os . path . join ( root , filename ) makedir_exist_ok ( root ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : print ( 'Using downloaded and verified file: ' + fpath ) else : try : print ( 'Downloading ' + url + ' to ' + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) ) except OSError : if url [ : 5 ] == 'https' : url = url . replace ( 'https:' , 'http:' ) print ( 'Failed download. Trying https -> http instead.' ' Downloading ' + url + ' to ' + fpath ) urllib . request . urlretrieve ( url , fpath , reporthook = gen_bar_updater ( ) )",Download a file from a url and place it in root .
"def list_dir ( root , prefix = False ) : root = os . path . expanduser ( root ) directories = list ( filter ( lambda p : os . path . isdir ( os . path . join ( root , p ) ) , os . listdir ( root ) ) ) if prefix is True : directories = [ os . path . join ( root , d ) for d in directories ] return directories",List all directories at a given root
"def list_files ( root , suffix , prefix = False ) : root = os . path . expanduser ( root ) files = list ( filter ( lambda p : os . path . isfile ( os . path . join ( root , p ) ) and p . endswith ( suffix ) , os . listdir ( root ) ) ) if prefix is True : files = [ os . path . join ( root , d ) for d in files ] return files",List all files ending with a suffix at a given root
"def download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) : import requests url = ""https://docs.google.com/uc?export=download"" root = os . path . expanduser ( root ) if not filename : filename = file_id fpath = os . path . join ( root , filename ) makedir_exist_ok ( root ) if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : print ( 'Using downloaded and verified file: ' + fpath ) else : session = requests . Session ( ) response = session . get ( url , params = { 'id' : file_id } , stream = True ) token = _get_confirm_token ( response ) if token : params = { 'id' : file_id , 'confirm' : token } response = session . get ( url , params = params , stream = True ) _save_response_content ( response , fpath )",Download a Google Drive file from and place it in root .
"def get_params ( img , output_size ) : w , h = img . size th , tw = output_size if w == tw and h == th : return 0 , 0 , h , w i = random . randint ( 0 , h - th ) j = random . randint ( 0 , w - tw ) return i , j , th , tw",Get parameters for crop for a random crop .
"def get_params ( width , height , distortion_scale ) : half_height = int ( height / 2 ) half_width = int ( width / 2 ) topleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , random . randint ( 0 , int ( distortion_scale * half_height ) ) ) topright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , random . randint ( 0 , int ( distortion_scale * half_height ) ) ) botright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , random . randint ( height - int ( distortion_scale * half_height ) - 1 , height - 1 ) ) botleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , random . randint ( height - int ( distortion_scale * half_height ) - 1 , height - 1 ) ) startpoints = [ ( 0 , 0 ) , ( width - 1 , 0 ) , ( width - 1 , height - 1 ) , ( 0 , height - 1 ) ] endpoints = [ topleft , topright , botright , botleft ] return startpoints , endpoints",Get parameters for perspective for a random perspective transform .
"def get_params ( img , scale , ratio ) : area = img . size [ 0 ] * img . size [ 1 ] for attempt in range ( 10 ) : target_area = random . uniform ( * scale ) * area log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) if w <= img . size [ 0 ] and h <= img . size [ 1 ] : i = random . randint ( 0 , img . size [ 1 ] - h ) j = random . randint ( 0 , img . size [ 0 ] - w ) return i , j , h , w in_ratio = img . size [ 0 ] / img . size [ 1 ] if ( in_ratio < min ( ratio ) ) : w = img . size [ 0 ] h = w / min ( ratio ) elif ( in_ratio > max ( ratio ) ) : h = img . size [ 1 ] w = h * max ( ratio ) else : w = img . size [ 0 ] h = img . size [ 1 ] i = ( img . size [ 1 ] - h ) // 2 j = ( img . size [ 0 ] - w ) // 2 return i , j , h , w",Get parameters for crop for a random sized crop .
"def get_params ( brightness , contrast , saturation , hue ) : transforms = [ ] if brightness is not None : brightness_factor = random . uniform ( brightness [ 0 ] , brightness [ 1 ] ) transforms . append ( Lambda ( lambda img : F . adjust_brightness ( img , brightness_factor ) ) ) if contrast is not None : contrast_factor = random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) transforms . append ( Lambda ( lambda img : F . adjust_contrast ( img , contrast_factor ) ) ) if saturation is not None : saturation_factor = random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) transforms . append ( Lambda ( lambda img : F . adjust_saturation ( img , saturation_factor ) ) ) if hue is not None : hue_factor = random . uniform ( hue [ 0 ] , hue [ 1 ] ) transforms . append ( Lambda ( lambda img : F . adjust_hue ( img , hue_factor ) ) ) random . shuffle ( transforms ) transform = Compose ( transforms ) return transform",Get a randomized transform to be applied on image .
"def get_params ( degrees , translate , scale_ranges , shears , img_size ) : angle = random . uniform ( degrees [ 0 ] , degrees [ 1 ] ) if translate is not None : max_dx = translate [ 0 ] * img_size [ 0 ] max_dy = translate [ 1 ] * img_size [ 1 ] translations = ( np . round ( random . uniform ( - max_dx , max_dx ) ) , np . round ( random . uniform ( - max_dy , max_dy ) ) ) else : translations = ( 0 , 0 ) if scale_ranges is not None : scale = random . uniform ( scale_ranges [ 0 ] , scale_ranges [ 1 ] ) else : scale = 1.0 if shears is not None : shear = random . uniform ( shears [ 0 ] , shears [ 1 ] ) else : shear = 0.0 return angle , translations , scale , shear",Get parameters for affine transformation
"def inception_v3 ( pretrained = False , * * kwargs ) : if pretrained : if 'transform_input' not in kwargs : kwargs [ 'transform_input' ] = True if 'aux_logits' in kwargs : original_aux_logits = kwargs [ 'aux_logits' ] kwargs [ 'aux_logits' ] = True else : original_aux_logits = True model = Inception3 ( * * kwargs ) model . load_state_dict ( model_zoo . load_url ( model_urls [ 'inception_v3_google' ] ) ) if not original_aux_logits : model . aux_logits = False del model . AuxLogits return model return Inception3 ( * * kwargs )",r Inception v3 model architecture from Rethinking the Inception Architecture for Computer Vision <http : // arxiv . org / abs / 1512 . 00567 > _ .
"def download ( self ) : import tarfile if self . _check_integrity ( ) : print ( 'Files already downloaded and verified' ) return download_url ( self . url , self . root , self . filename , self . md5_checksum ) with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : tar . extractall ( path = self . root ) with open ( os . path . join ( self . root , 'dataset' , 'SBU_captioned_photo_dataset_urls.txt' ) ) as fh : for line in fh : url = line . rstrip ( ) try : download_url ( url , os . path . join ( self . root , 'dataset' ) ) except OSError : pass",Download and extract the tarball and download each individual photo .
"def googlenet ( pretrained = False , * * kwargs ) : if pretrained : if 'transform_input' not in kwargs : kwargs [ 'transform_input' ] = True if 'aux_logits' not in kwargs : kwargs [ 'aux_logits' ] = False if kwargs [ 'aux_logits' ] : warnings . warn ( 'auxiliary heads in the pretrained googlenet model are NOT pretrained, ' 'so make sure to train them' ) original_aux_logits = kwargs [ 'aux_logits' ] kwargs [ 'aux_logits' ] = True kwargs [ 'init_weights' ] = False model = GoogLeNet ( * * kwargs ) model . load_state_dict ( model_zoo . load_url ( model_urls [ 'googlenet' ] ) ) if not original_aux_logits : model . aux_logits = False del model . aux1 , model . aux2 return model return GoogLeNet ( * * kwargs )",r GoogLeNet ( Inception v1 ) model architecture from Going Deeper with Convolutions <http : // arxiv . org / abs / 1409 . 4842 > _ .
"def download ( self ) : if self . _check_exists ( ) : return makedir_exist_ok ( self . raw_folder ) makedir_exist_ok ( self . processed_folder ) for url in self . urls : filename = url . rpartition ( '/' ) [ 2 ] file_path = os . path . join ( self . raw_folder , filename ) download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) self . extract_gzip ( gzip_path = file_path , remove_finished = True ) print ( 'Processing...' ) training_set = ( read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) ) test_set = ( read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) ) with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : torch . save ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : torch . save ( test_set , f ) print ( 'Done!' )",Download the MNIST data if it doesn t exist in processed_folder already .
"def download ( self ) : import shutil import zipfile if self . _check_exists ( ) : return makedir_exist_ok ( self . raw_folder ) makedir_exist_ok ( self . processed_folder ) filename = self . url . rpartition ( '/' ) [ 2 ] file_path = os . path . join ( self . raw_folder , filename ) download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) print ( 'Extracting zip archive' ) with zipfile . ZipFile ( file_path ) as zip_f : zip_f . extractall ( self . raw_folder ) os . unlink ( file_path ) gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) for gzip_file in os . listdir ( gzip_folder ) : if gzip_file . endswith ( '.gz' ) : self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) for split in self . splits : print ( 'Processing ' + split ) training_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) ) test_set = ( read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) ) with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : torch . save ( training_set , f ) with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : torch . save ( test_set , f ) shutil . rmtree ( gzip_folder ) print ( 'Done!' )",Download the EMNIST data if it doesn t exist in processed_folder already .
"def request ( method , url , * * kwargs ) : time_before_request = time ( ) session = SessionSinglePool ( ) kwargs [ 'proxies' ] = settings [ 'outgoing' ] . get ( 'proxies' ) or None if 'timeout' in kwargs : timeout = kwargs [ 'timeout' ] else : timeout = getattr ( threadLocal , 'timeout' , None ) if timeout is not None : kwargs [ 'timeout' ] = timeout response = session . request ( method = method , url = url , * * kwargs ) time_after_request = time ( ) if timeout is not None : timeout_overhead = 0.2 start_time = getattr ( threadLocal , 'start_time' , time_before_request ) search_duration = time_after_request - start_time if search_duration > timeout + timeout_overhead : raise requests . exceptions . Timeout ( response = response ) session . close ( ) if hasattr ( threadLocal , 'total_time' ) : threadLocal . total_time += time_after_request - time_before_request return response",same as requests / requests / api . py request ( ... )
"def get_current_theme_name ( override = None ) : if override and ( override in themes or override == '__common__' ) : return override theme_name = request . args . get ( 'theme' , request . preferences . get_value ( 'theme' ) ) if theme_name not in themes : theme_name = default_theme return theme_name",Returns theme name .
"def index ( ) : output_format = request . form . get ( 'format' , 'html' ) if output_format not in [ 'html' , 'csv' , 'json' , 'rss' ] : output_format = 'html' if request . form . get ( 'q' ) is None : if output_format == 'html' : return render ( 'index.html' , ) else : return index_error ( output_format , 'No query' ) , 400 search_query = None result_container = None try : search_query = get_search_query_from_webapp ( request . preferences , request . form ) search = SearchWithPlugins ( search_query , request . user_plugins , request ) result_container = search . search ( ) except Exception as e : logger . exception ( 'search error' ) if ( issubclass ( e . __class__ , SearxParameterException ) ) : return index_error ( output_format , e . message ) , 400 else : return index_error ( output_format , gettext ( 'search error' ) ) , 500 results = result_container . get_ordered_results ( ) number_of_results = result_container . results_number ( ) if number_of_results < result_container . results_length ( ) : number_of_results = 0 advanced_search = request . form . get ( 'advanced_search' , None ) for result in results : if output_format == 'html' : if 'content' in result and result [ 'content' ] : result [ 'content' ] = highlight_content ( escape ( result [ 'content' ] [ : 1024 ] ) , search_query . query ) result [ 'title' ] = highlight_content ( escape ( result [ 'title' ] or u'' ) , search_query . query ) else : if result . get ( 'content' ) : result [ 'content' ] = html_to_text ( result [ 'content' ] ) . strip ( ) result [ 'title' ] = ' ' . join ( html_to_text ( result [ 'title' ] ) . strip ( ) . split ( ) ) result [ 'pretty_url' ] = prettify_url ( result [ 'url' ] ) if 'publishedDate' in result : try : result [ 'pubdate' ] = result [ 'publishedDate' ] . strftime ( '%Y-%m-%d %H:%M:%S%z' ) except ValueError : result [ 'publishedDate' ] = None else : if result [ 'publishedDate' ] . replace ( tzinfo = None ) >= datetime . now ( ) - timedelta ( days = 1 ) : timedifference = datetime . now ( ) - result [ 'publishedDate' ] . replace ( tzinfo = None ) minutes = int ( ( timedifference . seconds / 60 ) % 60 ) hours = int ( timedifference . seconds / 60 / 60 ) if hours == 0 : result [ 'publishedDate' ] = gettext ( u'{minutes} minute(s) ago' ) . format ( minutes = minutes ) else : result [ 'publishedDate' ] = gettext ( u'{hours} hour(s), {minutes} minute(s) ago' ) . format ( hours = hours , minutes = minutes ) else : result [ 'publishedDate' ] = format_date ( result [ 'publishedDate' ] ) if output_format == 'json' : return Response ( json . dumps ( { 'query' : search_query . query . decode ( 'utf-8' ) , 'number_of_results' : number_of_results , 'results' : results , 'answers' : list ( result_container . answers ) , 'corrections' : list ( result_container . corrections ) , 'infoboxes' : result_container . infoboxes , 'suggestions' : list ( result_container . suggestions ) , 'unresponsive_engines' : list ( result_container . unresponsive_engines ) } , default = lambda item : list ( item ) if isinstance ( item , set ) else item ) , mimetype = 'application/json' ) elif output_format == 'csv' : csv = UnicodeWriter ( StringIO ( ) ) keys = ( 'title' , 'url' , 'content' , 'host' , 'engine' , 'score' ) csv . writerow ( keys ) for row in results : row [ 'host' ] = row [ 'parsed_url' ] . netloc csv . writerow ( [ row . get ( key , '' ) for key in keys ] ) csv . stream . seek ( 0 ) response = Response ( csv . stream . read ( ) , mimetype = 'application/csv' ) cont_disp = 'attachment;Filename=searx_-_{0}.csv' . format ( search_query . query ) response . headers . add ( 'Content-Disposition' , cont_disp ) return response elif output_format == 'rss' : response_rss = render ( 'opensearch_response_rss.xml' , results = results , q = request . form [ 'q' ] , number_of_results = number_of_results , base_url = get_base_url ( ) , override_theme = '__common__' , ) return Response ( response_rss , mimetype = 'text/xml' ) return render ( 'results.html' , results = results , q = request . form [ 'q' ] , selected_categories = search_query . categories , pageno = search_query . pageno , time_range = search_query . time_range , number_of_results = format_decimal ( number_of_results ) , advanced_search = advanced_search , suggestions = result_container . suggestions , answers = result_container . answers , corrections = result_container . corrections , infoboxes = result_container . infoboxes , paging = result_container . paging , unresponsive_engines = result_container . unresponsive_engines , current_language = match_language ( search_query . lang , LANGUAGE_CODES , fallback = settings [ 'search' ] [ 'language' ] ) , base_url = get_base_url ( ) , theme = get_current_theme_name ( ) , favicons = global_favicons [ themes . index ( get_current_theme_name ( ) ) ] )",Render index page .
"def autocompleter ( ) : disabled_engines = request . preferences . engines . get_disabled ( ) if PY3 : raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) else : raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) raw_text_query . parse_query ( ) if not raw_text_query . getSearchQuery ( ) : return '' , 400 completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) raw_results = searx_bang ( raw_text_query ) if len ( raw_results ) <= 3 and completer : language = request . preferences . get_value ( 'language' ) if not language or language == 'all' : language = 'en' else : language = language . split ( '-' ) [ 0 ] raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) results = [ ] for result in raw_results : raw_text_query . changeSearchQuery ( result ) results . append ( raw_text_query . getFullQuery ( ) ) if request . form . get ( 'format' ) == 'x-suggestions' : return Response ( json . dumps ( [ raw_text_query . query , results ] ) , mimetype = 'application/json' ) return Response ( json . dumps ( results ) , mimetype = 'application/json' )",Return autocompleter results
"def preferences ( ) : if request . method == 'POST' : resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) try : request . preferences . parse_form ( request . form ) except ValidationException : request . errors . append ( gettext ( 'Invalid settings, please edit your preferences' ) ) return resp return request . preferences . save ( resp ) image_proxy = request . preferences . get_value ( 'image_proxy' ) lang = request . preferences . get_value ( 'language' ) disabled_engines = request . preferences . engines . get_disabled ( ) allowed_plugins = request . preferences . plugins . get_enabled ( ) stats = { } for c in categories : for e in categories [ c ] : stats [ e . name ] = { 'time' : None , 'warn_timeout' : False , 'warn_time' : False } if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : stats [ e . name ] [ 'warn_timeout' ] = True stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True return render ( 'preferences.html' , locales = settings [ 'locales' ] , current_locale = get_locale ( ) , image_proxy = image_proxy , engines_by_category = categories , stats = stats , answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , disabled_engines = disabled_engines , autocomplete_backends = autocomplete_backends , shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , themes = themes , plugins = plugins , doi_resolvers = settings [ 'doi_resolvers' ] , current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , allowed_plugins = allowed_plugins , theme = get_current_theme_name ( ) , preferences_url_params = request . preferences . get_as_url_params ( ) , base_url = get_base_url ( ) , preferences = True )",Render preferences page && save user preferences
"def request ( query , params ) : offset = ( params [ 'pageno' ] - 1 ) params [ 'url' ] = search_url . format ( offset = offset , query = quote ( query ) ) return params",pre - request callback params<dict > : method : POST / GET headers : {} data : {} # if method == POST url : category : search category pageno : 1 # number of the requested page
"def response ( resp ) : results = [ ] dom = html . fromstring ( resp . text ) try : number_of_results_string = re . sub ( '[^0-9]' , '' , dom . xpath ( '//a[@class=""active"" and contains(@href,""/suchen/dudenonline"")]/span/text()' ) [ 0 ] ) results . append ( { 'number_of_results' : int ( number_of_results_string ) } ) except : logger . debug ( ""Couldn't read number of results."" ) pass for result in dom . xpath ( '//section[@class=""wide"" and not(contains(@style,""overflow:hidden""))]' ) : try : logger . debug ( ""running for %s"" % str ( result ) ) link = result . xpath ( './/h2/a' ) [ 0 ] url = link . attrib . get ( 'href' ) title = result . xpath ( 'string(.//h2/a)' ) content = extract_text ( result . xpath ( './/p' ) ) results . append ( { 'url' : url , 'title' : title , 'content' : content } ) except : logger . debug ( 'result parse error in:\n%s' , etree . tostring ( result , pretty_print = True ) ) continue return results",post - response callback resp : requests response object
def get_themes ( templates_path ) : themes = os . listdir ( templates_path ) if '__common__' in themes : themes . remove ( '__common__' ) return themes,Returns available themes list .
"def searx_bang ( full_query ) : if len ( full_query . getSearchQuery ( ) ) == 0 : return [ ] results = [ ] first_char = full_query . getSearchQuery ( ) [ 0 ] if first_char == '!' or first_char == '?' : if len ( full_query . getSearchQuery ( ) ) == 1 : results . append ( first_char + ""images"" ) results . append ( first_char + ""wikipedia"" ) results . append ( first_char + ""osm"" ) else : engine_query = full_query . getSearchQuery ( ) [ 1 : ] for categorie in categories : if categorie . startswith ( engine_query ) : results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) for engine in engines : if engine . startswith ( engine_query . replace ( '_' , ' ' ) ) : results . append ( first_char + '{engine}' . format ( engine = engine . replace ( ' ' , '_' ) ) ) for engine_shortcut in engine_shortcuts : if engine_shortcut . startswith ( engine_query ) : results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) elif first_char == ':' : if len ( full_query . getSearchQuery ( ) ) == 1 : results . append ( "":en"" ) results . append ( "":en_us"" ) results . append ( "":english"" ) results . append ( "":united_kingdom"" ) else : engine_query = full_query . getSearchQuery ( ) [ 1 : ] for lc in language_codes : lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) if lang_id . startswith ( engine_query ) : if len ( engine_query ) <= 2 : results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) else : results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) if country . startswith ( engine_query . replace ( '_' , ' ' ) ) : results . append ( u':{country}' . format ( country = country . replace ( ' ' , '_' ) ) ) result_set = set ( results ) for query_part in full_query . query_parts : if query_part in result_set : result_set . remove ( query_part ) return list ( result_set )",check if the searchQuery contain a bang and create fitting autocompleter results
"def response ( resp ) : json_resp = resp . text [ resp . text . find ( '\n' ) + 1 : resp . text . rfind ( '\n' ) - 2 ] results = [ ] try : conversion_rate = float ( json . loads ( json_resp ) [ 'conversion' ] [ 'converted-amount' ] ) except : return results answer = '{0} {1} = {2} {3}, 1 {1} ({5}) = {4} {3} ({6})' . format ( resp . search_params [ 'amount' ] , resp . search_params [ 'from' ] , resp . search_params [ 'amount' ] * conversion_rate , resp . search_params [ 'to' ] , conversion_rate , resp . search_params [ 'from_name' ] , resp . search_params [ 'to_name' ] , ) url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( resp . search_params [ 'from' ] . upper ( ) , resp . search_params [ 'to' ] ) results . append ( { 'answer' : answer , 'url' : url } ) return results",remove first and last lines to get only json
"def custom_gradient ( fx , gx , x , fx_gx_manually_stopped = False , name = None ) : def maybe_stop ( x ) : if fx_gx_manually_stopped : return x return tf . stop_gradient ( x ) with tf . compat . v1 . name_scope ( name , 'custom_gradient' , [ fx , gx , x ] ) : fx = tf . convert_to_tensor ( value = fx , name = 'fx' ) with tf . control_dependencies ( [ fx ] ) : if is_list_like ( x ) : x = [ identity ( x_ , name = 'x' ) for x_ in x ] else : x = [ identity ( x , name = 'x' ) ] if is_list_like ( gx ) : gx = [ identity ( gx_ , dtype = fx . dtype , name = 'gx' ) for gx_ in gx ] else : gx = [ identity ( gx , dtype = fx . dtype , name = 'gx' ) ] override_grad = [ ] for x_ , gx_ in zip ( x , gx ) : equal_shape = tf . compat . v1 . assert_equal ( tf . shape ( input = x_ ) , tf . shape ( input = gx_ ) , message = 'Each `x` must have the same shape as each `gx`.' ) with tf . control_dependencies ( [ equal_shape ] ) : zeros_like_x_ = x_ - tf . stop_gradient ( x_ ) override_grad . append ( tf . reduce_sum ( input_tensor = maybe_stop ( gx_ ) * zeros_like_x_ ) ) override_grad = sum ( override_grad ) override_grad /= tf . cast ( tf . size ( input = fx ) , dtype = fx . dtype . base_dtype ) return maybe_stop ( fx ) + override_grad",Embeds a custom gradient into a Tensor .
"def value_and_gradient ( f , xs , use_gradient_tape = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'value_and_gradient' , [ xs ] ) : is_xs_list_like = isinstance ( xs , ( tuple , list ) ) if not is_xs_list_like : xs = [ xs ] xs = [ tf . convert_to_tensor ( value = x , name = 'x{}' . format ( i ) ) for i , x in enumerate ( xs ) ] if tf . executing_eagerly ( ) or use_gradient_tape : with tf . GradientTape ( watch_accessed_variables = False ) as tape : for x in xs : tape . watch ( x ) y = f ( * xs ) dydx = tape . gradient ( y , xs ) else : y = f ( * xs ) dydx = tf . gradients ( ys = y , xs = xs ) if not is_xs_list_like : dydx = dydx [ 0 ] return y , dydx",Computes f ( * xs ) and its gradients wrt to * xs .
"def mvn ( * args , * * kwargs ) : return tfd . Independent ( tfd . Normal ( * args , * * kwargs ) , reinterpreted_batch_ndims = 1 )",Convenience function to efficiently construct a MultivariateNormalDiag .
"def eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) : rv_avg_effect = tfd . Normal ( loc = 0. , scale = 10. ) rv_avg_stddev = tfd . Normal ( loc = 5. , scale = 1. ) rv_school_effects_standard = mvn ( loc = tf . zeros_like ( school_effects_standard ) , scale = tf . ones_like ( school_effects_standard ) ) rv_treatment_effects = mvn ( loc = ( avg_effect + tf . exp ( avg_stddev ) * school_effects_standard ) , scale = treatment_stddevs ) return ( rv_avg_effect . log_prob ( avg_effect ) + rv_avg_stddev . log_prob ( avg_stddev ) + rv_school_effects_standard . log_prob ( school_effects_standard ) + rv_treatment_effects . log_prob ( treatment_effects ) )",Eight - schools joint log - prob .
"def benchmark_eight_schools_hmc ( num_results = int ( 5e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 , step_size = 0.4 ) : num_schools = 8 treatment_effects = tf . constant ( [ 28 , 8 , - 3 , 7 , - 1 , 1 , 18 , 12 ] , dtype = np . float32 , name = 'treatment_effects' ) treatment_stddevs = tf . constant ( [ 15 , 10 , 16 , 11 , 9 , 11 , 10 , 18 ] , dtype = np . float32 , name = 'treatment_stddevs' ) def unnormalized_posterior_log_prob ( avg_effect , avg_stddev , school_effects_standard ) : """"""Eight-schools unnormalized log posterior."""""" return eight_schools_joint_log_prob ( treatment_effects , treatment_stddevs , avg_effect , avg_stddev , school_effects_standard ) if tf . executing_eagerly ( ) : sample_chain = tf . function ( tfp . mcmc . sample_chain ) else : sample_chain = tfp . mcmc . sample_chain def computation ( ) : """"""The benchmark computation."""""" _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = ( tf . zeros ( [ ] , name = 'init_avg_effect' ) , tf . zeros ( [ ] , name = 'init_avg_stddev' ) , tf . ones ( [ num_schools ] , name = 'init_school_effects_standard' ) , ) , kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_posterior_log_prob , step_size = step_size , num_leapfrog_steps = num_leapfrog_steps ) ) return kernel_results . is_accepted is_accepted_tensor = computation ( ) if not tf . executing_eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( is_accepted_tensor ) start_time = time . time ( ) if tf . executing_eagerly ( ) : is_accepted = computation ( ) else : is_accepted = session . run ( is_accepted_tensor ) wall_time = time . time ( ) - start_time num_accepted = np . sum ( is_accepted ) acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time )",Runs HMC on the eight - schools unnormalized posterior .
"def expand_docstring ( * * kwargs ) : def _fn_wrapped ( fn ) : """"""Original function with modified `__doc__` attribute."""""" doc = inspect . cleandoc ( fn . __doc__ ) for k , v in six . iteritems ( kwargs ) : pattern = r'\$\{' + str ( k ) + r'\}' doc = re . sub ( pattern , lambda match : v , doc ) fn . __doc__ = doc return fn return _fn_wrapped",Decorator to programmatically expand the docstring .
def _simple_name ( distribution ) : simple_name = distribution . name if simple_name . endswith ( '/' ) : simple_name = simple_name . split ( '/' ) [ - 2 ] parts = simple_name . split ( '_' ) if parts [ - 1 ] . isdigit ( ) : simple_name = '_' . join ( parts [ : - 1 ] ) return simple_name,Infer the original name passed into a distribution constructor .
"def _build_custom_rv ( distribution , sample_shape , value , name ) : del name return RandomVariable ( distribution = distribution , sample_shape = sample_shape , value = value )",RandomVariable constructor with a dummy name argument .
"def as_random_variable ( distribution , sample_shape = ( ) , value = None ) : return _build_custom_rv ( distribution = distribution , sample_shape = sample_shape , value = value , name = _simple_name ( distribution ) )",Wrap an existing distribution as a traceable random variable .
"def _make_random_variable ( distribution_cls ) : @ interceptable @ functools . wraps ( distribution_cls , assigned = ( '__module__' , '__name__' ) ) @ docstring_util . expand_docstring ( cls = distribution_cls . __name__ , doc = inspect . cleandoc ( distribution_cls . __init__ . __doc__ or '' ) ) def func ( * args , * * kwargs ) : """"""Create a random variable for ${cls}.

    See ${cls} for more details.

    Returns:
      RandomVariable.

    #### Original Docstring for Distribution

    ${doc}
    """""" sample_shape = kwargs . pop ( 'sample_shape' , ( ) ) value = kwargs . pop ( 'value' , None ) return RandomVariable ( distribution = distribution_cls ( * args , * * kwargs ) , sample_shape = sample_shape , value = value ) return func",Factory function to make random variable given distribution class .
"def _mode_mean_shape ( self ) : shape = tensorshape_util . concatenate ( self . batch_shape , self . event_shape ) has_static_shape = tensorshape_util . is_fully_defined ( shape ) if not has_static_shape : shape = tf . concat ( [ self . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , 0 ) return shape",Shape for the mode / mean Tensors .
"def one_step_predictive ( model , observed_time_series , parameter_samples ) : with tf . compat . v1 . name_scope ( 'one_step_predictive' , values = [ observed_time_series , parameter_samples ] ) : [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] lgssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) ( _ , _ , _ , _ , _ , observation_means , observation_covs ) = lgssm . forward_filter ( observed_time_series , mask = is_missing ) return sts_util . mix_over_posterior_draws ( means = observation_means [ ... , 0 ] , variances = observation_covs [ ... , 0 , 0 ] )",Compute one - step - ahead predictive distributions for all timesteps .
"def forecast ( model , observed_time_series , parameter_samples , num_steps_forecast ) : with tf . compat . v1 . name_scope ( 'forecast' , values = [ observed_time_series , parameter_samples , num_steps_forecast ] ) : [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) num_observed_steps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] observed_data_ssm = model . make_state_space_model ( num_timesteps = num_observed_steps , param_vals = parameter_samples ) ( _ , _ , _ , predictive_means , predictive_covs , _ , _ ) = observed_data_ssm . forward_filter ( observed_time_series , mask = mask ) parameter_samples = model . _canonicalize_param_vals_as_map ( parameter_samples ) parameter_samples_with_reordered_batch_dimension = { param . name : dist_util . move_dimension ( parameter_samples [ param . name ] , 0 , - ( 1 + _prefer_static_event_ndims ( param . prior ) ) ) for param in model . parameters } forecast_prior = tfd . MultivariateNormalFullCovariance ( loc = dist_util . move_dimension ( predictive_means [ ... , - 1 , : ] , 0 , - 2 ) , covariance_matrix = dist_util . move_dimension ( predictive_covs [ ... , - 1 , : , : ] , 0 , - 3 ) ) kwargs = { } if hasattr ( model , 'constant_offset' ) : kwargs [ 'constant_offset' ] = tf . convert_to_tensor ( value = model . constant_offset , dtype = forecast_prior . dtype ) [ ... , tf . newaxis ] forecast_ssm = model . _make_state_space_model ( num_timesteps = num_steps_forecast , param_map = parameter_samples_with_reordered_batch_dimension , initial_state_prior = forecast_prior , initial_step = num_observed_steps , * * kwargs ) num_posterior_draws = dist_util . prefer_static_value ( forecast_ssm . batch_shape_tensor ( ) ) [ - 1 ] return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = forecast_ssm . dtype ) ) , components_distribution = forecast_ssm )",Construct predictive distribution over future observations .
"def _max_mask_non_finite ( x , axis = - 1 , keepdims = False , mask = 0 ) : m = np . max ( x , axis = _astuple ( axis ) , keepdims = keepdims ) needs_masking = ~ np . isfinite ( m ) if needs_masking . ndim > 0 : m [ needs_masking ] = mask elif needs_masking : m = mask return m",Returns max or mask if max is not finite .
"def _reduce_logsumexp ( input_tensor , axis = None , keepdims = False , name = None ) : try : return scipy_special . logsumexp ( input_tensor , axis = _astuple ( axis ) , keepdims = keepdims ) except NotImplementedError : m = _max_mask_non_finite ( input_tensor , axis = axis , keepdims = True ) y = input_tensor - m y = np . exp ( y , out = y ) return m + np . log ( np . sum ( y , axis = _astuple ( axis ) , keepdims = keepdims ) )",Computes log ( sum ( exp ( input_tensor ))) along the specified axis .
"def assert_finite ( x , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_finite' ) : x_ = tf . get_static_value ( x ) if x_ is not None : if ~ np . all ( np . isfinite ( x_ ) ) : raise ValueError ( message ) return x assertion = tf . compat . v1 . assert_equal ( tf . math . is_finite ( x ) , tf . ones_like ( x , tf . bool ) , data = data , summarize = summarize , message = message ) with tf . control_dependencies ( [ assertion ] ) : return tf . identity ( x )",Assert all elements of x are finite .
"def assert_rank_at_most ( x , rank , data = None , summarize = None , message = None , name = None ) : with tf . compat . v2 . name_scope ( name or 'assert_rank_at_most' ) : return tf . compat . v1 . assert_less_equal ( tf . rank ( x ) , rank , data = data , summarize = summarize , message = message )",Assert x has rank equal to rank or smaller .
"def _event_size ( event_shape , name = None ) : with tf . compat . v1 . name_scope ( name , 'event_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , dtype = tf . int32 , name = 'event_shape' ) event_shape_const = tf . get_static_value ( event_shape ) if event_shape_const is not None : return np . prod ( event_shape_const ) else : return tf . reduce_prod ( input_tensor = event_shape )",Computes the number of elements in a tensor with shape event_shape .
"def _eval_all_one_hot ( fn , dist , name = None ) : with tf . compat . v1 . name_scope ( name , 'eval_all_one_hot' ) : event_size = dist . event_shape_tensor ( ) [ - 1 ] batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) x = tf . reshape ( tf . eye ( event_size , dtype = dist . dtype ) , shape = tf . pad ( tensor = tf . ones ( batch_ndims , tf . int32 ) , paddings = [ [ 1 , 1 ] ] , constant_values = event_size ) ) perm = tf . pad ( tensor = tf . range ( 1 , batch_ndims + 1 ) , paddings = [ [ 0 , 1 ] ] ) return tf . transpose ( a = fn ( dist , x ) , perm = perm )",OneHotCategorical helper computing probs cdf etc over its support .
"def _make_kl_divergence_fn ( distribution_b , use_exact_kl = False , test_points_reduce_axis = ( ) , test_points_fn = tf . convert_to_tensor , weight = None ) : if use_exact_kl is None : kl_divergence_fn = tfd . kl_divergence else : def kl_divergence_fn ( distribution_a , distribution_b ) : z = test_points_fn ( distribution_a ) return tf . reduce_mean ( input_tensor = distribution_a . log_prob ( z ) - distribution_b . log_prob ( z ) , axis = test_points_reduce_axis ) def _fn ( distribution_a ) : """"""Closure that computes KLDiv as a function of `a` as in `KL[a, b]`."""""" with tf . compat . v1 . name_scope ( 'kldivergence_loss' ) : distribution_b_ = ( distribution_b ( ) if callable ( distribution_b ) else distribution_b ) kl = kl_divergence_fn ( distribution_a , distribution_b_ ) if weight is not None : kl = tf . cast ( weight , dtype = kl . dtype ) * kl return tf . reduce_sum ( input_tensor = kl , name = 'batch_total_kl_divergence' ) return _fn",Creates a callable computing KL [ a b ] from a a tfd . Distribution .
"def _get_convert_to_tensor_fn ( identifier ) : if identifier is None : return None if isinstance ( identifier , six . string_types ) : identifier = str ( identifier ) return _deserialize ( identifier ) if isinstance ( identifier , dict ) : return _deserialize ( identifier ) if isinstance ( identifier , property ) : identifier = identifier . fget if callable ( identifier ) : return identifier raise ValueError ( 'Could not interpret ' 'convert-to-tensor function identifier:' , identifier )",Return a convert - to - tensor func given a name config callable etc .
"def get_config ( self ) : config = { 'make_distribution_fn' : _serialize_function ( self . _make_distribution_fn ) , 'convert_to_tensor_fn' : _serialize ( self . _convert_to_tensor_fn ) , } base_config = super ( DistributionLambda , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) )",Returns the config of this layer .
"def new ( params , event_size , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'MultivariateNormalTriL' , [ params , event_size ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) scale_tril = tfb . ScaleTriL ( diag_shift = np . array ( 1e-5 , params . dtype . as_numpy_dtype ( ) ) , validate_args = validate_args ) return tfd . MultivariateNormalTriL ( loc = params [ ... , : event_size ] , scale_tril = scale_tril ( params [ ... , event_size : ] ) , validate_args = validate_args )",Create the distribution instance from a params vector .
"def params_size ( event_size , name = None ) : with tf . compat . v1 . name_scope ( name , 'MultivariateNormalTriL_params_size' , [ event_size ] ) : return event_size + event_size * ( event_size + 1 ) // 2",The number of params needed to create a single distribution .
"def new ( params , event_size , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'OneHotCategorical' , [ params , event_size ] ) : return tfd . OneHotCategorical ( logits = params , dtype = dtype or params . dtype . base_dtype , validate_args = validate_args )",Create the distribution instance from a params vector .
"def new ( params , event_size , num_components , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'CategoricalMixtureOfOneHotCategorical' , [ params , event_size , num_components ] ) : dist = MixtureSameFamily . new ( params , num_components , OneHotCategorical ( event_size , validate_args = False , name = name ) , validate_args = validate_args , name = name ) dist . _mean = functools . partial ( _eval_all_one_hot , tfd . Distribution . prob , dist ) dist . log_mean = functools . partial ( _eval_all_one_hot , tfd . Distribution . log_prob , dist ) return dist",Create the distribution instance from a params vector .
"def params_size ( event_size , num_components , name = None ) : with tf . compat . v1 . name_scope ( name , 'CategoricalMixtureOfOneHotCategorical_params_size' , [ event_size , num_components ] ) : return MixtureSameFamily . params_size ( num_components , OneHotCategorical . params_size ( event_size , name = name ) , name = name )",The number of params needed to create a single distribution .
"def new ( params , event_shape = ( ) , dtype = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentBernoulli' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) new_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) dist = tfd . Independent ( tfd . Bernoulli ( logits = tf . reshape ( params , new_shape ) , dtype = dtype or params . dtype . base_dtype , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args ) dist . _logits = dist . distribution . _logits dist . _probs = dist . distribution . _probs dist . logits = tfd . Bernoulli . logits dist . probs = tfd . Bernoulli . probs return dist",Create the distribution instance from a params vector .
"def get_config ( self ) : config = { 'event_shape' : self . _event_shape , 'convert_to_tensor_fn' : _serialize ( self . _convert_to_tensor_fn ) , 'sample_dtype' : self . _sample_dtype , 'validate_args' : self . _validate_args } base_config = super ( IndependentBernoulli , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) )",Returns the config of this layer .
"def new ( params , event_shape = ( ) , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentLogistic' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) output_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) loc_params , scale_params = tf . split ( params , 2 , axis = - 1 ) return tfd . Independent ( tfd . Logistic ( loc = tf . reshape ( loc_params , output_shape ) , scale = tf . math . softplus ( tf . reshape ( scale_params , output_shape ) ) , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args )",Create the distribution instance from a params vector .
"def params_size ( event_shape = ( ) , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentNormal_params_size' , [ event_shape ] ) : event_shape = tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) return 2 * _event_size ( event_shape , name = name or 'IndependentNormal_params_size' )",The number of params needed to create a single distribution .
"def new ( params , event_shape = ( ) , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'IndependentPoisson' , [ params , event_shape ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) event_shape = dist_util . expand_to_vector ( tf . convert_to_tensor ( value = event_shape , name = 'event_shape' , dtype_hint = tf . int32 ) , tensor_name = 'event_shape' ) output_shape = tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , event_shape , ] , axis = 0 ) return tfd . Independent ( tfd . Poisson ( log_rate = tf . reshape ( params , output_shape ) , validate_args = validate_args ) , reinterpreted_batch_ndims = tf . size ( input = event_shape ) , validate_args = validate_args )",Create the distribution instance from a params vector .
"def new ( params , num_components , component_layer , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily' , [ params , num_components , component_layer ] ) : params = tf . convert_to_tensor ( value = params , name = 'params' ) num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) components_dist = component_layer ( tf . reshape ( params [ ... , num_components : ] , tf . concat ( [ tf . shape ( input = params ) [ : - 1 ] , [ num_components , - 1 ] ] , axis = 0 ) ) ) mixture_dist = tfd . Categorical ( logits = params [ ... , : num_components ] ) return tfd . MixtureSameFamily ( mixture_dist , components_dist , validate_args = False )",Create the distribution instance from a params vector .
"def params_size ( num_components , component_params_size , name = None ) : with tf . compat . v1 . name_scope ( name , 'MixtureSameFamily_params_size' , [ num_components , component_params_size ] ) : num_components = tf . convert_to_tensor ( value = num_components , name = 'num_components' , dtype_hint = tf . int32 ) component_params_size = tf . convert_to_tensor ( value = component_params_size , name = 'component_params_size' ) num_components = dist_util . prefer_static_value ( num_components ) component_params_size = dist_util . prefer_static_value ( component_params_size ) return num_components + num_components * component_params_size",Number of params needed to create a MixtureSameFamily distribution .
"def params_size ( num_components , event_shape = ( ) , name = None ) : return MixtureSameFamily . params_size ( num_components , IndependentNormal . params_size ( event_shape , name = name ) , name = name )",The number of params needed to create a single distribution .
"def new ( params , num_components , event_shape = ( ) , validate_args = False , name = None ) : return MixtureSameFamily . new ( params , num_components , IndependentLogistic ( event_shape , validate_args = validate_args , name = name ) , validate_args = validate_args , name = name )",Create the distribution instance from a params vector .
"def params_size ( num_components , event_shape = ( ) , name = None ) : return MixtureSameFamily . params_size ( num_components , IndependentLogistic . params_size ( event_shape , name = name ) , name = name )",The number of params needed to create a single distribution .
def get_next_interceptor ( ) : try : interceptor = _interceptor_stack . stack . pop ( ) yield interceptor finally : _interceptor_stack . stack . append ( interceptor ),Yields the top - most interceptor on the thread - local interceptor stack .
"def interceptable ( func ) : @ functools . wraps ( func ) def func_wrapped ( * args , * * kwargs ) : with get_next_interceptor ( ) as interceptor : return interceptor ( func , * args , * * kwargs ) return func_wrapped",Decorator that wraps func so that its execution is intercepted .
"def tape ( ) : tape_data = collections . OrderedDict ( { } ) def record ( f , * args , * * kwargs ) : """"""Records execution to a tape."""""" name = kwargs . get ( ""name"" ) output = interceptable ( f ) ( * args , * * kwargs ) if name : tape_data [ name ] = output return output with interception ( record ) : yield tape_data",Context manager for recording interceptable executions onto a tape .
"def toy_logistic_data ( num_examples , input_size = 2 , weights_prior_stddev = 5.0 ) : random_weights = weights_prior_stddev * np . random . randn ( input_size ) random_bias = np . random . randn ( ) design_matrix = np . random . rand ( num_examples , input_size ) * 2 - 1 logits = np . reshape ( np . dot ( design_matrix , random_weights ) + random_bias , ( - 1 , 1 ) ) p_labels = 1. / ( 1 + np . exp ( - logits ) ) labels = np . int32 ( p_labels > np . random . rand ( num_examples , 1 ) ) return random_weights , random_bias , np . float32 ( design_matrix ) , labels",Generates synthetic data for binary classification .
"def visualize_decision ( features , labels , true_w_b , candidate_w_bs , fname ) : fig = figure . Figure ( figsize = ( 6 , 6 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) ax = fig . add_subplot ( 1 , 1 , 1 ) ax . scatter ( features [ : , 0 ] , features [ : , 1 ] , c = np . float32 ( labels [ : , 0 ] ) , cmap = cm . get_cmap ( ""binary"" ) , edgecolors = ""k"" ) def plot_weights ( w , b , * * kwargs ) : w1 , w2 = w x1s = np . linspace ( - 1 , 1 , 100 ) x2s = - ( w1 * x1s + b ) / w2 ax . plot ( x1s , x2s , * * kwargs ) for w , b in candidate_w_bs : plot_weights ( w , b , alpha = 1. / np . sqrt ( len ( candidate_w_bs ) ) , lw = 1 , color = ""blue"" ) if true_w_b is not None : plot_weights ( * true_w_b , lw = 4 , color = ""green"" , label = ""true separator"" ) ax . set_xlim ( [ - 1.5 , 1.5 ] ) ax . set_ylim ( [ - 1.5 , 1.5 ] ) ax . legend ( ) canvas . print_figure ( fname , format = ""png"" ) print ( ""saved {}"" . format ( fname ) )",Utility method to visualize decision boundaries in R^2 .
"def build_input_pipeline ( x , y , batch_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( ( x , y ) ) training_batches = training_dataset . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) batch_features , batch_labels = training_iterator . get_next ( ) return batch_features , batch_labels",Build a Dataset iterator for supervised classification .
"def _maybe_check_valid_map_values ( map_values , validate_args ) : assertions = [ ] message = 'Rank of map_values must be 1.' if tensorshape_util . rank ( map_values . shape ) is not None : if tensorshape_util . rank ( map_values . shape ) != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_rank ( map_values , 1 , message = message ) ) message = 'Size of map_values must be greater than 0.' if tensorshape_util . num_elements ( map_values . shape ) is not None : if tensorshape_util . num_elements ( map_values . shape ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( assert_util . assert_greater ( tf . size ( input = map_values ) , 0 , message = message ) ) if validate_args : assertions . append ( assert_util . assert_equal ( tf . math . is_strictly_increasing ( map_values ) , True , message = 'map_values is not strictly increasing.' ) ) return assertions",Validate map_values if validate_args == True .
"def trace ( state : State , fn : TransitionOperator , num_steps : IntTensor , trace_fn : Callable [ [ State , TensorNest ] , TensorNest ] ) -> Tuple [ State , TensorNest ] : def fn_wrapper ( args , _ ) : return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( fn , args [ 0 ] ) ) def trace_fn_wrapper ( args ) : return tf . nest . map_structure ( tf . convert_to_tensor , call_fn ( trace_fn , args ) ) state = call_fn ( fn , state ) first_trace = trace_fn_wrapper ( state ) state , full_trace = mcmc_util . trace_scan ( fn_wrapper , state , tf . ones ( num_steps - 1 ) , trace_fn = trace_fn_wrapper ) prepend = lambda x , y : tf . concat ( [ tf . convert_to_tensor ( value = x ) [ tf . newaxis ] , y ] , 0 ) return state , tf . nest . map_structure ( prepend , first_trace , full_trace )",TransitionOperator that runs fn repeatedly and traces its outputs .
"def call_fn ( fn : TransitionOperator , args : Union [ Tuple [ Any ] , Any ] ) -> Any : if isinstance ( args , ( list , tuple ) ) and not mcmc_util . is_namedtuple_like ( args ) : args = args return fn ( * args ) else : return fn ( args )",Calls a transition operator with args unpacking args if its a sequence .
"def call_and_grads ( fn : TransitionOperator , args : Union [ Tuple [ Any ] , Any ] ) -> Tuple [ tf . Tensor , TensorNest , TensorNest ] : with tf . GradientTape ( ) as tape : tape . watch ( args ) ret , extra = call_fn ( fn , args ) grads = tape . gradient ( ret , args ) return ret , extra , grads",Calls fn and returns the gradients with respect to fn s first output .
"def maybe_broadcast_structure ( from_structure : Any , to_structure : Any ) -> Any : flat_from = tf . nest . flatten ( from_structure ) flat_to = tf . nest . flatten ( to_structure ) if len ( flat_from ) == 1 : flat_from *= len ( flat_to ) return tf . nest . pack_sequence_as ( to_structure , flat_from )",Maybe broadcasts from_structure to to_structure .
"def transform_log_prob_fn ( log_prob_fn : PotentialFn , bijector : BijectorNest , init_state : State = None ) -> Union [ PotentialFn , Tuple [ PotentialFn , State ] ] : def wrapper ( * args ) : """"""Transformed wrapper."""""" bijector_ = bijector args = tf . nest . map_structure ( lambda x : 0. + x , args ) if len ( args ) == 1 : args = args [ 0 ] elif isinstance ( bijector_ , list ) : bijector_ = tuple ( bijector_ ) original_space_args = tf . nest . map_structure ( lambda b , x : b . forward ( x ) , bijector_ , args ) original_space_args = original_space_args original_space_log_prob , extra = call_fn ( log_prob_fn , original_space_args ) event_ndims = tf . nest . map_structure ( lambda x : tf . rank ( x ) - tf . rank ( original_space_log_prob ) , args ) return original_space_log_prob + sum ( tf . nest . flatten ( tf . nest . map_structure ( lambda b , x , e : b . forward_log_det_jacobian ( x , event_ndims = e ) , bijector_ , args , event_ndims ) ) ) , [ original_space_args , extra ] if init_state is None : return wrapper else : return wrapper , tf . nest . map_structure ( lambda b , s : b . inverse ( s ) , bijector , init_state )",Transforms a log - prob function using a bijector .
"def leapfrog_step ( leapfrog_step_state : LeapFrogStepState , step_size : FloatTensor , target_log_prob_fn : PotentialFn , kinetic_energy_fn : PotentialFn ) -> Tuple [ LeapFrogStepState , LeapFrogStepExtras ] : state = leapfrog_step_state . state state_grads = leapfrog_step_state . state_grads momentum = leapfrog_step_state . momentum step_size = maybe_broadcast_structure ( step_size , state ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) momentum = tf . nest . map_structure ( tf . convert_to_tensor , momentum ) state = tf . nest . map_structure ( tf . convert_to_tensor , state ) if state_grads is None : _ , _ , state_grads = call_and_grads ( target_log_prob_fn , state ) else : state_grads = tf . nest . map_structure ( tf . convert_to_tensor , state_grads ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) kinetic_energy , kinetic_energy_extra , momentum_grads = call_and_grads ( kinetic_energy_fn , momentum ) state = tf . nest . map_structure ( lambda x , mg , s : x + mg * s , state , momentum_grads , step_size ) target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) momentum = tf . nest . map_structure ( lambda m , sg , s : m + 0.5 * sg * s , momentum , state_grads , step_size ) return LeapFrogStepState ( state , state_grads , momentum ) , LeapFrogStepExtras ( target_log_prob , state_extra , kinetic_energy , kinetic_energy_extra )",Leapfrog TransitionOperator .
"def metropolis_hastings_step ( current_state : State , proposed_state : State , energy_change : FloatTensor , seed = None ) -> Tuple [ State , tf . Tensor , tf . Tensor ] : flat_current = tf . nest . flatten ( current_state ) flat_proposed = nest . flatten_up_to ( current_state , proposed_state ) flat_current = [ p if c is None else c for p , c in zip ( flat_proposed , flat_current ) ] current_state = tf . nest . pack_sequence_as ( current_state , flat_current ) current_state = tf . nest . map_structure ( tf . convert_to_tensor , current_state ) proposed_state = tf . nest . map_structure ( tf . convert_to_tensor , proposed_state ) energy_change = tf . convert_to_tensor ( value = energy_change ) log_accept_ratio = - energy_change log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( input = log_accept_ratio ) , dtype = log_accept_ratio . dtype . base_dtype , seed = seed ) ) is_accepted = log_uniform < log_accept_ratio next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) return next_state , is_accepted , log_uniform",Metropolis - Hastings step .
"def hamiltonian_monte_carlo ( hmc_state : HamiltonianMonteCarloState , target_log_prob_fn : PotentialFn , step_size : Any , num_leapfrog_steps : IntTensor , momentum : State = None , kinetic_energy_fn : PotentialFn = None , momentum_sample_fn : MomentumSampleFn = None , leapfrog_trace_fn : Callable [ [ LeapFrogStepState , LeapFrogStepExtras ] , TensorNest ] = lambda * args : ( ) , seed = None , ) -> Tuple [ HamiltonianMonteCarloState , HamiltonianMonteCarloExtra ] : state = hmc_state . state state_grads = hmc_state . state_grads target_log_prob = hmc_state . target_log_prob state_extra = hmc_state . state_extra if kinetic_energy_fn is None : def kinetic_energy_fn ( * momentum ) : return tf . add_n ( [ tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = - 1 ) / 2. for x in tf . nest . flatten ( momentum ) ] ) , ( ) if momentum_sample_fn is None : def momentum_sample_fn ( * momentum ) : ret = tf . nest . map_structure ( lambda x : tf . random . normal ( tf . shape ( input = x ) , dtype = x . dtype ) , momentum ) if len ( ret ) == 1 : return ret [ 0 ] else : return ret if momentum is None : momentum = call_fn ( momentum_sample_fn , tf . nest . map_structure ( tf . zeros_like , state ) ) if target_log_prob is None : target_log_prob , state_extra , state_grads = call_and_grads ( target_log_prob_fn , state ) kinetic_energy , _ = call_fn ( kinetic_energy_fn , momentum ) current_energy = - target_log_prob + kinetic_energy current_state = HamiltonianMonteCarloState ( state = state , state_grads = state_grads , state_extra = state_extra , target_log_prob = target_log_prob ) def leapfrog_wrapper ( leapfrog_state , target_log_prob , state_extra ) : """"""Leapfrog wrapper that tracks extra state."""""" del target_log_prob del state_extra leapfrog_state , leapfrog_extra = leapfrog_step ( leapfrog_state , step_size = step_size , target_log_prob_fn = target_log_prob_fn , kinetic_energy_fn = kinetic_energy_fn ) return [ leapfrog_state , leapfrog_extra . target_log_prob , leapfrog_extra . state_extra ] , leapfrog_extra def leapfrog_trace_wrapper_fn ( args , leapfrog_extra ) : return leapfrog_trace_fn ( args [ 0 ] , leapfrog_extra ) leapfrog_wrapper_state = ( LeapFrogStepState ( state , state_grads , momentum ) , target_log_prob , state_extra ) [ [ leapfrog_state , target_log_prob , state_extra ] , _ ] , leapfrog_trace = trace ( leapfrog_wrapper_state , leapfrog_wrapper , num_leapfrog_steps , trace_fn = leapfrog_trace_wrapper_fn ) kinetic_energy , _ = call_fn ( kinetic_energy_fn , leapfrog_state . momentum ) proposed_energy = - target_log_prob + kinetic_energy proposed_state = HamiltonianMonteCarloState ( state = leapfrog_state . state , state_grads = leapfrog_state . state_grads , target_log_prob = target_log_prob , state_extra = state_extra ) energy_change = proposed_energy - current_energy hmc_state , is_accepted , _ = metropolis_hastings_step ( current_state , proposed_state , energy_change , seed = seed ) hmc_state = hmc_state return hmc_state , HamiltonianMonteCarloExtra ( is_accepted = is_accepted , proposed_hmc_state = proposed_state , log_accept_ratio = - energy_change , leapfrog_trace = leapfrog_trace )",Hamiltonian Monte Carlo TransitionOperator .
"def sign_adaptation ( control : FloatNest , output : FloatTensor , set_point : FloatTensor , adaptation_rate : FloatTensor = 0.01 ) -> FloatNest : def _get_new_control ( control , output , set_point ) : new_control = mcmc_util . choose ( output > set_point , control * ( 1. + adaptation_rate ) , control / ( 1. + adaptation_rate ) ) return new_control output = maybe_broadcast_structure ( output , control ) set_point = maybe_broadcast_structure ( set_point , control ) return tf . nest . map_structure ( _get_new_control , control , output , set_point )",A function to do simple sign - based control of a variable .
"def compute_output_shape ( self , input_shape ) : input_shape = tf . TensorShape ( input_shape ) . as_list ( ) if self . data_format == 'channels_last' : space = input_shape [ 1 : - 1 ] new_space = [ ] for i in range ( len ( space ) ) : new_dim = tf_layers_util . conv_output_length ( space [ i ] , self . kernel_size [ i ] , padding = self . padding , stride = self . strides [ i ] , dilation = self . dilation_rate [ i ] ) new_space . append ( new_dim ) return tf . TensorShape ( [ input_shape [ 0 ] ] + new_space + [ self . filters ] ) else : space = input_shape [ 2 : ] new_space = [ ] for i in range ( len ( space ) ) : new_dim = tf_layers_util . conv_output_length ( space [ i ] , self . kernel_size [ i ] , padding = self . padding , stride = self . strides [ i ] , dilation = self . dilation_rate [ i ] ) new_space . append ( new_dim ) return tf . TensorShape ( [ input_shape [ 0 ] , self . filters ] + new_space )",Computes the output shape of the layer .
"def get_config ( self ) : config = { 'filters' : self . filters , 'kernel_size' : self . kernel_size , 'strides' : self . strides , 'padding' : self . padding , 'data_format' : self . data_format , 'dilation_rate' : self . dilation_rate , 'activation' : ( tf . keras . activations . serialize ( self . activation ) if self . activation else None ) , 'activity_regularizer' : tf . keras . initializers . serialize ( self . activity_regularizer ) , } function_keys = [ 'kernel_posterior_fn' , 'kernel_posterior_tensor_fn' , 'kernel_prior_fn' , 'kernel_divergence_fn' , 'bias_posterior_fn' , 'bias_posterior_tensor_fn' , 'bias_prior_fn' , 'bias_divergence_fn' , ] for function_key in function_keys : function = getattr ( self , function_key ) if function is None : function_name = None function_type = None else : function_name , function_type = tfp_layers_util . serialize_function ( function ) config [ function_key ] = function_name config [ function_key + '_type' ] = function_type base_config = super ( _ConvVariational , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) )",Returns the config of the layer .
"def from_config ( cls , config ) : config = config . copy ( ) function_keys = [ 'kernel_posterior_fn' , 'kernel_posterior_tensor_fn' , 'kernel_prior_fn' , 'kernel_divergence_fn' , 'bias_posterior_fn' , 'bias_posterior_tensor_fn' , 'bias_prior_fn' , 'bias_divergence_fn' , ] for function_key in function_keys : serial = config [ function_key ] function_type = config . pop ( function_key + '_type' ) if serial is not None : config [ function_key ] = tfp_layers_util . deserialize_function ( serial , function_type = function_type ) return cls ( * * config )",Creates a layer from its config .
"def get_config ( self ) : config = { 'seed' : self . seed , } base_config = super ( _ConvFlipout , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) )",Returns the config of the layer .
"def _as_tensor ( x , name , dtype ) : return None if x is None else tf . convert_to_tensor ( value = x , name = name , dtype = dtype )",Convenience to convert to Tensor or leave as None .
"def _create_scale_operator ( self , identity_multiplier , diag , tril , perturb_diag , perturb_factor , shift , validate_args , dtype ) : identity_multiplier = _as_tensor ( identity_multiplier , ""identity_multiplier"" , dtype ) diag = _as_tensor ( diag , ""diag"" , dtype ) tril = _as_tensor ( tril , ""tril"" , dtype ) perturb_diag = _as_tensor ( perturb_diag , ""perturb_diag"" , dtype ) perturb_factor = _as_tensor ( perturb_factor , ""perturb_factor"" , dtype ) shape_hint = None if perturb_factor is not None : shape_hint = distribution_util . dimension_size ( perturb_factor , axis = - 2 ) if self . _is_only_identity_multiplier : if validate_args : return distribution_util . with_dependencies ( [ assert_util . assert_none_equal ( identity_multiplier , tf . zeros ( [ ] , identity_multiplier . dtype ) , [ ""identity_multiplier should be non-zero."" ] ) ] , identity_multiplier ) return identity_multiplier scale = distribution_util . make_tril_scale ( loc = shift , scale_tril = tril , scale_diag = diag , scale_identity_multiplier = identity_multiplier , validate_args = validate_args , assert_positive = False , shape_hint = shape_hint ) if perturb_factor is not None : return tf . linalg . LinearOperatorLowRankUpdate ( scale , u = perturb_factor , diag_update = perturb_diag , is_diag_update_positive = perturb_diag is None , is_non_singular = True , is_self_adjoint = True , is_positive_definite = True , is_square = True ) return scale",Construct scale from various components .
"def random_walk_normal_fn ( scale = 1. , name = None ) : def _fn ( state_parts , seed ) : """"""Adds a normal perturbation to the input state.

    Args:
      state_parts: A list of `Tensor`s of any shape and real dtype representing
        the state parts of the `current_state` of the Markov chain.
      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
        applied.
        Default value: `None`.

    Returns:
      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same
        shape and type as the `state_parts`.

    Raises:
      ValueError: if `scale` does not broadcast with `state_parts`.
    """""" with tf . compat . v1 . name_scope ( name , 'random_walk_normal_fn' , values = [ state_parts , scale , seed ] ) : scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] if len ( scales ) == 1 : scales *= len ( state_parts ) if len ( state_parts ) != len ( scales ) : raise ValueError ( '`scale` must broadcast with `state_parts`.' ) seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkNormalFn' ) next_state_parts = [ tf . random . normal ( mean = state_part , stddev = scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] return next_state_parts return _fn",Returns a callable that adds a random normal perturbation to the input .
"def random_walk_uniform_fn ( scale = 1. , name = None ) : def _fn ( state_parts , seed ) : """"""Adds a uniform perturbation to the input state.

    Args:
      state_parts: A list of `Tensor`s of any shape and real dtype representing
        the state parts of the `current_state` of the Markov chain.
      seed: `int` or None. The random seed for this `Op`. If `None`, no seed is
        applied.
        Default value: `None`.

    Returns:
      perturbed_state_parts: A Python `list` of The `Tensor`s. Has the same
        shape and type as the `state_parts`.

    Raises:
      ValueError: if `scale` does not broadcast with `state_parts`.
    """""" with tf . compat . v1 . name_scope ( name , 'random_walk_uniform_fn' , values = [ state_parts , scale , seed ] ) : scales = scale if mcmc_util . is_list_like ( scale ) else [ scale ] if len ( scales ) == 1 : scales *= len ( state_parts ) if len ( state_parts ) != len ( scales ) : raise ValueError ( '`scale` must broadcast with `state_parts`.' ) seed_stream = distributions . SeedStream ( seed , salt = 'RandomWalkUniformFn' ) next_state_parts = [ tf . random . uniform ( minval = state_part - scale_part , maxval = state_part + scale_part , shape = tf . shape ( input = state_part ) , dtype = state_part . dtype . base_dtype , seed = seed_stream ( ) ) for scale_part , state_part in zip ( scales , state_parts ) ] return next_state_parts return _fn",Returns a callable that adds a random uniform perturbation to the input .
"def _kl_independent ( a , b , name = ""kl_independent"" ) : p = a . distribution q = b . distribution if ( tensorshape_util . is_fully_defined ( a . event_shape ) and tensorshape_util . is_fully_defined ( b . event_shape ) ) : if a . event_shape == b . event_shape : if p . event_shape == q . event_shape : num_reduce_dims = ( tensorshape_util . rank ( a . event_shape ) - tensorshape_util . rank ( p . event_shape ) ) reduce_dims = [ - i - 1 for i in range ( 0 , num_reduce_dims ) ] return tf . reduce_sum ( input_tensor = kullback_leibler . kl_divergence ( p , q , name = name ) , axis = reduce_dims ) else : raise NotImplementedError ( ""KL between Independents with different "" ""event shapes not supported."" ) else : raise ValueError ( ""Event shapes do not match."" ) else : with tf . control_dependencies ( [ assert_util . assert_equal ( a . event_shape_tensor ( ) , b . event_shape_tensor ( ) ) , assert_util . assert_equal ( p . event_shape_tensor ( ) , q . event_shape_tensor ( ) ) ] ) : num_reduce_dims = ( prefer_static . rank_from_shape ( a . event_shape_tensor , a . event_shape ) - prefer_static . rank_from_shape ( p . event_shape_tensor , a . event_shape ) ) reduce_dims = prefer_static . range ( - num_reduce_dims - 1 , - 1 , 1 ) return tf . reduce_sum ( input_tensor = kullback_leibler . kl_divergence ( p , q , name = name ) , axis = reduce_dims )",Batched KL divergence KL ( a || b ) for Independent distributions .
"def _get_default_reinterpreted_batch_ndims ( self , distribution ) : ndims = prefer_static . rank_from_shape ( distribution . batch_shape_tensor , distribution . batch_shape ) return prefer_static . maximum ( 0 , ndims - 1 )",Computes the default value for reinterpreted_batch_ndim __init__ arg .
"def _expand_to_event_rank ( self , x ) : expanded_x = x for _ in range ( tensorshape_util . rank ( self . event_shape ) ) : expanded_x = tf . expand_dims ( expanded_x , - 1 ) return expanded_x",Expand the rank of x up to static_event_rank times for broadcasting .
"def entropy_lower_bound ( self , name = ""entropy_lower_bound"" ) : with self . _name_scope ( name ) : with tf . control_dependencies ( self . _assertions ) : distribution_entropies = [ d . entropy ( ) for d in self . components ] cat_probs = self . _cat_probs ( log_probs = False ) partial_entropies = [ c_p * m for ( c_p , m ) in zip ( cat_probs , distribution_entropies ) ] return tf . add_n ( partial_entropies )",r A lower bound on the entropy of this mixture model .
"def _cat_probs ( self , log_probs ) : which_softmax = tf . nn . log_softmax if log_probs else tf . nn . softmax cat_probs = which_softmax ( self . cat . logits ) cat_probs = tf . unstack ( cat_probs , num = self . num_components , axis = - 1 ) return cat_probs",Get a list of num_components batchwise probabilities .
"def _maybe_validate_args ( outcomes , logits , probs , validate_args ) : assertions = [ ] def validate_equal_last_dim ( tensor_a , tensor_b , message ) : if tensor_a . shape . is_fully_defined ( ) and tensor_b . shape . is_fully_defined ( ) : if tensor_a . shape [ - 1 ] != tensor_b . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = tensor_a ) [ - 1 ] , tf . shape ( input = tensor_b ) [ - 1 ] , message = message ) ) if logits is not None : validate_equal_last_dim ( outcomes , logits , message = 'Last dimension of outcomes and logits must be equal size.' ) if probs is not None : validate_equal_last_dim ( outcomes , probs , message = 'Last dimension of outcomes and probs must be equal size.' ) message = 'Rank of outcomes must be 1.' if outcomes . shape . ndims is not None : if outcomes . shape . ndims != 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( outcomes , 1 , message = message ) ) message = 'Size of outcomes must be greater than 0.' if outcomes . shape . num_elements ( ) is not None : if outcomes . shape . num_elements ( ) == 0 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_greater ( tf . size ( input = outcomes ) , 0 , message = message ) ) if validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . math . is_strictly_increasing ( outcomes ) , True , message = 'outcomes is not strictly increasing.' ) ) return assertions",Validate outcomes logits and probs s shapes .
"def _ensure_tf_install ( ) : try : import tensorflow as tf except ImportError : print ( ""\n\nFailed to import TensorFlow. Please note that TensorFlow is not "" ""installed by default when you install TensorFlow Probability. This "" ""is so that users can decide whether to install the GPU-enabled "" ""TensorFlow package. To use TensorFlow Probability, please install "" ""the most recent version of TensorFlow, by following instructions at "" ""https://tensorflow.org/install.\n\n"" ) raise import distutils . version required_tensorflow_version = ""1.13"" if ( distutils . version . LooseVersion ( tf . __version__ ) < distutils . version . LooseVersion ( required_tensorflow_version ) ) : raise ImportError ( ""This version of TensorFlow Probability requires TensorFlow "" ""version >= {required}; Detected an installation of version {present}. "" ""Please upgrade TensorFlow to proceed."" . format ( required = required_tensorflow_version , present = tf . __version__ ) )",Attempt to import tensorflow and ensure its version is sufficient .
"def logistic_regression ( features ) : coeffs = ed . MultivariateNormalDiag ( loc = tf . zeros ( features . shape [ 1 ] ) , name = ""coeffs"" ) labels = ed . Bernoulli ( logits = tf . tensordot ( features , coeffs , [ [ 1 ] , [ 0 ] ] ) , name = ""labels"" ) return labels",Bayesian logistic regression which returns labels given features .
"def covertype ( ) : import sklearn . datasets data = sklearn . datasets . covtype . fetch_covtype ( ) features = data . data labels = data . target features -= features . mean ( 0 ) features /= features . std ( 0 ) features = np . hstack ( [ features , np . ones ( [ features . shape [ 0 ] , 1 ] ) ] ) features = tf . cast ( features , dtype = tf . float32 ) _ , counts = np . unique ( labels , return_counts = True ) specific_category = np . argmax ( counts ) labels = ( labels == specific_category ) labels = tf . cast ( labels , dtype = tf . int32 ) return features , labels",Builds the Covertype data set .
"def _kl_dirichlet_dirichlet ( d1 , d2 , name = None ) : with tf . name_scope ( name or ""kl_dirichlet_dirichlet"" ) : digamma_sum_d1 = tf . math . digamma ( tf . reduce_sum ( input_tensor = d1 . concentration , axis = - 1 , keepdims = True ) ) digamma_diff = tf . math . digamma ( d1 . concentration ) - digamma_sum_d1 concentration_diff = d1 . concentration - d2 . concentration return ( tf . reduce_sum ( input_tensor = concentration_diff * digamma_diff , axis = - 1 ) - tf . math . lbeta ( d1 . concentration ) + tf . math . lbeta ( d2 . concentration ) )",Batchwise KL divergence KL ( d1 || d2 ) with d1 and d2 Dirichlet .
"def _maybe_assert_valid_concentration ( self , concentration , validate_args ) : if not validate_args : return concentration return distribution_util . with_dependencies ( [ assert_util . assert_positive ( concentration , message = ""Concentration parameter must be positive."" ) , assert_util . assert_rank_at_least ( concentration , 1 , message = ""Concentration parameter must have >=1 dimensions."" ) , assert_util . assert_less ( 1 , tf . shape ( input = concentration ) [ - 1 ] , message = ""Concentration parameter must have event_size >= 2."" ) , ] , concentration )",Checks the validity of the concentration parameter .
"def _maybe_assert_valid_sample ( self , x ) : if not self . validate_args : return x return distribution_util . with_dependencies ( [ assert_util . assert_positive ( x , message = ""samples must be positive"" ) , assert_util . assert_near ( tf . ones ( [ ] , dtype = self . dtype ) , tf . reduce_sum ( input_tensor = x , axis = - 1 ) , message = ""sample last-dimension must sum to `1`"" ) , ] , x )",Checks the validity of a sample .
"def auto_correlation ( x , axis = - 1 , max_lags = None , center = True , normalize = True , name = 'auto_correlation' ) : with tf . compat . v1 . name_scope ( name , values = [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) rank = util . prefer_static_rank ( x ) if axis < 0 : axis = rank + axis shift = rank - 1 - axis x_rotated = util . rotate_transpose ( x , shift ) if center : x_rotated -= tf . reduce_mean ( input_tensor = x_rotated , axis = - 1 , keepdims = True ) x_len = util . prefer_static_shape ( x_rotated ) [ - 1 ] x_len_float64 = tf . cast ( x_len , np . float64 ) target_length = tf . pow ( np . float64 ( 2. ) , tf . math . ceil ( tf . math . log ( x_len_float64 * 2 ) / np . log ( 2. ) ) ) pad_length = tf . cast ( target_length - x_len_float64 , np . int32 ) x_rotated_pad = util . pad ( x_rotated , axis = - 1 , back = True , count = pad_length ) dtype = x . dtype if not dtype . is_complex : if not dtype . is_floating : raise TypeError ( 'Argument x must have either float or complex dtype' ' found: {}' . format ( dtype ) ) x_rotated_pad = tf . complex ( x_rotated_pad , dtype . real_dtype . as_numpy_dtype ( 0. ) ) fft_x_rotated_pad = tf . signal . fft ( x_rotated_pad ) spectral_density = fft_x_rotated_pad * tf . math . conj ( fft_x_rotated_pad ) shifted_product = tf . signal . ifft ( spectral_density ) shifted_product = tf . cast ( shifted_product , dtype ) know_static_shape = True if not x_rotated . shape . is_fully_defined ( ) : know_static_shape = False if max_lags is None : max_lags = x_len - 1 else : max_lags = tf . convert_to_tensor ( value = max_lags , name = 'max_lags' ) max_lags_ = tf . get_static_value ( max_lags ) if max_lags_ is None or not know_static_shape : know_static_shape = False max_lags = tf . minimum ( x_len - 1 , max_lags ) else : max_lags = min ( x_len - 1 , max_lags_ ) shifted_product_chopped = shifted_product [ ... , : max_lags + 1 ] if know_static_shape : chopped_shape = x_rotated . shape . as_list ( ) chopped_shape [ - 1 ] = min ( x_len , max_lags + 1 ) shifted_product_chopped . set_shape ( chopped_shape ) x_len = tf . cast ( x_len , dtype . real_dtype ) max_lags = tf . cast ( max_lags , dtype . real_dtype ) denominator = x_len - tf . range ( 0. , max_lags + 1. ) denominator = tf . cast ( denominator , dtype ) shifted_product_rotated = shifted_product_chopped / denominator if normalize : shifted_product_rotated /= shifted_product_rotated [ ... , : 1 ] return util . rotate_transpose ( shifted_product_rotated , - shift )",Auto correlation along one axis .
"def cholesky_covariance ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'cholesky_covariance' , values = [ x , sample_axis ] ) : sample_axis = tf . convert_to_tensor ( value = sample_axis , dtype = tf . int32 ) cov = covariance ( x , sample_axis = sample_axis , event_axis = - 1 , keepdims = keepdims ) return tf . linalg . cholesky ( cov )",Cholesky factor of the covariance matrix of vector - variate random samples .
"def covariance ( x , y = None , sample_axis = 0 , event_axis = - 1 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'covariance' , values = [ x , y , event_axis , sample_axis ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) x -= tf . reduce_mean ( input_tensor = x , axis = sample_axis , keepdims = True ) if y is None : y = x else : y = tf . convert_to_tensor ( value = y , name = 'y' , dtype = x . dtype ) x . shape . assert_is_compatible_with ( y . shape ) y -= tf . reduce_mean ( input_tensor = y , axis = sample_axis , keepdims = True ) if event_axis is None : return tf . reduce_mean ( input_tensor = x * tf . math . conj ( y ) , axis = sample_axis , keepdims = keepdims ) if sample_axis is None : raise ValueError ( 'sample_axis was None, which means all axis hold events, and this ' 'overlaps with event_axis ({})' . format ( event_axis ) ) event_axis = _make_positive_axis ( event_axis , tf . rank ( x ) ) sample_axis = _make_positive_axis ( sample_axis , tf . rank ( x ) ) if _is_list_like ( event_axis ) and _is_list_like ( sample_axis ) : if set ( event_axis ) . intersection ( sample_axis ) : raise ValueError ( 'sample_axis ({}) and event_axis ({}) overlapped' . format ( sample_axis , event_axis ) ) if ( np . diff ( sorted ( event_axis ) ) > 1 ) . any ( ) : raise ValueError ( 'event_axis must be contiguous. Found: {}' . format ( event_axis ) ) batch_axis = list ( sorted ( set ( range ( x . shape . ndims ) ) . difference ( sample_axis + event_axis ) ) ) else : batch_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , tf . concat ( ( sample_axis , event_axis ) , 0 ) ) event_axis = tf . convert_to_tensor ( value = event_axis , name = 'event_axis' , dtype = tf . int32 ) sample_axis = tf . convert_to_tensor ( value = sample_axis , name = 'sample_axis' , dtype = tf . int32 ) batch_axis = tf . convert_to_tensor ( value = batch_axis , name = 'batch_axis' , dtype = tf . int32 ) perm_for_xy = tf . concat ( ( batch_axis , event_axis , sample_axis ) , 0 ) x_permed = tf . transpose ( a = x , perm = perm_for_xy ) y_permed = tf . transpose ( a = y , perm = perm_for_xy ) batch_ndims = tf . size ( input = batch_axis ) batch_shape = tf . shape ( input = x_permed ) [ : batch_ndims ] event_ndims = tf . size ( input = event_axis ) event_shape = tf . shape ( input = x_permed ) [ batch_ndims : batch_ndims + event_ndims ] sample_shape = tf . shape ( input = x_permed ) [ batch_ndims + event_ndims : ] sample_ndims = tf . size ( input = sample_shape ) n_samples = tf . reduce_prod ( input_tensor = sample_shape ) n_events = tf . reduce_prod ( input_tensor = event_shape ) x_permed_flat = tf . reshape ( x_permed , tf . concat ( ( batch_shape , event_shape , [ n_samples ] ) , 0 ) ) y_permed_flat = tf . reshape ( y_permed , tf . concat ( ( batch_shape , event_shape , [ n_samples ] ) , 0 ) ) x_permed_flat = tf . reshape ( x_permed , tf . concat ( ( batch_shape , [ n_events ] , [ n_samples ] ) , 0 ) ) y_permed_flat = tf . reshape ( y_permed , tf . concat ( ( batch_shape , [ n_events ] , [ n_samples ] ) , 0 ) ) cov = tf . matmul ( x_permed_flat , y_permed_flat , adjoint_b = True ) / tf . cast ( n_samples , x . dtype ) cov = tf . reshape ( cov , tf . concat ( ( batch_shape , event_shape ** 2 , tf . ones ( [ sample_ndims ] , tf . int32 ) ) , 0 ) ) cov = tf . transpose ( a = cov , perm = tf . math . invert_permutation ( perm_for_xy ) ) e_start = event_axis [ 0 ] e_len = 1 + event_axis [ - 1 ] - event_axis [ 0 ] cov = tf . reshape ( cov , tf . concat ( ( tf . shape ( input = cov ) [ : e_start ] , event_shape , event_shape , tf . shape ( input = cov ) [ e_start + e_len : ] ) , 0 ) ) if not keepdims : squeeze_axis = tf . where ( sample_axis < e_start , sample_axis , sample_axis + e_len ) cov = _squeeze ( cov , axis = squeeze_axis ) return cov",Sample covariance between observations indexed by event_axis .
"def correlation ( x , y = None , sample_axis = 0 , event_axis = - 1 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'correlation' , values = [ x , y , event_axis , sample_axis ] ) : x /= stddev ( x , sample_axis = sample_axis , keepdims = True ) if y is not None : y /= stddev ( y , sample_axis = sample_axis , keepdims = True ) return covariance ( x = x , y = y , event_axis = event_axis , sample_axis = sample_axis , keepdims = keepdims )",Sample correlation ( Pearson ) between observations indexed by event_axis .
"def stddev ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'stddev' , values = [ x , sample_axis ] ) : return tf . sqrt ( variance ( x , sample_axis = sample_axis , keepdims = keepdims ) )",Estimate standard deviation using samples .
"def variance ( x , sample_axis = 0 , keepdims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'variance' , values = [ x , sample_axis ] ) : return covariance ( x , y = None , sample_axis = sample_axis , event_axis = None , keepdims = keepdims )",Estimate variance using samples .
"def _make_list_or_1d_tensor ( values ) : values = tf . convert_to_tensor ( value = values , name = 'values' ) values_ = tf . get_static_value ( values ) if values_ is None : return values + tf . zeros ( [ 1 ] , dtype = values . dtype ) if values_ . ndim > 1 : raise ValueError ( 'values had > 1 dim: {}' . format ( values_ . shape ) ) values_ = values_ + np . zeros ( [ 1 ] , dtype = values_ . dtype ) return list ( values_ )",Return a list ( preferred ) or 1d Tensor from values if values . ndims < 2 .
"def _make_positive_axis ( axis , ndims ) : axis = _make_list_or_1d_tensor ( axis ) ndims = tf . convert_to_tensor ( value = ndims , name = 'ndims' , dtype = tf . int32 ) ndims_ = tf . get_static_value ( ndims ) if _is_list_like ( axis ) and ndims_ is not None : positive_axis = [ ] for a in axis : if a < 0 : a = ndims_ + a positive_axis . append ( a ) else : axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) positive_axis = tf . where ( axis >= 0 , axis , axis + ndims ) return positive_axis",Rectify possibly negatively axis . Prefer return Python list .
"def _squeeze ( x , axis ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if axis is None : return tf . squeeze ( x , axis = None ) axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis += tf . zeros ( [ 1 ] , dtype = axis . dtype ) keep_axis , _ = tf . compat . v1 . setdiff1d ( tf . range ( 0 , tf . rank ( x ) ) , axis ) return tf . reshape ( x , tf . gather ( tf . shape ( input = x ) , keep_axis ) )",A version of squeeze that works with dynamic axis .
"def _kl_normal_normal ( n_a , n_b , name = None ) : with tf . name_scope ( name or ""kl_normal_normal"" ) : one = tf . constant ( 1 , dtype = n_a . dtype ) two = tf . constant ( 2 , dtype = n_a . dtype ) half = tf . constant ( 0.5 , dtype = n_a . dtype ) s_a_squared = tf . square ( n_a . scale ) s_b_squared = tf . square ( n_b . scale ) ratio = s_a_squared / s_b_squared return ( tf . square ( n_a . loc - n_b . loc ) / ( two * s_b_squared ) + half * ( ratio - one - tf . math . log ( ratio ) ) )",Calculate the batched KL divergence KL ( n_a || n_b ) with n_a and n_b Normal .
"def _z ( self , x ) : with tf . name_scope ( ""standardize"" ) : return ( x - self . loc ) / self . scale",Standardize input x to a unit normal .
"def _inv_z ( self , z ) : with tf . name_scope ( ""reconstruct"" ) : return z * self . scale + self . loc",Reconstruct input x from a its normalized version .
"def semilocal_linear_trend_transition_matrix ( autoregressive_coef ) : fixed_entries = tf . constant ( [ [ 1. , 1. ] , [ 0. , 0. ] ] , dtype = autoregressive_coef . dtype ) autoregressive_coef_mask = tf . constant ( [ [ 0. , 0. ] , [ 0. , 1. ] ] , dtype = autoregressive_coef . dtype ) bottom_right_entry = ( autoregressive_coef [ ... , tf . newaxis , tf . newaxis ] * autoregressive_coef_mask ) return tf . linalg . LinearOperatorFullMatrix ( fixed_entries + bottom_right_entry )",Build the transition matrix for a semi - local linear trend model .
"def semilocal_linear_trend_transition_noise ( level_scale , slope_mean , slope_scale , autoregressive_coef ) : broadcast_batch_shape = dist_util . get_broadcast_shape ( level_scale , slope_mean , slope_scale , autoregressive_coef ) broadcast_ones = tf . ones ( broadcast_batch_shape , dtype = level_scale . dtype ) scale_diag = tf . stack ( [ level_scale * broadcast_ones , slope_scale * broadcast_ones ] , axis = - 1 ) bias = tf . stack ( [ tf . zeros_like ( broadcast_ones ) , slope_mean * ( 1 - autoregressive_coef ) * broadcast_ones ] , axis = - 1 ) return tfd . MultivariateNormalDiag ( loc = bias , scale_diag = scale_diag )",Build the transition noise model for a semi - local linear trend model .
"def sample_halton_sequence ( dim , num_results = None , sequence_indices = None , dtype = tf . float32 , randomized = True , seed = None , name = None ) : if dim < 1 or dim > _MAX_DIMENSION : raise ValueError ( 'Dimension must be between 1 and {}. Supplied {}' . format ( _MAX_DIMENSION , dim ) ) if ( num_results is None ) == ( sequence_indices is None ) : raise ValueError ( 'Either `num_results` or `sequence_indices` must be' ' specified but not both.' ) if not dtype . is_floating : raise ValueError ( 'dtype must be of `float`-type' ) with tf . compat . v1 . name_scope ( name , 'sample' , values = [ num_results , sequence_indices ] ) : if num_results is not None : num_results = tf . convert_to_tensor ( value = num_results ) if sequence_indices is not None : sequence_indices = tf . convert_to_tensor ( value = sequence_indices ) indices = _get_indices ( num_results , sequence_indices , dtype ) radixes = tf . constant ( _PRIMES [ 0 : dim ] , dtype = dtype , shape = [ dim , 1 ] ) max_sizes_by_axes = _base_expansion_size ( tf . reduce_max ( input_tensor = indices ) , radixes ) max_size = tf . reduce_max ( input_tensor = max_sizes_by_axes ) exponents_by_axes = tf . tile ( [ tf . range ( max_size ) ] , [ dim , 1 ] ) weight_mask = exponents_by_axes >= max_sizes_by_axes capped_exponents = tf . where ( weight_mask , tf . zeros_like ( exponents_by_axes ) , exponents_by_axes ) weights = radixes ** capped_exponents coeffs = tf . math . floordiv ( indices , weights ) coeffs *= 1. - tf . cast ( weight_mask , dtype ) coeffs %= radixes if not randomized : coeffs /= radixes return tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence' ) coeffs = _randomize ( coeffs , radixes , seed = stream ( ) ) coeffs *= 1. - tf . cast ( weight_mask , dtype ) coeffs /= radixes base_values = tf . reduce_sum ( input_tensor = coeffs / weights , axis = - 1 ) zero_correction = tf . random . uniform ( [ dim , 1 ] , seed = stream ( ) , dtype = dtype ) zero_correction /= radixes ** max_sizes_by_axes return base_values + tf . reshape ( zero_correction , [ - 1 ] )",r Returns a sample from the dim dimensional Halton sequence .
"def _randomize ( coeffs , radixes , seed = None ) : given_dtype = coeffs . dtype coeffs = tf . cast ( coeffs , dtype = tf . int32 ) num_coeffs = tf . shape ( input = coeffs ) [ - 1 ] radixes = tf . reshape ( tf . cast ( radixes , dtype = tf . int32 ) , shape = [ - 1 ] ) stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence2' ) perms = _get_permutations ( num_coeffs , radixes , seed = stream ( ) ) perms = tf . reshape ( perms , shape = [ - 1 ] ) radix_sum = tf . reduce_sum ( input_tensor = radixes ) radix_offsets = tf . reshape ( tf . cumsum ( radixes , exclusive = True ) , shape = [ - 1 , 1 ] ) offsets = radix_offsets + tf . range ( num_coeffs ) * radix_sum permuted_coeffs = tf . gather ( perms , coeffs + offsets ) return tf . cast ( permuted_coeffs , dtype = given_dtype )",Applies the Owen ( 2017 ) randomization to the coefficients .
"def _get_permutations ( num_results , dims , seed = None ) : sample_range = tf . range ( num_results ) stream = distributions . SeedStream ( seed , salt = 'MCMCSampleHaltonSequence3' ) def generate_one ( d ) : seed = stream ( ) fn = lambda _ : tf . random . shuffle ( tf . range ( d ) , seed = seed ) return tf . map_fn ( fn , sample_range , parallel_iterations = 1 if seed is not None else 10 ) return tf . concat ( [ generate_one ( d ) for d in tf . unstack ( dims ) ] , axis = - 1 )",Uniform iid sample from the space of permutations .
"def _get_indices ( num_results , sequence_indices , dtype , name = None ) : with tf . compat . v1 . name_scope ( name , '_get_indices' , [ num_results , sequence_indices ] ) : if sequence_indices is None : num_results = tf . cast ( num_results , dtype = dtype ) sequence_indices = tf . range ( num_results , dtype = dtype ) else : sequence_indices = tf . cast ( sequence_indices , dtype ) indices = sequence_indices + 1 return tf . reshape ( indices , [ - 1 , 1 , 1 ] )",Generates starting points for the Halton sequence procedure .
"def _base_expansion_size ( num , bases ) : return tf . floor ( tf . math . log ( num ) / tf . math . log ( bases ) ) + 1",Computes the number of terms in the place value expansion .
"def _primes_less_than ( n ) : small_primes = np . array ( ( 2 , 3 , 5 ) ) if n <= 6 : return small_primes [ small_primes < n ] sieve = np . ones ( n // 3 + ( n % 6 == 2 ) , dtype = np . bool ) sieve [ 0 ] = False m = int ( n ** 0.5 ) // 3 + 1 for i in range ( m ) : if not sieve [ i ] : continue k = 3 * i + 1 | 1 sieve [ k ** 2 // 3 : : 2 * k ] = False sieve [ ( k ** 2 + 4 * k - 2 * k * ( i & 1 ) ) // 3 : : 2 * k ] = False return np . r_ [ 2 , 3 , 3 * np . nonzero ( sieve ) [ 0 ] + 1 | 1 ]",Returns sorted array of primes such that 2 < = prime < n .
"def _machine_eps ( dtype ) : if isinstance ( dtype , tf . DType ) : dtype = dtype . as_numpy_dtype ( ) return np . finfo ( dtype ) . eps",Returns the machine epsilon for the supplied dtype .
"def hager_zhang ( value_and_gradients_function , initial_step_size = None , value_at_initial_step = None , value_at_zero = None , converged = None , threshold_use_approximate_wolfe_condition = 1e-6 , shrinkage_param = 0.66 , expansion_param = 5.0 , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , step_size_shrink_param = 0.1 , max_iterations = 50 , name = None ) : with tf . compat . v1 . name_scope ( name , 'hager_zhang' , [ initial_step_size , value_at_initial_step , value_at_zero , converged , threshold_use_approximate_wolfe_condition , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ] ) : val_0 , val_initial , f_lim , prepare_evals = _prepare_args ( value_and_gradients_function , initial_step_size , value_at_initial_step , value_at_zero , threshold_use_approximate_wolfe_condition ) valid_inputs = ( hzl . is_finite ( val_0 ) & ( val_0 . df < 0 ) & tf . math . is_finite ( val_initial . x ) & ( val_initial . x > 0 ) ) if converged is None : init_converged = tf . zeros_like ( valid_inputs ) else : init_converged = tf . convert_to_tensor ( value = converged ) failed = ~ init_converged & ~ valid_inputs active = ~ init_converged & valid_inputs fix_step_evals , val_c , fix_failed = _fix_step_size ( value_and_gradients_function , val_initial , active , step_size_shrink_param ) init_interval = HagerZhangLineSearchResult ( converged = init_converged , failed = failed | fix_failed , func_evals = prepare_evals + fix_step_evals , iterations = tf . convert_to_tensor ( value = 0 ) , left = val_0 , right = hzl . val_where ( init_converged , val_0 , val_c ) ) def _apply_bracket_and_search ( ) : """"""Bracketing and searching to do for valid inputs."""""" return _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) init_active = ~ init_interval . failed & ~ init_interval . converged return prefer_static . cond ( tf . reduce_any ( input_tensor = init_active ) , _apply_bracket_and_search , lambda : init_interval )",The Hager Zhang line search algorithm .
"def _fix_step_size ( value_and_gradients_function , val_c_input , active , step_size_shrink_param ) : iter_max = np . ceil ( - np . log2 ( _machine_eps ( val_c_input . x . dtype ) ) ) def _cond ( i , val_c , to_fix ) : del val_c return ( i < iter_max ) & tf . reduce_any ( input_tensor = to_fix ) def _body ( i , val_c , to_fix ) : next_c = tf . where ( to_fix , val_c . x * step_size_shrink_param , val_c . x ) next_val_c = value_and_gradients_function ( next_c ) still_to_fix = to_fix & ~ hzl . is_finite ( next_val_c ) return ( i + 1 , next_val_c , still_to_fix ) to_fix = active & ~ hzl . is_finite ( val_c_input ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = ( 0 , val_c_input , to_fix ) )",Shrinks the input step size until the value and grad become finite .
"def _bracket_and_search ( value_and_gradients_function , init_interval , f_lim , max_iterations , shrinkage_param , expansion_param , sufficient_decrease_param , curvature_param ) : bracket_result = hzl . bracket ( value_and_gradients_function , init_interval , f_lim , max_iterations , expansion_param ) converged = init_interval . converged | _very_close ( bracket_result . left . x , bracket_result . right . x ) exhausted_iterations = ~ converged & tf . greater_equal ( bracket_result . iteration , max_iterations ) line_search_args = HagerZhangLineSearchResult ( converged = converged , failed = bracket_result . failed | exhausted_iterations , iterations = bracket_result . iteration , func_evals = bracket_result . num_evals , left = bracket_result . left , right = bracket_result . right ) return _line_search_after_bracketing ( value_and_gradients_function , line_search_args , init_interval . left , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param )",Brackets the minimum and performs a line search .
"def _line_search_after_bracketing ( value_and_gradients_function , search_interval , val_0 , f_lim , max_iterations , sufficient_decrease_param , curvature_param , shrinkage_param ) : def _loop_cond ( curr_interval ) : """"""Loop condition."""""" active = ~ ( curr_interval . converged | curr_interval . failed ) return ( curr_interval . iterations < max_iterations ) & tf . reduce_any ( input_tensor = active ) def _loop_body ( curr_interval ) : """"""The loop body."""""" secant2_raw_result = hzl . secant2 ( value_and_gradients_function , val_0 , curr_interval , f_lim , sufficient_decrease_param , curvature_param ) secant2_result = HagerZhangLineSearchResult ( converged = secant2_raw_result . converged , failed = secant2_raw_result . failed , iterations = curr_interval . iterations + 1 , func_evals = secant2_raw_result . num_evals , left = secant2_raw_result . left , right = secant2_raw_result . right ) should_check_shrinkage = ~ ( secant2_result . converged | secant2_result . failed ) def _do_check_shrinkage ( ) : """"""Check if interval has shrinked enough."""""" old_width = curr_interval . right . x - curr_interval . left . x new_width = secant2_result . right . x - secant2_result . left . x sufficient_shrinkage = new_width < old_width * shrinkage_param func_is_flat = ( _very_close ( curr_interval . left . f , curr_interval . right . f ) & _very_close ( secant2_result . left . f , secant2_result . right . f ) ) new_converged = ( should_check_shrinkage & sufficient_shrinkage & func_is_flat ) needs_inner_bisect = should_check_shrinkage & ~ sufficient_shrinkage inner_bisect_args = secant2_result . _replace ( converged = secant2_result . converged | new_converged ) def _apply_inner_bisect ( ) : return _line_search_inner_bisection ( value_and_gradients_function , inner_bisect_args , needs_inner_bisect , f_lim ) return prefer_static . cond ( tf . reduce_any ( input_tensor = needs_inner_bisect ) , _apply_inner_bisect , lambda : inner_bisect_args ) next_args = prefer_static . cond ( tf . reduce_any ( input_tensor = should_check_shrinkage ) , _do_check_shrinkage , lambda : secant2_result ) interval_shrunk = ( ~ next_args . failed & _very_close ( next_args . left . x , next_args . right . x ) ) return [ next_args . _replace ( converged = next_args . converged | interval_shrunk ) ] return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ search_interval ] , parallel_iterations = 1 ) [ 0 ]",The main loop of line search after the minimum has been bracketed .
"def _line_search_inner_bisection ( value_and_gradients_function , search_interval , active , f_lim ) : midpoint = ( search_interval . left . x + search_interval . right . x ) / 2 val_mid = value_and_gradients_function ( midpoint ) is_valid_mid = hzl . is_finite ( val_mid ) still_active = active & is_valid_mid new_failed = active & ~ is_valid_mid next_inteval = search_interval . _replace ( failed = search_interval . failed | new_failed , func_evals = search_interval . func_evals + 1 ) def _apply_update ( ) : update_result = hzl . update ( value_and_gradients_function , next_inteval . left , next_inteval . right , val_mid , f_lim , active = still_active ) return HagerZhangLineSearchResult ( converged = next_inteval . converged , failed = next_inteval . failed | update_result . failed , iterations = next_inteval . iterations + update_result . iteration , func_evals = next_inteval . func_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = still_active ) , _apply_update , lambda : next_inteval )",Performs bisection and updates the interval .
"def _prepare_args ( value_and_gradients_function , initial_step_size , val_initial , val_0 , approximate_wolfe_threshold ) : eval_count = 0 if val_initial is None : if initial_step_size is not None : initial_step_size = tf . convert_to_tensor ( value = initial_step_size ) else : initial_step_size = tf . convert_to_tensor ( value = 1.0 , dtype = tf . float32 ) val_initial = value_and_gradients_function ( initial_step_size ) eval_count += 1 if val_0 is None : x_0 = tf . zeros_like ( val_initial . x ) val_0 = value_and_gradients_function ( x_0 ) eval_count += 1 f_lim = val_0 . f + ( approximate_wolfe_threshold * tf . abs ( val_0 . f ) ) return val_0 , val_initial , f_lim , tf . convert_to_tensor ( value = eval_count )",Prepares the arguments for the line search initialization .
"def _to_str ( x ) : x = tf . convert_to_tensor ( value = x ) if x . dtype == tf . bool : return tf . where ( x , tf . fill ( x . shape , 'True' ) , tf . fill ( x . shape , 'False' ) ) return x",Converts a bool tensor to a string with True / False values .
"def _print ( pass_through_tensor , values ) : flat_values = [ ] for value in values : if hasattr ( value , '_fields' ) : for field in value . _fields : flat_values . extend ( [ field , _to_str ( getattr ( value , field ) ) ] ) continue if isinstance ( value , ( list , tuple ) ) : for v in value : flat_values . append ( _to_str ( v ) ) continue flat_values . append ( _to_str ( value ) ) return tf . compat . v1 . Print ( pass_through_tensor , flat_values )",Wrapper for tf . Print which supports lists and namedtuples for printing .
"def _kl_brute_force ( a , b , name = None ) : def squared_frobenius_norm ( x ) : """"""Helper to make KL calculation slightly more readable."""""" return tf . reduce_sum ( input_tensor = tf . square ( x ) , axis = [ - 2 , - 1 ] ) def is_diagonal ( x ) : """"""Helper to identify if `LinearOperator` has only a diagonal component."""""" return ( isinstance ( x , tf . linalg . LinearOperatorIdentity ) or isinstance ( x , tf . linalg . LinearOperatorScaledIdentity ) or isinstance ( x , tf . linalg . LinearOperatorDiag ) ) with tf . name_scope ( name or ""kl_mvn"" ) : if is_diagonal ( a . scale ) and is_diagonal ( b . scale ) : b_inv_a = ( a . stddev ( ) / b . stddev ( ) ) [ ... , tf . newaxis ] else : b_inv_a = b . scale . solve ( a . scale . to_dense ( ) ) kl_div = ( b . scale . log_abs_determinant ( ) - a . scale . log_abs_determinant ( ) + 0.5 * ( - tf . cast ( a . scale . domain_dimension_tensor ( ) , a . dtype ) + squared_frobenius_norm ( b_inv_a ) + squared_frobenius_norm ( b . scale . solve ( ( b . mean ( ) - a . mean ( ) ) [ ... , tf . newaxis ] ) ) ) ) tensorshape_util . set_shape ( kl_div , tf . broadcast_static_shape ( a . batch_shape , b . batch_shape ) ) return kl_div",Batched KL divergence KL ( a || b ) for multivariate Normals .
"def quadrature_scheme_softmaxnormal_gauss_hermite ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or ""quadrature_scheme_softmaxnormal_gauss_hermite"" ) : normal_loc = tf . convert_to_tensor ( value = normal_loc , name = ""normal_loc"" ) npdt = dtype_util . as_numpy_dtype ( normal_loc . dtype ) normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = npdt , name = ""normal_scale"" ) normal_scale = maybe_check_quadrature_param ( normal_scale , ""normal_scale"" , validate_args ) grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) grid = grid . astype ( npdt ) probs = probs . astype ( npdt ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) probs = tf . convert_to_tensor ( value = probs , name = ""probs"" , dtype = npdt ) grid = softmax ( - distribution_util . pad ( ( normal_loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * normal_scale [ ... , tf . newaxis ] * grid ) , axis = - 2 , front = True ) , axis = - 2 ) return grid , probs",Use Gauss - Hermite quadrature to form quadrature on K - 1 simplex .
"def quadrature_scheme_softmaxnormal_quantiles ( normal_loc , normal_scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or ""softmax_normal_grid_and_probs"" ) : normal_loc = tf . convert_to_tensor ( value = normal_loc , name = ""normal_loc"" ) dt = dtype_util . base_dtype ( normal_loc . dtype ) normal_scale = tf . convert_to_tensor ( value = normal_scale , dtype = dt , name = ""normal_scale"" ) normal_scale = maybe_check_quadrature_param ( normal_scale , ""normal_scale"" , validate_args ) dist = normal . Normal ( loc = normal_loc , scale = normal_scale ) def _get_batch_ndims ( ) : """"""Helper to get rank(dist.batch_shape), statically if possible."""""" ndims = tensorshape_util . rank ( dist . batch_shape ) if ndims is None : ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] return ndims batch_ndims = _get_batch_ndims ( ) def _get_final_shape ( qs ) : """"""Helper to build `TensorShape`."""""" bs = tensorshape_util . with_rank_at_least ( dist . batch_shape , 1 ) num_components = tf . compat . dimension_value ( bs [ - 1 ] ) if num_components is not None : num_components += 1 tail = tf . TensorShape ( [ num_components , qs ] ) return bs [ : - 1 ] . concatenate ( tail ) def _compute_quantiles ( ) : """"""Helper to build quantiles."""""" zero = tf . zeros ( [ ] , dtype = dist . dtype ) edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) quantiles = dist . quantile ( edges ) quantiles = softmax_centered_bijector . SoftmaxCentered ( ) . forward ( quantiles ) perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) quantiles = tf . transpose ( a = quantiles , perm = perm ) tensorshape_util . set_shape ( quantiles , _get_final_shape ( quadrature_size + 1 ) ) return quantiles quantiles = _compute_quantiles ( ) grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. tensorshape_util . set_shape ( grid , _get_final_shape ( quadrature_size ) ) probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) return grid , probs",Use SoftmaxNormal quantiles to form quadrature on K - 1 simplex .
"def maybe_check_quadrature_param ( param , name , validate_args ) : with tf . name_scope ( ""check_"" + name ) : assertions = [ ] if tensorshape_util . rank ( param . shape ) is not None : if tensorshape_util . rank ( param . shape ) == 0 : raise ValueError ( ""Mixing params must be a (batch of) vector; "" ""{}.rank={} is not at least one."" . format ( name , tensorshape_util . rank ( param . shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_rank_at_least ( param , 1 , message = ( ""Mixing params must be a (batch of) vector; "" ""{}.rank is not at least one."" . format ( name ) ) ) ) if tensorshape_util . with_rank_at_least ( param . shape , 1 ) [ - 1 ] is not None : if tf . compat . dimension_value ( param . shape [ - 1 ] ) != 1 : raise NotImplementedError ( ""Currently only bimixtures are supported; "" ""{}.shape[-1]={} is not 1."" . format ( name , tf . compat . dimension_value ( param . shape [ - 1 ] ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( tf . shape ( input = param ) [ - 1 ] , 1 , message = ( ""Currently only bimixtures are supported; "" ""{}.shape[-1] is not 1."" . format ( name ) ) ) ) if assertions : return distribution_util . with_dependencies ( assertions , param ) return param",Helper which checks validity of loc and scale init args .
"def determine_batch_event_shapes ( grid , endpoint_affine ) : with tf . name_scope ( ""determine_batch_event_shapes"" ) : batch_shape = grid . shape [ : - 2 ] batch_shape_tensor = tf . shape ( input = grid ) [ : - 2 ] event_shape = None event_shape_tensor = None def _set_event_shape ( shape , shape_tensor ) : if event_shape is None : return shape , shape_tensor return ( tf . broadcast_static_shape ( event_shape , shape ) , tf . broadcast_dynamic_shape ( event_shape_tensor , shape_tensor ) ) for aff in endpoint_affine : if aff . shift is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . shift . shape [ : - 1 ] ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , tf . shape ( input = aff . shift ) [ : - 1 ] ) event_shape , event_shape_tensor = _set_event_shape ( aff . shift . shape [ - 1 : ] , tf . shape ( input = aff . shift ) [ - 1 : ] ) if aff . scale is not None : batch_shape = tf . broadcast_static_shape ( batch_shape , aff . scale . batch_shape ) batch_shape_tensor = tf . broadcast_dynamic_shape ( batch_shape_tensor , aff . scale . batch_shape_tensor ( ) ) event_shape , event_shape_tensor = _set_event_shape ( tf . TensorShape ( [ aff . scale . range_dimension ] ) , aff . scale . range_dimension_tensor ( ) [ tf . newaxis ] ) return batch_shape , batch_shape_tensor , event_shape , event_shape_tensor",Helper to infer batch_shape and event_shape .
"def interpolate_loc ( grid , loc ) : if len ( loc ) != 2 : raise NotImplementedError ( ""Currently only bimixtures are supported; "" ""len(scale)={} is not 2."" . format ( len ( loc ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( ""Num quadrature grid points must be known prior "" ""to graph execution."" ) with tf . name_scope ( ""interpolate_loc"" ) : if loc is None or loc [ 0 ] is None and loc [ 1 ] is None : return [ None ] * deg w = grid [ ... , tf . newaxis , : , : ] loc = [ x [ ... , tf . newaxis ] if x is not None else None for x in loc ] if loc [ 0 ] is None : x = w [ ... , 1 , : ] * loc [ 1 ] elif loc [ 1 ] is None : x = w [ ... , 0 , : ] * loc [ 0 ] else : delta = loc [ 0 ] - loc [ 1 ] x = w [ ... , 0 , : ] * delta + loc [ 1 ] return [ x [ ... , k ] for k in range ( deg ) ]",Helper which interpolates between two locs .
"def interpolate_scale ( grid , scale ) : if len ( scale ) != 2 : raise NotImplementedError ( ""Currently only bimixtures are supported; "" ""len(scale)={} is not 2."" . format ( len ( scale ) ) ) deg = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( grid . shape , 1 ) [ - 1 ] ) if deg is None : raise ValueError ( ""Num quadrature grid points must be known prior "" ""to graph execution."" ) with tf . name_scope ( ""interpolate_scale"" ) : return [ linop_add_lib . add_operators ( [ linop_scale ( grid [ ... , k , q ] , s ) for k , s in enumerate ( scale ) ] ) [ 0 ] for q in range ( deg ) ]",Helper which interpolates between two scales .
"def linop_scale ( w , op ) : with tf . name_scope ( ""linop_scale"" ) : def scaled_identity ( w ) : return tf . linalg . LinearOperatorScaledIdentity ( num_rows = op . range_dimension_tensor ( ) , multiplier = w , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorIdentity ) : return scaled_identity ( w ) if isinstance ( op , tf . linalg . LinearOperatorScaledIdentity ) : return scaled_identity ( w * op . multiplier ) if isinstance ( op , tf . linalg . LinearOperatorDiag ) : return tf . linalg . LinearOperatorDiag ( diag = w [ ... , tf . newaxis ] * op . diag_part ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) if isinstance ( op , tf . linalg . LinearOperatorLowerTriangular ) : return tf . linalg . LinearOperatorLowerTriangular ( tril = w [ ... , tf . newaxis , tf . newaxis ] * op . to_dense ( ) , is_non_singular = op . is_non_singular , is_self_adjoint = op . is_self_adjoint , is_positive_definite = op . is_positive_definite ) raise NotImplementedError ( ""Unsupported Linop type ({})"" . format ( type ( op ) . __name__ ) )",Creates weighted LinOp from existing LinOp .
"def concat_vectors ( * args ) : args_ = [ tf . get_static_value ( x ) for x in args ] if any ( vec is None for vec in args_ ) : return tf . concat ( args , axis = 0 ) return [ val for vec in args_ for val in vec ]",Concatenates input vectors statically if possible .
"def softmax ( x , axis , name = None ) : with tf . name_scope ( name or ""softmax"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) ndims = ( tensorshape_util . rank ( x . shape ) if tensorshape_util . rank ( x . shape ) is not None else tf . rank ( x , name = ""ndims"" ) ) axis = tf . convert_to_tensor ( value = axis , dtype = tf . int32 , name = ""axis"" ) axis_ = tf . get_static_value ( axis ) if axis_ is not None : axis = np . int ( ndims + axis_ if axis_ < 0 else axis_ ) else : axis = tf . where ( axis < 0 , ndims + axis , axis ) return tf . nn . softmax ( x , axis = axis )",Equivalent to tf . nn . softmax but works around b / 70297725 .
"def _expand_base_distribution_mean ( self ) : single_draw_shape = concat_vectors ( self . batch_shape_tensor ( ) , self . event_shape_tensor ( ) ) m = tf . reshape ( self . distribution . mean ( ) , shape = tf . ones_like ( single_draw_shape , dtype = tf . int32 ) ) m = tf . tile ( m , multiples = single_draw_shape ) tensorshape_util . set_shape ( m , tensorshape_util . concatenate ( self . batch_shape , self . event_shape ) ) return m",Ensures self . distribution . mean () has [ batch event ] shape .
"def _log_vector_matrix ( vs , ms ) : return tf . reduce_logsumexp ( input_tensor = vs [ ... , tf . newaxis ] + ms , axis = - 2 )",Multiply tensor of vectors by matrices assuming values stored are logs .
"def _log_matrix_vector ( ms , vs ) : return tf . reduce_logsumexp ( input_tensor = ms + vs [ ... , tf . newaxis , : ] , axis = - 1 )",Multiply tensor of matrices by vectors assuming values stored are logs .
"def _vector_matrix ( vs , ms ) : return tf . reduce_sum ( input_tensor = vs [ ... , tf . newaxis ] * ms , axis = - 2 )",Multiply tensor of vectors by matrices .
"def _extract_log_probs ( num_states , dist ) : states = tf . reshape ( tf . range ( num_states ) , tf . concat ( [ [ num_states ] , tf . ones_like ( dist . batch_shape_tensor ( ) ) ] , axis = 0 ) ) return distribution_util . move_dimension ( dist . log_prob ( states ) , 0 , - 1 )",Tabulate log probabilities from a batch of distributions .
"def _marginal_hidden_probs ( self ) : initial_log_probs = tf . broadcast_to ( self . _log_init , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . _num_states ] ] , axis = 0 ) ) if self . _num_steps > 1 : transition_log_probs = self . _log_trans def forward_step ( log_probs , _ ) : return _log_vector_matrix ( log_probs , transition_log_probs ) dummy_index = tf . zeros ( self . _num_steps - 1 , dtype = tf . float32 ) forward_log_probs = tf . scan ( forward_step , dummy_index , initializer = initial_log_probs , name = ""forward_log_probs"" ) forward_log_probs = tf . concat ( [ [ initial_log_probs ] , forward_log_probs ] , axis = 0 ) else : forward_log_probs = initial_log_probs [ tf . newaxis , ... ] return tf . exp ( forward_log_probs )",Compute marginal pdf for each individual observable .
"def posterior_marginals ( self , observations , name = None ) : with tf . name_scope ( name or ""posterior_marginals"" ) : with tf . control_dependencies ( self . _runtime_assertions ) : observation_tensor_shape = tf . shape ( input = observations ) with self . _observation_shape_preconditions ( observation_tensor_shape ) : observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) log_transition = self . _log_trans observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) observation_rank = tf . rank ( observations ) underlying_event_rank = self . _underlying_event_rank observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) observation_log_probs = self . _observation_distribution . log_prob ( observations ) log_adjoint_prob = tf . zeros_like ( log_init ) def forward_step ( log_previous_step , log_prob_observation ) : return _log_vector_matrix ( log_previous_step , log_transition ) + log_prob_observation log_prob = log_init + observation_log_probs [ 0 ] forward_log_probs = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = log_prob , name = ""forward_log_probs"" ) forward_log_probs = tf . concat ( [ [ log_prob ] , forward_log_probs ] , axis = 0 ) def backward_step ( log_previous_step , log_prob_observation ) : return _log_matrix_vector ( log_transition , log_prob_observation + log_previous_step ) backward_log_adjoint_probs = tf . scan ( backward_step , observation_log_probs [ 1 : ] , initializer = log_adjoint_prob , reverse = True , name = ""backward_log_adjoint_probs"" ) total_log_prob = tf . reduce_logsumexp ( input_tensor = forward_log_probs [ - 1 ] , axis = - 1 ) backward_log_adjoint_probs = tf . concat ( [ backward_log_adjoint_probs , [ log_adjoint_prob ] ] , axis = 0 ) log_likelihoods = forward_log_probs + backward_log_adjoint_probs marginal_log_probs = distribution_util . move_dimension ( log_likelihoods - total_log_prob [ ... , tf . newaxis ] , 0 , - 2 ) return categorical . Categorical ( logits = marginal_log_probs )",Compute marginal posterior distribution for each state .
"def posterior_mode ( self , observations , name = None ) : with tf . name_scope ( name or ""posterior_mode"" ) : with tf . control_dependencies ( self . _runtime_assertions ) : observation_tensor_shape = tf . shape ( input = observations ) with self . _observation_shape_preconditions ( observation_tensor_shape ) : observation_batch_shape = observation_tensor_shape [ : - 1 - self . _underlying_event_rank ] observation_event_shape = observation_tensor_shape [ - 1 - self . _underlying_event_rank : ] batch_shape = tf . broadcast_dynamic_shape ( observation_batch_shape , self . batch_shape_tensor ( ) ) log_init = tf . broadcast_to ( self . _log_init , tf . concat ( [ batch_shape , [ self . _num_states ] ] , axis = 0 ) ) observations = tf . broadcast_to ( observations , tf . concat ( [ batch_shape , observation_event_shape ] , axis = 0 ) ) observation_rank = tf . rank ( observations ) underlying_event_rank = self . _underlying_event_rank observations = distribution_util . move_dimension ( observations , observation_rank - underlying_event_rank - 1 , 0 ) observations = tf . expand_dims ( observations , observation_rank - underlying_event_rank ) observation_log_probs = self . _observation_distribution . log_prob ( observations ) log_prob = log_init + observation_log_probs [ 0 ] if self . _num_steps == 1 : most_likely_end = tf . argmax ( input = log_prob , axis = - 1 ) return most_likely_end [ ... , tf . newaxis ] def forward_step ( previous_step_pair , log_prob_observation ) : log_prob_previous = previous_step_pair [ 0 ] log_prob = ( log_prob_previous [ ... , tf . newaxis ] + self . _log_trans + log_prob_observation [ ... , tf . newaxis , : ] ) most_likely_given_successor = tf . argmax ( input = log_prob , axis = - 2 ) max_log_p_given_successor = tf . reduce_max ( input_tensor = log_prob , axis = - 2 ) return ( max_log_p_given_successor , most_likely_given_successor ) forward_log_probs , all_most_likely_given_successor = tf . scan ( forward_step , observation_log_probs [ 1 : ] , initializer = ( log_prob , tf . zeros ( tf . shape ( input = log_init ) , dtype = tf . int64 ) ) , name = ""forward_log_probs"" ) most_likely_end = tf . argmax ( input = forward_log_probs [ - 1 ] , axis = - 1 ) def backward_step ( most_likely_successor , most_likely_given_successor ) : return tf . reduce_sum ( input_tensor = ( most_likely_given_successor * tf . one_hot ( most_likely_successor , self . _num_states , dtype = tf . int64 ) ) , axis = - 1 ) backward_scan = tf . scan ( backward_step , all_most_likely_given_successor , most_likely_end , reverse = True ) most_likely_sequences = tf . concat ( [ backward_scan , [ most_likely_end ] ] , axis = 0 ) return distribution_util . move_dimension ( most_likely_sequences , 0 , - 1 )",Compute maximum likelihood sequence of hidden states .
"def _choose_random_direction ( current_state_parts , batch_rank , seed = None ) : seed_gen = distributions . SeedStream ( seed , salt = '_choose_random_direction' ) rnd_direction_parts = [ tf . random . normal ( tf . shape ( input = current_state_part ) , dtype = tf . float32 , seed = seed_gen ( ) ) for current_state_part in current_state_parts ] sum_squares = sum ( tf . reduce_sum ( input_tensor = rnd_direction ** 2. , axis = tf . range ( batch_rank , tf . rank ( rnd_direction ) ) , keepdims = True ) for rnd_direction in rnd_direction_parts ) rnd_direction_parts = [ rnd_direction / tf . sqrt ( sum_squares ) for rnd_direction in rnd_direction_parts ] return rnd_direction_parts",Chooses a random direction in the event space .
"def _sample_next ( target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'sample_next' , [ current_state_parts , step_sizes , max_doublings , current_target_log_prob , batch_rank ] ) : direction = _choose_random_direction ( current_state_parts , batch_rank = batch_rank , seed = seed ) reduce_axes = [ tf . range ( batch_rank , tf . rank ( dirn_part ) ) for dirn_part in direction ] components = [ tf . reduce_sum ( input_tensor = ( dirn_part / step_size ) ** 2 , axis = reduce_axes [ i ] ) for i , ( step_size , dirn_part ) in enumerate ( zip ( step_sizes , direction ) ) ] step_size = tf . math . rsqrt ( tf . add_n ( components ) ) def _get_rank ( x ) : return ( len ( x . shape . as_list ( ) ) if x . shape . dims is not None else tf . rank ( x ) ) state_part_ranks = [ _get_rank ( part ) for part in current_state_parts ] def _step_along_direction ( alpha ) : """"""Converts the scalar alpha into an n-dim vector with full state info.

      Computes x_0 + alpha * direction where x_0 is the current state and
      direction is the direction chosen above.

      Args:
        alpha: A tensor of shape equal to the batch dimensions of
          `current_state_parts`.

      Returns:
        state_parts: Tensor or Python list of `Tensor`s representing the
          state(s) of the Markov chain(s) for a given alpha and a given chosen
          direction. Has the same shape as `current_state_parts`.
      """""" padded_alphas = [ _right_pad ( alpha , final_rank = part_rank ) for part_rank in state_part_ranks ] state_parts = [ state_part + padded_alpha * direction_part for state_part , direction_part , padded_alpha in zip ( current_state_parts , direction , padded_alphas ) ] return state_parts def projected_target_log_prob_fn ( alpha ) : """"""The target log density projected along the chosen direction.

      Args:
        alpha: A tensor of shape equal to the batch dimensions of
          `current_state_parts`.

      Returns:
        Target log density evaluated at x_0 + alpha * direction where x_0 is the
        current state and direction is the direction chosen above. Has the same
        shape as `alpha`.
      """""" return target_log_prob_fn ( * _step_along_direction ( alpha ) ) alpha_init = tf . zeros_like ( current_target_log_prob , dtype = current_state_parts [ 0 ] . dtype . base_dtype ) [ next_alpha , next_target_log_prob , bounds_satisfied , upper_bounds , lower_bounds ] = ssu . slice_sampler_one_dim ( projected_target_log_prob_fn , x_initial = alpha_init , max_doublings = max_doublings , step_size = step_size , seed = seed ) return [ _step_along_direction ( next_alpha ) , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ]",Applies a single iteration of slice sampling update .
"def _maybe_call_fn ( fn , fn_arg_list , fn_result = None , description = 'target_log_prob' ) : fn_arg_list = ( list ( fn_arg_list ) if mcmc_util . is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) if fn_result is None : fn_result = fn ( * fn_arg_list ) if not fn_result . dtype . is_floating : raise TypeError ( '`{}` must be a `Tensor` with `float` `dtype`.' . format ( description ) ) return fn_result",Helper which computes fn_result if needed .
"def _right_pad ( x , final_rank ) : padded_shape = tf . concat ( [ tf . shape ( input = x ) , tf . ones ( final_rank - tf . rank ( x ) , dtype = tf . int32 ) ] , axis = 0 ) static_padded_shape = None if x . shape . is_fully_defined ( ) and isinstance ( final_rank , int ) : static_padded_shape = x . shape . as_list ( ) extra_dims = final_rank - len ( static_padded_shape ) static_padded_shape . extend ( [ 1 ] * extra_dims ) padded_x = tf . reshape ( x , static_padded_shape or padded_shape ) return padded_x",Pads the shape of x to the right to be of rank final_rank .
"def _prepare_args ( target_log_prob_fn , state , step_size , target_log_prob = None , maybe_expand = False , description = 'target_log_prob' ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( value = s , name = 'current_state' ) for s in state_parts ] target_log_prob = _maybe_call_fn ( target_log_prob_fn , state_parts , target_log_prob , description ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) def maybe_flatten ( x ) : return x if maybe_expand or mcmc_util . is_list_like ( state ) else x [ 0 ] return [ maybe_flatten ( state_parts ) , maybe_flatten ( step_sizes ) , target_log_prob ]",Processes input args to meet list - like assumptions .
"def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'slice' , 'one_step' ) , values = [ self . step_size , self . max_doublings , self . _seed_stream , current_state , previous_kernel_results . target_log_prob ] ) : with tf . compat . v1 . name_scope ( 'initialize' ) : [ current_state_parts , step_sizes , current_target_log_prob ] = _prepare_args ( self . target_log_prob_fn , current_state , self . step_size , previous_kernel_results . target_log_prob , maybe_expand = True ) max_doublings = tf . convert_to_tensor ( value = self . max_doublings , dtype = tf . int32 , name = 'max_doublings' ) independent_chain_ndims = distribution_util . prefer_static_rank ( current_target_log_prob ) [ next_state_parts , next_target_log_prob , bounds_satisfied , direction , upper_bounds , lower_bounds ] = _sample_next ( self . target_log_prob_fn , current_state_parts , step_sizes , max_doublings , current_target_log_prob , independent_chain_ndims , seed = self . _seed_stream ( ) ) def maybe_flatten ( x ) : return x if mcmc_util . is_list_like ( current_state ) else x [ 0 ] return [ maybe_flatten ( next_state_parts ) , SliceSamplerKernelResults ( target_log_prob = next_target_log_prob , bounds_satisfied = bounds_satisfied , direction = direction , upper_bounds = upper_bounds , lower_bounds = lower_bounds ) , ]",Runs one iteration of Slice Sampler .
"def sample_uniform_initial_state ( parameter , return_constrained = True , init_sample_shape = ( ) , seed = None ) : unconstrained_prior_sample = parameter . bijector . inverse ( parameter . prior . sample ( init_sample_shape , seed = seed ) ) uniform_initializer = 4 * tf . random . uniform ( tf . shape ( input = unconstrained_prior_sample ) , dtype = unconstrained_prior_sample . dtype , seed = seed ) - 2 if return_constrained : return parameter . bijector . forward ( uniform_initializer ) else : return uniform_initializer",Initialize from a uniform [ - 2 2 ] distribution in unconstrained space .
"def _build_trainable_posterior ( param , initial_loc_fn ) : loc = tf . compat . v1 . get_variable ( param . name + '_loc' , initializer = lambda : initial_loc_fn ( param ) , dtype = param . prior . dtype , use_resource = True ) scale = tf . nn . softplus ( tf . compat . v1 . get_variable ( param . name + '_scale' , initializer = lambda : - 4 * tf . ones_like ( initial_loc_fn ( param ) ) , dtype = param . prior . dtype , use_resource = True ) ) q = tfd . Normal ( loc = loc , scale = scale ) if ( param . prior . event_shape . ndims is None or param . prior . event_shape . ndims > 0 ) : q = tfd . Independent ( q , reinterpreted_batch_ndims = param . prior . event_shape . ndims ) return tfd . TransformedDistribution ( q , param . bijector )",Built a transformed - normal variational dist over a parameter s support .
"def build_factored_variational_loss ( model , observed_time_series , init_batch_shape = ( ) , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'build_factored_variational_loss' , values = [ observed_time_series ] ) as name : seed = tfd . SeedStream ( seed , salt = 'StructuralTimeSeries_build_factored_variational_loss' ) variational_distributions = collections . OrderedDict ( ) variational_samples = [ ] for param in model . parameters : def initial_loc_fn ( param ) : return sample_uniform_initial_state ( param , return_constrained = True , init_sample_shape = init_batch_shape , seed = seed ( ) ) q = _build_trainable_posterior ( param , initial_loc_fn = initial_loc_fn ) variational_distributions [ param . name ] = q variational_samples . append ( q . sample ( seed = seed ( ) ) ) observed_time_series = sts_util . pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape = init_batch_shape ) log_prob_fn = model . joint_log_prob ( observed_time_series ) expected_log_joint = log_prob_fn ( * variational_samples ) entropy = tf . reduce_sum ( input_tensor = [ - q . log_prob ( sample ) for ( q , sample ) in zip ( variational_distributions . values ( ) , variational_samples ) ] , axis = 0 ) variational_loss = - ( expected_log_joint + entropy ) return variational_loss , variational_distributions",Build a loss function for variational inference in STS models .
"def _minimize_in_graph ( build_loss_fn , num_steps = 200 , optimizer = None ) : optimizer = tf . compat . v1 . train . AdamOptimizer ( 0.1 ) if optimizer is None else optimizer def train_loop_body ( step ) : train_op = optimizer . minimize ( build_loss_fn if tf . executing_eagerly ( ) else build_loss_fn ( ) ) return tf . tuple ( tensors = [ tf . add ( step , 1 ) ] , control_inputs = [ train_op ] ) minimize_op = tf . compat . v1 . while_loop ( cond = lambda step : step < num_steps , body = train_loop_body , loop_vars = [ tf . constant ( 0 ) ] , return_same_structure = True ) [ 0 ] return minimize_op",Run an optimizer within the graph to minimize a loss function .
"def fit_with_hmc ( model , observed_time_series , num_results = 100 , num_warmup_steps = 50 , num_leapfrog_steps = 15 , initial_state = None , initial_step_size = None , chain_batch_shape = ( ) , num_variational_steps = 150 , variational_optimizer = None , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'fit_with_hmc' , values = [ observed_time_series ] ) as name : seed = tfd . SeedStream ( seed , salt = 'StructuralTimeSeries_fit_with_hmc' ) if initial_step_size is None or initial_state is None : def make_variational ( ) : return build_factored_variational_loss ( model , observed_time_series , init_batch_shape = chain_batch_shape , seed = seed ( ) ) make_variational = tf . compat . v1 . make_template ( 'make_variational' , make_variational ) _ , variational_distributions = make_variational ( ) minimize_op = _minimize_in_graph ( build_loss_fn = lambda : make_variational ( ) [ 0 ] , num_steps = num_variational_steps , optimizer = variational_optimizer ) with tf . control_dependencies ( [ minimize_op ] ) : if initial_state is None : initial_state = [ tf . stop_gradient ( d . sample ( ) ) for d in variational_distributions . values ( ) ] if initial_step_size is None : initial_step_size = [ transformed_q . distribution . stddev ( ) for transformed_q in variational_distributions . values ( ) ] observed_time_series = sts_util . pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape = chain_batch_shape ) samples , kernel_results = mcmc . sample_chain ( num_results = num_results , current_state = initial_state , num_burnin_steps = num_warmup_steps , kernel = mcmc . SimpleStepSizeAdaptation ( inner_kernel = mcmc . TransformedTransitionKernel ( inner_kernel = mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = model . joint_log_prob ( observed_time_series ) , step_size = initial_step_size , num_leapfrog_steps = num_leapfrog_steps , state_gradients_are_stopped = True , seed = seed ( ) ) , bijector = [ param . bijector for param in model . parameters ] ) , num_adaptation_steps = int ( num_warmup_steps * 0.8 ) , adaptation_rate = tf . convert_to_tensor ( value = 0.1 , dtype = initial_state [ 0 ] . dtype ) ) , parallel_iterations = 1 if seed is not None else 10 ) return samples , kernel_results",Draw posterior samples using Hamiltonian Monte Carlo ( HMC ) .
"def moments_of_masked_time_series ( time_series_tensor , broadcast_mask ) : num_unmasked_entries = tf . cast ( tf . reduce_sum ( input_tensor = tf . cast ( ~ broadcast_mask , tf . int32 ) , axis = - 1 ) , time_series_tensor . dtype ) mean = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , time_series_tensor ) , axis = - 1 ) / num_unmasked_entries ) variance = ( tf . reduce_sum ( input_tensor = tf . where ( broadcast_mask , tf . zeros_like ( time_series_tensor ) , ( time_series_tensor - mean [ ... , tf . newaxis ] ) ** 2 ) , axis = - 1 ) / num_unmasked_entries ) return mean , variance",Compute mean and variance accounting for a mask .
"def initial_value_of_masked_time_series ( time_series_tensor , broadcast_mask ) : num_timesteps = tf . shape ( input = time_series_tensor ) [ - 1 ] unmasked_negindices = ( tf . cast ( ~ broadcast_mask , tf . int32 ) * tf . range ( num_timesteps , 0 , - 1 ) ) first_unmasked_indices = num_timesteps - tf . reduce_max ( input_tensor = unmasked_negindices , axis = - 1 ) if first_unmasked_indices . shape . ndims is None : raise NotImplementedError ( 'Cannot compute initial values of a masked time series with' 'dynamic rank.' ) return tf . squeeze ( tf . compat . v1 . batch_gather ( params = time_series_tensor , indices = first_unmasked_indices [ ... , tf . newaxis ] ) , axis = - 1 )",Get the first unmasked entry of each time series in the batch .
"def broadcast_batch_shape ( distributions ) : batch_shape = distributions [ 0 ] . batch_shape for distribution in distributions : batch_shape = tf . broadcast_static_shape ( batch_shape , distribution . batch_shape ) if batch_shape . is_fully_defined ( ) : return batch_shape . as_list ( ) batch_shape = distributions [ 0 ] . batch_shape_tensor ( ) for distribution in distributions : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , distribution . batch_shape_tensor ( ) ) return tf . convert_to_tensor ( value = batch_shape )",Get broadcast batch shape from distributions statically if possible .
"def pad_batch_dimension_for_multiple_chains ( observed_time_series , model , chain_batch_shape ) : [ observed_time_series , is_missing ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) event_ndims = 2 model_batch_ndims = ( model . batch_shape . ndims if model . batch_shape . ndims is not None else tf . shape ( input = model . batch_shape_tensor ( ) ) [ 0 ] ) chain_batch_shape = tf . convert_to_tensor ( value = chain_batch_shape , name = 'chain_batch_shape' , dtype = tf . int32 ) if not chain_batch_shape . shape . is_fully_defined ( ) : raise ValueError ( 'Batch shape must have static rank. (given: {})' . format ( chain_batch_shape ) ) if chain_batch_shape . shape . ndims == 0 : chain_batch_shape = chain_batch_shape [ tf . newaxis ] chain_batch_ndims = tf . compat . dimension_value ( chain_batch_shape . shape [ 0 ] ) def do_padding ( observed_time_series_tensor ) : current_sample_shape = tf . shape ( input = observed_time_series_tensor ) [ : - ( model_batch_ndims + event_ndims ) ] current_batch_and_event_shape = tf . shape ( input = observed_time_series_tensor ) [ - ( model_batch_ndims + event_ndims ) : ] return tf . reshape ( tensor = observed_time_series_tensor , shape = tf . concat ( [ current_sample_shape , tf . ones ( [ chain_batch_ndims ] , dtype = tf . int32 ) , current_batch_and_event_shape ] , axis = 0 ) ) observed_time_series = prefer_static . cond ( ( dist_util . prefer_static_rank ( observed_time_series ) > model_batch_ndims + event_ndims ) , lambda : do_padding ( observed_time_series ) , lambda : observed_time_series ) if is_missing is not None : is_missing = prefer_static . cond ( ( dist_util . prefer_static_rank ( is_missing ) > model_batch_ndims + event_ndims ) , lambda : do_padding ( is_missing ) , lambda : is_missing ) return missing_values_util . MaskedTimeSeries ( observed_time_series , is_missing = is_missing ) return observed_time_series",Expand the observed time series with extra batch dimension ( s ) .
"def factored_joint_mvn ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] with tf . compat . v1 . name_scope ( 'factored_joint_mvn' , values = graph_parents ) : dtype = tf . debugging . assert_same_float_dtype ( distributions ) broadcast_ones = tf . ones ( broadcast_batch_shape ( distributions ) , dtype = dtype ) [ ... , tf . newaxis ] return MultivariateNormalLinearOperator ( loc = tf . concat ( [ mvn . mean ( ) * broadcast_ones for mvn in distributions ] , axis = - 1 ) , scale = tfl . LinearOperatorBlockDiag ( [ mvn . scale for mvn in distributions ] , is_square = True ) )",Combine MultivariateNormals into a factored joint distribution .
"def sum_mvns ( distributions ) : graph_parents = [ tensor for distribution in distributions for tensor in distribution . _graph_parents ] with tf . compat . v1 . name_scope ( 'sum_mvns' , values = graph_parents ) : if all ( [ isinstance ( mvn , tfd . MultivariateNormalDiag ) for mvn in distributions ] ) : return tfd . MultivariateNormalDiag ( loc = sum ( [ mvn . mean ( ) for mvn in distributions ] ) , scale_diag = tf . sqrt ( sum ( [ mvn . scale . diag ** 2 for mvn in distributions ] ) ) ) else : raise NotImplementedError ( 'Sums of distributions other than MultivariateNormalDiag are not ' 'currently implemented. (given: {})' . format ( distributions ) )",Attempt to sum MultivariateNormal distributions .
"def empirical_statistics ( observed_time_series ) : with tf . compat . v1 . name_scope ( 'empirical_statistics' , values = [ observed_time_series ] ) : [ observed_time_series , mask ] = canonicalize_observed_time_series_with_mask ( observed_time_series ) squeezed_series = observed_time_series [ ... , 0 ] if mask is None : observed_mean , observed_variance = tf . nn . moments ( x = squeezed_series , axes = - 1 ) observed_initial = squeezed_series [ ... , 0 ] else : broadcast_mask = tf . broadcast_to ( tf . cast ( mask , tf . bool ) , tf . shape ( input = squeezed_series ) ) observed_mean , observed_variance = ( missing_values_util . moments_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) try : observed_initial = ( missing_values_util . initial_value_of_masked_time_series ( squeezed_series , broadcast_mask = broadcast_mask ) ) except NotImplementedError : tf . compat . v1 . logging . warn ( 'Cannot compute initial values for a masked time series' 'with dynamic shape; using the mean instead. This will' 'affect heuristic priors and may change the results of' 'inference.' ) observed_initial = observed_mean observed_stddev = tf . sqrt ( observed_variance ) observed_initial_centered = observed_initial - observed_mean return observed_mean , observed_stddev , observed_initial_centered",Compute statistics of a provided time series as heuristic initialization .
"def _maybe_expand_trailing_dim ( observed_time_series_tensor ) : with tf . compat . v1 . name_scope ( 'maybe_expand_trailing_dim' , values = [ observed_time_series_tensor ] ) : if ( observed_time_series_tensor . shape . ndims is not None and tf . compat . dimension_value ( observed_time_series_tensor . shape [ - 1 ] ) is not None ) : expanded_time_series = ( observed_time_series_tensor if observed_time_series_tensor . shape [ - 1 ] == 1 else observed_time_series_tensor [ ... , tf . newaxis ] ) else : expanded_time_series = tf . cond ( pred = tf . equal ( tf . shape ( input = observed_time_series_tensor ) [ - 1 ] , 1 ) , true_fn = lambda : observed_time_series_tensor , false_fn = lambda : observed_time_series_tensor [ ... , tf . newaxis ] ) return expanded_time_series",Ensures observed_time_series_tensor has a trailing dimension of size 1 .
"def canonicalize_observed_time_series_with_mask ( maybe_masked_observed_time_series ) : with tf . compat . v1 . name_scope ( 'canonicalize_observed_time_series_with_mask' ) : if hasattr ( maybe_masked_observed_time_series , 'is_missing' ) : observed_time_series = ( maybe_masked_observed_time_series . time_series ) is_missing = maybe_masked_observed_time_series . is_missing else : observed_time_series = maybe_masked_observed_time_series is_missing = None observed_time_series = tf . convert_to_tensor ( value = observed_time_series , name = 'observed_time_series' ) observed_time_series = _maybe_expand_trailing_dim ( observed_time_series ) if is_missing is not None : is_missing = tf . convert_to_tensor ( value = is_missing , name = 'is_missing' , dtype_hint = tf . bool ) return missing_values_util . MaskedTimeSeries ( observed_time_series , is_missing = is_missing )",Extract a Tensor with canonical shape and optional mask .
"def mix_over_posterior_draws ( means , variances ) : with tf . compat . v1 . name_scope ( 'mix_over_posterior_draws' , values = [ means , variances ] ) : num_posterior_draws = dist_util . prefer_static_value ( tf . shape ( input = means ) ) [ 0 ] component_observations = tfd . Independent ( distribution = tfd . Normal ( loc = dist_util . move_dimension ( means , 0 , - 2 ) , scale = tf . sqrt ( dist_util . move_dimension ( variances , 0 , - 2 ) ) ) , reinterpreted_batch_ndims = 1 ) return tfd . MixtureSameFamily ( mixture_distribution = tfd . Categorical ( logits = tf . zeros ( [ num_posterior_draws ] , dtype = component_observations . dtype ) ) , components_distribution = component_observations )",Construct a predictive normal distribution that mixes over posterior draws .
"def _kl_uniform_uniform ( a , b , name = None ) : with tf . name_scope ( name or ""kl_uniform_uniform"" ) : final_batch_shape = distribution_util . get_broadcast_shape ( a . low , b . low , a . high , b . high ) dtype = dtype_util . common_dtype ( [ a . low , a . high , b . low , b . high ] , tf . float32 ) return tf . where ( ( b . low <= a . low ) & ( a . high <= b . high ) , tf . math . log ( b . high - b . low ) - tf . math . log ( a . high - a . low ) , tf . broadcast_to ( dtype_util . as_numpy_dtype ( dtype ) ( np . inf ) , final_batch_shape ) )",Calculate the batched KL divergence KL ( a || b ) with a and b Uniform .
"def range ( self , name = ""range"" ) : with self . _name_scope ( name ) : return self . high - self . low",high - low .
"def _make_summary_statistic ( attr ) : def _fn ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute ' + attr + ' when all distributions are ' 'independent; {}' . format ( self . model ) ) return self . _unflatten ( getattr ( d ( ) , attr ) ( ) for d in self . _dist_fn_wrapped ) return _fn",Factory for making summary statistics eg mean mode stddev .
"def _unify_call_signature ( i , dist_fn ) : if distribution_util . is_distribution_instance ( dist_fn ) : return ( lambda * _ : dist_fn ) , None if not callable ( dist_fn ) : raise TypeError ( '{} must be either `tfd.Distribution`-like or ' '`callable`.' . format ( dist_fn ) ) args = _get_required_args ( dist_fn ) if not args : return ( lambda * _ : dist_fn ( ) ) , ( ) @ functools . wraps ( dist_fn ) def dist_fn_wrapped ( * xs ) : """"""Calls `dist_fn` with reversed and truncated args."""""" if i != len ( xs ) : raise ValueError ( 'Internal Error: Unexpected number of inputs provided to {}-th ' 'distribution maker (dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , i , len ( xs ) ) ) if len ( xs ) < len ( args ) : raise ValueError ( 'Internal Error: Too few inputs provided to {}-th distribution maker ' '(dist_fn: {}, expected: {}, saw: {}).' . format ( i , dist_fn , len ( args ) , len ( xs ) ) ) return dist_fn ( * reversed ( xs [ - len ( args ) : ] ) ) return dist_fn_wrapped , args",Creates dist_fn_wrapped which calls dist_fn with all prev nodes .
"def _resolve_distribution_names ( dist_fn_args , dist_names , leaf_name ) : if dist_names is None : dist_names = [ ] else : dist_names = dist_names . copy ( ) n = len ( dist_fn_args ) dist_names . extend ( [ None ] * ( n - len ( dist_names ) ) ) for i_ , args in enumerate ( reversed ( dist_fn_args ) ) : if not args : continue i = n - i_ - 1 for j , arg_name in enumerate ( args ) : dist_names [ i - j - 1 ] = arg_name j = 0 for i_ in range ( len ( dist_names ) ) : i = n - i_ - 1 if dist_names [ i ] is None : dist_names [ i ] = leaf_name if j == 0 else leaf_name + str ( j ) j += 1 return tuple ( dist_names )",Uses arg names to resolve distribution names .
def _get_required_args ( fn ) : argspec = tf_inspect . getfullargspec ( fn ) args = argspec . args if tf_inspect . isclass ( fn ) : args = args [ 1 : ] if argspec . defaults : args = args [ : - len ( argspec . defaults ) ] return tuple ( args ),Returns the distribution s required args .
"def _kl_joint_joint ( d0 , d1 , name = None ) : if len ( d0 . _dist_fn_wrapped ) != len ( d1 . _dist_fn_wrapped ) : raise ValueError ( 'Can only compute KL divergence between when each has the' 'same number of component distributions.' ) if ( not all ( a is None for a in d0 . _dist_fn_args ) or not all ( a is None for a in d1 . _dist_fn_args ) ) : raise ValueError ( 'Can only compute KL divergence when all distributions are ' 'independent.' ) with tf . name_scope ( name or 'kl_jointseq_jointseq' ) : return sum ( kullback_leibler . kl_divergence ( d0_ ( ) , d1_ ( ) ) for d0_ , d1_ in zip ( d0 . _dist_fn_wrapped , d1 . _dist_fn_wrapped ) )",Calculate the KL divergence between two JointDistributionSequential s .
"def _build ( self , model ) : if not isinstance ( model , collections . Sequence ) : raise TypeError ( '`model` must be `list`-like (saw: {}).' . format ( type ( model ) . __name__ ) ) self . _dist_fn = model self . _dist_fn_wrapped , self . _dist_fn_args = zip ( * [ _unify_call_signature ( i , dist_fn ) for i , dist_fn in enumerate ( model ) ] )",Creates dist_fn dist_fn_wrapped dist_fn_args .
"def _resolve_graph ( self , distribution_names = None , leaf_name = 'x' ) : if distribution_names is None or any ( self . _dist_fn_args ) : distribution_names = _resolve_distribution_names ( self . _dist_fn_args , distribution_names , leaf_name ) if len ( set ( distribution_names ) ) != len ( distribution_names ) : raise ValueError ( 'Distribution names must be unique: {}' . format ( distribution_names ) ) if len ( distribution_names ) != len ( self . _dist_fn_wrapped ) : raise ValueError ( 'Distribution names must be 1:1 with `rvs`.' ) return tuple ( zip ( distribution_names , tuple ( ( ) if a is None else a for a in self . _dist_fn_args ) ) )",Creates a tuple of tuple s of dependencies .
"def _entropy ( self ) : if any ( self . _dist_fn_args ) : raise ValueError ( 'Can only compute entropy when all distributions are independent.' ) return sum ( joint_distribution_lib . maybe_check_wont_broadcast ( ( d ( ) . entropy ( ) for d in self . _dist_fn_wrapped ) , self . validate_args ) )",Shannon entropy in nats .
"def check_arg_in_support ( f ) : @ functools . wraps ( f ) def _check_arg_and_apply_f ( * args , * * kwargs ) : dist = args [ 0 ] x = args [ 1 ] with tf . control_dependencies ( [ assert_util . assert_greater_equal ( x , dist . loc , message = ""x is not in the support of the distribution"" ) ] if dist . validate_args else [ ] ) : return f ( * args , * * kwargs ) return _check_arg_and_apply_f",Decorator function for argument bounds checking .
"def _extend_support_with_default_value ( self , x , f , default_value ) : with tf . name_scope ( ""extend_support_with_default_value"" ) : x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = ""x"" ) loc = self . loc + tf . zeros_like ( self . scale ) + tf . zeros_like ( x ) x = x + tf . zeros_like ( loc ) y = f ( tf . where ( x < loc , self . _inv_z ( 0.5 ) + tf . zeros_like ( x ) , x ) ) if default_value == 0. : default_value = tf . zeros_like ( y ) elif default_value == 1. : default_value = tf . ones_like ( y ) else : default_value = tf . fill ( dims = tf . shape ( input = y ) , value = dtype_util . as_numpy_dtype ( self . dtype ) ( default_value ) ) return tf . where ( x < loc , default_value , y )",Returns f ( x ) if x is in the support and default_value otherwise .
"def _prepare_args ( log_likelihood_fn , state , log_likelihood = None , description = 'log_likelihood' ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( s , name = 'current_state' ) for s in state_parts ] log_likelihood = _maybe_call_fn ( log_likelihood_fn , state_parts , log_likelihood , description ) return [ state_parts , log_likelihood ]",Processes input args to meet list - like assumptions .
"def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'elliptical_slice' , 'one_step' ) , values = [ self . _seed_stream , current_state , previous_kernel_results . log_likelihood ] ) : with tf . compat . v1 . name_scope ( 'initialize' ) : [ init_state_parts , init_log_likelihood ] = _prepare_args ( self . log_likelihood_fn , current_state , previous_kernel_results . log_likelihood ) normal_samples = self . normal_sampler_fn ( self . _seed_stream ( ) ) normal_samples = list ( normal_samples ) if mcmc_util . is_list_like ( normal_samples ) else [ normal_samples ] u = tf . random . uniform ( shape = tf . shape ( init_log_likelihood ) , seed = self . _seed_stream ( ) , dtype = init_log_likelihood . dtype . base_dtype , ) threshold = init_log_likelihood + tf . math . log ( u ) starting_angle = tf . random . uniform ( shape = tf . shape ( init_log_likelihood ) , minval = 0. , maxval = 2 * np . pi , name = 'angle' , seed = self . _seed_stream ( ) , dtype = init_log_likelihood . dtype . base_dtype , ) starting_angle_min = starting_angle - 2 * np . pi starting_angle_max = starting_angle starting_state_parts = _rotate_on_ellipse ( init_state_parts , normal_samples , starting_angle ) starting_log_likelihood = self . log_likelihood_fn ( * starting_state_parts ) def chain_not_done ( angle , angle_min , angle_max , current_state_parts , current_log_likelihood ) : del angle , angle_min , angle_max , current_state_parts return tf . reduce_any ( current_log_likelihood < threshold ) def sample_next_angle ( angle , angle_min , angle_max , current_state_parts , current_log_likelihood ) : """"""Slice sample a new angle, and rotate init_state by that amount."""""" chain_not_done = current_log_likelihood < threshold angle_min = tf . where ( tf . math . logical_and ( angle < 0 , chain_not_done ) , angle , angle_min ) angle_max = tf . where ( tf . math . logical_and ( angle >= 0 , chain_not_done ) , angle , angle_max ) new_angle = tf . random . uniform ( shape = tf . shape ( current_log_likelihood ) , minval = angle_min , maxval = angle_max , seed = self . _seed_stream ( ) , dtype = angle . dtype . base_dtype ) angle = tf . where ( chain_not_done , new_angle , angle ) next_state_parts = _rotate_on_ellipse ( init_state_parts , normal_samples , angle ) new_state_parts = [ ] broadcasted_chain_not_done = _right_pad_with_ones ( chain_not_done , tf . rank ( next_state_parts [ 0 ] ) ) for n_state , c_state in zip ( next_state_parts , current_state_parts ) : new_state_part = tf . where ( tf . broadcast_to ( broadcasted_chain_not_done , tf . shape ( n_state ) ) , n_state , c_state ) new_state_parts . append ( new_state_part ) return ( angle , angle_min , angle_max , new_state_parts , self . log_likelihood_fn ( * new_state_parts ) ) [ next_angle , _ , _ , next_state_parts , next_log_likelihood , ] = tf . while_loop ( cond = chain_not_done , body = sample_next_angle , loop_vars = [ starting_angle , starting_angle_min , starting_angle_max , starting_state_parts , starting_log_likelihood ] ) return [ next_state_parts if mcmc_util . is_list_like ( current_state ) else next_state_parts [ 0 ] , EllipticalSliceSamplerKernelResults ( log_likelihood = next_log_likelihood , angle = next_angle , normal_samples = normal_samples , ) , ]",Runs one iteration of the Elliptical Slice Sampler .
"def image_summary ( seqs , name , num = None ) : seqs = tf . clip_by_value ( seqs , 0. , 1. ) seqs = tf . unstack ( seqs [ : num ] ) joined_seqs = [ tf . concat ( tf . unstack ( seq ) , 1 ) for seq in seqs ] joined_seqs = tf . expand_dims ( tf . concat ( joined_seqs , 0 ) , 0 ) tf . compat . v2 . summary . image ( name , joined_seqs , max_outputs = 1 , step = tf . compat . v1 . train . get_or_create_global_step ( ) )",Visualizes sequences as TensorBoard summaries .
"def visualize_reconstruction ( inputs , reconstruct , num = 3 , name = ""reconstruction"" ) : reconstruct = tf . clip_by_value ( reconstruct , 0. , 1. ) inputs_and_reconstruct = tf . concat ( ( inputs [ : num ] , reconstruct [ : num ] ) , axis = 0 ) image_summary ( inputs_and_reconstruct , name )",Visualizes the reconstruction of inputs in TensorBoard .
"def visualize_qualitative_analysis ( inputs , model , samples = 1 , batch_size = 3 , length = 8 ) : average = lambda dist : tf . reduce_mean ( input_tensor = dist . mean ( ) , axis = 0 ) with tf . compat . v1 . name_scope ( ""val_reconstruction"" ) : reconstruct = functools . partial ( model . reconstruct , inputs = inputs , samples = samples ) visualize_reconstruction ( inputs , average ( reconstruct ( ) ) ) visualize_reconstruction ( inputs , average ( reconstruct ( sample_static = True ) ) , name = ""static_prior"" ) visualize_reconstruction ( inputs , average ( reconstruct ( sample_dynamic = True ) ) , name = ""dynamic_prior"" ) visualize_reconstruction ( inputs , average ( reconstruct ( swap_static = True ) ) , name = ""swap_static"" ) visualize_reconstruction ( inputs , average ( reconstruct ( swap_dynamic = True ) ) , name = ""swap_dynamic"" ) with tf . compat . v1 . name_scope ( ""generation"" ) : generate = functools . partial ( model . generate , batch_size = batch_size , length = length , samples = samples ) image_summary ( average ( generate ( fix_static = True ) ) , ""fix_static"" ) image_summary ( average ( generate ( fix_dynamic = True ) ) , ""fix_dynamic"" )",Visualizes a qualitative analysis of a given model .
"def summarize_dist_params ( dist , name , name_scope = ""dist_params"" ) : with tf . compat . v1 . name_scope ( name_scope ) : tf . compat . v2 . summary . histogram ( name = ""{}/{}"" . format ( name , ""mean"" ) , data = dist . mean ( ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) ) tf . compat . v2 . summary . histogram ( name = ""{}/{}"" . format ( name , ""stddev"" ) , data = dist . stddev ( ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) )",Summarize the parameters of a distribution .
"def summarize_mean_in_nats_and_bits ( inputs , units , name , nats_name_scope = ""nats"" , bits_name_scope = ""bits_per_dim"" ) : mean = tf . reduce_mean ( input_tensor = inputs ) with tf . compat . v1 . name_scope ( nats_name_scope ) : tf . compat . v2 . summary . scalar ( name , mean , step = tf . compat . v1 . train . get_or_create_global_step ( ) ) with tf . compat . v1 . name_scope ( bits_name_scope ) : tf . compat . v2 . summary . scalar ( name , mean / units / tf . math . log ( 2. ) , step = tf . compat . v1 . train . get_or_create_global_step ( ) )",Summarize the mean of a tensor in nats and bits per unit .
"def call ( self , inputs ) : del inputs with tf . compat . v1 . name_scope ( self . _name ) : return tfd . MultivariateNormalDiag ( self . loc , self . scale_diag )",Runs the model to generate multivariate normal distribution .
"def zero_state ( self , sample_batch_shape = ( ) ) : h0 = tf . zeros ( [ 1 , self . hidden_size ] ) c0 = tf . zeros ( [ 1 , self . hidden_size ] ) combined_shape = tf . concat ( ( tf . convert_to_tensor ( value = sample_batch_shape , dtype = tf . int32 ) , [ self . dimensions ] ) , axis = - 1 ) previous_output = tf . zeros ( combined_shape ) return previous_output , ( h0 , c0 )",Returns an initial state for the LSTM cell .
"def call ( self , inputs , state ) : original_shape = inputs . shape if len ( original_shape ) < 2 : inputs = tf . reshape ( inputs , [ 1 , - 1 ] ) out , state = self . lstm_cell ( inputs , state ) out = self . output_layer ( out ) correct_shape = tf . concat ( ( original_shape [ : - 1 ] , tf . shape ( input = out ) [ - 1 : ] ) , 0 ) out = tf . reshape ( out , correct_shape ) loc = out [ ... , : self . dimensions ] scale_diag = tf . nn . softplus ( out [ ... , self . dimensions : ] ) + 1e-5 return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag ) , state",Runs the model to generate a distribution for a single timestep .
"def call ( self , inputs ) : dynamic , static = inputs timesteps = tf . shape ( input = dynamic ) [ - 2 ] static = static [ ... , tf . newaxis , : ] + tf . zeros ( [ timesteps , 1 ] ) latents = tf . concat ( [ dynamic , static ] , axis = - 1 ) out = self . dense ( latents ) out = tf . reshape ( out , ( - 1 , 1 , 1 , self . hidden_size ) ) out = self . conv_transpose1 ( out ) out = self . conv_transpose2 ( out ) out = self . conv_transpose3 ( out ) out = self . conv_transpose4 ( out ) expanded_shape = tf . concat ( ( tf . shape ( input = latents ) [ : - 1 ] , tf . shape ( input = out ) [ 1 : ] ) , axis = 0 ) out = tf . reshape ( out , expanded_shape ) return tfd . Independent ( distribution = tfd . Normal ( loc = out , scale = 1. ) , reinterpreted_batch_ndims = 3 , name = ""decoded_image"" )",Runs the model to generate a distribution p ( x_t | z_t f ) .
"def call ( self , inputs ) : image_shape = tf . shape ( input = inputs ) [ - 3 : ] collapsed_shape = tf . concat ( ( [ - 1 ] , image_shape ) , axis = 0 ) out = tf . reshape ( inputs , collapsed_shape ) out = self . conv1 ( out ) out = self . conv2 ( out ) out = self . conv3 ( out ) out = self . conv4 ( out ) expanded_shape = tf . concat ( ( tf . shape ( input = inputs ) [ : - 3 ] , [ - 1 ] ) , axis = 0 ) return tf . reshape ( out , expanded_shape )",Runs the model to generate an intermediate representation of x_t .
"def call ( self , inputs ) : collapsed_shape = tf . concat ( ( [ - 1 ] , tf . shape ( input = inputs ) [ - 2 : ] ) , axis = 0 ) out = tf . reshape ( inputs , collapsed_shape ) out = self . bilstm ( out ) expanded_shape = tf . concat ( ( tf . shape ( input = inputs ) [ : - 2 ] , [ - 1 ] ) , axis = 0 ) out = tf . reshape ( out , expanded_shape ) out = self . output_layer ( out ) loc = out [ ... , : self . latent_size ] scale_diag = tf . nn . softplus ( out [ ... , self . latent_size : ] ) + 1e-5 return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag )",Runs the model to generate a distribution q ( f | x_ { 1 : T } ) .
"def call ( self , inputs ) : out = self . dense ( inputs ) out = self . output_layer ( out ) loc = out [ ... , : self . latent_size ] scale_diag = tf . nn . softplus ( out [ ... , self . latent_size : ] ) + 1e-5 return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag )",Runs the model to generate a distribution q ( z_ { 1 : T } | x_ { 1 : T } ) .
"def call ( self , inputs ) : features , static_sample = inputs length = tf . shape ( input = features ) [ - 2 ] static_sample = static_sample [ ... , tf . newaxis , : ] + tf . zeros ( [ length , 1 ] ) sample_shape_static = tf . shape ( input = static_sample ) [ : - 3 ] sample_shape_inputs = tf . shape ( input = features ) [ : - 3 ] broadcast_shape_inputs = tf . concat ( ( sample_shape_static , [ 1 , 1 , 1 ] ) , 0 ) broadcast_shape_static = tf . concat ( ( sample_shape_inputs , [ 1 , 1 , 1 ] ) , 0 ) features = features + tf . zeros ( broadcast_shape_inputs ) static_sample = static_sample + tf . zeros ( broadcast_shape_static ) combined = tf . concat ( ( features , static_sample ) , axis = - 1 ) collapsed_shape = tf . concat ( ( [ - 1 ] , tf . shape ( input = combined ) [ - 2 : ] ) , axis = 0 ) out = tf . reshape ( combined , collapsed_shape ) out = self . bilstm ( out ) out = self . rnn ( out ) expanded_shape = tf . concat ( ( tf . shape ( input = combined ) [ : - 2 ] , tf . shape ( input = out ) [ 1 : ] ) , axis = 0 ) out = tf . reshape ( out , expanded_shape ) out = self . output_layer ( out ) loc = out [ ... , : self . latent_size ] scale_diag = tf . nn . softplus ( out [ ... , self . latent_size : ] ) + 1e-5 return tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag )",Runs the model to generate a distribution q ( z_ { 1 : T } | x_ { 1 : T } f ) .
"def generate ( self , batch_size , length , samples = 1 , fix_static = False , fix_dynamic = False ) : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood",Generate new sequences .
"def reconstruct ( self , inputs , samples = 1 , sample_static = False , sample_dynamic = False , swap_static = False , swap_dynamic = False , fix_static = False , fix_dynamic = False ) : batch_size = tf . shape ( input = inputs ) [ - 5 ] length = len ( tf . unstack ( inputs , axis = - 4 ) ) features = self . compressor ( inputs ) if sample_static : static_sample , _ = self . sample_static_prior ( samples , batch_size , fix_static ) else : static_sample , _ = self . sample_static_posterior ( features , samples ) if swap_static : static_sample = tf . reverse ( static_sample , axis = [ 1 ] ) if sample_dynamic : dynamic_sample , _ = self . sample_dynamic_prior ( samples , batch_size , length , fix_dynamic ) else : dynamic_sample , _ = self . sample_dynamic_posterior ( features , samples , static_sample ) if swap_dynamic : dynamic_sample = tf . reverse ( dynamic_sample , axis = [ 1 ] ) likelihood = self . decoder ( ( dynamic_sample , static_sample ) ) return likelihood",Reconstruct the given input sequences .
"def sample_static_prior ( self , samples , batch_size , fixed = False ) : dist = self . static_prior ( ) if fixed : sample = dist . sample ( ( samples , 1 ) ) + tf . zeros ( [ batch_size , 1 ] ) else : sample = dist . sample ( ( samples , batch_size ) ) return sample , dist",Sample the static latent prior .
"def sample_static_posterior ( self , inputs , samples ) : dist = self . static_encoder ( inputs ) sample = dist . sample ( samples ) return sample , dist",Sample the static latent posterior .
"def sample_dynamic_prior ( self , samples , batch_size , length , fixed = False ) : if fixed : sample_batch_size = 1 else : sample_batch_size = batch_size sample , state = self . dynamic_prior . zero_state ( [ samples , sample_batch_size ] ) locs = [ ] scale_diags = [ ] sample_list = [ ] for _ in range ( length ) : dist , state = self . dynamic_prior ( sample , state ) sample = dist . sample ( ) locs . append ( dist . parameters [ ""loc"" ] ) scale_diags . append ( dist . parameters [ ""scale_diag"" ] ) sample_list . append ( sample ) sample = tf . stack ( sample_list , axis = 2 ) loc = tf . stack ( locs , axis = 2 ) scale_diag = tf . stack ( scale_diags , axis = 2 ) if fixed : sample = sample + tf . zeros ( [ batch_size , 1 , 1 ] ) return sample , tfd . MultivariateNormalDiag ( loc = loc , scale_diag = scale_diag )",Sample the dynamic latent prior .
"def sample_dynamic_posterior ( self , inputs , samples , static_sample = None ) : if self . latent_posterior == ""factorized"" : dist = self . dynamic_encoder ( inputs ) samples = dist . sample ( samples ) else : if static_sample is None : raise ValueError ( ""The full dynamic posterior requires a static latent sample"" ) dist = self . dynamic_encoder ( ( inputs , static_sample ) ) samples = dist . sample ( ) return samples , dist",Sample the static latent posterior .
"def batch_shape ( self ) : batch_shape = tf . TensorShape ( [ ] ) for param in self . parameters : batch_shape = tf . broadcast_static_shape ( batch_shape , param . prior . batch_shape ) return batch_shape",Static batch shape of models represented by this component .
"def batch_shape_tensor ( self ) : batch_shape = tf . constant ( [ ] , dtype = tf . int32 ) for param in self . parameters : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , param . prior . batch_shape_tensor ( ) ) return batch_shape",Runtime batch shape of models represented by this component .
"def _canonicalize_param_vals_as_map ( self , param_vals ) : if hasattr ( param_vals , 'keys' ) : param_map = param_vals else : param_map = { p . name : v for ( p , v ) in zip ( self . parameters , param_vals ) } return param_map",If given an ordered list of parameter values build a name : value map .
"def make_state_space_model ( self , num_timesteps , param_vals = None , initial_state_prior = None , initial_step = 0 ) : return self . _make_state_space_model ( num_timesteps = num_timesteps , param_map = self . _canonicalize_param_vals_as_map ( param_vals ) , initial_state_prior = initial_state_prior , initial_step = initial_step )",Instantiate this model as a Distribution over specified num_timesteps .
"def prior_sample ( self , num_timesteps , initial_step = 0 , params_sample_shape = ( ) , trajectories_sample_shape = ( ) , seed = None ) : seed = distributions . SeedStream ( seed , salt = 'StructuralTimeSeries_prior_sample' ) with tf . compat . v1 . name_scope ( 'prior_sample' , values = [ num_timesteps , params_sample_shape , trajectories_sample_shape ] ) : param_samples = [ p . prior . sample ( params_sample_shape , seed = seed ( ) , name = p . name ) for p in self . parameters ] model = self . make_state_space_model ( num_timesteps = num_timesteps , initial_step = initial_step , param_vals = param_samples ) return model . sample ( trajectories_sample_shape , seed = seed ( ) ) , param_samples",Sample from the joint prior over model parameters and trajectories .
"def joint_log_prob ( self , observed_time_series ) : with tf . compat . v1 . name_scope ( 'joint_log_prob' , values = [ observed_time_series ] ) : [ observed_time_series , mask ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) num_timesteps = distribution_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] def log_joint_fn ( * param_vals ) : """"""Generated log-density function."""""" param_lp = sum ( [ param . prior . log_prob ( param_val ) for ( param , param_val ) in zip ( self . parameters , param_vals ) ] ) lgssm = self . make_state_space_model ( param_vals = param_vals , num_timesteps = num_timesteps ) observation_lp = lgssm . log_prob ( observed_time_series , mask = mask ) sample_ndims = tf . maximum ( 0 , tf . rank ( observation_lp ) - tf . rank ( param_lp ) ) observation_lp = tf . reduce_sum ( input_tensor = observation_lp , axis = tf . range ( sample_ndims ) ) return param_lp + observation_lp return log_joint_fn",Build the joint density log p ( params ) + log p ( y|params ) as a callable .
"def _compute_min_event_ndims ( bijector_list , compute_forward = True ) : min_event_ndims = 0 rank_changed_adjusted_max_min_event_ndims = 0 if compute_forward : bijector_list = reversed ( bijector_list ) for b in bijector_list : if compute_forward : current_min_event_ndims = b . forward_min_event_ndims current_inverse_min_event_ndims = b . inverse_min_event_ndims else : current_min_event_ndims = b . inverse_min_event_ndims current_inverse_min_event_ndims = b . forward_min_event_ndims if rank_changed_adjusted_max_min_event_ndims < current_min_event_ndims : min_event_ndims += ( current_min_event_ndims - rank_changed_adjusted_max_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims = max ( current_min_event_ndims , rank_changed_adjusted_max_min_event_ndims ) number_of_changed_dimensions = ( current_min_event_ndims - current_inverse_min_event_ndims ) rank_changed_adjusted_max_min_event_ndims -= number_of_changed_dimensions return min_event_ndims",Computes the min_event_ndims associated with the give list of bijectors .
"def vector_size_to_square_matrix_size ( d , validate_args , name = None ) : if isinstance ( d , ( float , int , np . generic , np . ndarray ) ) : n = ( - 1 + np . sqrt ( 1 + 8 * d ) ) / 2. if float ( int ( n ) ) != n : raise ValueError ( ""Vector length is not a triangular number."" ) return int ( n ) else : with tf . name_scope ( name or ""vector_size_to_square_matrix_size"" ) as name : n = ( - 1. + tf . sqrt ( 1 + 8. * tf . cast ( d , dtype = tf . float32 ) ) ) / 2. if validate_args : with tf . control_dependencies ( [ assert_util . assert_equal ( tf . cast ( tf . cast ( n , dtype = tf . int32 ) , dtype = tf . float32 ) , n , message = ""Vector length is not a triangular number"" ) ] ) : n = tf . identity ( n ) return tf . cast ( n , d . dtype )",Convert a vector size to a matrix size .
"def _argsort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) return np . argsort ( values , axis , kind = 'stable' if stable else 'quicksort' )",Numpy implementation of tf . argsort .
"def _sort ( values , axis = - 1 , direction = 'ASCENDING' , stable = False , name = None ) : if direction == 'ASCENDING' : pass elif direction == 'DESCENDING' : values = np . negative ( values ) else : raise ValueError ( 'Unrecognized direction: {}.' . format ( direction ) ) result = np . sort ( values , axis , kind = 'stable' if stable else 'quicksort' ) if direction == 'DESCENDING' : return np . negative ( result ) return result",Numpy implementation of tf . sort .
"def _kl_gumbel_gumbel ( a , b , name = None ) : with tf . name_scope ( name or ""kl_gumbel_gumbel"" ) : return ( tf . math . log ( b . scale ) - tf . math . log ( a . scale ) + np . euler_gamma * ( a . scale / b . scale - 1. ) + tf . math . expm1 ( ( b . loc - a . loc ) / b . scale + tf . math . lgamma ( a . scale / b . scale + 1. ) ) + ( a . loc - b . loc ) / b . scale )",Calculate the batched KL divergence KL ( a || b ) with a and b Gumbel .
"def ndtr ( x , name = ""ndtr"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if dtype_util . as_numpy_dtype ( x . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( ""x.dtype=%s is not handled, see docstring for supported types."" % x . dtype ) return _ndtr ( x )",Normal distribution function .
"def _ndtr ( x ) : half_sqrt_2 = tf . constant ( 0.5 * np . sqrt ( 2. ) , dtype = x . dtype , name = ""half_sqrt_2"" ) w = x * half_sqrt_2 z = tf . abs ( w ) y = tf . where ( tf . less ( z , half_sqrt_2 ) , 1. + tf . math . erf ( w ) , tf . where ( tf . greater ( w , 0. ) , 2. - tf . math . erfc ( z ) , tf . math . erfc ( z ) ) ) return 0.5 * y",Implements ndtr core logic .
"def ndtri ( p , name = ""ndtri"" ) : with tf . name_scope ( name ) : p = tf . convert_to_tensor ( value = p , name = ""p"" ) if dtype_util . as_numpy_dtype ( p . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( ""p.dtype=%s is not handled, see docstring for supported types."" % p . dtype ) return _ndtri ( p )",The inverse of the CDF of the Normal distribution function .
"def _ndtri ( p ) : p0 = list ( reversed ( [ - 5.99633501014107895267E1 , 9.80010754185999661536E1 , - 5.66762857469070293439E1 , 1.39312609387279679503E1 , - 1.23916583867381258016E0 ] ) ) q0 = list ( reversed ( [ 1.0 , 1.95448858338141759834E0 , 4.67627912898881538453E0 , 8.63602421390890590575E1 , - 2.25462687854119370527E2 , 2.00260212380060660359E2 , - 8.20372256168333339912E1 , 1.59056225126211695515E1 , - 1.18331621121330003142E0 ] ) ) p1 = list ( reversed ( [ 4.05544892305962419923E0 , 3.15251094599893866154E1 , 5.71628192246421288162E1 , 4.40805073893200834700E1 , 1.46849561928858024014E1 , 2.18663306850790267539E0 , - 1.40256079171354495875E-1 , - 3.50424626827848203418E-2 , - 8.57456785154685413611E-4 ] ) ) q1 = list ( reversed ( [ 1.0 , 1.57799883256466749731E1 , 4.53907635128879210584E1 , 4.13172038254672030440E1 , 1.50425385692907503408E1 , 2.50464946208309415979E0 , - 1.42182922854787788574E-1 , - 3.80806407691578277194E-2 , - 9.33259480895457427372E-4 ] ) ) p2 = list ( reversed ( [ 3.23774891776946035970E0 , 6.91522889068984211695E0 , 3.93881025292474443415E0 , 1.33303460815807542389E0 , 2.01485389549179081538E-1 , 1.23716634817820021358E-2 , 3.01581553508235416007E-4 , 2.65806974686737550832E-6 , 6.23974539184983293730E-9 ] ) ) q2 = list ( reversed ( [ 1.0 , 6.02427039364742014255E0 , 3.67983563856160859403E0 , 1.37702099489081330271E0 , 2.16236993594496635890E-1 , 1.34204006088543189037E-2 , 3.28014464682127739104E-4 , 2.89247864745380683936E-6 , 6.79019408009981274425E-9 ] ) ) def _create_polynomial ( var , coeffs ) : """"""Compute n_th order polynomial via Horner's method."""""" coeffs = np . array ( coeffs , dtype_util . as_numpy_dtype ( var . dtype ) ) if not coeffs . size : return tf . zeros_like ( var ) return coeffs [ 0 ] + _create_polynomial ( var , coeffs [ 1 : ] ) * var maybe_complement_p = tf . where ( p > - np . expm1 ( - 2. ) , 1. - p , p ) sanitized_mcp = tf . where ( maybe_complement_p <= 0. , tf . fill ( tf . shape ( input = p ) , dtype_util . as_numpy_dtype ( p . dtype ) ( 0.5 ) ) , maybe_complement_p ) w = sanitized_mcp - 0.5 ww = w ** 2 x_for_big_p = w + w * ww * ( _create_polynomial ( ww , p0 ) / _create_polynomial ( ww , q0 ) ) x_for_big_p *= - np . sqrt ( 2. * np . pi ) z = tf . sqrt ( - 2. * tf . math . log ( sanitized_mcp ) ) first_term = z - tf . math . log ( z ) / z second_term_small_p = ( _create_polynomial ( 1. / z , p2 ) / _create_polynomial ( 1. / z , q2 ) / z ) second_term_otherwise = ( _create_polynomial ( 1. / z , p1 ) / _create_polynomial ( 1. / z , q1 ) / z ) x_for_small_p = first_term - second_term_small_p x_otherwise = first_term - second_term_otherwise x = tf . where ( sanitized_mcp > np . exp ( - 2. ) , x_for_big_p , tf . where ( z >= 8.0 , x_for_small_p , x_otherwise ) ) x = tf . where ( p > 1. - np . exp ( - 2. ) , x , - x ) infinity_scalar = tf . constant ( np . inf , dtype = p . dtype ) infinity = tf . fill ( tf . shape ( input = p ) , infinity_scalar ) x_nan_replaced = tf . where ( p <= 0.0 , - infinity , tf . where ( p >= 1.0 , infinity , x ) ) return x_nan_replaced",Implements ndtri core logic .
"def log_ndtr ( x , series_order = 3 , name = ""log_ndtr"" ) : if not isinstance ( series_order , int ) : raise TypeError ( ""series_order must be a Python integer."" ) if series_order < 0 : raise ValueError ( ""series_order must be non-negative."" ) if series_order > 30 : raise ValueError ( ""series_order must be <= 30."" ) with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if dtype_util . base_equal ( x . dtype , tf . float64 ) : lower_segment = LOGNDTR_FLOAT64_LOWER upper_segment = LOGNDTR_FLOAT64_UPPER elif dtype_util . base_equal ( x . dtype , tf . float32 ) : lower_segment = LOGNDTR_FLOAT32_LOWER upper_segment = LOGNDTR_FLOAT32_UPPER else : raise TypeError ( ""x.dtype=%s is not supported."" % x . dtype ) return tf . where ( tf . greater ( x , upper_segment ) , - _ndtr ( - x ) , tf . where ( tf . greater ( x , lower_segment ) , tf . math . log ( _ndtr ( tf . maximum ( x , lower_segment ) ) ) , _log_ndtr_lower ( tf . minimum ( x , lower_segment ) , series_order ) ) )",Log Normal distribution function .
"def _log_ndtr_lower ( x , series_order ) : x_2 = tf . square ( x ) log_scale = - 0.5 * x_2 - tf . math . log ( - x ) - 0.5 * np . log ( 2. * np . pi ) return log_scale + tf . math . log ( _log_ndtr_asymptotic_series ( x , series_order ) )",Asymptotic expansion version of Log [ cdf ( x ) ] appropriate for x<< - 1 .
"def _log_ndtr_asymptotic_series ( x , series_order ) : npdt = dtype_util . as_numpy_dtype ( x . dtype ) if series_order <= 0 : return npdt ( 1 ) x_2 = tf . square ( x ) even_sum = tf . zeros_like ( x ) odd_sum = tf . zeros_like ( x ) x_2n = x_2 for n in range ( 1 , series_order + 1 ) : y = npdt ( _double_factorial ( 2 * n - 1 ) ) / x_2n if n % 2 : odd_sum += y else : even_sum += y x_2n *= x_2 return 1. + even_sum - odd_sum",Calculates the asymptotic series used in log_ndtr .
"def erfinv ( x , name = ""erfinv"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if dtype_util . as_numpy_dtype ( x . dtype ) not in [ np . float32 , np . float64 ] : raise TypeError ( ""x.dtype={} is not handled, see docstring for supported "" ""types."" . format ( dtype_util . name ( x . dtype ) ) ) return ndtri ( ( x + 1. ) / 2. ) / np . sqrt ( 2. )",The inverse function for erf the error function .
"def log_cdf_laplace ( x , name = ""log_cdf_laplace"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) lower_solution = - np . log ( 2. ) + x safe_exp_neg_x = tf . exp ( - tf . abs ( x ) ) upper_solution = tf . math . log1p ( - 0.5 * safe_exp_neg_x ) return tf . where ( x < 0. , lower_solution , upper_solution )",Log Laplace distribution function .
"def text_messages_joint_log_prob ( count_data , lambda_1 , lambda_2 , tau ) : alpha = ( 1. / tf . reduce_mean ( input_tensor = count_data ) ) rv_lambda = tfd . Exponential ( rate = alpha ) rv_tau = tfd . Uniform ( ) lambda_ = tf . gather ( [ lambda_1 , lambda_2 ] , indices = tf . cast ( tau * tf . cast ( tf . size ( input = count_data ) , dtype = tf . float32 ) <= tf . cast ( tf . range ( tf . size ( input = count_data ) ) , dtype = tf . float32 ) , dtype = tf . int32 ) ) rv_observation = tfd . Poisson ( rate = lambda_ ) return ( rv_lambda . log_prob ( lambda_1 ) + rv_lambda . log_prob ( lambda_2 ) + rv_tau . log_prob ( tau ) + tf . reduce_sum ( input_tensor = rv_observation . log_prob ( count_data ) ) )",Joint log probability function .
"def benchmark_text_messages_hmc ( num_results = int ( 3e3 ) , num_burnin_steps = int ( 3e3 ) , num_leapfrog_steps = 3 ) : if not tf . executing_eagerly ( ) : tf . compat . v1 . reset_default_graph ( ) count_data = tf . cast ( tf . concat ( [ tfd . Poisson ( rate = 15. ) . sample ( 43 ) , tfd . Poisson ( rate = 25. ) . sample ( 31 ) ] , axis = 0 ) , dtype = tf . float32 ) if tf . executing_eagerly ( ) : count_data = count_data . numpy ( ) else : with tf . compat . v1 . Session ( ) : count_data = count_data . eval ( ) def unnormalized_log_posterior ( lambda1 , lambda2 , tau ) : return text_messages_joint_log_prob ( count_data , lambda1 , lambda2 , tau ) if tf . executing_eagerly ( ) : sample_chain = tf . function ( tfp . mcmc . sample_chain ) else : sample_chain = tfp . mcmc . sample_chain step_size = tf . compat . v2 . Variable ( name = 'step_size' , initial_value = tf . constant ( 0.05 , dtype = tf . float32 ) , trainable = False ) def computation ( ) : """"""The benchmark computation."""""" initial_chain_state = [ tf . constant ( count_data . mean ( ) , name = 'init_lambda1' ) , tf . constant ( count_data . mean ( ) , name = 'init_lambda2' ) , tf . constant ( 0.5 , name = 'init_tau' ) , ] unconstraining_bijectors = [ tfp . bijectors . Exp ( ) , tfp . bijectors . Exp ( ) , tfp . bijectors . Sigmoid ( ) , ] _ , kernel_results = sample_chain ( num_results = num_results , num_burnin_steps = num_burnin_steps , current_state = initial_chain_state , kernel = tfp . mcmc . TransformedTransitionKernel ( inner_kernel = tfp . mcmc . HamiltonianMonteCarlo ( target_log_prob_fn = unnormalized_log_posterior , num_leapfrog_steps = num_leapfrog_steps , step_size = step_size , step_size_update_fn = tfp . mcmc . make_simple_step_size_update_policy ( num_burnin_steps ) , state_gradients_are_stopped = True ) , bijector = unconstraining_bijectors ) ) return kernel_results . inner_results . is_accepted is_accepted_tensor = computation ( ) if not tf . executing_eagerly ( ) : session = tf . compat . v1 . Session ( ) session . run ( tf . compat . v1 . global_variables_initializer ( ) ) session . run ( is_accepted_tensor ) start_time = time . time ( ) if tf . executing_eagerly ( ) : is_accepted = computation ( ) else : is_accepted = session . run ( is_accepted_tensor ) wall_time = time . time ( ) - start_time num_accepted = np . sum ( is_accepted ) acceptance_rate = np . float32 ( num_accepted ) / np . float32 ( num_results ) return dict ( iters = ( num_results + num_burnin_steps ) * num_leapfrog_steps , extras = { 'acceptance_rate' : acceptance_rate } , wall_time = wall_time )",Runs HMC on the text - messages unnormalized posterior .
"def _is_univariate_marginal ( self , index_points ) : num_index_points = tf . compat . dimension_value ( index_points . shape [ - ( self . kernel . feature_ndims + 1 ) ] ) if num_index_points is None : warnings . warn ( 'Unable to detect statically whether the number of index_points is ' '1. As a result, defaulting to treating the marginal GP at ' '`index_points` as a multivariate Gaussian. This makes some methods, ' 'like `cdf` unavailable.' ) return num_index_points == 1",True if the given index_points would yield a univariate marginal .
"def get_marginal_distribution ( self , index_points = None ) : with self . _name_scope ( 'get_marginal_distribution' ) : index_points = self . _get_index_points ( index_points ) covariance = self . _compute_covariance ( index_points ) loc = self . _mean_fn ( index_points ) if self . _is_univariate_marginal ( index_points ) : scale = tf . sqrt ( covariance ) loc = tf . squeeze ( loc , axis = - 1 ) return normal . Normal ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' ) else : scale = tf . linalg . LinearOperatorLowerTriangular ( tf . linalg . cholesky ( _add_diagonal_shift ( covariance , self . jitter ) ) , is_non_singular = True , name = 'GaussianProcessScaleLinearOperator' ) return mvn_linear_operator . MultivariateNormalLinearOperator ( loc = loc , scale = scale , validate_args = self . _validate_args , allow_nan_stats = self . _allow_nan_stats , name = 'marginal_distribution' )",Compute the marginal of this GP over function values at index_points .
"def _get_index_points ( self , index_points = None ) : if self . _index_points is None and index_points is None : raise ValueError ( 'This GaussianProcess instance was not instantiated with a value for ' 'index_points. One must therefore be provided when calling sample, ' 'log_prob, and other such methods. In particular, one can\'t compute ' 'KL divergences to/from an instance of `GaussianProccess` with ' 'unspecified `index_points` directly. Instead, use the ' '`get_marginal_distribution` function, which takes `index_points` as ' 'an argument and returns a `Normal` or ' '`MultivariateNormalLinearOperator` instance, whose KL can be ' 'computed.' ) return index_points if index_points is not None else self . _index_points",Return index_points if not None else self . _index_points .
"def _logsum_expbig_minus_expsmall ( big , small ) : with tf . name_scope ( ""logsum_expbig_minus_expsmall"" ) : return tf . math . log1p ( - tf . exp ( small - big ) ) + big",Stable evaluation of Log [ exp { big } - exp { small } ] .
"def _log_prob_with_logsf_and_logcdf ( self , y ) : logsf_y = self . log_survival_function ( y ) logsf_y_minus_1 = self . log_survival_function ( y - 1 ) logcdf_y = self . log_cdf ( y ) logcdf_y_minus_1 = self . log_cdf ( y - 1 ) big = tf . where ( logsf_y < logcdf_y , logsf_y_minus_1 , logcdf_y ) small = tf . where ( logsf_y < logcdf_y , logsf_y , logcdf_y_minus_1 ) return _logsum_expbig_minus_expsmall ( big , small )",Compute log_prob ( y ) using log survival_function and cdf together .
"def make_iaf_stack ( total_event_size , num_hidden_layers = 2 , seed = None , dtype = tf . float32 ) : seed = tfd . SeedStream ( seed , 'make_iaf_stack' ) def make_iaf ( ) : """"""Create an IAF."""""" initializer = tf . compat . v2 . keras . initializers . VarianceScaling ( 2 * 0.01 , seed = seed ( ) % ( 2 ** 31 - 1 ) ) made = tfb . AutoregressiveLayer ( params = 2 , event_shape = [ total_event_size ] , hidden_units = [ total_event_size ] * num_hidden_layers , activation = tf . nn . elu , kernel_initializer = initializer , dtype = dtype ) def shift_and_scale ( x ) : x . set_shape ( x . shape . merge_with ( [ None ] * ( x . shape . ndims - 1 ) + [ total_event_size ] ) ) return tf . unstack ( made ( x ) , num = 2 , axis = - 1 ) return tfb . Invert ( tfb . MaskedAutoregressiveFlow ( shift_and_scale ) ) def make_swap ( ) : """"""Create an swap."""""" permutation = list ( reversed ( range ( total_event_size ) ) ) return tfb . Permute ( permutation ) bijector = make_iaf ( ) bijector = make_swap ( ) ( bijector ) bijector = make_iaf ( ) ( bijector ) bijector = make_swap ( ) ( bijector ) bijector = make_iaf ( ) ( bijector ) bijector = make_swap ( ) ( bijector ) return bijector",Creates an stacked IAF bijector .
"def one_step ( self , current_state , previous_kernel_results ) : @ tfp . mcmc . internal . util . make_innermost_setter def set_num_leapfrog_steps ( kernel_results , num_leapfrog_steps ) : return kernel_results . _replace ( accepted_results = kernel_results . accepted_results . _replace ( num_leapfrog_steps = num_leapfrog_steps ) ) step_size = previous_kernel_results . new_step_size previous_kernel_results = set_num_leapfrog_steps ( previous_kernel_results , self . _num_leapfrog_steps ( step_size ) ) new_state , kernel_results = self . _kernel . one_step ( self . _flatten_state ( current_state ) , previous_kernel_results ) return self . _unflatten_state ( new_state ) , kernel_results",Runs one iteration of NeuTra .
"def bootstrap_results ( self , state ) : def loss ( ) : q = self . _flattened_variational_distribution ( ) samples = q . sample ( self . train_batch_size ) return tf . reduce_mean ( input_tensor = q . log_prob ( samples ) - self . _flattened_target_log_prob ( samples ) , axis = - 1 ) lr = tf . convert_to_tensor ( value = self . learning_rate , dtype = self . _dtype ) dtype = lr . dtype learning_rate = tf . compat . v2 . optimizers . schedules . PiecewiseConstantDecay ( list ( self . num_train_steps * np . array ( [ 0.2 , 0.8 ] ) . astype ( dtype . as_numpy_dtype ( ) ) ) , [ lr , lr * 0.1 , lr * 0.01 ] ) opt = tf . compat . v2 . optimizers . Adam ( learning_rate ) @ tf . function ( autograph = False ) def train_step ( ) : with tf . GradientTape ( ) as tape : loss_val = loss ( ) vals = tape . watched_variables ( ) grads = tape . gradient ( loss_val , vals ) grads_and_vals = list ( zip ( grads , vals ) ) opt . apply_gradients ( grads_and_vals ) return loss_val for step in range ( self . num_train_steps ) : loss_val = train_step ( ) tf . debugging . assert_all_finite ( loss_val , 'NeuTra loss is NaN at step {}' . format ( step ) ) if self . train_debug_fn : self . train_debug_fn ( self , step , loss_val ) state_parts = tf . nest . flatten ( state ) flat_state_shapes = tf . nest . flatten ( self . state_shape ) batch_shape = tf . shape ( input = state_parts [ 0 ] ) [ : - flat_state_shapes [ 0 ] . ndims ] return self . _kernel . bootstrap_results ( self . _flattened_variational_distribution ( ) . sample ( batch_shape , seed = self . seed ) )",Trains the bijector and creates initial previous_kernel_results .
"def mean_log_det ( self , name = ""mean_log_det"" ) : with self . _name_scope ( name ) : return ( self . _multi_digamma ( 0.5 * self . df , self . dimension ) + self . dimension * math . log ( 2. ) + 2 * self . scale_operator . log_abs_determinant ( ) )",Computes E [ log ( det ( X )) ] under this Wishart distribution .
"def log_normalization ( self , name = ""log_normalization"" ) : with self . _name_scope ( name ) : return ( self . df * self . scale_operator . log_abs_determinant ( ) + 0.5 * self . df * self . dimension * math . log ( 2. ) + self . _multi_lgamma ( 0.5 * self . df , self . dimension ) )",Computes the log normalizing constant log ( Z ) .
"def _multi_gamma_sequence ( self , a , p , name = ""multi_gamma_sequence"" ) : with self . _name_scope ( name ) : seq = tf . linspace ( tf . constant ( 0. , dtype = self . dtype ) , 0.5 - 0.5 * p , tf . cast ( p , tf . int32 ) ) return seq + tf . expand_dims ( a , [ - 1 ] )",Creates sequence used in multivariate ( di ) gamma ; shape = shape ( a ) + [ p ] .
"def _multi_lgamma ( self , a , p , name = ""multi_lgamma"" ) : with self . _name_scope ( name ) : seq = self . _multi_gamma_sequence ( a , p ) return ( 0.25 * p * ( p - 1. ) * math . log ( math . pi ) + tf . reduce_sum ( input_tensor = tf . math . lgamma ( seq ) , axis = [ - 1 ] ) )",Computes the log multivariate gamma function ; log ( Gamma_p ( a )) .
"def _multi_digamma ( self , a , p , name = ""multi_digamma"" ) : with self . _name_scope ( name ) : seq = self . _multi_gamma_sequence ( a , p ) return tf . reduce_sum ( input_tensor = tf . math . digamma ( seq ) , axis = [ - 1 ] )",Computes the multivariate digamma function ; Psi_p ( a ) .
"def _outer_squared_difference ( x , y ) : z = x - y return z [ ... , tf . newaxis , : ] * z [ ... , tf . newaxis ]",Convenience function analogous to tf . squared_difference .
"def _value_and_batch_jacobian ( f , x ) : if tf . executing_eagerly ( ) : with tf . GradientTape ( ) as tape : tape . watch ( x ) value = f ( x ) batch_jacobian = tape . batch_jacobian ( value , x ) else : value = f ( x ) batch_jacobian = gradients . batch_jacobian ( value , x ) return value , batch_jacobian",Enables uniform interface to value and batch jacobian calculation .
"def _prevent_2nd_derivative ( x ) : def grad ( dy ) : return array_ops . prevent_gradient ( dy , message = ""Second derivative is not implemented."" ) return tf . identity ( x ) , grad",Disables computation of the second derivatives for a tensor .
"def _reparameterize_sample ( self , x ) : x = tf . stop_gradient ( x ) x_2d_shape = [ - 1 , self . _event_size ] def reshaped_distributional_transform ( x_2d ) : return tf . reshape ( self . _distributional_transform ( tf . reshape ( x_2d , tf . shape ( input = x ) ) ) , x_2d_shape ) transform_2d , jacobian = _value_and_batch_jacobian ( reshaped_distributional_transform , tf . reshape ( x , x_2d_shape ) ) transform_2d = _prevent_2nd_derivative ( transform_2d ) surrogate_x_2d = - tf . linalg . triangular_solve ( tf . stop_gradient ( jacobian ) , tf . expand_dims ( transform_2d , axis = - 1 ) , lower = True ) surrogate_x = tf . reshape ( surrogate_x_2d , tf . shape ( input = x ) ) return x + ( surrogate_x - tf . stop_gradient ( surrogate_x ) )",Adds reparameterization ( pathwise ) gradients to samples of the mixture .
"def _distributional_transform ( self , x ) : if tensorshape_util . rank ( x . shape ) is None : raise ValueError ( ""Distributional transform does not support inputs of "" ""undefined rank."" ) if isinstance ( self . _components_distribution , independent . Independent ) : univariate_components = self . _components_distribution . distribution else : univariate_components = self . _components_distribution with tf . control_dependencies ( [ assert_util . assert_equal ( univariate_components . is_scalar_event ( ) , True , message = ""`univariate_components` must have scalar event"" ) ] ) : x_padded = self . _pad_sample_dims ( x ) log_prob_x = univariate_components . log_prob ( x_padded ) cdf_x = univariate_components . cdf ( x_padded ) cumsum_log_prob_x = tf . reshape ( tf . math . cumsum ( tf . reshape ( log_prob_x , [ - 1 , self . _event_size ] ) , exclusive = True , axis = - 1 ) , tf . shape ( input = log_prob_x ) ) logits_mix_prob = distribution_utils . pad_mixture_dimensions ( self . mixture_distribution . logits , self , self . mixture_distribution , self . _event_ndims ) log_posterior_weights_x = logits_mix_prob + cumsum_log_prob_x component_axis = tensorshape_util . rank ( x . shape ) - self . _event_ndims posterior_weights_x = tf . nn . softmax ( log_posterior_weights_x , axis = component_axis ) return tf . reduce_sum ( input_tensor = posterior_weights_x * cdf_x , axis = component_axis )",Performs distributional transform of the mixture samples .
"def _split_covariance_into_marginals ( covariance , block_sizes ) : start_dim = 0 marginals = [ ] for size in block_sizes : end_dim = start_dim + size marginals . append ( covariance [ ... , start_dim : end_dim , start_dim : end_dim ] ) start_dim = end_dim return marginals",Split a covariance matrix into block - diagonal marginals of given sizes .
"def _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples ) : try : model . components except AttributeError : raise ValueError ( 'Model decomposed into components must be an instance of' '`tfp.sts.Sum` (passed model {})' . format ( model ) ) with tf . compat . v1 . name_scope ( 'decompose_from_posterior_marginals' ) : latent_sizes = [ component . latent_size for component in model . components ] component_means = tf . split ( posterior_means , latent_sizes , axis = - 1 ) component_covs = _split_covariance_into_marginals ( posterior_covs , latent_sizes ) num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = posterior_means ) ) [ - 2 ] component_ssms = model . make_component_state_space_models ( num_timesteps = num_timesteps , param_vals = parameter_samples ) component_predictive_dists = collections . OrderedDict ( ) for ( component , component_ssm , component_mean , component_cov ) in zip ( model . components , component_ssms , component_means , component_covs ) : component_obs_mean , component_obs_cov = ( component_ssm . latents_to_observations ( latent_means = component_mean , latent_covs = component_cov ) ) component_predictive_dists [ component ] = sts_util . mix_over_posterior_draws ( means = component_obs_mean [ ... , 0 ] , variances = component_obs_cov [ ... , 0 , 0 ] ) return component_predictive_dists",Utility method to decompose a joint posterior into components .
"def decompose_by_component ( model , observed_time_series , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_by_component' , values = [ observed_time_series ] ) : [ observed_time_series , is_missing ] = sts_util . canonicalize_observed_time_series_with_mask ( observed_time_series ) num_timesteps = dist_util . prefer_static_value ( tf . shape ( input = observed_time_series ) ) [ - 2 ] ssm = model . make_state_space_model ( num_timesteps = num_timesteps , param_vals = parameter_samples ) posterior_means , posterior_covs = ssm . posterior_marginals ( observed_time_series , mask = is_missing ) return _decompose_from_posterior_marginals ( model , posterior_means , posterior_covs , parameter_samples )",Decompose an observed time series into contributions from each component .
"def decompose_forecast_by_component ( model , forecast_dist , parameter_samples ) : with tf . compat . v1 . name_scope ( 'decompose_forecast_by_component' ) : try : forecast_lgssm = forecast_dist . components_distribution forecast_latent_mean , _ = forecast_lgssm . _joint_mean ( ) forecast_latent_covs , _ = forecast_lgssm . _joint_covariances ( ) except AttributeError as e : raise ValueError ( 'Forecast distribution must be a MixtureSameFamily of' 'LinearGaussianStateSpaceModel distributions, such as returned by' '`tfp.sts.forecast()`. (saw exception: {})' . format ( e ) ) forecast_latent_mean = dist_util . move_dimension ( forecast_latent_mean , source_idx = - 3 , dest_idx = 0 ) forecast_latent_covs = dist_util . move_dimension ( forecast_latent_covs , source_idx = - 4 , dest_idx = 0 ) return _decompose_from_posterior_marginals ( model , forecast_latent_mean , forecast_latent_covs , parameter_samples )",Decompose a forecast distribution into contributions from each component .
"def dense_to_sparse ( x , ignore_value = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'dense_to_sparse' , [ x , ignore_value ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if ignore_value is None : if x . dtype . base_dtype == tf . string : ignore_value = '' else : ignore_value = x . dtype . as_numpy_dtype ( 0 ) ignore_value = tf . cast ( ignore_value , x . dtype , name = 'ignore_value' ) indices = tf . where ( tf . not_equal ( x , ignore_value ) , name = 'indices' ) return tf . SparseTensor ( indices = indices , values = tf . gather_nd ( x , indices , name = 'values' ) , dense_shape = tf . shape ( input = x , out_type = tf . int64 , name = 'dense_shape' ) )",Converts dense Tensor to SparseTensor dropping ignore_value cells .
"def _operator ( attr ) : @ functools . wraps ( attr ) def func ( a , * args ) : return attr ( a . value , * args ) return func",Defers an operator overload to attr .
"def _numpy_text ( tensor , is_repr = False ) : if tensor . dtype . is_numpy_compatible : text = repr ( tensor . numpy ( ) ) if is_repr else str ( tensor . numpy ( ) ) else : text = ""<unprintable>"" if ""\n"" in text : text = ""\n"" + text return text",Human - readable representation of a tensor s numpy value .
"def sample_shape ( self ) : if isinstance ( self . _sample_shape , tf . Tensor ) : return tf . TensorShape ( tf . get_static_value ( self . _sample_shape ) ) return tf . TensorShape ( self . _sample_shape )",Sample shape of random variable as a TensorShape .
"def sample_shape_tensor ( self , name = ""sample_shape_tensor"" ) : with tf . compat . v1 . name_scope ( name ) : if isinstance ( self . _sample_shape , tf . Tensor ) : return self . _sample_shape return tf . convert_to_tensor ( value = self . sample_shape . as_list ( ) , dtype = tf . int32 )",Sample shape of random variable as a 1 - D Tensor .
"def value ( self ) : if self . _value is None : try : self . _value = self . distribution . sample ( self . sample_shape_tensor ( ) ) except NotImplementedError : raise NotImplementedError ( ""sample is not implemented for {0}. You must either pass in the "" ""value argument or implement sample for {0}."" . format ( self . distribution . __class__ . __name__ ) ) return self . _value",Get tensor that the random variable corresponds to .
"def eval ( self , session = None , feed_dict = None ) : return self . value . eval ( session = session , feed_dict = feed_dict )",In a session computes and returns the value of this random variable .
"def numpy ( self ) : if not isinstance ( self . value , ops . EagerTensor ) : raise NotImplementedError ( ""value argument must be a EagerTensor."" ) return self . value . numpy ( )",Value as NumPy array only available for TF Eager .
"def normal_conjugates_known_scale_posterior ( prior , scale , s , n ) : if not isinstance ( prior , normal . Normal ) : raise TypeError ( ""Expected prior to be an instance of type Normal"" ) if s . dtype != prior . dtype : raise TypeError ( ""Observation sum s.dtype does not match prior dtype: %s vs. %s"" % ( s . dtype , prior . dtype ) ) n = tf . cast ( n , prior . dtype ) scale0_2 = tf . square ( prior . scale ) scale_2 = tf . square ( scale ) scalep_2 = 1.0 / ( 1 / scale0_2 + n / scale_2 ) return normal . Normal ( loc = ( prior . loc / scale0_2 + s / scale_2 ) * scalep_2 , scale = tf . sqrt ( scalep_2 ) )",Posterior Normal distribution with conjugate prior on the mean .
"def real_nvp_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , name = None , * args , * * kwargs ) : with tf . compat . v2 . name_scope ( name or ""real_nvp_default_template"" ) : def _fn ( x , output_units , * * condition_kwargs ) : """"""Fully connected MLP parameterized via `real_nvp_template`."""""" if condition_kwargs : raise NotImplementedError ( ""Conditioning not implemented in the default template."" ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] reshape_output = lambda x : x [ 0 ] else : reshape_output = lambda x : x for units in hidden_layers : x = tf . compat . v1 . layers . dense ( inputs = x , units = units , activation = activation , * args , * * kwargs ) x = tf . compat . v1 . layers . dense ( inputs = x , units = ( 1 if shift_only else 2 ) * output_units , activation = None , * args , * * kwargs ) if shift_only : return reshape_output ( x ) , None shift , log_scale = tf . split ( x , 2 , axis = - 1 ) return reshape_output ( shift ) , reshape_output ( log_scale ) return tf . compat . v1 . make_template ( ""real_nvp_default_template"" , _fn )",Build a scale - and - shift function using a multi - layer neural network .
"def _uniform_unit_norm ( dimension , shape , dtype , seed ) : raw = normal . Normal ( loc = dtype_util . as_numpy_dtype ( dtype ) ( 0 ) , scale = dtype_util . as_numpy_dtype ( dtype ) ( 1 ) ) . sample ( tf . concat ( [ shape , [ dimension ] ] , axis = 0 ) , seed = seed ( ) ) unit_norm = raw / tf . norm ( tensor = raw , ord = 2 , axis = - 1 ) [ ... , tf . newaxis ] return unit_norm",Returns a batch of points chosen uniformly from the unit hypersphere .
"def _replicate ( n , tensor ) : multiples = tf . concat ( [ [ n ] , tf . ones_like ( tensor . shape ) ] , axis = 0 ) return tf . tile ( tf . expand_dims ( tensor , axis = 0 ) , multiples )",Replicate the input tensor n times along a new ( major ) dimension .
"def _sample_n ( self , num_samples , seed = None , name = None ) : if self . dimension < 0 : raise ValueError ( 'Cannot sample negative-dimension correlation matrices.' ) seed = seed_stream . SeedStream ( seed , 'sample_lkj' ) with tf . name_scope ( 'sample_lkj' or name ) : if not dtype_util . is_floating ( self . concentration . dtype ) : raise TypeError ( 'The concentration argument should have floating type, not ' '{}' . format ( dtype_util . name ( self . concentration . dtype ) ) ) concentration = _replicate ( num_samples , self . concentration ) concentration_shape = tf . shape ( input = concentration ) if self . dimension <= 1 : shape = tf . concat ( [ concentration_shape , [ self . dimension , self . dimension ] ] , axis = 0 ) return tf . ones ( shape = shape , dtype = self . concentration . dtype ) beta_conc = concentration + ( self . dimension - 2. ) / 2. beta_dist = beta . Beta ( concentration1 = beta_conc , concentration0 = beta_conc ) corr12 = 2. * beta_dist . sample ( seed = seed ( ) ) - 1. first_row = tf . concat ( [ tf . ones_like ( corr12 ) [ ... , tf . newaxis ] , tf . zeros_like ( corr12 ) [ ... , tf . newaxis ] ] , axis = - 1 ) second_row = tf . concat ( [ corr12 [ ... , tf . newaxis ] , tf . sqrt ( 1 - corr12 ** 2 ) [ ... , tf . newaxis ] ] , axis = - 1 ) chol_result = tf . concat ( [ first_row [ ... , tf . newaxis , : ] , second_row [ ... , tf . newaxis , : ] ] , axis = - 2 ) for n in range ( 2 , self . dimension ) : beta_conc -= 0.5 norm = beta . Beta ( concentration1 = n / 2. , concentration0 = beta_conc ) . sample ( seed = seed ( ) ) distance = tf . sqrt ( norm ) [ ... , tf . newaxis ] direction = _uniform_unit_norm ( n , concentration_shape , self . concentration . dtype , seed ) raw_correlation = distance * direction new_row = tf . concat ( [ raw_correlation , tf . sqrt ( 1. - norm [ ... , tf . newaxis ] ) ] , axis = - 1 ) chol_result = tf . concat ( [ chol_result , tf . zeros_like ( chol_result [ ... , 0 ] [ ... , tf . newaxis ] ) ] , axis = - 1 ) chol_result = tf . concat ( [ chol_result , new_row [ ... , tf . newaxis , : ] ] , axis = - 2 ) if self . input_output_cholesky : return chol_result result = tf . matmul ( chol_result , chol_result , transpose_b = True ) result = tf . linalg . set_diag ( result , tf . ones ( shape = tf . shape ( input = result ) [ : - 1 ] , dtype = result . dtype ) ) return result",Returns a Tensor of samples from an LKJ distribution .
"def _log_unnorm_prob ( self , x , name = None ) : with tf . name_scope ( name or 'log_unnorm_prob_lkj' ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if self . input_output_cholesky : logdet = 2.0 * tf . reduce_sum ( input_tensor = tf . math . log ( tf . linalg . diag_part ( x ) ) , axis = [ - 1 ] ) else : _ , logdet = tf . linalg . slogdet ( x ) answer = ( self . concentration - 1. ) * logdet return answer",Returns the unnormalized log density of an LKJ distribution .
"def _log_normalization ( self , name = 'log_normalization' ) : with tf . name_scope ( name or 'log_normalization_lkj' ) : logpi = np . log ( np . pi ) ans = tf . zeros_like ( self . concentration ) for k in range ( 1 , self . dimension ) : ans += logpi * ( k / 2. ) ans += tf . math . lgamma ( self . concentration + ( self . dimension - 1 - k ) / 2. ) ans -= tf . math . lgamma ( self . concentration + ( self . dimension - 1 ) / 2. ) return ans",Returns the log normalization of an LKJ distribution .
"def common_dtype ( args_list , preferred_dtype = None ) : dtype = None preferred_dtype = ( None if preferred_dtype is None else tf . as_dtype ( preferred_dtype ) ) for a in tf . nest . flatten ( args_list ) : if hasattr ( a , 'dtype' ) : dt = tf . as_dtype ( a . dtype ) else : continue if dtype is None : dtype = dt elif dtype != dt : raise TypeError ( 'Found incompatible dtypes, {} and {}.' . format ( dtype , dt ) ) if dtype is None and preferred_dtype is None : return None return ( preferred_dtype if dtype is None else dtype ) . as_numpy_dtype",Returns explict dtype from args_list if exists else preferred_dtype .
"def _make_summary_statistic ( attr ) : def _fn ( self , * * kwargs ) : """"""Implements summary statistic, eg, mean, stddev, mode."""""" x = getattr ( self . distribution , attr ) ( * * kwargs ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , prefer_static . ones ( prefer_static . rank_from_shape ( self . sample_shape ) , dtype = self . sample_shape . dtype ) , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) x = tf . reshape ( x , shape = shape ) shape = prefer_static . concat ( [ self . distribution . batch_shape_tensor ( ) , self . sample_shape , self . distribution . event_shape_tensor ( ) , ] , axis = 0 ) return tf . broadcast_to ( x , shape ) return _fn",Factory for implementing summary statistics eg mean stddev mode .
"def _kl_sample ( a , b , name = 'kl_sample' ) : assertions = [ ] a_ss = tf . get_static_value ( a . sample_shape ) b_ss = tf . get_static_value ( b . sample_shape ) msg = '`a.sample_shape` must be identical to `b.sample_shape`.' if a_ss is not None and b_ss is not None : if not np . array_equal ( a_ss , b_ss ) : raise ValueError ( msg ) elif a . validate_args or b . validate_args : assertions . append ( assert_util . assert_equal ( a . sample_shape , b . sample_shape , message = msg ) ) with tf . control_dependencies ( assertions ) : kl = kullback_leibler . kl_divergence ( a . distribution , b . distribution , name = name ) n = prefer_static . reduce_prod ( a . sample_shape ) return tf . cast ( x = n , dtype = kl . dtype ) * kl",Batched KL divergence KL ( a || b ) for Sample distributions .
"def _broadcast_to ( tensor_to_broadcast , target_tensors ) : output = tensor_to_broadcast for tensor in target_tensors : output += tf . zeros_like ( tensor ) return output",Helper to broadcast a tensor using a list of target tensors .
def _pdf_at_peak ( self ) : return ( self . peak - self . low ) / ( self . high - self . low ),Pdf evaluated at the peak .
"def effective_sample_size ( states , filter_threshold = 0. , filter_beyond_lag = None , name = None ) : states_was_list = _is_list_like ( states ) if not states_was_list : states = [ states ] filter_beyond_lag = _broadcast_maybelist_arg ( states , filter_beyond_lag , 'filter_beyond_lag' ) filter_threshold = _broadcast_maybelist_arg ( states , filter_threshold , 'filter_threshold' ) with tf . compat . v1 . name_scope ( name , 'effective_sample_size' ) : ess_list = [ _effective_sample_size_single_state ( s , ml , mlt ) for ( s , ml , mlt ) in zip ( states , filter_beyond_lag , filter_threshold ) ] if states_was_list : return ess_list return ess_list [ 0 ]",Estimate a lower bound on effective sample size for each independent chain .
"def _effective_sample_size_single_state ( states , filter_beyond_lag , filter_threshold ) : with tf . compat . v1 . name_scope ( 'effective_sample_size_single_state' , values = [ states , filter_beyond_lag , filter_threshold ] ) : states = tf . convert_to_tensor ( value = states , name = 'states' ) dt = states . dtype auto_corr = stats . auto_correlation ( states , axis = 0 , max_lags = filter_beyond_lag ) if filter_threshold is not None : filter_threshold = tf . convert_to_tensor ( value = filter_threshold , dtype = dt , name = 'filter_threshold' ) mask = auto_corr < filter_threshold mask = tf . cast ( mask , dtype = dt ) mask = tf . cumsum ( mask , axis = 0 ) mask = tf . maximum ( 1. - mask , 0. ) auto_corr *= mask n = _axis_size ( states , axis = 0 ) k = tf . range ( 0. , _axis_size ( auto_corr , axis = 0 ) ) nk_factor = ( n - k ) / n if auto_corr . shape . ndims is not None : new_shape = [ - 1 ] + [ 1 ] * ( auto_corr . shape . ndims - 1 ) else : new_shape = tf . concat ( ( [ - 1 ] , tf . ones ( [ tf . rank ( auto_corr ) - 1 ] , dtype = tf . int32 ) ) , axis = 0 ) nk_factor = tf . reshape ( nk_factor , new_shape ) return n / ( - 1 + 2 * tf . reduce_sum ( input_tensor = nk_factor * auto_corr , axis = 0 ) )",ESS computation for one single Tensor argument .
"def potential_scale_reduction ( chains_states , independent_chain_ndims = 1 , name = None ) : chains_states_was_list = _is_list_like ( chains_states ) if not chains_states_was_list : chains_states = [ chains_states ] icn_const_ = tf . get_static_value ( tf . convert_to_tensor ( value = independent_chain_ndims ) ) if icn_const_ is not None : independent_chain_ndims = icn_const_ if icn_const_ < 1 : raise ValueError ( 'Argument `independent_chain_ndims` must be `>= 1`, found: {}' . format ( independent_chain_ndims ) ) with tf . compat . v1 . name_scope ( name , 'potential_scale_reduction' ) : rhat_list = [ _potential_scale_reduction_single_state ( s , independent_chain_ndims ) for s in chains_states ] if chains_states_was_list : return rhat_list return rhat_list [ 0 ]",Gelman and Rubin ( 1992 ) s potential scale reduction for chain convergence .
"def _potential_scale_reduction_single_state ( state , independent_chain_ndims ) : with tf . compat . v1 . name_scope ( 'potential_scale_reduction_single_state' , values = [ state , independent_chain_ndims ] ) : state = tf . convert_to_tensor ( value = state , name = 'state' ) sample_ndims = 1 sample_axis = tf . range ( 0 , sample_ndims ) chain_axis = tf . range ( sample_ndims , sample_ndims + independent_chain_ndims ) sample_and_chain_axis = tf . range ( 0 , sample_ndims + independent_chain_ndims ) n = _axis_size ( state , sample_axis ) m = _axis_size ( state , chain_axis ) b_div_n = _reduce_variance ( tf . reduce_mean ( input_tensor = state , axis = sample_axis , keepdims = True ) , sample_and_chain_axis , biased = False ) w = tf . reduce_mean ( input_tensor = _reduce_variance ( state , sample_axis , keepdims = True , biased = True ) , axis = sample_and_chain_axis ) sigma_2_plus = w + b_div_n return ( ( m + 1. ) / m ) * sigma_2_plus / w - ( n - 1. ) / ( m * n )",potential_scale_reduction for one single state Tensor .
"def _axis_size ( x , axis = None ) : if axis is None : return tf . cast ( tf . size ( input = x ) , x . dtype ) return tf . cast ( tf . reduce_prod ( input_tensor = tf . gather ( tf . shape ( input = x ) , axis ) ) , x . dtype )",Get number of elements of x in axis as type x . dtype .
"def _broadcast_maybelist_arg ( states , secondary_arg , name ) : if _is_list_like ( secondary_arg ) : if len ( secondary_arg ) != len ( states ) : raise ValueError ( 'Argument `%s` was a list of different length ({}) than ' '`states` ({})' . format ( name , len ( states ) ) ) else : secondary_arg = [ secondary_arg ] * len ( states ) return secondary_arg",Broadcast a listable secondary_arg to that of states .
"def quadrature_scheme_lognormal_gauss_hermite ( loc , scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or ""vector_diffeomixture_quadrature_gauss_hermite"" ) : grid , probs = np . polynomial . hermite . hermgauss ( deg = quadrature_size ) npdt = dtype_util . as_numpy_dtype ( loc . dtype ) grid = grid . astype ( npdt ) probs = probs . astype ( npdt ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) probs = tf . convert_to_tensor ( value = probs , name = ""probs"" , dtype = loc . dtype ) grid = ( loc [ ... , tf . newaxis ] + np . sqrt ( 2. ) * scale [ ... , tf . newaxis ] * grid ) return grid , probs",Use Gauss - Hermite quadrature to form quadrature on positive - reals .
"def quadrature_scheme_lognormal_quantiles ( loc , scale , quadrature_size , validate_args = False , name = None ) : with tf . name_scope ( name or ""quadrature_scheme_lognormal_quantiles"" ) : dist = transformed_distribution . TransformedDistribution ( distribution = normal . Normal ( loc = loc , scale = scale ) , bijector = exp_bijector . Exp ( ) , validate_args = validate_args ) batch_ndims = tensorshape_util . rank ( dist . batch_shape ) if batch_ndims is None : batch_ndims = tf . shape ( input = dist . batch_shape_tensor ( ) ) [ 0 ] def _compute_quantiles ( ) : """"""Helper to build quantiles."""""" zero = tf . zeros ( [ ] , dtype = dist . dtype ) edges = tf . linspace ( zero , 1. , quadrature_size + 3 ) [ 1 : - 1 ] edges = tf . reshape ( edges , shape = tf . concat ( [ [ - 1 ] , tf . ones ( [ batch_ndims ] , dtype = tf . int32 ) ] , axis = 0 ) ) quantiles = dist . quantile ( edges ) perm = tf . concat ( [ tf . range ( 1 , 1 + batch_ndims ) , [ 0 ] ] , axis = 0 ) quantiles = tf . transpose ( a = quantiles , perm = perm ) return quantiles quantiles = _compute_quantiles ( ) grid = ( quantiles [ ... , : - 1 ] + quantiles [ ... , 1 : ] ) / 2. new_shape = tensorshape_util . concatenate ( dist . batch_shape , [ quadrature_size ] ) tensorshape_util . set_shape ( grid , new_shape ) probs = tf . fill ( dims = [ quadrature_size ] , value = 1. / tf . cast ( quadrature_size , dist . dtype ) ) return grid , probs",Use LogNormal quantiles to form quadrature on positive - reals .
"def merge ( self , x = None , y = None , ildj = None , kwargs = None , mapping = None ) : if mapping is None : mapping = _Mapping ( x = x , y = y , ildj = ildj , kwargs = kwargs ) elif any ( arg is not None for arg in [ x , y , ildj , kwargs ] ) : raise ValueError ( ""Cannot simultaneously specify mapping and individual "" ""arguments."" ) return _Mapping ( x = self . _merge ( self . x , mapping . x ) , y = self . _merge ( self . y , mapping . y ) , ildj = self . _merge ( self . ildj , mapping . ildj ) , kwargs = self . _merge ( self . kwargs , mapping . kwargs , use_equals = True ) )",Returns new _Mapping with args merged with self .
"def remove ( self , field ) : return _Mapping ( x = None if field == ""x"" else self . x , y = None if field == ""y"" else self . y , ildj = self . ildj , kwargs = self . kwargs )",To support weak referencing removes cache key from the cache value .
"def _merge ( self , old , new , use_equals = False ) : if old is None : return new if new is None : return old if ( old == new ) if use_equals else ( old is new ) : return old raise ValueError ( ""Incompatible values: %s != %s"" % ( old , new ) )",Helper to merge which handles merging one value .
"def _deep_tuple ( self , x ) : if isinstance ( x , dict ) : return self . _deep_tuple ( tuple ( sorted ( x . items ( ) ) ) ) elif isinstance ( x , ( list , tuple ) ) : return tuple ( map ( self . _deep_tuple , x ) ) return x",Converts nested tuple list or dict to nested tuple .
"def _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'left_doubling_increments' , [ batch_shape , max_doublings , step_size ] ) : step_size = tf . convert_to_tensor ( value = step_size ) dtype = step_size . dtype . base_dtype output_shape = tf . concat ( ( [ max_doublings + 1 ] , batch_shape ) , axis = 0 ) expand_left = distributions . Bernoulli ( 0.5 , dtype = dtype ) . sample ( sample_shape = output_shape , seed = seed ) width_multipliers = tf . cast ( 2 ** tf . range ( 0 , max_doublings + 1 ) , dtype = dtype ) widths_shape = tf . concat ( ( [ max_doublings + 1 ] , tf . ones_like ( batch_shape ) ) , axis = 0 ) width_multipliers = tf . reshape ( width_multipliers , shape = widths_shape ) widths = width_multipliers * step_size left_increments = tf . cumsum ( widths * expand_left , exclusive = True , axis = 0 ) return left_increments , widths",Computes the doubling increments for the left end point .
"def _find_best_interval_idx ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'find_best_interval_idx' , [ x ] ) : k = tf . shape ( input = x ) [ 0 ] dtype = x . dtype . base_dtype mults = tf . range ( 2 * k , k , - 1 , dtype = dtype ) [ : , tf . newaxis ] shifts = tf . range ( k , dtype = dtype ) [ : , tf . newaxis ] indices = tf . argmax ( input = mults * x + shifts , axis = 0 , output_type = dtype ) return indices",Finds the index of the optimal set of bounds for each chain .
"def slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'slice_bounds_by_doubling' , [ x_initial , log_slice_heights , max_doublings , step_size ] ) : seed_gen = distributions . SeedStream ( seed , salt = 'slice_bounds_by_doubling' ) x_initial = tf . convert_to_tensor ( value = x_initial ) batch_shape = tf . shape ( input = x_initial ) dtype = step_size . dtype . base_dtype left_endpoints = x_initial + step_size * tf . random . uniform ( batch_shape , minval = - 1.0 , maxval = 0.0 , dtype = dtype , seed = seed_gen ( ) ) left_increments , widths = _left_doubling_increments ( batch_shape , max_doublings , step_size , seed = seed_gen ( ) ) left_endpoints -= left_increments right_endpoints = left_endpoints + widths left_ep_values = tf . map_fn ( target_log_prob , left_endpoints ) right_ep_values = tf . map_fn ( target_log_prob , right_endpoints ) left_ok = left_ep_values < log_slice_heights right_ok = right_ep_values < log_slice_heights both_ok = left_ok & right_ok both_ok_f = tf . reshape ( both_ok , [ max_doublings + 1 , - 1 ] ) best_interval_idx = _find_best_interval_idx ( tf . cast ( both_ok_f , dtype = tf . int32 ) ) point_index_gather = tf . stack ( [ best_interval_idx , tf . range ( tf . size ( input = best_interval_idx ) ) ] , axis = 1 , name = 'point_index_gather' ) left_ep_f = tf . reshape ( left_endpoints , [ max_doublings + 1 , - 1 ] ) right_ep_f = tf . reshape ( right_endpoints , [ max_doublings + 1 , - 1 ] ) lower_bounds = tf . reshape ( tf . gather_nd ( left_ep_f , point_index_gather ) , batch_shape ) upper_bounds = tf . reshape ( tf . gather_nd ( right_ep_f , point_index_gather ) , batch_shape ) both_ok = tf . reduce_any ( input_tensor = both_ok , axis = 0 ) return upper_bounds , lower_bounds , both_ok",Returns the bounds of the slice at each stage of doubling procedure .
"def _sample_with_shrinkage ( x_initial , target_log_prob , log_slice_heights , step_size , lower_bounds , upper_bounds , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'sample_with_shrinkage' , [ x_initial , log_slice_heights , step_size , lower_bounds , upper_bounds ] ) : seed_gen = distributions . SeedStream ( seed , salt = '_sample_with_shrinkage' ) found = tf . zeros_like ( x_initial , dtype = tf . bool ) cond = lambda found , * ignored_args : ~ tf . reduce_all ( input_tensor = found ) x_next = tf . identity ( x_initial ) x_initial_shape = tf . shape ( input = x_initial ) x_initial_dtype = x_initial . dtype . base_dtype def _body ( found , left , right , x_next ) : """"""Iterates until every chain has found a suitable next state."""""" proportions = tf . random . uniform ( x_initial_shape , dtype = x_initial_dtype , seed = seed_gen ( ) ) x_proposed = tf . where ( ~ found , left + proportions * ( right - left ) , x_next ) accept_res = _test_acceptance ( x_initial , target_log_prob = target_log_prob , decided = found , log_slice_heights = log_slice_heights , x_proposed = x_proposed , step_size = step_size , lower_bounds = left , upper_bounds = right ) boundary_test = log_slice_heights < target_log_prob ( x_proposed ) can_accept = boundary_test & accept_res next_found = found | can_accept next_left = tf . where ( x_proposed < x_initial , x_proposed , left ) next_right = tf . where ( x_proposed >= x_initial , x_proposed , right ) return next_found , next_left , next_right , x_proposed return tf . while_loop ( cond = cond , body = _body , loop_vars = ( found , lower_bounds , upper_bounds , x_next ) ) [ - 1 ]",Samples from the slice by applying shrinkage for rejected points .
"def slice_sampler_one_dim ( target_log_prob , x_initial , step_size = 0.01 , max_doublings = 30 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'slice_sampler_one_dim' , [ x_initial , step_size , max_doublings ] ) : x_initial = tf . convert_to_tensor ( value = x_initial ) dtype = x_initial . dtype . base_dtype log_slice_heights = target_log_prob ( x_initial ) - tf . random . gamma ( tf . shape ( input = x_initial ) , alpha = 1 , dtype = dtype , seed = seed ) upper_bounds , lower_bounds , bounds_satisfied = slice_bounds_by_doubling ( x_initial , target_log_prob , log_slice_heights , max_doublings , step_size , seed = seed ) retval = _sample_with_shrinkage ( x_initial , target_log_prob = target_log_prob , log_slice_heights = log_slice_heights , step_size = step_size , lower_bounds = lower_bounds , upper_bounds = upper_bounds , seed = seed ) return ( retval , target_log_prob ( retval ) , bounds_satisfied , upper_bounds , lower_bounds )",For a given x position in each Markov chain returns the next x .
"def sample_annealed_importance_chain ( num_steps , proposal_log_prob_fn , target_log_prob_fn , current_state , make_kernel_fn , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , ""sample_annealed_importance_chain"" , [ num_steps , current_state ] ) : num_steps = tf . convert_to_tensor ( value = num_steps , dtype = tf . int32 , name = ""num_steps"" ) if mcmc_util . is_list_like ( current_state ) : current_state = [ tf . convert_to_tensor ( value = s , name = ""current_state"" ) for s in current_state ] else : current_state = tf . convert_to_tensor ( value = current_state , name = ""current_state"" ) def _make_convex_combined_log_prob_fn ( iter_ ) : def _fn ( * args ) : p = tf . identity ( proposal_log_prob_fn ( * args ) , name = ""proposal_log_prob"" ) t = tf . identity ( target_log_prob_fn ( * args ) , name = ""target_log_prob"" ) dtype = p . dtype . base_dtype beta = tf . cast ( iter_ + 1 , dtype ) / tf . cast ( num_steps , dtype ) return tf . identity ( beta * t + ( 1. - beta ) * p , name = ""convex_combined_log_prob"" ) return _fn def _loop_body ( iter_ , ais_weights , current_state , kernel_results ) : """"""Closure which implements `tf.while_loop` body."""""" x = ( current_state if mcmc_util . is_list_like ( current_state ) else [ current_state ] ) proposal_log_prob = proposal_log_prob_fn ( * x ) target_log_prob = target_log_prob_fn ( * x ) ais_weights += ( ( target_log_prob - proposal_log_prob ) / tf . cast ( num_steps , ais_weights . dtype ) ) kernel = make_kernel_fn ( _make_convex_combined_log_prob_fn ( iter_ ) ) next_state , inner_results = kernel . one_step ( current_state , kernel_results . inner_results ) kernel_results = AISResults ( proposal_log_prob = proposal_log_prob , target_log_prob = target_log_prob , inner_results = inner_results , ) return [ iter_ + 1 , ais_weights , next_state , kernel_results ] def _bootstrap_results ( init_state ) : """"""Creates first version of `previous_kernel_results`."""""" kernel = make_kernel_fn ( _make_convex_combined_log_prob_fn ( iter_ = 0 ) ) inner_results = kernel . bootstrap_results ( init_state ) convex_combined_log_prob = inner_results . accepted_results . target_log_prob dtype = convex_combined_log_prob . dtype . as_numpy_dtype shape = tf . shape ( input = convex_combined_log_prob ) proposal_log_prob = tf . fill ( shape , dtype ( np . nan ) , name = ""bootstrap_proposal_log_prob"" ) target_log_prob = tf . fill ( shape , dtype ( np . nan ) , name = ""target_target_log_prob"" ) return AISResults ( proposal_log_prob = proposal_log_prob , target_log_prob = target_log_prob , inner_results = inner_results , ) previous_kernel_results = _bootstrap_results ( current_state ) inner_results = previous_kernel_results . inner_results ais_weights = tf . zeros ( shape = tf . broadcast_dynamic_shape ( tf . shape ( input = inner_results . proposed_results . target_log_prob ) , tf . shape ( input = inner_results . accepted_results . target_log_prob ) ) , dtype = inner_results . proposed_results . target_log_prob . dtype . base_dtype ) [ _ , ais_weights , current_state , kernel_results ] = tf . while_loop ( cond = lambda iter_ , * args : iter_ < num_steps , body = _loop_body , loop_vars = [ np . int32 ( 0 ) , ais_weights , current_state , previous_kernel_results , ] , parallel_iterations = parallel_iterations ) return [ current_state , ais_weights , kernel_results ]",Runs annealed importance sampling ( AIS ) to estimate normalizing constants .
"def make_value_setter ( * * model_kwargs ) : def set_values ( f , * args , * * kwargs ) : """"""Sets random variable values to its aligned value."""""" name = kwargs . get ( ""name"" ) if name in model_kwargs : kwargs [ ""value"" ] = model_kwargs [ name ] return interceptable ( f ) ( * args , * * kwargs ) return set_values",Creates a value - setting interceptor .
"def make_log_joint_fn ( model ) : def log_joint_fn ( * args , * * kwargs ) : """"""Log-probability of inputs according to a joint probability distribution.

    Args:
      *args: Positional arguments. They are the model's original inputs and can
        alternatively be specified as part of `kwargs`.
      **kwargs: Keyword arguments, where for each key-value pair `k` and `v`,
        `v` is passed as a `value` to the random variable(s) whose keyword
        argument `name` during construction is equal to `k`.

    Returns:
      Scalar tf.Tensor, which represents the model's log-probability summed
      over all Edward random variables and their dimensions.

    Raises:
      TypeError: If a random variable in the model has no specified value in
        `**kwargs`.
    """""" log_probs = [ ] def interceptor ( rv_constructor , * rv_args , * * rv_kwargs ) : """"""Overrides a random variable's `value` and accumulates its log-prob."""""" rv_name = rv_kwargs . get ( ""name"" ) if rv_name is None : raise KeyError ( ""Random variable constructor {} has no name "" ""in its arguments."" . format ( rv_constructor . __name__ ) ) previously_specified_value = rv_kwargs . get ( ""value"" ) value = kwargs . get ( rv_name , previously_specified_value ) if value is None : raise LookupError ( ""Keyword argument specifying value for {} is "" ""missing."" . format ( rv_name ) ) rv_kwargs [ ""value"" ] = value rv = rv_constructor ( * rv_args , * * rv_kwargs ) log_prob = tf . reduce_sum ( input_tensor = rv . distribution . log_prob ( rv . value ) ) log_probs . append ( log_prob ) return rv model_kwargs = _get_function_inputs ( model , kwargs ) with interception ( interceptor ) : model ( * args , * * model_kwargs ) log_prob = sum ( log_probs ) return log_prob return log_joint_fn",Takes Edward probabilistic program and returns its log joint function .
"def _get_function_inputs ( f , src_kwargs ) : if hasattr ( f , ""_func"" ) : f = f . _func try : argspec = inspect . getfullargspec ( f ) except AttributeError : argspec = inspect . getargspec ( f ) fkwargs = { k : v for k , v in six . iteritems ( src_kwargs ) if k in argspec . args } return fkwargs",Filters inputs to be compatible with function f s signature .
"def _vggconv_block ( x , filters , kernel , stride , kernel_posterior_fn ) : out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tfp . layers . Convolution2DFlipout ( filters , kernel , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( out ) out = tf . keras . layers . BatchNormalization ( ) ( out ) out = tf . keras . layers . Activation ( 'relu' ) ( out ) out = tf . keras . layers . MaxPooling2D ( pool_size = ( 2 , 2 ) , strides = stride ) ( out ) return out",Network block for VGG .
"def compute_output_shape ( self , input_shape ) : input_shape = tf . TensorShape ( input_shape ) input_shape = input_shape . with_rank_at_least ( 2 ) if tf . compat . dimension_value ( input_shape [ - 1 ] ) is None : raise ValueError ( 'The innermost dimension of `input_shape` must be defined, ' 'but saw: {}' . format ( input_shape ) ) return input_shape [ : - 1 ] . concatenate ( self . units )",Computes the output shape of the layer .
"def get_config ( self ) : config = { 'units' : self . units , 'activation' : ( tf . keras . activations . serialize ( self . activation ) if self . activation else None ) , 'activity_regularizer' : tf . keras . initializers . serialize ( self . activity_regularizer ) , } function_keys = [ 'kernel_posterior_fn' , 'kernel_posterior_tensor_fn' , 'kernel_prior_fn' , 'kernel_divergence_fn' , 'bias_posterior_fn' , 'bias_posterior_tensor_fn' , 'bias_prior_fn' , 'bias_divergence_fn' , ] for function_key in function_keys : function = getattr ( self , function_key ) if function is None : function_name = None function_type = None else : function_name , function_type = tfp_layers_util . serialize_function ( function ) config [ function_key ] = function_name config [ function_key + '_type' ] = function_type base_config = super ( _DenseVariational , self ) . get_config ( ) return dict ( list ( base_config . items ( ) ) + list ( config . items ( ) ) )",Returns the config of the layer .
"def kernel ( target_log_prob_fn , current_state , step_size , seed = None , current_target_log_prob = None , current_grads_target_log_prob = None , name = None ) : if not tf . executing_eagerly ( ) : raise NotImplementedError ( ""`kernel` is only available in Eager mode."" ) with tf . compat . v1 . name_scope ( name , default_name = ""nuts_kernel"" , values = [ current_state , step_size , seed , current_target_log_prob , current_grads_target_log_prob ] ) : with tf . compat . v1 . name_scope ( ""initialize"" ) : current_state = [ tf . convert_to_tensor ( value = s ) for s in current_state ] step_size = [ tf . convert_to_tensor ( value = s ) for s in step_size ] value_and_gradients_fn = lambda * args : tfp . math . value_and_gradient ( target_log_prob_fn , args ) value_and_gradients_fn = _embed_no_none_gradient_check ( value_and_gradients_fn ) if ( current_target_log_prob is None or current_grads_target_log_prob is None ) : ( current_target_log_prob , current_grads_target_log_prob ) = value_and_gradients_fn ( * current_state ) seed_stream = tfd . SeedStream ( seed , ""nuts_kernel"" ) current_momentum = [ ] for state_tensor in current_state : momentum_tensor = tf . random . normal ( shape = tf . shape ( input = state_tensor ) , dtype = state_tensor . dtype , seed = seed_stream ( ) ) current_momentum . append ( momentum_tensor ) log_slice_sample = tf . math . log ( tf . random . uniform ( [ ] , seed = seed_stream ( ) ) ) log_slice_sample += _log_joint ( current_target_log_prob , current_momentum ) reverse_state = current_state reverse_target_log_prob = current_target_log_prob reverse_grads_target_log_prob = current_grads_target_log_prob reverse_momentum = current_momentum forward_state = current_state forward_target_log_prob = current_target_log_prob forward_grads_target_log_prob = current_grads_target_log_prob forward_momentum = current_momentum next_state = current_state next_target_log_prob = current_target_log_prob next_grads_target_log_prob = current_grads_target_log_prob depth = 0 num_states = 1 continue_trajectory = True while continue_trajectory : direction = tfp . math . random_rademacher ( [ ] , seed = seed_stream ( ) ) if direction < 0 : [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , next_state_in_subtree , next_target_log_prob_in_subtree , next_grads_target_log_prob_in_subtree , num_states_in_subtree , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) else : [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state_in_subtree , next_target_log_prob_in_subtree , next_grads_target_log_prob_in_subtree , num_states_in_subtree , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) if continue_trajectory : accept_state_in_subtree = _random_bernoulli ( [ ] , probs = tf . minimum ( 1. , num_states_in_subtree / num_states ) , dtype = tf . bool , seed = seed_stream ( ) ) if accept_state_in_subtree : next_state = next_state_in_subtree next_target_log_prob = next_target_log_prob_in_subtree next_grads_target_log_prob = next_grads_target_log_prob_in_subtree has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) continue_trajectory = continue_trajectory and has_no_u_turn num_states += num_states_in_subtree depth += 1 return next_state , next_target_log_prob , next_grads_target_log_prob",Simulates a No - U - Turn Sampler ( NUTS ) trajectory .
"def _build_tree ( value_and_gradients_fn , current_state , current_target_log_prob , current_grads_target_log_prob , current_momentum , direction , depth , step_size , log_slice_sample , max_simulation_error = 1000. , seed = None ) : if depth == 0 : [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ] = _leapfrog ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , step_size = direction * step_size ) next_log_joint = _log_joint ( next_target_log_prob , next_momentum ) num_states = tf . cast ( next_log_joint > log_slice_sample , dtype = tf . int32 ) continue_trajectory = ( next_log_joint > log_slice_sample - max_simulation_error ) return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] seed_stream = tfd . SeedStream ( seed , ""build_tree"" ) [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = current_state , current_target_log_prob = current_target_log_prob , current_grads_target_log_prob = current_grads_target_log_prob , current_momentum = current_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) if continue_trajectory : if direction < 0 : [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , _ , _ , _ , _ , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = reverse_state , current_target_log_prob = reverse_target_log_prob , current_grads_target_log_prob = reverse_grads_target_log_prob , current_momentum = reverse_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) else : [ _ , _ , _ , _ , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , far_state , far_target_log_prob , far_grads_target_log_prob , far_num_states , far_continue_trajectory , ] = _build_tree ( value_and_gradients_fn = value_and_gradients_fn , current_state = forward_state , current_target_log_prob = forward_target_log_prob , current_grads_target_log_prob = forward_grads_target_log_prob , current_momentum = forward_momentum , direction = direction , depth = depth - 1 , step_size = step_size , log_slice_sample = log_slice_sample , seed = seed_stream ( ) ) num_states += far_num_states accept_far_state = _random_bernoulli ( [ ] , probs = far_num_states / num_states , dtype = tf . bool , seed = seed_stream ( ) ) if accept_far_state : next_state = far_state next_target_log_prob = far_target_log_prob next_grads_target_log_prob = far_grads_target_log_prob has_no_u_turn = tf . logical_and ( _has_no_u_turn ( forward_state , reverse_state , forward_momentum ) , _has_no_u_turn ( forward_state , reverse_state , reverse_momentum ) ) continue_trajectory = far_continue_trajectory and has_no_u_turn return [ reverse_state , reverse_target_log_prob , reverse_grads_target_log_prob , reverse_momentum , forward_state , forward_target_log_prob , forward_grads_target_log_prob , forward_momentum , next_state , next_target_log_prob , next_grads_target_log_prob , num_states , continue_trajectory , ]",Builds a tree at a given tree depth and at a given state .
"def _embed_no_none_gradient_check ( value_and_gradients_fn ) : @ functools . wraps ( value_and_gradients_fn ) def func_wrapped ( * args , * * kwargs ) : """"""Wrapped function which checks for None gradients."""""" value , grads = value_and_gradients_fn ( * args , * * kwargs ) if any ( grad is None for grad in grads ) : raise ValueError ( ""Gradient is None for a state."" ) return value , grads return func_wrapped",Wraps value and gradients function to assist with None gradients .
"def _has_no_u_turn ( state_one , state_two , momentum ) : dot_product = sum ( [ tf . reduce_sum ( input_tensor = ( s1 - s2 ) * m ) for s1 , s2 , m in zip ( state_one , state_two , momentum ) ] ) return dot_product > 0",If two given states and momentum do not exhibit a U - turn pattern .
"def _leapfrog ( value_and_gradients_fn , current_state , current_grads_target_log_prob , current_momentum , step_size ) : mid_momentum = [ m + 0.5 * step * g for m , step , g in zip ( current_momentum , step_size , current_grads_target_log_prob ) ] next_state = [ s + step * m for s , step , m in zip ( current_state , step_size , mid_momentum ) ] next_target_log_prob , next_grads_target_log_prob = value_and_gradients_fn ( * next_state ) next_momentum = [ m + 0.5 * step * g for m , step , g in zip ( mid_momentum , step_size , next_grads_target_log_prob ) ] return [ next_state , next_target_log_prob , next_grads_target_log_prob , next_momentum , ]",Runs one step of leapfrog integration .
"def _log_joint ( current_target_log_prob , current_momentum ) : momentum_log_prob = - sum ( [ tf . reduce_sum ( input_tensor = 0.5 * ( m ** 2. ) ) for m in current_momentum ] ) return current_target_log_prob + momentum_log_prob",Log - joint probability given a state s log - probability and momentum .
"def _random_bernoulli ( shape , probs , dtype = tf . int32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , ""random_bernoulli"" , [ shape , probs ] ) : probs = tf . convert_to_tensor ( value = probs ) random_uniform = tf . random . uniform ( shape , dtype = probs . dtype , seed = seed ) return tf . cast ( tf . less ( random_uniform , probs ) , dtype )",Returns samples from a Bernoulli distribution .
"def default_loc_scale_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : def _fn ( dtype , shape , name , trainable , add_variable_fn ) : """"""Creates `loc`, `scale` parameters."""""" loc = add_variable_fn ( name = name + '_loc' , shape = shape , initializer = loc_initializer , regularizer = loc_regularizer , constraint = loc_constraint , dtype = dtype , trainable = trainable ) if is_singular : return loc , None untransformed_scale = add_variable_fn ( name = name + '_untransformed_scale' , shape = shape , initializer = untransformed_scale_initializer , regularizer = untransformed_scale_regularizer , constraint = untransformed_scale_constraint , dtype = dtype , trainable = trainable ) scale = ( np . finfo ( dtype . as_numpy_dtype ) . eps + tf . nn . softplus ( untransformed_scale ) ) return loc , scale return _fn",Makes closure which creates loc scale params from tf . get_variable .
"def default_mean_field_normal_fn ( is_singular = False , loc_initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) , untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = - 3. , stddev = 0.1 ) , loc_regularizer = None , untransformed_scale_regularizer = None , loc_constraint = None , untransformed_scale_constraint = None ) : loc_scale_fn = default_loc_scale_fn ( is_singular = is_singular , loc_initializer = loc_initializer , untransformed_scale_initializer = untransformed_scale_initializer , loc_regularizer = loc_regularizer , untransformed_scale_regularizer = untransformed_scale_regularizer , loc_constraint = loc_constraint , untransformed_scale_constraint = untransformed_scale_constraint ) def _fn ( dtype , shape , name , trainable , add_variable_fn ) : """"""Creates multivariate `Deterministic` or `Normal` distribution.

    Args:
      dtype: Type of parameter's event.
      shape: Python `list`-like representing the parameter's event shape.
      name: Python `str` name prepended to any created (or existing)
        `tf.Variable`s.
      trainable: Python `bool` indicating all created `tf.Variable`s should be
        added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.
      add_variable_fn: `tf.get_variable`-like `callable` used to create (or
        access existing) `tf.Variable`s.

    Returns:
      Multivariate `Deterministic` or `Normal` distribution.
    """""" loc , scale = loc_scale_fn ( dtype , shape , name , trainable , add_variable_fn ) if scale is None : dist = tfd . Deterministic ( loc = loc ) else : dist = tfd . Normal ( loc = loc , scale = scale ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims ) return _fn",Creates a function to build Normal distributions with trainable params .
"def default_multivariate_normal_fn ( dtype , shape , name , trainable , add_variable_fn ) : del name , trainable , add_variable_fn dist = tfd . Normal ( loc = tf . zeros ( shape , dtype ) , scale = dtype . as_numpy_dtype ( 1 ) ) batch_ndims = tf . size ( input = dist . batch_shape_tensor ( ) ) return tfd . Independent ( dist , reinterpreted_batch_ndims = batch_ndims )",Creates multivariate standard Normal distribution .
"def deserialize_function ( serial , function_type ) : if function_type == 'function' : function = tf . keras . utils . deserialize_keras_object ( serial ) elif function_type == 'lambda' : function = generic_utils . func_load ( serial ) else : raise TypeError ( 'Unknown function type:' , function_type ) return function",Deserializes the Keras - serialized function .
"def serialize_function ( func ) : if isinstance ( func , types . LambdaType ) : return generic_utils . func_dump ( func ) , 'lambda' return func . __name__ , 'function'",Serializes function for Keras .
"def broadcast_structure ( to_structure , from_structure ) : from_parts = tf . nest . flatten ( from_structure ) if len ( from_parts ) == 1 : from_structure = tf . nest . map_structure ( lambda _ : from_parts [ 0 ] , to_structure ) return from_structure",Broadcasts from_structure to to_structure .
"def expand_as_args ( args ) : return ( isinstance ( args , collections . Sequence ) and not _is_namedtuple ( args ) and not _force_leaf ( args ) )",Returns True if args should be expanded as * args .
"def _nested_convert_to_tensor ( struct , dtype = None , name = None ) : if dtype is not None or not tf . nest . is_nested ( struct ) : return tf . convert_to_tensor ( struct , dtype = dtype ) if _maybe_convertible_to_tensor ( struct ) : try : return tf . convert_to_tensor ( value = struct , name = name ) except ( ValueError , TypeError ) : pass shallow_struct = _get_shallow_structure ( struct ) return nest . map_structure_up_to ( shallow_struct , lambda s : _nested_convert_to_tensor ( s , name = name ) , struct )",Eagerly converts struct to Tensor recursing upon failure .
"def convert_args_to_tensor ( args , dtype = None , name = None ) : if dtype is None : if expand_as_args ( args ) or _expand_as_kwargs ( args ) : shallow_args = _get_shallow_structure ( args ) return nest . map_structure_up_to ( shallow_args , lambda s : _nested_convert_to_tensor ( s , name = name ) , args ) else : return _nested_convert_to_tensor ( args , name = name ) else : return nest . map_structure_up_to ( dtype , lambda s , dtype : _nested_convert_to_tensor ( s , dtype , name ) , args , dtype )",Converts args to Tensor s .
"def call_fn ( fn , args ) : if expand_as_args ( args ) : return fn ( * args ) elif _expand_as_kwargs ( args ) : return fn ( * * args ) else : return fn ( args )",Calls fn with args possibly expanding args .
"def _wrap_method ( cls , attr ) : fn = getattr ( cls , attr ) is_property = isinstance ( fn , property ) if is_property : fn = fn . fget @ functools . wraps ( fn ) def wrapped ( self , * args , * * kwargs ) : return fn ( self . _value ( ) , * args , * * kwargs ) return property ( wrapped ) if is_property else wrapped",Replaces member function s first arg self to self . _value () .
"def _get_tensor_like_attributes ( ) : attrs = dict ( ) attrs . update ( ( attr , _wrap_method ( tf . Tensor , attr ) ) for attr in tf . Tensor . OVERLOADABLE_OPERATORS . union ( { '__iter__' } ) ) attrs . update ( ( attr , getattr ( tf . Tensor , attr ) ) for attr in { '__nonzero__' , '__bool__' , '__array_priority__' } ) return attrs",Returns Tensor attributes related to shape and Python builtins .
"def _value ( self , dtype = None , name = None , as_ref = False ) : if as_ref : raise NotImplementedError ( 'Cannot convert a `Distribution` to a reference ' '(e.g., `tf.Variable`).' ) if self . _concrete_value is None : if self . _convert_to_tensor_fn is None : raise NotImplementedError ( 'Failed to convert object of type {} to Tensor. Contents: {}. ' 'Call `distribution.set_tensor_conversion(lambda self: ...)` to ' 'enable `tf.convert_to_tensor` capability. For example: ' '`x = tfd.Normal(0,1).set_tensor_conversion(tfd.Distribution.mean)`' ' results in `tf.convert_to_tensor(x)` being identical to ' '`x.mean()`.' . format ( type ( self ) , self ) ) with self . _name_scope ( 'value' ) : self . _concrete_value = ( self . _convert_to_tensor_fn ( self ) if callable ( self . _convert_to_tensor_fn ) else self . _convert_to_tensor_fn ) if not tf . is_tensor ( self . _concrete_value ) : self . _concrete_value = tfd . _convert_to_tensor ( value = self . _concrete_value , name = name or 'concrete_value' , dtype = dtype , dtype_hint = self . dtype ) return self . _concrete_value",Get the value returned by tf . convert_to_tensor ( distribution ) .
"def make_encoder ( activation , latent_size , base_depth ) : conv = functools . partial ( tf . keras . layers . Conv2D , padding = ""SAME"" , activation = activation ) encoder_net = tf . keras . Sequential ( [ conv ( base_depth , 5 , 1 ) , conv ( base_depth , 5 , 2 ) , conv ( 2 * base_depth , 5 , 1 ) , conv ( 2 * base_depth , 5 , 2 ) , conv ( 4 * latent_size , 7 , padding = ""VALID"" ) , tf . keras . layers . Flatten ( ) , tf . keras . layers . Dense ( 2 * latent_size , activation = None ) , ] ) def encoder ( images ) : images = 2 * tf . cast ( images , dtype = tf . float32 ) - 1 net = encoder_net ( images ) return tfd . MultivariateNormalDiag ( loc = net [ ... , : latent_size ] , scale_diag = tf . nn . softplus ( net [ ... , latent_size : ] + _softplus_inverse ( 1.0 ) ) , name = ""code"" ) return encoder",Creates the encoder function .
"def make_decoder ( activation , latent_size , output_shape , base_depth ) : deconv = functools . partial ( tf . keras . layers . Conv2DTranspose , padding = ""SAME"" , activation = activation ) conv = functools . partial ( tf . keras . layers . Conv2D , padding = ""SAME"" , activation = activation ) decoder_net = tf . keras . Sequential ( [ deconv ( 2 * base_depth , 7 , padding = ""VALID"" ) , deconv ( 2 * base_depth , 5 ) , deconv ( 2 * base_depth , 5 , 2 ) , deconv ( base_depth , 5 ) , deconv ( base_depth , 5 , 2 ) , deconv ( base_depth , 5 ) , conv ( output_shape [ - 1 ] , 5 , activation = None ) , ] ) def decoder ( codes ) : original_shape = tf . shape ( input = codes ) codes = tf . reshape ( codes , ( - 1 , 1 , 1 , latent_size ) ) logits = decoder_net ( codes ) logits = tf . reshape ( logits , shape = tf . concat ( [ original_shape [ : - 1 ] , output_shape ] , axis = 0 ) ) return tfd . Independent ( tfd . Bernoulli ( logits = logits ) , reinterpreted_batch_ndims = len ( output_shape ) , name = ""image"" ) return decoder",Creates the decoder function .
"def make_mixture_prior ( latent_size , mixture_components ) : if mixture_components == 1 : return tfd . MultivariateNormalDiag ( loc = tf . zeros ( [ latent_size ] ) , scale_identity_multiplier = 1.0 ) loc = tf . compat . v1 . get_variable ( name = ""loc"" , shape = [ mixture_components , latent_size ] ) raw_scale_diag = tf . compat . v1 . get_variable ( name = ""raw_scale_diag"" , shape = [ mixture_components , latent_size ] ) mixture_logits = tf . compat . v1 . get_variable ( name = ""mixture_logits"" , shape = [ mixture_components ] ) return tfd . MixtureSameFamily ( components_distribution = tfd . MultivariateNormalDiag ( loc = loc , scale_diag = tf . nn . softplus ( raw_scale_diag ) ) , mixture_distribution = tfd . Categorical ( logits = mixture_logits ) , name = ""prior"" )",Creates the mixture of Gaussians prior distribution .
"def pack_images ( images , rows , cols ) : shape = tf . shape ( input = images ) width = shape [ - 3 ] height = shape [ - 2 ] depth = shape [ - 1 ] images = tf . reshape ( images , ( - 1 , width , height , depth ) ) batch = tf . shape ( input = images ) [ 0 ] rows = tf . minimum ( rows , batch ) cols = tf . minimum ( batch // rows , cols ) images = images [ : rows * cols ] images = tf . reshape ( images , ( rows , cols , width , height , depth ) ) images = tf . transpose ( a = images , perm = [ 0 , 2 , 1 , 3 , 4 ] ) images = tf . reshape ( images , [ 1 , rows * width , cols * height , depth ] ) return images",Helper utility to make a field of images .
"def model_fn ( features , labels , mode , params , config ) : del labels , config if params [ ""analytic_kl"" ] and params [ ""mixture_components"" ] != 1 : raise NotImplementedError ( ""Using `analytic_kl` is only supported when `mixture_components = 1` "" ""since there's no closed form otherwise."" ) encoder = make_encoder ( params [ ""activation"" ] , params [ ""latent_size"" ] , params [ ""base_depth"" ] ) decoder = make_decoder ( params [ ""activation"" ] , params [ ""latent_size"" ] , IMAGE_SHAPE , params [ ""base_depth"" ] ) latent_prior = make_mixture_prior ( params [ ""latent_size"" ] , params [ ""mixture_components"" ] ) image_tile_summary ( ""input"" , tf . cast ( features , dtype = tf . float32 ) , rows = 1 , cols = 16 ) approx_posterior = encoder ( features ) approx_posterior_sample = approx_posterior . sample ( params [ ""n_samples"" ] ) decoder_likelihood = decoder ( approx_posterior_sample ) image_tile_summary ( ""recon/sample"" , tf . cast ( decoder_likelihood . sample ( ) [ : 3 , : 16 ] , dtype = tf . float32 ) , rows = 3 , cols = 16 ) image_tile_summary ( ""recon/mean"" , decoder_likelihood . mean ( ) [ : 3 , : 16 ] , rows = 3 , cols = 16 ) distortion = - decoder_likelihood . log_prob ( features ) avg_distortion = tf . reduce_mean ( input_tensor = distortion ) tf . compat . v1 . summary . scalar ( ""distortion"" , avg_distortion ) if params [ ""analytic_kl"" ] : rate = tfd . kl_divergence ( approx_posterior , latent_prior ) else : rate = ( approx_posterior . log_prob ( approx_posterior_sample ) - latent_prior . log_prob ( approx_posterior_sample ) ) avg_rate = tf . reduce_mean ( input_tensor = rate ) tf . compat . v1 . summary . scalar ( ""rate"" , avg_rate ) elbo_local = - ( rate + distortion ) elbo = tf . reduce_mean ( input_tensor = elbo_local ) loss = - elbo tf . compat . v1 . summary . scalar ( ""elbo"" , elbo ) importance_weighted_elbo = tf . reduce_mean ( input_tensor = tf . reduce_logsumexp ( input_tensor = elbo_local , axis = 0 ) - tf . math . log ( tf . cast ( params [ ""n_samples"" ] , dtype = tf . float32 ) ) ) tf . compat . v1 . summary . scalar ( ""elbo/importance_weighted"" , importance_weighted_elbo ) random_image = decoder ( latent_prior . sample ( 16 ) ) image_tile_summary ( ""random/sample"" , tf . cast ( random_image . sample ( ) , dtype = tf . float32 ) , rows = 4 , cols = 4 ) image_tile_summary ( ""random/mean"" , random_image . mean ( ) , rows = 4 , cols = 4 ) global_step = tf . compat . v1 . train . get_or_create_global_step ( ) learning_rate = tf . compat . v1 . train . cosine_decay ( params [ ""learning_rate"" ] , global_step , params [ ""max_steps"" ] ) tf . compat . v1 . summary . scalar ( ""learning_rate"" , learning_rate ) optimizer = tf . compat . v1 . train . AdamOptimizer ( learning_rate ) train_op = optimizer . minimize ( loss , global_step = global_step ) return tf . estimator . EstimatorSpec ( mode = mode , loss = loss , train_op = train_op , eval_metric_ops = { ""elbo"" : tf . compat . v1 . metrics . mean ( elbo ) , ""elbo/importance_weighted"" : tf . compat . v1 . metrics . mean ( importance_weighted_elbo ) , ""rate"" : tf . compat . v1 . metrics . mean ( avg_rate ) , ""distortion"" : tf . compat . v1 . metrics . mean ( avg_distortion ) , } , )",Builds the model function for use in an estimator .
"def download ( directory , filename ) : filepath = os . path . join ( directory , filename ) if tf . io . gfile . exists ( filepath ) : return filepath if not tf . io . gfile . exists ( directory ) : tf . io . gfile . makedirs ( directory ) url = os . path . join ( ROOT_PATH , filename ) print ( ""Downloading %s to %s"" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) return filepath",Downloads a file .
"def build_fake_input_fns ( batch_size ) : random_sample = np . random . rand ( batch_size , * IMAGE_SHAPE ) . astype ( ""float32"" ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) . map ( lambda row : ( row , 0 ) ) . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn",Builds fake MNIST - style data for unit testing .
"def build_input_fns ( data_dir , batch_size ) : def train_input_fn ( ) : dataset = static_mnist_dataset ( data_dir , ""train"" ) dataset = dataset . shuffle ( 50000 ) . repeat ( ) . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : eval_dataset = static_mnist_dataset ( data_dir , ""valid"" ) eval_dataset = eval_dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( eval_dataset ) . get_next ( ) return train_input_fn , eval_input_fn",Builds an Iterator switching between train and heldout data .
"def _validate_block_sizes ( block_sizes , bijectors , validate_args ) : block_sizes_shape = block_sizes . shape if tensorshape_util . is_fully_defined ( block_sizes_shape ) : if ( tensorshape_util . rank ( block_sizes_shape ) != 1 or ( tensorshape_util . num_elements ( block_sizes_shape ) != len ( bijectors ) ) ) : raise ValueError ( '`block_sizes` must be `None`, or a vector of the same length as ' '`bijectors`. Got a `Tensor` with shape {} and `bijectors` of ' 'length {}' . format ( block_sizes_shape , len ( bijectors ) ) ) return block_sizes elif validate_args : message = ( '`block_sizes` must be `None`, or a vector of the same length ' 'as `bijectors`.' ) with tf . control_dependencies ( [ assert_util . assert_equal ( tf . size ( input = block_sizes ) , len ( bijectors ) , message = message ) , assert_util . assert_equal ( tf . rank ( block_sizes ) , 1 ) ] ) : return tf . identity ( block_sizes ) else : return block_sizes",Helper to validate block sizes .
"def maybe_check_wont_broadcast ( flat_xs , validate_args ) : flat_xs = tuple ( flat_xs ) if not validate_args : return flat_xs msg = 'Broadcasting probably indicates an error in model specification.' s = tuple ( x . shape for x in flat_xs ) if all ( tensorshape_util . is_fully_defined ( s_ ) for s_ in s ) : if not all ( a == b for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ) : raise ValueError ( msg ) return flat_xs assertions = [ assert_util . assert_equal ( a , b , message = msg ) for a , b in zip ( s [ 1 : ] , s [ : - 1 ] ) ] with tf . control_dependencies ( assertions ) : return tuple ( tf . identity ( x ) for x in flat_xs )",Verifies that parts don t broadcast .
"def softplus_and_shift ( x , shift = 1e-5 , name = None ) : with tf . compat . v1 . name_scope ( name , 'softplus_and_shift' , [ x , shift ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) y = tf . nn . softplus ( x ) if shift is not None : y += shift return y",Converts ( batch of ) scalars to ( batch of ) positive valued scalars .
"def tril_with_diag_softplus_and_shift ( x , diag_shift = 1e-5 , name = None ) : with tf . compat . v1 . name_scope ( name , 'tril_with_diag_softplus_and_shift' , [ x , diag_shift ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) x = tfd . fill_triangular ( x ) diag = softplus_and_shift ( tf . linalg . diag_part ( x ) , diag_shift ) x = tf . linalg . set_diag ( x , diag ) return x",Converts ( batch of ) vectors to ( batch of ) lower - triangular scale matrices .
"def multivariate_normal_tril ( x , dims , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = tril_with_diag_softplus_and_shift , name = None ) : with tf . compat . v1 . name_scope ( name , 'multivariate_normal_tril' , [ x , dims ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) x = layer_fn ( x , dims + dims * ( dims + 1 ) // 2 ) return tfd . MultivariateNormalTriL ( loc = loc_fn ( x [ ... , : dims ] ) , scale_tril = scale_fn ( x [ ... , dims : ] ) )",Constructs a trainable tfd . MultivariateNormalTriL distribution .
"def bernoulli ( x , layer_fn = tf . compat . v1 . layers . dense , name = None ) : with tf . compat . v1 . name_scope ( name , 'bernoulli' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) logits = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) return tfd . Bernoulli ( logits = logits )",Constructs a trainable tfd . Bernoulli distribution .
"def normal ( x , layer_fn = tf . compat . v1 . layers . dense , loc_fn = lambda x : x , scale_fn = 1. , name = None ) : with tf . compat . v1 . name_scope ( name , 'normal' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if callable ( scale_fn ) : y = layer_fn ( x , 2 ) loc = loc_fn ( y [ ... , 0 ] ) scale = scale_fn ( y [ ... , 1 ] ) else : y = tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) loc = loc_fn ( y ) scale = tf . cast ( scale_fn , loc . dtype . base_dtype ) return tfd . Normal ( loc = loc , scale = scale )",Constructs a trainable tfd . Normal distribution .
"def poisson ( x , layer_fn = tf . compat . v1 . layers . dense , log_rate_fn = lambda x : x , name = None ) : with tf . compat . v1 . name_scope ( name , 'poisson' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) log_rate = log_rate_fn ( tf . squeeze ( layer_fn ( x , 1 ) , axis = - 1 ) ) return tfd . Poisson ( log_rate = log_rate )",Constructs a trainable tfd . Poisson distribution .
"def secant_root ( objective_fn , initial_position , next_position = None , value_at_position = None , position_tolerance = 1e-8 , value_tolerance = 1e-8 , max_iterations = 50 , stopping_policy_fn = tf . reduce_all , validate_args = False , name = None ) : if not callable ( stopping_policy_fn ) : raise ValueError ( 'stopping_policy_fn must be callable' ) position = tf . convert_to_tensor ( value = initial_position , name = 'position' , ) value_at_position = tf . convert_to_tensor ( value = value_at_position or objective_fn ( position ) , name = 'value_at_position' , dtype = position . dtype . base_dtype ) zero = tf . zeros_like ( position ) position_tolerance = tf . convert_to_tensor ( value = position_tolerance , name = 'position_tolerance' , dtype = position . dtype ) value_tolerance = tf . convert_to_tensor ( value = value_tolerance , name = 'value_tolerance' , dtype = position . dtype ) num_iterations = tf . zeros_like ( position , dtype = tf . int32 ) max_iterations = tf . convert_to_tensor ( value = max_iterations , dtype = tf . int32 ) max_iterations = tf . broadcast_to ( max_iterations , name = 'max_iterations' , shape = position . shape ) if next_position is None : epsilon = tf . constant ( 1e-4 , dtype = position . dtype , shape = position . shape ) step = position * epsilon + tf . sign ( position ) * epsilon else : step = next_position - initial_position finished = tf . constant ( False , shape = position . shape ) def _should_continue ( position , value_at_position , num_iterations , step , finished ) : """"""Indicates whether the overall search should continue.

    Args:
      position: `Tensor` containing the current root estimates.
      value_at_position: `Tensor` containing the value of `objective_fn` at
        `position`.
      num_iterations: `Tensor` containing the current iteration index for each
        point.
      step: `Tensor` containing the size of the step to take for each point.
      finished: `Tensor` indicating for which points the search is finished.

    Returns:
      A boolean value indicating whether the overall search should continue.
    """""" del position , value_at_position , num_iterations , step return ~ tf . convert_to_tensor ( value = stopping_policy_fn ( finished ) , name = 'should_stop' , dtype = tf . bool ) def _body ( position , value_at_position , num_iterations , step , finished ) : """"""Performs one iteration of the secant root-finding algorithm.

    Args:
      position: `Tensor` containing the current root estimates.
      value_at_position: `Tensor` containing the value of `objective_fn` at
        `position`.
      num_iterations: `Tensor` containing the current iteration index for each
        point.
      step: `Tensor` containing the size of the step to take for each point.
      finished: `Tensor` indicating for which points the search is finished.

    Returns:
      The `Tensor`s to use for the next iteration of the algorithm.
    """""" was_finished = finished | ( num_iterations >= max_iterations ) | ( tf . abs ( step ) < position_tolerance ) | ( tf . abs ( value_at_position ) < value_tolerance ) next_position = tf . where ( was_finished , position , position + step ) value_at_next_position = tf . where ( was_finished , value_at_position , objective_fn ( next_position ) ) is_finished = tf . equal ( value_at_position , value_at_next_position ) next_position = tf . where ( is_finished & ~ was_finished , ( position + next_position ) * 0.5 , next_position ) num_iterations = tf . where ( is_finished , num_iterations , num_iterations + 1 ) next_step = tf . where ( is_finished , zero , step * value_at_next_position / ( value_at_position - value_at_next_position ) ) return ( next_position , value_at_next_position , num_iterations , next_step , is_finished ) with tf . compat . v1 . name_scope ( name , 'secant_root' , [ position , next_position , value_at_position , max_iterations ] ) : assertions = [ ] if validate_args : assertions += [ tf . Assert ( tf . reduce_all ( input_tensor = position_tolerance > zero ) , [ position_tolerance ] ) , tf . Assert ( tf . reduce_all ( input_tensor = value_tolerance > zero ) , [ value_tolerance ] ) , tf . Assert ( tf . reduce_all ( input_tensor = max_iterations >= num_iterations ) , [ max_iterations ] ) , ] with tf . control_dependencies ( assertions ) : root , value_at_root , num_iterations , _ , _ = tf . while_loop ( cond = _should_continue , body = _body , loop_vars = [ position , value_at_position , num_iterations , step , finished ] ) return RootSearchResults ( estimated_root = root , objective_at_estimated_root = value_at_root , num_iterations = num_iterations )",r Finds root ( s ) of a function of single variable using the secant method .
"def _euler_method ( random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts , name = None ) : with tf . compat . v1 . name_scope ( name , 'mala_euler_method' , [ random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts ] ) : proposed_state_parts = [ ] for random_draw , state , drift , step_size , volatility in zip ( random_draw_parts , state_parts , drift_parts , step_size_parts , volatility_parts ) : proposal = state + drift + volatility * tf . sqrt ( step_size ) * random_draw proposed_state_parts . append ( proposal ) return proposed_state_parts",Applies one step of Euler - Maruyama method .
"def _get_drift ( step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob , name = None ) : with tf . compat . v1 . name_scope ( name , 'mala_get_drift' , [ step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob ] ) : drift_parts = [ ] for step_size , volatility , grad_volatility , grad_target_log_prob in ( zip ( step_size_parts , volatility_parts , grads_volatility , grads_target_log_prob ) ) : volatility_squared = tf . square ( volatility ) drift = 0.5 * step_size * ( volatility_squared * grad_target_log_prob + grad_volatility ) drift_parts . append ( drift ) return drift_parts",Compute diffusion drift at the current location current_state .
"def _compute_log_acceptance_correction ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims , name = None ) : with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , independent_chain_ndims ] ) : proposed_log_density_parts = [ ] dual_log_density_parts = [ ] for [ current_state , proposed_state , current_volatility , proposed_volatility , current_drift , proposed_drift , step_size , ] in zip ( current_state_parts , proposed_state_parts , current_volatility_parts , proposed_volatility_parts , current_drift_parts , proposed_drift_parts , step_size_parts , ) : axis = tf . range ( independent_chain_ndims , tf . rank ( current_state ) ) state_diff = proposed_state - current_state current_volatility *= tf . sqrt ( step_size ) proposed_energy = ( state_diff - current_drift ) / current_volatility proposed_volatility *= tf . sqrt ( step_size ) proposed_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( current_volatility ) , 0.5 * ( proposed_energy ** 2 ) ] ) , axis = axis ) ) proposed_log_density_parts . append ( - proposed_energy ) dual_energy = ( state_diff + proposed_drift ) / proposed_volatility dual_energy = ( tf . reduce_sum ( input_tensor = mcmc_util . safe_sum ( [ tf . math . log ( proposed_volatility ) , 0.5 * ( dual_energy ** 2 ) ] ) , axis = axis ) ) dual_log_density_parts . append ( - dual_energy ) proposed_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( proposed_log_density_parts , axis = - 1 ) , axis = - 1 ) dual_log_density_reduce = tf . reduce_sum ( input_tensor = tf . stack ( dual_log_density_parts , axis = - 1 ) , axis = - 1 ) return mcmc_util . safe_sum ( [ dual_log_density_reduce , - proposed_log_density_reduce ] )",r Helper to kernel which computes the log acceptance - correction .
"def _maybe_call_volatility_fn_and_grads ( volatility_fn , state , volatility_fn_results = None , grads_volatility_fn = None , sample_shape = None , parallel_iterations = 10 ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] needs_volatility_fn_gradients = grads_volatility_fn is None if volatility_fn_results is None : volatility_fn_results = volatility_fn ( * state_parts ) volatility_fn_results = ( list ( volatility_fn_results ) if mcmc_util . is_list_like ( volatility_fn_results ) else [ volatility_fn_results ] ) if len ( volatility_fn_results ) == 1 : volatility_fn_results *= len ( state_parts ) if len ( state_parts ) != len ( volatility_fn_results ) : raise ValueError ( '`volatility_fn` should return a tensor or a list ' 'of the same length as `current_state`.' ) volatility_fn_results = _maybe_broadcast_volatility ( volatility_fn_results , state_parts ) if grads_volatility_fn is None : [ _ , grads_volatility_fn , ] = diag_jacobian ( xs = state_parts , ys = volatility_fn_results , sample_shape = sample_shape , parallel_iterations = parallel_iterations , fn = volatility_fn ) if needs_volatility_fn_gradients : grads_volatility_fn = [ 2. * g * volatility if g is not None else tf . zeros_like ( fn_arg , dtype = fn_arg . dtype . base_dtype ) for g , volatility , fn_arg in zip ( grads_volatility_fn , volatility_fn_results , state_parts ) ] return volatility_fn_results , grads_volatility_fn",Helper which computes volatility_fn results and grads if needed .
"def _maybe_broadcast_volatility ( volatility_parts , state_parts ) : return [ v + tf . zeros_like ( sp , dtype = sp . dtype . base_dtype ) for v , sp in zip ( volatility_parts , state_parts ) ]",Helper to broadcast volatility_parts to the shape of state_parts .
"def _prepare_args ( target_log_prob_fn , volatility_fn , state , step_size , target_log_prob = None , grads_target_log_prob = None , volatility = None , grads_volatility_fn = None , diffusion_drift = None , parallel_iterations = 10 ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] [ target_log_prob , grads_target_log_prob , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , state_parts , target_log_prob , grads_target_log_prob ) [ volatility_parts , grads_volatility , ] = _maybe_call_volatility_fn_and_grads ( volatility_fn , state_parts , volatility , grads_volatility_fn , distribution_util . prefer_static_shape ( target_log_prob ) , parallel_iterations ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) if diffusion_drift is None : diffusion_drift_parts = _get_drift ( step_sizes , volatility_parts , grads_volatility , grads_target_log_prob ) else : diffusion_drift_parts = ( list ( diffusion_drift ) if mcmc_util . is_list_like ( diffusion_drift ) else [ diffusion_drift ] ) if len ( state_parts ) != len ( diffusion_drift ) : raise ValueError ( 'There should be exactly one `diffusion_drift` or it ' 'should have same length as list-like `current_state`.' ) return [ state_parts , step_sizes , target_log_prob , grads_target_log_prob , volatility_parts , grads_volatility , diffusion_drift_parts , ]",Helper which processes input args to meet list - like assumptions .
"def make_ar_transition_matrix ( coefficients ) : top_row = tf . expand_dims ( coefficients , - 2 ) coef_shape = dist_util . prefer_static_shape ( coefficients ) batch_shape , order = coef_shape [ : - 1 ] , coef_shape [ - 1 ] remaining_rows = tf . concat ( [ tf . eye ( order - 1 , dtype = coefficients . dtype , batch_shape = batch_shape ) , tf . zeros ( tf . concat ( [ batch_shape , ( order - 1 , 1 ) ] , axis = 0 ) , dtype = coefficients . dtype ) ] , axis = - 1 ) ar_matrix = tf . concat ( [ top_row , remaining_rows ] , axis = - 2 ) return ar_matrix",Build transition matrix for an autoregressive StateSpaceModel .
"def diag_jacobian ( xs , ys = None , sample_shape = None , fn = None , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , 'jacobians_diag' , [ xs , ys ] ) : if sample_shape is None : sample_shape = [ 1 ] jacobians_diag_res = [ ] xs = list ( xs ) if _is_list_like ( xs ) else [ xs ] xs = [ tf . convert_to_tensor ( value = x ) for x in xs ] if not tf . executing_eagerly ( ) : if ys is None : if fn is None : raise ValueError ( 'Both `ys` and `fn` can not be `None`' ) else : ys = fn ( * xs ) ys = list ( ys ) if _is_list_like ( ys ) else [ ys ] if len ( xs ) != len ( ys ) : raise ValueError ( '`xs` and `ys` should have the same length' ) for y , x in zip ( ys , xs ) : y_ = y + tf . zeros_like ( x ) y_ = tf . reshape ( y , tf . concat ( [ sample_shape , [ - 1 ] ] , - 1 ) ) n = tf . size ( input = x ) / tf . cast ( tf . reduce_prod ( input_tensor = sample_shape ) , dtype = tf . int32 ) n = tf . cast ( n , dtype = tf . int32 ) loop_vars = [ 0 , tf . TensorArray ( x . dtype , n ) ] def loop_body ( j ) : """"""Loop function to compute gradients of the each direction."""""" res = tf . gradients ( ys = y_ [ ... , j ] , xs = x ) [ 0 ] if res is None : res = tf . zeros ( tf . concat ( [ sample_shape , [ 1 ] ] , - 1 ) , dtype = x . dtype ) else : res = tf . reshape ( res , tf . concat ( [ sample_shape , [ - 1 ] ] , - 1 ) ) res = tf . expand_dims ( res , 0 ) res = res [ ... , j ] return res _ , jacobian_diag_res = tf . while_loop ( cond = lambda j , _ : j < n , body = lambda j , result : ( j + 1 , result . write ( j , loop_body ( j ) ) ) , loop_vars = loop_vars , parallel_iterations = parallel_iterations ) shape_x = tf . shape ( input = x ) reshaped_jacobian_diag = tf . transpose ( a = jacobian_diag_res . stack ( ) ) reshaped_jacobian_diag = tf . reshape ( reshaped_jacobian_diag , shape_x ) jacobians_diag_res . append ( reshaped_jacobian_diag ) else : if fn is None : raise ValueError ( '`fn` can not be `None` when eager execution is ' 'enabled' ) if ys is None : ys = fn ( * xs ) def fn_slice ( i , j ) : """"""Broadcast y[i], flatten event shape of y[i], return y[i][..., j]."""""" def fn_broadcast ( * state ) : res = fn ( * state ) res = list ( res ) if _is_list_like ( res ) else [ res ] if len ( res ) != len ( state ) : res *= len ( state ) res = [ tf . reshape ( r + tf . zeros_like ( s ) , tf . concat ( [ sample_shape , [ - 1 ] ] , - 1 ) ) for r , s in zip ( res , state ) ] return res return lambda * state : tf . expand_dims ( fn_broadcast ( * state ) [ i ] , 0 ) [ ... , j ] def make_loop_body ( i , x ) : """"""Loop function to compute gradients of the each direction."""""" def _fn ( j , result ) : res = value_and_gradient ( fn_slice ( i , j ) , xs ) [ 1 ] [ i ] if res is None : res = tf . zeros ( tf . concat ( [ sample_shape , [ 1 ] ] , - 1 ) , dtype = x . dtype ) else : res = tf . reshape ( res , tf . concat ( [ sample_shape , [ - 1 ] ] , - 1 ) ) res = res [ ... , j ] return j + 1 , result . write ( j , res ) return _fn for i , x in enumerate ( xs ) : n = tf . size ( input = x ) / tf . cast ( tf . reduce_prod ( input_tensor = sample_shape ) , dtype = tf . int32 ) n = tf . cast ( n , dtype = tf . int32 ) loop_vars = [ 0 , tf . TensorArray ( x . dtype , n ) ] _ , jacobian_diag_res = tf . while_loop ( cond = lambda j , _ : j < n , body = make_loop_body ( i , x ) , loop_vars = loop_vars , parallel_iterations = parallel_iterations ) shape_x = tf . shape ( input = x ) reshaped_jacobian_diag = tf . transpose ( a = jacobian_diag_res . stack ( ) ) reshaped_jacobian_diag = tf . reshape ( reshaped_jacobian_diag , shape_x ) jacobians_diag_res . append ( reshaped_jacobian_diag ) return ys , jacobians_diag_res",Computes diagonal of the Jacobian matrix of ys = fn ( xs ) wrt xs .
"def calculate_reshape ( original_shape , new_shape , validate = False , name = None ) : batch_shape_static = tensorshape_util . constant_value_as_shape ( new_shape ) if tensorshape_util . is_fully_defined ( batch_shape_static ) : return np . int32 ( batch_shape_static ) , batch_shape_static , [ ] with tf . name_scope ( name or ""calculate_reshape"" ) : original_size = tf . reduce_prod ( input_tensor = original_shape ) implicit_dim = tf . equal ( new_shape , - 1 ) size_implicit_dim = ( original_size // tf . maximum ( 1 , - tf . reduce_prod ( input_tensor = new_shape ) ) ) new_ndims = tf . shape ( input = new_shape ) expanded_new_shape = tf . where ( implicit_dim , tf . fill ( new_ndims , size_implicit_dim ) , new_shape ) validations = [ ] if not validate else [ assert_util . assert_rank ( original_shape , 1 , message = ""Original shape must be a vector."" ) , assert_util . assert_rank ( new_shape , 1 , message = ""New shape must be a vector."" ) , assert_util . assert_less_equal ( tf . math . count_nonzero ( implicit_dim , dtype = tf . int32 ) , 1 , message = ""At most one dimension can be unknown."" ) , assert_util . assert_positive ( expanded_new_shape , message = ""Shape elements must be >=-1."" ) , assert_util . assert_equal ( tf . reduce_prod ( input_tensor = expanded_new_shape ) , original_size , message = ""Shape sizes do not match."" ) , ] return expanded_new_shape , batch_shape_static , validations",Calculates the reshaped dimensions ( replacing up to one - 1 in reshape ) .
"def validate_init_args_statically ( distribution , batch_shape ) : if tensorshape_util . rank ( batch_shape . shape ) is not None : if tensorshape_util . rank ( batch_shape . shape ) != 1 : raise ValueError ( ""`batch_shape` must be a vector "" ""(saw rank: {})."" . format ( tensorshape_util . rank ( batch_shape . shape ) ) ) batch_shape_static = tensorshape_util . constant_value_as_shape ( batch_shape ) batch_size_static = tensorshape_util . num_elements ( batch_shape_static ) dist_batch_size_static = tensorshape_util . num_elements ( distribution . batch_shape ) if batch_size_static is not None and dist_batch_size_static is not None : if batch_size_static != dist_batch_size_static : raise ValueError ( ""`batch_shape` size ({}) must match "" ""`distribution.batch_shape` size ({})."" . format ( batch_size_static , dist_batch_size_static ) ) if tensorshape_util . dims ( batch_shape_static ) is not None : if any ( tf . compat . dimension_value ( dim ) is not None and tf . compat . dimension_value ( dim ) < 1 for dim in batch_shape_static ) : raise ValueError ( ""`batch_shape` elements must be >=-1."" )",Helper to __init__ which makes or raises assertions .
"def _sample_shape ( self , x ) : x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) event_ndims = ( tf . size ( input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) batch_ndims = ( tf . size ( input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) sample_ndims = x_ndims - batch_ndims - event_ndims if isinstance ( sample_ndims , int ) : static_sample_shape = x . shape [ : sample_ndims ] else : static_sample_shape = tf . TensorShape ( None ) if tensorshape_util . is_fully_defined ( static_sample_shape ) : sample_shape = np . int32 ( static_sample_shape ) else : sample_shape = tf . shape ( input = x ) [ : sample_ndims ] return sample_shape , static_sample_shape",Computes graph and static sample_shape .
"def _call_reshape_input_output ( self , fn , x , extra_kwargs = None ) : with tf . control_dependencies ( self . _runtime_assertions + self . _validate_sample_arg ( x ) ) : sample_shape , static_sample_shape = self . _sample_shape ( x ) old_shape = tf . concat ( [ sample_shape , self . distribution . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) x_reshape = tf . reshape ( x , old_shape ) result = fn ( x_reshape , * * extra_kwargs ) if extra_kwargs else fn ( x_reshape ) new_shape = tf . concat ( [ sample_shape , self . _batch_shape_unexpanded , ] , axis = 0 ) result = tf . reshape ( result , new_shape ) if ( tensorshape_util . rank ( static_sample_shape ) is not None and tensorshape_util . rank ( self . batch_shape ) is not None ) : new_shape = tensorshape_util . concatenate ( static_sample_shape , self . batch_shape ) tensorshape_util . set_shape ( result , new_shape ) return result",Calls fn appropriately reshaping its input x and output .
"def _call_and_reshape_output ( self , fn , event_shape_list = None , static_event_shape_list = None , extra_kwargs = None ) : with tf . control_dependencies ( self . _runtime_assertions ) : if event_shape_list is None : event_shape_list = [ self . _event_shape_tensor ( ) ] if static_event_shape_list is None : static_event_shape_list = [ self . event_shape ] new_shape = tf . concat ( [ self . _batch_shape_unexpanded ] + event_shape_list , axis = 0 ) result = tf . reshape ( fn ( * * extra_kwargs ) if extra_kwargs else fn ( ) , new_shape ) if ( tensorshape_util . rank ( self . batch_shape ) is not None and tensorshape_util . rank ( self . event_shape ) is not None ) : event_shape = tf . TensorShape ( [ ] ) for rss in static_event_shape_list : event_shape = tensorshape_util . concatenate ( event_shape , rss ) static_shape = tensorshape_util . concatenate ( self . batch_shape , event_shape ) tensorshape_util . set_shape ( result , static_shape ) return result",Calls fn and appropriately reshapes its output .
"def _validate_sample_arg ( self , x ) : with tf . name_scope ( ""validate_sample_arg"" ) : x_ndims = ( tf . rank ( x ) if tensorshape_util . rank ( x . shape ) is None else tensorshape_util . rank ( x . shape ) ) event_ndims = ( tf . size ( input = self . event_shape_tensor ( ) ) if tensorshape_util . rank ( self . event_shape ) is None else tensorshape_util . rank ( self . event_shape ) ) batch_ndims = ( tf . size ( input = self . _batch_shape_unexpanded ) if tensorshape_util . rank ( self . batch_shape ) is None else tensorshape_util . rank ( self . batch_shape ) ) expected_batch_event_ndims = batch_ndims + event_ndims if ( isinstance ( x_ndims , int ) and isinstance ( expected_batch_event_ndims , int ) ) : if x_ndims < expected_batch_event_ndims : raise NotImplementedError ( ""Broadcasting is not supported; too few batch and event dims "" ""(expected at least {}, saw {})."" . format ( expected_batch_event_ndims , x_ndims ) ) ndims_assertion = [ ] elif self . validate_args : ndims_assertion = [ assert_util . assert_greater_equal ( x_ndims , expected_batch_event_ndims , message = ( ""Broadcasting is not supported; too few "" ""batch and event dims."" ) , name = ""assert_batch_and_event_ndims_large_enough"" ) , ] if ( tensorshape_util . is_fully_defined ( self . batch_shape ) and tensorshape_util . is_fully_defined ( self . event_shape ) ) : expected_batch_event_shape = np . int32 ( tensorshape_util . concatenate ( self . batch_shape , self . event_shape ) ) else : expected_batch_event_shape = tf . concat ( [ self . batch_shape_tensor ( ) , self . event_shape_tensor ( ) , ] , axis = 0 ) sample_ndims = x_ndims - expected_batch_event_ndims if isinstance ( sample_ndims , int ) : sample_ndims = max ( sample_ndims , 0 ) if ( isinstance ( sample_ndims , int ) and tensorshape_util . is_fully_defined ( x . shape [ sample_ndims : ] ) ) : actual_batch_event_shape = np . int32 ( x . shape [ sample_ndims : ] ) else : sample_ndims = tf . maximum ( sample_ndims , 0 ) actual_batch_event_shape = tf . shape ( input = x ) [ sample_ndims : ] if ( isinstance ( expected_batch_event_shape , np . ndarray ) and isinstance ( actual_batch_event_shape , np . ndarray ) ) : if any ( expected_batch_event_shape != actual_batch_event_shape ) : raise NotImplementedError ( ""Broadcasting is not supported; "" ""unexpected batch and event shape "" ""(expected {}, saw {})."" . format ( expected_batch_event_shape , actual_batch_event_shape ) ) runtime_assertions = ndims_assertion elif self . validate_args : with tf . control_dependencies ( ndims_assertion ) : shape_assertion = assert_util . assert_equal ( expected_batch_event_shape , actual_batch_event_shape , message = ( ""Broadcasting is not supported; "" ""unexpected batch and event shape."" ) , name = ""assert_batch_and_event_shape_same"" ) runtime_assertions = [ shape_assertion ] else : runtime_assertions = [ ] return runtime_assertions",Helper which validates sample arg e . g . input to log_prob .
"def _bdtr ( k , n , p ) : ones = tf . ones_like ( n - k ) k_eq_n = tf . equal ( k , n ) safe_dn = tf . where ( k_eq_n , ones , n - k ) dk = tf . math . betainc ( a = safe_dn , b = k + 1 , x = 1 - p ) return tf . where ( k_eq_n , ones , dk )",The binomial cumulative distribution function .
"def _maybe_assert_valid_sample ( self , counts ) : if not self . validate_args : return counts counts = distribution_util . embed_check_nonnegative_integer_form ( counts ) return distribution_util . with_dependencies ( [ assert_util . assert_less_equal ( counts , self . total_count , message = ""counts are not less than or equal to n."" ) , ] , counts )",Check counts for proper shape values then return tensor version .
"def _flat_sample_distributions ( self , sample_shape = ( ) , seed = None , value = None ) : ds = [ ] values_out = [ ] seed = seed_stream . SeedStream ( 'JointDistributionCoroutine' , seed ) gen = self . _model ( ) index = 0 d = next ( gen ) try : while True : actual_distribution = d . distribution if isinstance ( d , self . Root ) else d ds . append ( actual_distribution ) if ( value is not None and len ( value ) > index and value [ index ] is not None ) : seed ( ) next_value = value [ index ] else : next_value = actual_distribution . sample ( sample_shape = sample_shape if isinstance ( d , self . Root ) else ( ) , seed = seed ( ) ) values_out . append ( next_value ) index += 1 d = gen . send ( next_value ) except StopIteration : pass return ds , values_out",Executes model creating both samples and distributions .
"def _kl_pareto_pareto ( a , b , name = None ) : with tf . name_scope ( name or ""kl_pareto_pareto"" ) : final_batch_shape = distribution_util . get_broadcast_shape ( a . concentration , b . concentration , a . scale , b . scale ) common_type = dtype_util . common_dtype ( [ a . concentration , b . concentration , a . scale , b . scale ] , tf . float32 ) return tf . where ( a . scale >= b . scale , b . concentration * ( tf . math . log ( a . scale ) - tf . math . log ( b . scale ) ) + tf . math . log ( a . concentration ) - tf . math . log ( b . concentration ) + b . concentration / a . concentration - 1.0 , tf . broadcast_to ( tf . cast ( np . inf , common_type ) , final_batch_shape ) )",Calculate the batched KL divergence KL ( a || b ) with a and b Pareto .
"def _extend_support ( self , x , f , alt ) : scale = self . scale + tf . zeros_like ( self . concentration ) is_invalid = x < scale scale = scale + tf . zeros_like ( x ) x = x + tf . zeros_like ( scale ) y = f ( tf . where ( is_invalid , scale , x ) ) if alt == 0. : alt = tf . zeros_like ( y ) elif alt == 1. : alt = tf . ones_like ( y ) else : alt = tf . fill ( dims = tf . shape ( input = y ) , value = dtype_util . as_numpy_dtype ( self . dtype ) ( alt ) ) return tf . where ( is_invalid , alt , y )",Returns f ( x ) if x is in the support and alt otherwise .
"def latent_dirichlet_allocation ( concentration , topics_words ) : topics = ed . Dirichlet ( concentration = concentration , name = ""topics"" ) word_probs = tf . matmul ( topics , topics_words ) bag_of_words = ed . OneHotCategorical ( probs = word_probs , name = ""bag_of_words"" ) return bag_of_words",Latent Dirichlet Allocation in terms of its generative process .
"def make_lda_variational ( activation , num_topics , layer_sizes ) : encoder_net = tf . keras . Sequential ( ) for num_hidden_units in layer_sizes : encoder_net . add ( tf . keras . layers . Dense ( num_hidden_units , activation = activation , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) encoder_net . add ( tf . keras . layers . Dense ( num_topics , activation = tf . nn . softplus , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) def lda_variational ( bag_of_words ) : concentration = _clip_dirichlet_parameters ( encoder_net ( bag_of_words ) ) return ed . Dirichlet ( concentration = concentration , name = ""topics_posterior"" ) return lda_variational",Creates the variational distribution for LDA .
"def model_fn ( features , labels , mode , params , config ) : del labels , config logit_concentration = tf . compat . v1 . get_variable ( ""logit_concentration"" , shape = [ 1 , params [ ""num_topics"" ] ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( params [ ""prior_initial_value"" ] ) ) ) concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) num_words = features . shape [ 1 ] topics_words_logits = tf . compat . v1 . get_variable ( ""topics_words_logits"" , shape = [ params [ ""num_topics"" ] , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) topics_words = tf . nn . softmax ( topics_words_logits , axis = - 1 ) lda_variational = make_lda_variational ( params [ ""activation"" ] , params [ ""num_topics"" ] , params [ ""layer_sizes"" ] ) with ed . tape ( ) as variational_tape : _ = lda_variational ( features ) with ed . tape ( ) as model_tape : with ed . interception ( make_value_setter ( topics = variational_tape [ ""topics_posterior"" ] ) ) : posterior_predictive = latent_dirichlet_allocation ( concentration , topics_words ) log_likelihood = posterior_predictive . distribution . log_prob ( features ) tf . compat . v1 . summary . scalar ( ""log_likelihood"" , tf . reduce_mean ( input_tensor = log_likelihood ) ) kl = variational_tape [ ""topics_posterior"" ] . distribution . kl_divergence ( model_tape [ ""topics"" ] . distribution ) tf . compat . v1 . summary . scalar ( ""kl"" , tf . reduce_mean ( input_tensor = kl ) ) with tf . control_dependencies ( [ tf . compat . v1 . assert_greater ( kl , - 1e-3 , message = ""kl"" ) ] ) : kl = tf . identity ( kl ) elbo = log_likelihood - kl avg_elbo = tf . reduce_mean ( input_tensor = elbo ) tf . compat . v1 . summary . scalar ( ""elbo"" , avg_elbo ) loss = - avg_elbo global_step = tf . compat . v1 . train . get_or_create_global_step ( ) optimizer = tf . compat . v1 . train . AdamOptimizer ( params [ ""learning_rate"" ] ) grads_and_vars = optimizer . compute_gradients ( loss ) grads_and_vars_except_prior = [ x for x in grads_and_vars if x [ 1 ] != logit_concentration ] def train_op_except_prior ( ) : return optimizer . apply_gradients ( grads_and_vars_except_prior , global_step = global_step ) def train_op_all ( ) : return optimizer . apply_gradients ( grads_and_vars , global_step = global_step ) train_op = tf . cond ( pred = global_step < params [ ""prior_burn_in_steps"" ] , true_fn = train_op_except_prior , false_fn = train_op_all ) words_per_document = tf . reduce_sum ( input_tensor = features , axis = 1 ) log_perplexity = - elbo / words_per_document tf . compat . v1 . summary . scalar ( ""perplexity"" , tf . exp ( tf . reduce_mean ( input_tensor = log_perplexity ) ) ) ( log_perplexity_tensor , log_perplexity_update ) = tf . compat . v1 . metrics . mean ( log_perplexity ) perplexity_tensor = tf . exp ( log_perplexity_tensor ) topics = tf . compat . v1 . py_func ( functools . partial ( get_topics_strings , vocabulary = params [ ""vocabulary"" ] ) , [ topics_words , concentration ] , tf . string , stateful = False ) tf . compat . v1 . summary . text ( ""topics"" , topics ) return tf . estimator . EstimatorSpec ( mode = mode , loss = loss , train_op = train_op , eval_metric_ops = { ""elbo"" : tf . compat . v1 . metrics . mean ( elbo ) , ""log_likelihood"" : tf . compat . v1 . metrics . mean ( log_likelihood ) , ""kl"" : tf . compat . v1 . metrics . mean ( kl ) , ""perplexity"" : ( perplexity_tensor , log_perplexity_update ) , ""topics"" : ( topics , tf . no_op ( ) ) , } , )",Builds the model function for use in an Estimator .
"def get_topics_strings ( topics_words , alpha , vocabulary , topics_to_print = 10 , words_per_topic = 10 ) : alpha = np . squeeze ( alpha , axis = 0 ) highest_weight_topics = np . argsort ( - alpha , kind = ""mergesort"" ) top_words = np . argsort ( - topics_words , axis = 1 ) res = [ ] for topic_idx in highest_weight_topics [ : topics_to_print ] : l = [ ""index={} alpha={:.2f}"" . format ( topic_idx , alpha [ topic_idx ] ) ] l += [ vocabulary [ word ] for word in top_words [ topic_idx , : words_per_topic ] ] res . append ( "" "" . join ( l ) ) return np . array ( res )",Returns the summary of the learned topics .
"def newsgroups_dataset ( directory , split_name , num_words , shuffle_and_repeat ) : data = np . load ( download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) ) data = data [ : - 1 ] num_documents = data . shape [ 0 ] indices = np . array ( [ ( row_idx , column_idx ) for row_idx , row in enumerate ( data ) for column_idx in row ] ) sparse_matrix = scipy . sparse . coo_matrix ( ( np . ones ( indices . shape [ 0 ] ) , ( indices [ : , 0 ] , indices [ : , 1 ] ) ) , shape = ( num_documents , num_words ) , dtype = np . float32 ) sparse_matrix = sparse_matrix . tocsr ( ) dataset = tf . data . Dataset . range ( num_documents ) if shuffle_and_repeat : dataset = dataset . shuffle ( num_documents ) . repeat ( ) def get_row_py_func ( idx ) : def get_row_python ( idx_py ) : return np . squeeze ( np . array ( sparse_matrix [ idx_py ] . todense ( ) ) , axis = 0 ) py_func = tf . compat . v1 . py_func ( get_row_python , [ idx ] , tf . float32 , stateful = False ) py_func . set_shape ( ( num_words , ) ) return py_func dataset = dataset . map ( get_row_py_func ) return dataset",20 newsgroups as a tf . data . Dataset .
"def build_fake_input_fns ( batch_size ) : num_words = 1000 vocabulary = [ str ( i ) for i in range ( num_words ) ] random_sample = np . random . randint ( 10 , size = ( batch_size , num_words ) ) . astype ( np . float32 ) def train_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) . repeat ( ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = tf . data . Dataset . from_tensor_slices ( random_sample ) dataset = dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn , vocabulary",Builds fake data for unit testing .
"def build_input_fns ( data_dir , batch_size ) : with open ( download ( data_dir , ""vocab.pkl"" ) , ""r"" ) as f : words_to_idx = pickle . load ( f ) num_words = len ( words_to_idx ) vocabulary = [ None ] * num_words for word , idx in words_to_idx . items ( ) : vocabulary [ idx ] = word def train_input_fn ( ) : dataset = newsgroups_dataset ( data_dir , ""train"" , num_words , shuffle_and_repeat = True ) dataset = dataset . batch ( batch_size ) . prefetch ( 32 ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) def eval_input_fn ( ) : dataset = newsgroups_dataset ( data_dir , ""test"" , num_words , shuffle_and_repeat = False ) dataset = dataset . batch ( batch_size ) return tf . compat . v1 . data . make_one_shot_iterator ( dataset ) . get_next ( ) return train_input_fn , eval_input_fn , vocabulary",Builds iterators for train and evaluation data .
"def _kl_chi_chi ( a , b , name = None ) : with tf . name_scope ( name or ""kl_chi_chi"" ) : return ( 0.5 * tf . math . digamma ( 0.5 * a . df ) * ( a . df - b . df ) + tf . math . lgamma ( 0.5 * b . df ) - tf . math . lgamma ( 0.5 * a . df ) )",Calculate the batched KL divergence KL ( a || b ) with a and b Chi .
"def _sparse_or_dense_matmul_onehot ( sparse_or_dense_matrix , col_index ) : if isinstance ( sparse_or_dense_matrix , ( tf . SparseTensor , tf . compat . v1 . SparseTensorValue ) ) : num_rows = _get_shape ( sparse_or_dense_matrix ) [ - 2 ] batch_shape = _get_shape ( sparse_or_dense_matrix ) [ : - 2 ] slice_start = tf . concat ( [ tf . zeros_like ( batch_shape ) , [ 0 , col_index ] ] , axis = 0 ) slice_size = tf . concat ( [ batch_shape , [ num_rows , 1 ] ] , axis = 0 ) sparse_slice = tf . sparse . slice ( sparse_or_dense_matrix , tf . cast ( slice_start , tf . int64 ) , tf . cast ( slice_size , tf . int64 ) ) output_shape = tf . concat ( [ batch_shape , [ num_rows ] ] , axis = 0 ) return tf . reshape ( tf . sparse . to_dense ( sparse_slice ) , output_shape ) else : return tf . gather ( sparse_or_dense_matrix , col_index , axis = - 1 )",Returns a ( dense ) column of a Tensor or SparseTensor .
"def minimize_one_step ( gradient_unregularized_loss , hessian_unregularized_loss_outer , hessian_unregularized_loss_middle , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_full_sweeps = 1 , learning_rate = None , name = None ) : graph_deps = [ gradient_unregularized_loss , hessian_unregularized_loss_outer , hessian_unregularized_loss_middle , x_start , l1_regularizer , l2_regularizer , maximum_full_sweeps , tolerance , learning_rate , ] with tf . compat . v1 . name_scope ( name , 'minimize_one_step' , graph_deps ) : x_shape = _get_shape ( x_start ) batch_shape = x_shape [ : - 1 ] dims = x_shape [ - 1 ] def _hessian_diag_elt_with_l2 ( coord ) : inner_square = tf . reduce_sum ( input_tensor = _sparse_or_dense_matmul_onehot ( hessian_unregularized_loss_outer , coord ) ** 2 , axis = - 1 ) unregularized_component = ( hessian_unregularized_loss_middle [ ... , coord ] * inner_square ) l2_component = _mul_or_none ( 2. , l2_regularizer ) return _add_ignoring_nones ( unregularized_component , l2_component ) grad_loss_with_l2 = _add_ignoring_nones ( gradient_unregularized_loss , _mul_or_none ( 2. , l2_regularizer , x_start ) ) x_update_diff_norm_sq_convergence_threshold = ( tolerance * ( 1. + tf . norm ( tensor = x_start , ord = 2 , axis = - 1 ) ) ** 2 ) update_shape = tf . concat ( [ [ dims ] , batch_shape ] , axis = - 1 ) def _loop_cond ( iter_ , x_update_diff_norm_sq , x_update , hess_matmul_x_update ) : del x_update del hess_matmul_x_update sweep_complete = ( iter_ > 0 ) & tf . equal ( iter_ % dims , 0 ) small_delta = ( x_update_diff_norm_sq < x_update_diff_norm_sq_convergence_threshold ) converged = sweep_complete & small_delta allowed_more_iterations = iter_ < maximum_full_sweeps * dims return allowed_more_iterations & tf . reduce_any ( input_tensor = ~ converged ) def _loop_body ( iter_ , x_update_diff_norm_sq , x_update , hess_matmul_x_update ) : coord = iter_ % dims x_update_diff_norm_sq = tf . where ( tf . equal ( coord , 0 ) , tf . zeros_like ( x_update_diff_norm_sq ) , x_update_diff_norm_sq ) w_old = x_start [ ... , coord ] + x_update [ coord , ... ] second_deriv = _hessian_diag_elt_with_l2 ( coord ) newton_step = - _mul_ignoring_nones ( learning_rate , grad_loss_with_l2 [ ... , coord ] + hess_matmul_x_update [ coord , ... ] ) / second_deriv delta = ( soft_threshold ( w_old + newton_step , _mul_ignoring_nones ( learning_rate , l1_regularizer ) / second_deriv ) - w_old ) def _do_update ( x_update_diff_norm_sq , x_update , hess_matmul_x_update ) : hessian_column_with_l2 = sparse_or_dense_matvecmul ( hessian_unregularized_loss_outer , hessian_unregularized_loss_middle * _sparse_or_dense_matmul_onehot ( hessian_unregularized_loss_outer , coord ) , adjoint_a = True ) if l2_regularizer is not None : hessian_column_with_l2 += _one_hot_like ( hessian_column_with_l2 , coord , on_value = 2. * l2_regularizer ) n = tf . rank ( hessian_column_with_l2 ) perm = tf . roll ( tf . range ( n ) , shift = 1 , axis = 0 ) hessian_column_with_l2 = tf . transpose ( a = hessian_column_with_l2 , perm = perm ) x_update = tf . tensor_scatter_nd_add ( x_update , [ [ coord ] ] , [ delta ] ) with tf . control_dependencies ( [ x_update ] ) : x_update_diff_norm_sq_ = x_update_diff_norm_sq + delta ** 2 hess_matmul_x_update_ = ( hess_matmul_x_update + delta * hessian_column_with_l2 ) x_update_diff_norm_sq_ . set_shape ( x_update_diff_norm_sq_ . shape . merge_with ( x_update_diff_norm_sq . shape ) ) hess_matmul_x_update_ . set_shape ( hess_matmul_x_update_ . shape . merge_with ( hess_matmul_x_update . shape ) ) return [ x_update_diff_norm_sq_ , x_update , hess_matmul_x_update_ ] inputs_to_update = [ x_update_diff_norm_sq , x_update , hess_matmul_x_update ] return [ iter_ + 1 ] + prefer_static . cond ( tf . reduce_all ( input_tensor = tf . equal ( delta , 0. ) ) , lambda : inputs_to_update , lambda : _do_update ( * inputs_to_update ) ) base_dtype = x_start . dtype . base_dtype iter_ , x_update_diff_norm_sq , x_update , _ = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ tf . zeros ( [ ] , dtype = np . int32 , name = 'iter' ) , tf . zeros ( batch_shape , dtype = base_dtype , name = 'x_update_diff_norm_sq' ) , tf . zeros ( update_shape , dtype = base_dtype , name = 'x_update' ) , tf . zeros ( update_shape , dtype = base_dtype , name = 'hess_matmul_x_update' ) , ] ) n = tf . rank ( x_update ) perm = tf . roll ( tf . range ( n ) , shift = - 1 , axis = 0 ) x_update = tf . transpose ( a = x_update , perm = perm ) converged = tf . reduce_all ( input_tensor = x_update_diff_norm_sq < x_update_diff_norm_sq_convergence_threshold ) return x_start + x_update , converged , iter_ / dims",One step of ( the outer loop of ) the minimization algorithm .
"def minimize ( grad_and_hessian_loss_fn , x_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = 1 , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : graph_deps = [ x_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] , with tf . compat . v1 . name_scope ( name , 'minimize' , graph_deps ) : def _loop_cond ( x_start , converged , iter_ ) : del x_start return tf . logical_and ( iter_ < maximum_iterations , tf . logical_not ( converged ) ) def _loop_body ( x_start , converged , iter_ ) : g , h_outer , h_middle = grad_and_hessian_loss_fn ( x_start ) x_start , converged , _ = minimize_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = h_outer , hessian_unregularized_loss_middle = h_middle , x_start = x_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps_per_iteration , tolerance = tolerance , learning_rate = learning_rate ) return x_start , converged , iter_ + 1 return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ x_start , tf . zeros ( [ ] , np . bool , name = 'converged' ) , tf . zeros ( [ ] , np . int32 , name = 'iter' ) , ] )",Minimize using Hessian - informed proximal gradient descent .
"def make_encoder ( base_depth , activation , latent_size , code_size ) : conv = functools . partial ( tf . keras . layers . Conv2D , padding = ""SAME"" , activation = activation ) encoder_net = tf . keras . Sequential ( [ conv ( base_depth , 5 , 1 ) , conv ( base_depth , 5 , 2 ) , conv ( 2 * base_depth , 5 , 1 ) , conv ( 2 * base_depth , 5 , 2 ) , conv ( 4 * latent_size , 7 , padding = ""VALID"" ) , tf . keras . layers . Flatten ( ) , tf . keras . layers . Dense ( latent_size * code_size , activation = None ) , tf . keras . layers . Reshape ( [ latent_size , code_size ] ) ] ) def encoder ( images ) : """"""Encodes a batch of images.

    Args:
      images: A `Tensor` representing the inputs to be encoded, of shape `[...,
        channels]`.

    Returns:
      codes: A `float`-like `Tensor` of shape `[..., latent_size, code_size]`.
        It represents latent vectors to be matched with the codebook.
    """""" images = 2 * tf . cast ( images , dtype = tf . float32 ) - 1 codes = encoder_net ( images ) return codes return encoder",Creates the encoder function .
"def make_decoder ( base_depth , activation , input_size , output_shape ) : deconv = functools . partial ( tf . keras . layers . Conv2DTranspose , padding = ""SAME"" , activation = activation ) conv = functools . partial ( tf . keras . layers . Conv2D , padding = ""SAME"" , activation = activation ) decoder_net = tf . keras . Sequential ( [ tf . keras . layers . Reshape ( ( 1 , 1 , input_size ) ) , deconv ( 2 * base_depth , 7 , padding = ""VALID"" ) , deconv ( 2 * base_depth , 5 ) , deconv ( 2 * base_depth , 5 , 2 ) , deconv ( base_depth , 5 ) , deconv ( base_depth , 5 , 2 ) , deconv ( base_depth , 5 ) , conv ( output_shape [ - 1 ] , 5 , activation = None ) , tf . keras . layers . Reshape ( output_shape ) , ] ) def decoder ( codes ) : """"""Builds a distribution over images given codes.

    Args:
      codes: A `Tensor` representing the inputs to be decoded, of shape `[...,
        code_size]`.

    Returns:
      decoder_distribution: A multivariate `Bernoulli` distribution.
    """""" logits = decoder_net ( codes ) return tfd . Independent ( tfd . Bernoulli ( logits = logits ) , reinterpreted_batch_ndims = len ( output_shape ) , name = ""decoder_distribution"" ) return decoder",Creates the decoder function .
"def add_ema_control_dependencies ( vector_quantizer , one_hot_assignments , codes , commitment_loss , decay ) : updated_ema_count = moving_averages . assign_moving_average ( vector_quantizer . ema_count , tf . reduce_sum ( input_tensor = one_hot_assignments , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) updated_ema_means = moving_averages . assign_moving_average ( vector_quantizer . ema_means , tf . reduce_sum ( input_tensor = tf . expand_dims ( codes , 2 ) * tf . expand_dims ( one_hot_assignments , 3 ) , axis = [ 0 , 1 ] ) , decay , zero_debias = False ) perturbed_ema_count = updated_ema_count + 1e-5 with tf . control_dependencies ( [ commitment_loss ] ) : update_means = tf . compat . v1 . assign ( vector_quantizer . codebook , updated_ema_means / perturbed_ema_count [ ... , tf . newaxis ] ) with tf . control_dependencies ( [ update_means ] ) : return tf . identity ( commitment_loss )",Add control dependencies to the commmitment loss to update the codebook .
"def save_imgs ( x , fname ) : n = x . shape [ 0 ] fig = figure . Figure ( figsize = ( n , 1 ) , frameon = False ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i in range ( n ) : ax = fig . add_subplot ( 1 , n , i + 1 ) ax . imshow ( x [ i ] . squeeze ( ) , interpolation = ""none"" , cmap = cm . get_cmap ( ""binary"" ) ) ax . axis ( ""off"" ) canvas . print_figure ( fname , format = ""png"" ) print ( ""saved %s"" % fname )",Helper method to save a grid of images to a PNG file .
"def visualize_training ( images_val , reconstructed_images_val , random_images_val , log_dir , prefix , viz_n = 10 ) : save_imgs ( images_val [ : viz_n ] , os . path . join ( log_dir , ""{}_inputs.png"" . format ( prefix ) ) ) save_imgs ( reconstructed_images_val [ : viz_n ] , os . path . join ( log_dir , ""{}_reconstructions.png"" . format ( prefix ) ) ) if random_images_val is not None : save_imgs ( random_images_val [ : viz_n ] , os . path . join ( log_dir , ""{}_prior_samples.png"" . format ( prefix ) ) )",Helper method to save images visualizing model reconstructions .
"def load_bernoulli_mnist_dataset ( directory , split_name ) : amat_file = download ( directory , FILE_TEMPLATE . format ( split = split_name ) ) dataset = tf . data . TextLineDataset ( amat_file ) str_to_arr = lambda string : np . array ( [ c == b""1"" for c in string . split ( ) ] ) def _parser ( s ) : booltensor = tf . compat . v1 . py_func ( str_to_arr , [ s ] , tf . bool ) reshaped = tf . reshape ( booltensor , [ 28 , 28 , 1 ] ) return tf . cast ( reshaped , dtype = tf . float32 ) , tf . constant ( 0 , tf . int32 ) return dataset . map ( _parser )",Returns Hugo Larochelle s binary static MNIST tf . data . Dataset .
"def build_input_pipeline ( data_dir , batch_size , heldout_size , mnist_type ) : if mnist_type in [ MnistType . FAKE_DATA , MnistType . THRESHOLD ] : if mnist_type == MnistType . FAKE_DATA : mnist_data = build_fake_data ( ) else : mnist_data = mnist . read_data_sets ( data_dir ) training_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . train . images , np . int32 ( mnist_data . train . labels ) ) ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . validation . images , np . int32 ( mnist_data . validation . labels ) ) ) elif mnist_type == MnistType . BERNOULLI : training_dataset = load_bernoulli_mnist_dataset ( data_dir , ""train"" ) heldout_dataset = load_bernoulli_mnist_dataset ( data_dir , ""valid"" ) else : raise ValueError ( ""Unknown MNIST type."" ) training_batches = training_dataset . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_frozen = ( heldout_dataset . take ( heldout_size ) . repeat ( ) . batch ( heldout_size ) ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) images = tf . reshape ( images , shape = [ - 1 ] + IMAGE_SHAPE ) if mnist_type in [ MnistType . FAKE_DATA , MnistType . THRESHOLD ] : images = tf . cast ( images > 0.5 , dtype = tf . int32 ) return images , labels , handle , training_iterator , heldout_iterator",Builds an Iterator switching between train and heldout data .
"def as_numpy_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'as_numpy_dtype' ) : return dtype . as_numpy_dtype return dtype",Returns a np . dtype based on this dtype .
"def base_dtype ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'base_dtype' ) : return dtype . base_dtype return dtype",Returns a non - reference dtype based on this dtype .
"def is_bool ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_bool' ) : return dtype . is_bool return np . dtype ( dtype ) . kind == 'b'",Returns whether this is a boolean data type .
"def is_complex ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_complex' ) : return dtype . is_complex return np . issubdtype ( np . dtype ( dtype ) , np . complex )",Returns whether this is a complex floating point type .
"def is_floating ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_floating' ) : return dtype . is_floating return np . issubdtype ( np . dtype ( dtype ) , np . float )",Returns whether this is a ( non - quantized real ) floating point type .
"def is_integer ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'is_integer' ) : return dtype . is_integer return np . issubdtype ( np . dtype ( dtype ) , np . integer )",Returns whether this is a ( non - quantized ) integer type .
"def max ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'max' ) : return dtype . max use_finfo = is_floating ( dtype ) or is_complex ( dtype ) return np . finfo ( dtype ) . max if use_finfo else np . iinfo ( dtype ) . max",Returns the maximum representable value in this data type .
"def name ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'name' ) : return dtype . name if hasattr ( dtype , '__name__' ) : return dtype . __name__ return str ( dtype )",Returns the string name for this dtype .
"def size ( dtype ) : dtype = tf . as_dtype ( dtype ) if hasattr ( dtype , 'size' ) : return dtype . size return np . dtype ( dtype ) . itemsize",Returns the number of bytes to represent this dtype .
"def _assert_same_base_type ( items , expected_type = None ) : original_expected_type = expected_type mismatch = False for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type elif expected_type != item_type : mismatch = True break if mismatch : expected_type = original_expected_type original_item_str = None get_name = lambda x : x . name if hasattr ( x , 'name' ) else str ( x ) for item in items : if item is not None : item_type = base_dtype ( item . dtype ) if not expected_type : expected_type = item_type original_item_str = get_name ( item ) elif expected_type != item_type : raise ValueError ( '{}, type={}, must be of the same type ({}){}.' . format ( get_name ( item ) , item_type , expected_type , ( ( ' as {}' . format ( original_item_str ) ) if original_item_str else '' ) ) ) return expected_type else : return expected_type",r Asserts all items are of the same base type .
"def assert_same_float_dtype ( tensors = None , dtype = None ) : if tensors : dtype = _assert_same_base_type ( tensors , dtype ) if not dtype : dtype = tf . float32 elif not is_floating ( dtype ) : raise ValueError ( 'Expected floating point type, got {}.' . format ( dtype ) ) return dtype",Validate and return float type based on tensors and dtype .
"def _kl_categorical_categorical ( a , b , name = None ) : with tf . name_scope ( name or ""kl_categorical_categorical"" ) : return tf . reduce_sum ( input_tensor = tf . nn . softmax ( a . logits ) * ( tf . nn . log_softmax ( a . logits ) - tf . nn . log_softmax ( b . logits ) ) , axis = - 1 )",Calculate the batched KL divergence KL ( a || b ) with a b OneHotCategorical .
"def minimize ( objective_function , initial_simplex = None , initial_vertex = None , step_sizes = None , objective_at_initial_simplex = None , objective_at_initial_vertex = None , batch_evaluate_objective = False , func_tolerance = 1e-8 , position_tolerance = 1e-8 , parallel_iterations = 1 , max_iterations = None , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , func_tolerance , position_tolerance ] ) : ( dim , _ , simplex , objective_at_simplex , num_evaluations ) = _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) domain_dtype = simplex . dtype ( reflection , expansion , contraction , shrinkage ) = _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , domain_dtype ) closure_kwargs = dict ( objective_function = objective_function , dim = dim , func_tolerance = func_tolerance , position_tolerance = position_tolerance , batch_evaluate_objective = batch_evaluate_objective , reflection = reflection , expansion = expansion , contraction = contraction , shrinkage = shrinkage ) def _loop_body ( _ , iterations , simplex , objective_at_simplex , num_evaluations ) : ( converged , next_simplex , next_objective , evaluations ) = nelder_mead_one_step ( simplex , objective_at_simplex , * * closure_kwargs ) return ( converged , iterations + 1 , next_simplex , next_objective , num_evaluations + evaluations ) initial_args = ( False , 0 , simplex , objective_at_simplex , num_evaluations ) def _is_converged ( converged , num_iterations , * ignored_args ) : not_converged = tf . logical_not ( converged ) return ( not_converged if max_iterations is None else ( not_converged & ( num_iterations < max_iterations ) ) ) ( converged , num_iterations , final_simplex , final_objective_values , final_evaluations ) = tf . while_loop ( cond = _is_converged , body = _loop_body , loop_vars = initial_args , parallel_iterations = parallel_iterations ) order = tf . argsort ( final_objective_values , direction = 'ASCENDING' , stable = True ) best_index = order [ 0 ] return NelderMeadOptimizerResults ( converged = tf . convert_to_tensor ( value = converged ) , num_objective_evaluations = final_evaluations , position = final_simplex [ best_index ] , objective_value = final_objective_values [ best_index ] , final_simplex = final_simplex , final_objective_values = final_objective_values , num_iterations = tf . convert_to_tensor ( value = num_iterations ) , initial_simplex = simplex , initial_objective_values = objective_at_simplex )",Minimum of the objective function using the Nelder Mead simplex algorithm .
"def nelder_mead_one_step ( current_simplex , current_objective_values , objective_function = None , dim = None , func_tolerance = None , position_tolerance = None , batch_evaluate_objective = False , reflection = None , expansion = None , contraction = None , shrinkage = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'nelder_mead_one_step' ) : domain_dtype = current_simplex . dtype . base_dtype order = tf . argsort ( current_objective_values , direction = 'ASCENDING' , stable = True ) ( best_index , worst_index , second_worst_index ) = order [ 0 ] , order [ - 1 ] , order [ - 2 ] worst_vertex = current_simplex [ worst_index ] ( best_objective_value , worst_objective_value , second_worst_objective_value ) = ( current_objective_values [ best_index ] , current_objective_values [ worst_index ] , current_objective_values [ second_worst_index ] ) face_centroid = tf . reduce_sum ( input_tensor = current_simplex , axis = 0 ) - worst_vertex face_centroid /= tf . cast ( dim , domain_dtype ) reflected = face_centroid + reflection * ( face_centroid - worst_vertex ) objective_at_reflected = objective_function ( reflected ) num_evaluations = 1 has_converged = _check_convergence ( current_simplex , current_simplex [ best_index ] , best_objective_value , worst_objective_value , func_tolerance , position_tolerance ) def _converged_fn ( ) : return ( True , current_simplex , current_objective_values , 0 ) case0 = has_converged , _converged_fn accept_reflected = ( ( objective_at_reflected < second_worst_objective_value ) & ( objective_at_reflected >= best_objective_value ) ) accept_reflected_fn = _accept_reflected_fn ( current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected ) case1 = accept_reflected , accept_reflected_fn do_expansion = objective_at_reflected < best_objective_value expansion_fn = _expansion_fn ( objective_function , current_simplex , current_objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) case2 = do_expansion , expansion_fn do_outside_contraction = ( ( objective_at_reflected < worst_objective_value ) & ( objective_at_reflected >= second_worst_objective_value ) ) outside_contraction_fn = _outside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) case3 = do_outside_contraction , outside_contraction_fn default_fn = _inside_contraction_fn ( objective_function , current_simplex , current_objective_values , face_centroid , best_index , worst_index , worst_objective_value , contraction , shrinkage , batch_evaluate_objective ) ( converged , next_simplex , next_objective_at_simplex , case_evals ) = prefer_static . case ( [ case0 , case1 , case2 , case3 ] , default = default_fn , exclusive = False ) next_simplex . set_shape ( current_simplex . shape ) next_objective_at_simplex . set_shape ( current_objective_values . shape ) return ( converged , next_simplex , next_objective_at_simplex , num_evaluations + case_evals )",A single iteration of the Nelder Mead algorithm .
"def _accept_reflected_fn ( simplex , objective_values , worst_index , reflected , objective_at_reflected ) : def _replace_worst_with_reflected ( ) : next_simplex = _replace_at_index ( simplex , worst_index , reflected ) next_objective_values = _replace_at_index ( objective_values , worst_index , objective_at_reflected ) return False , next_simplex , next_objective_values , 0 return _replace_worst_with_reflected",Creates the condition function pair for a reflection to be accepted .
"def _expansion_fn ( objective_function , simplex , objective_values , worst_index , reflected , objective_at_reflected , face_centroid , expansion ) : def _expand_and_maybe_replace ( ) : """"""Performs the expansion step."""""" expanded = face_centroid + expansion * ( reflected - face_centroid ) expanded_objective_value = objective_function ( expanded ) expanded_is_better = ( expanded_objective_value < objective_at_reflected ) accept_expanded_fn = lambda : ( expanded , expanded_objective_value ) accept_reflected_fn = lambda : ( reflected , objective_at_reflected ) next_pt , next_objective_value = prefer_static . cond ( expanded_is_better , accept_expanded_fn , accept_reflected_fn ) next_simplex = _replace_at_index ( simplex , worst_index , next_pt ) next_objective_at_simplex = _replace_at_index ( objective_values , worst_index , next_objective_value ) return False , next_simplex , next_objective_at_simplex , 1 return _expand_and_maybe_replace",Creates the condition function pair for an expansion .
"def _outside_contraction_fn ( objective_function , simplex , objective_values , face_centroid , best_index , worst_index , reflected , objective_at_reflected , contraction , shrinkage , batch_evaluate_objective ) : def _contraction ( ) : """"""Performs a contraction."""""" contracted = face_centroid + contraction * ( reflected - face_centroid ) objective_at_contracted = objective_function ( contracted ) is_contracted_acceptable = objective_at_contracted <= objective_at_reflected def _accept_contraction ( ) : next_simplex = _replace_at_index ( simplex , worst_index , contracted ) objective_at_next_simplex = _replace_at_index ( objective_values , worst_index , objective_at_contracted ) return ( False , next_simplex , objective_at_next_simplex , 1 ) def _reject_contraction ( ) : return _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) return prefer_static . cond ( is_contracted_acceptable , _accept_contraction , _reject_contraction ) return _contraction",Creates the condition function pair for an outside contraction .
"def _shrink_towards_best ( objective_function , simplex , best_index , shrinkage , batch_evaluate_objective ) : best_vertex = simplex [ best_index ] shrunk_simplex = best_vertex + shrinkage * ( simplex - best_vertex ) objective_at_shrunk_simplex , evals = _evaluate_objective_multiple ( objective_function , shrunk_simplex , batch_evaluate_objective ) return ( False , shrunk_simplex , objective_at_shrunk_simplex , evals )",Shrinks the simplex around the best vertex .
"def _replace_at_index ( x , index , replacement ) : x_new = tf . concat ( [ x [ : index ] , tf . expand_dims ( replacement , axis = 0 ) , x [ ( index + 1 ) : ] ] , axis = 0 ) return x_new",Replaces an element at supplied index .
"def _check_convergence ( simplex , best_vertex , best_objective , worst_objective , func_tolerance , position_tolerance ) : objective_convergence = tf . abs ( worst_objective - best_objective ) < func_tolerance simplex_degeneracy = tf . reduce_max ( input_tensor = tf . abs ( simplex - best_vertex ) ) < position_tolerance return objective_convergence | simplex_degeneracy",Returns True if the simplex has converged .
"def _prepare_args ( objective_function , initial_simplex , initial_vertex , step_sizes , objective_at_initial_simplex , objective_at_initial_vertex , batch_evaluate_objective ) : if objective_at_initial_simplex is not None and initial_simplex is None : raise ValueError ( '`objective_at_initial_simplex` specified but the' '`initial_simplex` was not.' ) if objective_at_initial_vertex is not None and initial_vertex is None : raise ValueError ( '`objective_at_initial_vertex` specified but the' '`initial_vertex` was not.' ) if initial_simplex is not None : if initial_vertex is not None : raise ValueError ( 'Both `initial_simplex` and `initial_vertex` specified.' ' Only one of the two should be specified.' ) if step_sizes is not None : raise ValueError ( '`step_sizes` must not be specified when an' ' `initial_simplex` has been specified.' ) return _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) if initial_vertex is None : raise ValueError ( 'One of `initial_simplex` or `initial_vertex`' ' must be supplied' ) if step_sizes is None : step_sizes = _default_step_sizes ( initial_vertex ) return _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective )",Computes the initial simplex and the objective values at the simplex .
"def _default_step_sizes ( reference_vertex ) : small_sizes = tf . ones_like ( reference_vertex ) * 0.00025 large_sizes = reference_vertex * 0.05 return tf . where ( tf . abs ( reference_vertex ) < _EPSILON , small_sizes , large_sizes )",Chooses default step sizes according to [ Gao and Han ( 2010 ) ] [ 3 ] .
"def _prepare_args_with_initial_simplex ( objective_function , initial_simplex , objective_at_initial_simplex , batch_evaluate_objective ) : initial_simplex = tf . convert_to_tensor ( value = initial_simplex ) num_vertices = tf . shape ( input = initial_simplex ) [ 0 ] dim = num_vertices - 1 num_evaluations = 0 if objective_at_initial_simplex is None : objective_at_initial_simplex , n_evals = _evaluate_objective_multiple ( objective_function , initial_simplex , batch_evaluate_objective ) num_evaluations += n_evals objective_at_initial_simplex = tf . convert_to_tensor ( value = objective_at_initial_simplex ) return ( dim , num_vertices , initial_simplex , objective_at_initial_simplex , num_evaluations )",Evaluates the objective function at the specified initial simplex .
"def _prepare_args_with_initial_vertex ( objective_function , initial_vertex , step_sizes , objective_at_initial_vertex , batch_evaluate_objective ) : dim = tf . size ( input = initial_vertex ) num_vertices = dim + 1 unit_vectors_along_axes = tf . reshape ( tf . eye ( dim , dim , dtype = initial_vertex . dtype . base_dtype ) , tf . concat ( [ [ dim ] , tf . shape ( input = initial_vertex ) ] , axis = 0 ) ) simplex_face = initial_vertex + step_sizes * unit_vectors_along_axes simplex = tf . concat ( [ tf . expand_dims ( initial_vertex , axis = 0 ) , simplex_face ] , axis = 0 ) num_evaluations = 0 if objective_at_initial_vertex is None : objective_at_initial_vertex = objective_function ( initial_vertex ) num_evaluations += 1 objective_at_simplex_face , num_evals = _evaluate_objective_multiple ( objective_function , simplex_face , batch_evaluate_objective ) num_evaluations += num_evals objective_at_simplex = tf . concat ( [ tf . expand_dims ( objective_at_initial_vertex , axis = 0 ) , objective_at_simplex_face ] , axis = 0 ) return ( dim , num_vertices , simplex , objective_at_simplex , num_evaluations )",Constructs a standard axes aligned simplex .
"def _resolve_parameters ( dim , reflection , expansion , contraction , shrinkage , dtype ) : dim = tf . cast ( dim , dtype = dtype ) reflection = 1. if reflection is None else reflection expansion = ( 1. + 2. / dim ) if expansion is None else expansion contraction = ( 0.75 - 1. / ( 2 * dim ) ) if contraction is None else contraction shrinkage = ( 1. - 1. / dim ) if shrinkage is None else shrinkage return reflection , expansion , contraction , shrinkage",Applies the [ Gao and Han ] [ 3 ] presciption to the unspecified parameters .
"def _evaluate_objective_multiple ( objective_function , arg_batch , batch_evaluate_objective ) : n_points = tf . shape ( input = arg_batch ) [ 0 ] if batch_evaluate_objective : return objective_function ( arg_batch ) , n_points return tf . map_fn ( objective_function , arg_batch ) , n_points",Evaluates the objective function on a batch of points .
"def plot_weight_posteriors ( names , qm_vals , qs_vals , fname ) : fig = figure . Figure ( figsize = ( 6 , 3 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) ax = fig . add_subplot ( 1 , 2 , 1 ) for n , qm in zip ( names , qm_vals ) : sns . distplot ( qm . flatten ( ) , ax = ax , label = n ) ax . set_title ( ""weight means"" ) ax . set_xlim ( [ - 1.5 , 1.5 ] ) ax . legend ( ) ax = fig . add_subplot ( 1 , 2 , 2 ) for n , qs in zip ( names , qs_vals ) : sns . distplot ( qs . flatten ( ) , ax = ax ) ax . set_title ( ""weight stddevs"" ) ax . set_xlim ( [ 0 , 1. ] ) fig . tight_layout ( ) canvas . print_figure ( fname , format = ""png"" ) print ( ""saved {}"" . format ( fname ) )",Save a PNG plot with histograms of weight means and stddevs .
"def plot_heldout_prediction ( input_vals , probs , fname , n = 10 , title = """" ) : fig = figure . Figure ( figsize = ( 9 , 3 * n ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i in range ( n ) : ax = fig . add_subplot ( n , 3 , 3 * i + 1 ) ax . imshow ( input_vals [ i , : ] . reshape ( IMAGE_SHAPE [ : - 1 ] ) , interpolation = ""None"" ) ax = fig . add_subplot ( n , 3 , 3 * i + 2 ) for prob_sample in probs : sns . barplot ( np . arange ( 10 ) , prob_sample [ i , : ] , alpha = 0.1 , ax = ax ) ax . set_ylim ( [ 0 , 1 ] ) ax . set_title ( ""posterior samples"" ) ax = fig . add_subplot ( n , 3 , 3 * i + 3 ) sns . barplot ( np . arange ( 10 ) , np . mean ( probs [ : , i , : ] , axis = 0 ) , ax = ax ) ax . set_ylim ( [ 0 , 1 ] ) ax . set_title ( ""predictive probs"" ) fig . suptitle ( title ) fig . tight_layout ( ) canvas . print_figure ( fname , format = ""png"" ) print ( ""saved {}"" . format ( fname ) )",Save a PNG plot visualizing posterior uncertainty on heldout data .
"def build_input_pipeline ( mnist_data , batch_size , heldout_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . train . images , np . int32 ( mnist_data . train . labels ) ) ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( mnist_data . validation . images , np . int32 ( mnist_data . validation . labels ) ) ) heldout_frozen = ( heldout_dataset . take ( heldout_size ) . repeat ( ) . batch ( heldout_size ) ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_frozen ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) return images , labels , handle , training_iterator , heldout_iterator",Build an Iterator switching between train and heldout data .
"def build_fake_data ( num_examples = 10 ) : class Dummy ( object ) : pass num_examples = 10 mnist_data = Dummy ( ) mnist_data . train = Dummy ( ) mnist_data . train . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . train . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . train . num_examples = num_examples mnist_data . validation = Dummy ( ) mnist_data . validation . images = np . float32 ( np . random . randn ( num_examples , * IMAGE_SHAPE ) ) mnist_data . validation . labels = np . int32 ( np . random . permutation ( np . arange ( num_examples ) ) ) mnist_data . validation . num_examples = num_examples return mnist_data",Build fake MNIST - style data for unit testing .
"def _kl_bernoulli_bernoulli ( a , b , name = None ) : with tf . name_scope ( name or ""kl_bernoulli_bernoulli"" ) : delta_probs0 = tf . nn . softplus ( - b . logits ) - tf . nn . softplus ( - a . logits ) delta_probs1 = tf . nn . softplus ( b . logits ) - tf . nn . softplus ( a . logits ) return ( tf . sigmoid ( a . logits ) * delta_probs0 + tf . sigmoid ( - a . logits ) * delta_probs1 )",Calculate the batched KL divergence KL ( a || b ) with a and b Bernoulli .
"def get_config ( self ) : return { 'initializers' : [ tf . compat . v2 . initializers . serialize ( tf . keras . initializers . get ( init ) ) for init in self . initializers ] , 'sizes' : self . sizes , 'validate_args' : self . validate_args , }",Returns initializer configuration as a JSON - serializable dict .
"def from_config ( cls , config ) : return cls ( * * { 'initializers' : [ tf . compat . v2 . initializers . deserialize ( init ) for init in config . get ( 'initializers' , [ ] ) ] , 'sizes' : config . get ( 'sizes' , [ ] ) , 'validate_args' : config . get ( 'validate_args' , False ) , } )",Instantiates an initializer from a configuration dictionary .
"def _matmul ( a , b , transpose_a = False , transpose_b = False , adjoint_a = False , adjoint_b = False , a_is_sparse = False , b_is_sparse = False , name = None ) : if a_is_sparse or b_is_sparse : raise NotImplementedError ( 'Numpy backend does not support sparse matmul.' ) if transpose_a or adjoint_a : a = _matrix_transpose ( a , conjugate = adjoint_a ) if transpose_b or adjoint_b : b = _matrix_transpose ( b , conjugate = adjoint_b ) return np . matmul ( a , b )",Numpy matmul wrapper .
"def _std_var_helper ( self , statistic , statistic_name , statistic_ndims , df_factor_fn ) : df = tf . reshape ( self . df , tf . concat ( [ tf . shape ( input = self . df ) , tf . ones ( [ statistic_ndims ] , dtype = tf . int32 ) ] , - 1 ) ) df = _broadcast_to_shape ( df , tf . shape ( input = statistic ) ) denom = tf . where ( df > 2. , df - 2. , tf . ones_like ( df ) ) statistic = statistic * df_factor_fn ( df / denom ) inf = dtype_util . as_numpy_dtype ( self . dtype ) ( np . inf ) result_where_defined = tf . where ( df > 2. , statistic , tf . fill ( tf . shape ( input = statistic ) , inf , name = ""inf"" ) ) if self . allow_nan_stats : nan = dtype_util . as_numpy_dtype ( self . dtype ) ( np . nan ) return tf . where ( df > 1. , result_where_defined , tf . fill ( tf . shape ( input = statistic ) , nan , name = ""nan"" ) ) else : with tf . control_dependencies ( [ assert_util . assert_less ( tf . cast ( 1. , self . dtype ) , df , message = statistic_name + "" not defined for components of df <= 1"" ) , ] ) : return tf . identity ( result_where_defined )",Helper to compute stddev covariance and variance .
"def assign_moving_mean_variance ( mean_var , variance_var , value , decay , name = None ) : with tf . compat . v1 . name_scope ( name , ""assign_moving_mean_variance"" , [ variance_var , mean_var , value , decay ] ) : with tf . compat . v1 . colocate_with ( variance_var ) : with tf . compat . v1 . colocate_with ( mean_var ) : base_dtype = mean_var . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( ""mean_var.base_dtype({}) does not have float type "" ""`dtype`."" . format ( base_dtype . name ) ) if base_dtype != variance_var . dtype . base_dtype : raise TypeError ( ""mean_var.base_dtype({}) != variance_var.base_dtype({})"" . format ( base_dtype . name , variance_var . dtype . base_dtype . name ) ) value = tf . convert_to_tensor ( value = value , dtype = base_dtype , name = ""value"" ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = ""decay"" ) delta = value - mean_var with tf . control_dependencies ( [ delta ] ) : mean_var = mean_var . assign_add ( ( 1. - decay ) * delta ) variance_var = variance_var . assign_sub ( ( 1. - decay ) * ( variance_var - decay * tf . square ( delta ) ) ) return mean_var , variance_var",Compute exponentially weighted moving { mean variance } of a streaming value .
"def assign_log_moving_mean_exp ( log_mean_exp_var , log_value , decay , name = None ) : with tf . compat . v1 . name_scope ( name , ""assign_log_moving_mean_exp"" , [ log_mean_exp_var , log_value , decay ] ) : with tf . compat . v1 . colocate_with ( log_mean_exp_var ) : base_dtype = log_mean_exp_var . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( ""log_mean_exp_var.base_dtype({}) does not have float type "" ""`dtype`."" . format ( base_dtype . name ) ) log_value = tf . convert_to_tensor ( value = log_value , dtype = base_dtype , name = ""log_value"" ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = ""decay"" ) delta = ( log_value - log_mean_exp_var ) [ tf . newaxis , ... ] x = tf . concat ( [ tf . math . log ( decay ) * tf . ones_like ( delta ) , delta + tf . math . log1p ( - decay ) ] , axis = 0 ) x = tf . reduce_logsumexp ( input_tensor = x , axis = 0 ) return log_mean_exp_var . assign_add ( x )",Compute the log of the exponentially weighted moving mean of the exp .
"def moving_mean_variance ( value , decay , name = None ) : with tf . compat . v1 . variable_scope ( name , ""moving_mean_variance"" , [ value , decay ] ) : value = tf . convert_to_tensor ( value = value , name = ""value"" ) base_dtype = value . dtype . base_dtype if not base_dtype . is_floating : raise TypeError ( ""value.base_dtype({}) does not have float type `dtype`."" . format ( base_dtype . name ) ) decay = tf . convert_to_tensor ( value = decay , dtype = base_dtype , name = ""decay"" ) variance_var = tf . compat . v2 . Variable ( name = ""moving_variance"" , initial_value = tf . zeros ( shape = value . shape , dtype = value . dtype ) , trainable = False ) mean_var = tf . compat . v2 . Variable ( name = ""moving_mean"" , initial_value = tf . zeros ( shape = value . shape , dtype = value . dtype ) , trainable = False ) return assign_moving_mean_variance ( mean_var , variance_var , value , decay )",Compute exponentially weighted moving { mean variance } of a streaming value .
"def _make_columnar ( self , x ) : if tensorshape_util . rank ( x . shape ) is not None : if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , : ] return x shape = tf . shape ( input = x ) maybe_expanded_shape = tf . concat ( [ shape [ : - 1 ] , distribution_util . pick_vector ( tf . equal ( tf . rank ( x ) , 1 ) , [ 1 ] , np . array ( [ ] , dtype = np . int32 ) ) , shape [ - 1 : ] , ] , 0 ) return tf . reshape ( x , maybe_expanded_shape )",Ensures non - scalar input has at least one column .
"def _kl_laplace_laplace ( a , b , name = None ) : with tf . name_scope ( name or ""kl_laplace_laplace"" ) : distance = tf . abs ( a . loc - b . loc ) ratio = a . scale / b . scale return ( - tf . math . log ( ratio ) - 1 + distance / b . scale + ratio * tf . exp ( - distance / a . scale ) )",Calculate the batched KL divergence KL ( a || b ) with a and b Laplace .
"def random_rademacher ( shape , dtype = tf . float32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'random_rademacher' , [ shape , seed ] ) : generation_dtype = tf . int64 if tf . as_dtype ( dtype ) != tf . int32 else tf . int32 random_bernoulli = tf . random . uniform ( shape , minval = 0 , maxval = 2 , dtype = generation_dtype , seed = seed ) return tf . cast ( 2 * random_bernoulli - 1 , dtype )",Generates Tensor consisting of - 1 or + 1 chosen uniformly at random .
"def random_rayleigh ( shape , scale = None , dtype = tf . float32 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'random_rayleigh' , [ shape , scale , seed ] ) : if scale is not None : scale = tf . convert_to_tensor ( value = scale , dtype = dtype , name = 'scale' ) shape = tf . broadcast_dynamic_shape ( shape , tf . shape ( input = scale ) ) x = tf . sqrt ( - 2. * tf . math . log ( tf . random . uniform ( shape , minval = 0 , maxval = 1 , dtype = dtype , seed = seed ) ) ) if scale is None : return x return x * scale",Generates Tensor of positive reals drawn from a Rayleigh distributions .
"def _pick_scalar_condition ( pred , cond_true , cond_false ) : pred_ = tf . get_static_value ( tf . convert_to_tensor ( value = pred ) ) if pred_ is None : return tf . where ( pred , cond_true , cond_false ) return cond_true if pred_ else cond_false",Convenience function which chooses the condition based on the predicate .
"def _finish_log_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) log_prob = self . distribution . log_prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : log_prob = tf . reduce_sum ( input_tensor = log_prob , axis = self . _reduce_event_indices ) log_prob += tf . cast ( ildj , log_prob . dtype ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( log_prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return log_prob",Finish computation of log_prob on one element of the inverse image .
"def _finish_prob_for_one_fiber ( self , y , x , ildj , event_ndims , * * distribution_kwargs ) : x = self . _maybe_rotate_dims ( x , rotate_right = True ) prob = self . distribution . prob ( x , * * distribution_kwargs ) if self . _is_maybe_event_override : prob = tf . reduce_prod ( input_tensor = prob , axis = self . _reduce_event_indices ) prob *= tf . exp ( tf . cast ( ildj , prob . dtype ) ) if self . _is_maybe_event_override and isinstance ( event_ndims , int ) : tensorshape_util . set_shape ( prob , tf . broadcast_static_shape ( tensorshape_util . with_rank_at_least ( y . shape , 1 ) [ : - event_ndims ] , self . batch_shape ) ) return prob",Finish computation of prob on one element of the inverse image .
"def _maybe_validate_shape_override ( self , override_shape , base_is_scalar , validate_args , name ) : if override_shape is None : override_shape = [ ] override_shape = tf . convert_to_tensor ( value = override_shape , dtype = tf . int32 , name = name ) if not dtype_util . is_integer ( override_shape . dtype ) : raise TypeError ( ""shape override must be an integer"" ) override_is_scalar = _is_scalar_from_shape_tensor ( override_shape ) if tf . get_static_value ( override_is_scalar ) : return self . _empty dynamic_assertions = [ ] if tensorshape_util . rank ( override_shape . shape ) is not None : if tensorshape_util . rank ( override_shape . shape ) != 1 : raise ValueError ( ""shape override must be a vector"" ) elif validate_args : dynamic_assertions += [ assert_util . assert_rank ( override_shape , 1 , message = ""shape override must be a vector"" ) ] if tf . get_static_value ( override_shape ) is not None : if any ( s < 0 for s in tf . get_static_value ( override_shape ) ) : raise ValueError ( ""shape override must have non-negative elements"" ) elif validate_args : dynamic_assertions += [ assert_util . assert_non_negative ( override_shape , message = ""shape override must have non-negative elements"" ) ] is_both_nonscalar = prefer_static . logical_and ( prefer_static . logical_not ( base_is_scalar ) , prefer_static . logical_not ( override_is_scalar ) ) if tf . get_static_value ( is_both_nonscalar ) is not None : if tf . get_static_value ( is_both_nonscalar ) : raise ValueError ( ""base distribution not scalar"" ) elif validate_args : dynamic_assertions += [ assert_util . assert_equal ( is_both_nonscalar , False , message = ""base distribution not scalar"" ) ] if not dynamic_assertions : return override_shape return distribution_util . with_dependencies ( dynamic_assertions , override_shape )",Helper to __init__ which ensures override batch / event_shape are valid .
"def _maybe_rotate_dims ( self , x , rotate_right = False ) : needs_rotation_const = tf . get_static_value ( self . _needs_rotation ) if needs_rotation_const is not None and not needs_rotation_const : return x ndims = prefer_static . rank ( x ) n = ( ndims - self . _rotate_ndims ) if rotate_right else self . _rotate_ndims perm = prefer_static . concat ( [ prefer_static . range ( n , ndims ) , prefer_static . range ( 0 , n ) ] , axis = 0 ) return tf . transpose ( a = x , perm = perm )",Helper which rolls left event_dims left or right event_dims right .
"def _undo_batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , name = None ) : with tf . compat . v2 . name_scope ( name or ""undo_batchnorm"" ) : rescale = tf . sqrt ( variance + variance_epsilon ) if scale is not None : rescale /= scale batch_unnormalized = x * rescale + ( mean - offset * rescale if offset is not None else mean ) return batch_unnormalized",r Inverse of tf . nn . batch_normalization .
"def _validate_bn_layer ( self , layer ) : if ( not isinstance ( layer , tf . keras . layers . BatchNormalization ) and not isinstance ( layer , tf . compat . v1 . layers . BatchNormalization ) ) : raise ValueError ( ""batchnorm_layer must be an instance of BatchNormalization layer."" ) if layer . renorm : raise ValueError ( ""BatchNorm Bijector does not support renormalization."" ) if layer . virtual_batch_size : raise ValueError ( ""BatchNorm Bijector does not support virtual batch sizes."" )",Check for valid BatchNormalization layer .
"def _slice_single_param ( param , param_event_ndims , slices , dist_batch_shape ) : param_shape = tf . shape ( input = param ) insert_ones = tf . ones ( [ tf . size ( input = dist_batch_shape ) + param_event_ndims - tf . rank ( param ) ] , dtype = param_shape . dtype ) new_param_shape = tf . concat ( [ insert_ones , param_shape ] , axis = 0 ) full_batch_param = tf . reshape ( param , new_param_shape ) param_slices = [ ] param_dim_idx = 0 batch_dim_idx = 0 for slc in slices : if slc is tf . newaxis : param_slices . append ( slc ) continue if slc is Ellipsis : if batch_dim_idx < 0 : raise ValueError ( 'Found multiple `...` in slices {}' . format ( slices ) ) param_slices . append ( slc ) num_remaining_non_newaxis_slices = sum ( [ s is not tf . newaxis for s in slices [ slices . index ( Ellipsis ) + 1 : ] ] ) batch_dim_idx = - num_remaining_non_newaxis_slices param_dim_idx = batch_dim_idx - param_event_ndims continue param_dim_size = new_param_shape [ param_dim_idx ] batch_dim_size = dist_batch_shape [ batch_dim_idx ] is_broadcast = batch_dim_size > param_dim_size if isinstance ( slc , slice ) : start , stop , step = slc . start , slc . stop , slc . step if start is not None : start = tf . where ( is_broadcast , 0 , start ) if stop is not None : stop = tf . where ( is_broadcast , 1 , stop ) if step is not None : step = tf . where ( is_broadcast , 1 , step ) param_slices . append ( slice ( start , stop , step ) ) else : param_slices . append ( tf . where ( is_broadcast , 0 , slc ) ) param_dim_idx += 1 batch_dim_idx += 1 param_slices . extend ( [ ALL_SLICE ] * param_event_ndims ) return full_batch_param . __getitem__ ( param_slices )",Slices a single parameter of a distribution .
"def _slice_params_to_dict ( dist , params_event_ndims , slices ) : override_dict = { } for param_name , param_event_ndims in six . iteritems ( params_event_ndims ) : if param_name not in dist . parameters : raise ValueError ( 'Distribution {} is missing advertised ' 'parameter {}' . format ( dist , param_name ) ) param = dist . parameters [ param_name ] if param is None : continue dtype = None if hasattr ( dist , param_name ) : attr = getattr ( dist , param_name ) dtype = getattr ( attr , 'dtype' , None ) if dtype is None : dtype = dist . dtype warnings . warn ( 'Unable to find property getter for parameter Tensor {} ' 'on {}, falling back to Distribution.dtype {}' . format ( param_name , dist , dtype ) ) param = tf . convert_to_tensor ( value = param , dtype = dtype ) override_dict [ param_name ] = _slice_single_param ( param , param_event_ndims , slices , dist . batch_shape_tensor ( ) ) return override_dict",Computes the override dictionary of sliced parameters .
"def _apply_single_step ( dist , params_event_ndims , slices , params_overrides ) : if len ( slices ) == 1 and slices [ 0 ] == Ellipsis : override_dict = { } else : override_dict = _slice_params_to_dict ( dist , params_event_ndims , slices ) override_dict . update ( params_overrides ) parameters = dict ( dist . parameters , * * override_dict ) new_dist = type ( dist ) ( * * parameters ) return new_dist",Applies a single slicing step to dist returning a new instance .
"def _apply_slice_sequence ( dist , params_event_ndims , slice_overrides_seq ) : for slices , overrides in slice_overrides_seq : dist = _apply_single_step ( dist , params_event_ndims , slices , overrides ) return dist",Applies a sequence of slice or copy - with - overrides operations to dist .
"def batch_slice ( dist , params_event_ndims , params_overrides , slices ) : if not isinstance ( slices , collections . Sequence ) : slices = ( slices , ) orig_dist , slice_overrides_seq = getattr ( dist , PROVENANCE_ATTR , ( dist , [ ] ) ) slice_overrides_seq += [ ( slices , params_overrides ) ] dist = _apply_slice_sequence ( orig_dist , params_event_ndims , slice_overrides_seq ) setattr ( dist , PROVENANCE_ATTR , ( orig_dist , slice_overrides_seq ) ) return dist",Slices dist along its batch dimensions . Helper for tfd . Distribution .
"def fit ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , convergence_criteria_fn = None , learning_rate = None , fast_unsafe_numerics = True , maximum_iterations = None , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , offset , learning_rate , maximum_iterations ] with tf . compat . v1 . name_scope ( name , 'fit' , graph_deps ) : [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) if convergence_criteria_fn is None : convergence_criteria_fn = ( convergence_criteria_small_relative_norm_weights_change ( ) ) def _body ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous ) : """"""`tf.while_loop` body."""""" model_coefficients_next , predicted_linear_response_next = fit_one_step ( model_matrix , response , model , model_coefficients_previous , predicted_linear_response_previous , l2_regularizer , dispersion , offset , learning_rate , fast_unsafe_numerics ) is_converged_next = convergence_criteria_fn ( is_converged_previous = is_converged_previous , iter_ = iter_ , model_coefficients_previous = model_coefficients_previous , predicted_linear_response_previous = predicted_linear_response_previous , model_coefficients_next = model_coefficients_next , predicted_linear_response_next = predicted_linear_response_next , response = response , model = model , dispersion = dispersion ) return [ is_converged_next , iter_ + 1 , model_coefficients_next , predicted_linear_response_next , ] [ is_converged , iter_ , model_coefficients , predicted_linear_response , ] = tf . while_loop ( cond = lambda is_converged , * args : tf . logical_not ( is_converged ) , body = _body , loop_vars = [ tf . zeros ( [ ] , np . bool ) , tf . zeros ( [ ] , np . int32 ) , model_coefficients_start , predicted_linear_response_start , ] , maximum_iterations = maximum_iterations ) return [ model_coefficients , predicted_linear_response , is_converged , iter_ ]",Runs multiple Fisher scoring steps .
"def fit_one_step ( model_matrix , response , model , model_coefficients_start = None , predicted_linear_response_start = None , l2_regularizer = None , dispersion = None , offset = None , learning_rate = None , fast_unsafe_numerics = True , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , dispersion , learning_rate ] with tf . compat . v1 . name_scope ( name , 'fit_one_step' , graph_deps ) : [ model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset , ] = prepare_args ( model_matrix , response , model_coefficients_start , predicted_linear_response_start , offset ) mean , variance , grad_mean = model ( predicted_linear_response_start ) is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) def mask_if_invalid ( x , mask ) : mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) return tf . where ( is_valid , x , mask ) z = ( response - mean ) / mask_if_invalid ( grad_mean , 1. ) if learning_rate is not None : z *= learning_rate [ ... , tf . newaxis ] z += predicted_linear_response_start if offset is not None : z -= offset if dispersion is not None : variance *= dispersion w = ( mask_if_invalid ( grad_mean , 0. ) * tf . math . rsqrt ( mask_if_invalid ( variance , np . inf ) ) ) a = model_matrix * w [ ... , tf . newaxis ] b = z * w if l2_regularizer is None : l2_regularizer = np . array ( 0 , a . dtype . as_numpy_dtype ) else : l2_regularizer_ = distribution_util . maybe_get_static_value ( l2_regularizer , a . dtype . as_numpy_dtype ) if l2_regularizer_ is not None : l2_regularizer = l2_regularizer_ def _embed_l2_regularization ( ) : """"""Adds synthetic observations to implement L2 regularization."""""" num_model_coefficients = num_cols ( model_matrix ) batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] eye = tf . eye ( num_model_coefficients , batch_shape = batch_shape , dtype = a . dtype ) a_ = tf . concat ( [ a , tf . sqrt ( l2_regularizer ) * eye ] , axis = - 2 ) b_ = distribution_util . pad ( b , count = num_model_coefficients , axis = - 1 , back = True ) l2_regularizer_ = np . array ( 0 , a . dtype . as_numpy_dtype ) return a_ , b_ , l2_regularizer_ a , b , l2_regularizer = prefer_static . cond ( prefer_static . reduce_all ( [ not ( fast_unsafe_numerics ) , l2_regularizer > 0. ] ) , _embed_l2_regularization , lambda : ( a , b , l2_regularizer ) ) model_coefficients_next = tf . linalg . lstsq ( a , b [ ... , tf . newaxis ] , fast = fast_unsafe_numerics , l2_regularizer = l2_regularizer , name = 'model_coefficients_next' ) model_coefficients_next = model_coefficients_next [ ... , 0 ] predicted_linear_response_next = calculate_linear_predictor ( model_matrix , model_coefficients_next , offset , name = 'predicted_linear_response_next' ) return model_coefficients_next , predicted_linear_response_next",Runs one step of Fisher scoring .
"def convergence_criteria_small_relative_norm_weights_change ( tolerance = 1e-5 , norm_order = 2 ) : def convergence_criteria_fn ( is_converged_previous , iter_ , model_coefficients_previous , predicted_linear_response_previous , model_coefficients_next , predicted_linear_response_next , response , model , dispersion ) : """"""Returns `bool` `Tensor` indicating if fitting procedure has converged.

    Args:
      is_converged_previous: ""old"" convergence results.
      iter_: Iteration number.
      model_coefficients_previous: ""old"" `model_coefficients`.
      predicted_linear_response_previous: ""old"" `predicted_linear_response`.
      model_coefficients_next: ""new"" `model_coefficients`.
      predicted_linear_response_next: ""new: `predicted_linear_response`.
      response: (Batch of) vector-shaped `Tensor` where each element represents
        a sample's observed response (to the corresponding row of features).
        Must have same `dtype` as `model_matrix`.
      model: `tfp.glm.ExponentialFamily`-like instance used to construct the
        negative log-likelihood loss, gradient, and expected Hessian (i.e., the
        Fisher information matrix).
      dispersion: `Tensor` representing `response` dispersion, i.e., as in:
        `p(y|theta) := exp((y theta - A(theta)) / dispersion)`. Must broadcast
        with rows of `model_matrix`.
        Default value: `None` (i.e., ""no dispersion"").

    Returns:
      is_converged: `bool` `Tensor`.
    """""" relative_euclidean_norm = ( tf . norm ( tensor = model_coefficients_previous - model_coefficients_next , ord = norm_order , axis = - 1 ) / ( 1. + tf . norm ( tensor = model_coefficients_previous , ord = norm_order , axis = - 1 ) ) ) return ( iter_ > 0 ) & tf . reduce_all ( input_tensor = relative_euclidean_norm < tolerance ) return convergence_criteria_fn",Returns Python callable which indicates fitting procedure has converged .
"def prepare_args ( model_matrix , response , model_coefficients , predicted_linear_response , offset , name = None ) : graph_deps = [ model_matrix , response , model_coefficients , predicted_linear_response , offset ] with tf . compat . v1 . name_scope ( name , 'prepare_args' , graph_deps ) : dtype = dtype_util . common_dtype ( graph_deps , np . float32 ) model_matrix = tf . convert_to_tensor ( value = model_matrix , dtype = dtype , name = 'model_matrix' ) if offset is not None : offset = tf . convert_to_tensor ( value = offset , dtype = dtype , name = 'offset' ) response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) use_default_model_coefficients = model_coefficients is None if use_default_model_coefficients : batch_shape = tf . shape ( input = model_matrix ) [ : - 2 ] num_columns = tf . shape ( input = model_matrix ) [ - 1 ] model_coefficients = tf . zeros ( shape = tf . concat ( [ batch_shape , [ num_columns ] ] , axis = 0 ) , dtype = dtype , name = 'model_coefficients' ) else : model_coefficients = tf . convert_to_tensor ( value = model_coefficients , dtype = dtype , name = 'model_coefficients' ) if predicted_linear_response is None : if use_default_model_coefficients : if offset is None : predicted_linear_response = tf . zeros_like ( response , dtype , name = 'predicted_linear_response' ) else : predicted_linear_response = tf . broadcast_to ( offset , tf . shape ( input = response ) , name = 'predicted_linear_response' ) else : predicted_linear_response = calculate_linear_predictor ( model_matrix , model_coefficients , offset ) else : predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , dtype = dtype , name = 'predicted_linear_response' ) return [ model_matrix , response , model_coefficients , predicted_linear_response , offset , ]",Helper to fit which sanitizes input args .
"def calculate_linear_predictor ( model_matrix , model_coefficients , offset = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'calculate_linear_predictor' , [ model_matrix , model_coefficients , offset ] ) : predicted_linear_response = tf . linalg . matvec ( model_matrix , model_coefficients ) if offset is not None : predicted_linear_response += offset return predicted_linear_response",Computes model_matrix
def num_cols ( x ) : if tf . compat . dimension_value ( x . shape [ - 1 ] ) is not None : return tf . compat . dimension_value ( x . shape [ - 1 ] ) return tf . shape ( input = x ) [ - 1 ],Returns number of cols in a given Tensor .
"def _prefer_static ( original_fn , static_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) static_spec = tf_inspect . getfullargspec ( static_fn ) if original_spec != static_spec : raise ValueError ( 'Arg specs do not match: original={}, static={}, fn={}' . format ( original_spec , static_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn [ args_ , kwargs_ ] , all_static = _maybe_get_static_args ( [ args , kwargs ] ) if all_static : return static_fn ( * args_ , * * kwargs_ ) return original_fn ( * args , * * kwargs ) return wrap ( original_fn )",Wraps original_fn preferring to call static_fn when inputs are static .
"def _copy_docstring ( original_fn , new_fn ) : original_spec = tf_inspect . getfullargspec ( original_fn ) new_spec = tf_inspect . getfullargspec ( new_fn ) if original_spec != new_spec : raise ValueError ( 'Arg specs do not match: original={}, new={}, fn={}' . format ( original_spec , new_spec , original_fn ) ) @ decorator . decorator def wrap ( wrapped_fn , * args , * * kwargs ) : del wrapped_fn return new_fn ( * args , * * kwargs ) return wrap ( original_fn )",Wraps new_fn with the doc of original_fn .
"def _get_static_predicate ( pred ) : if pred in { 0 , 1 } : pred_value = bool ( pred ) elif isinstance ( pred , bool ) : pred_value = pred elif isinstance ( pred , tf . Tensor ) : pred_value = tf . get_static_value ( pred ) if pred_value is None : pred_value = c_api . TF_TryEvaluateConstant_wrapper ( pred . graph . _c_graph , pred . _as_tf_output ( ) ) else : raise TypeError ( '`pred` must be a Tensor, or a Python bool, or 1 or 0. ' 'Found instead: {}' . format ( pred ) ) return pred_value",Helper function for statically evaluating predicates in cond .
"def rank_from_shape ( shape_tensor_fn , tensorshape = None ) : if tensorshape is None : shape_tensor = ( shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) if ( hasattr ( shape_tensor , 'shape' ) and hasattr ( shape_tensor . shape , 'num_elements' ) ) : ndims_ = tensorshape_util . num_elements ( shape_tensor . shape ) else : ndims_ = len ( shape_tensor ) ndims_fn = lambda : tf . size ( input = shape_tensor ) else : ndims_ = tensorshape_util . rank ( tensorshape ) ndims_fn = lambda : tf . size ( input = shape_tensor_fn ( ) if callable ( shape_tensor_fn ) else shape_tensor_fn ) return ndims_fn ( ) if ndims_ is None else ndims_",Computes rank given a Tensor s shape .
"def cond ( pred , true_fn = None , false_fn = None , name = None ) : if not callable ( true_fn ) : raise TypeError ( '`true_fn` must be callable.' ) if not callable ( false_fn ) : raise TypeError ( '`false_fn` must be callable.' ) pred_value = _get_static_predicate ( pred ) if pred_value is not None : if pred_value : return true_fn ( ) else : return false_fn ( ) else : return tf . cond ( pred = pred , true_fn = true_fn , false_fn = false_fn , name = name )",Return either true_fn () if predicate pred is true else false_fn () .
"def case ( pred_fn_pairs , default = None , exclusive = False , name = 'smart_case' ) : return control_flow_ops . _case_helper ( cond , pred_fn_pairs , default , exclusive , name , allow_python_preds = True )",Like tf . case except attempts to statically evaluate predicates .
"def log_prob ( self , response , predicted_linear_response , name = None ) : with self . _name_scope ( name , 'log_prob' , [ response , predicted_linear_response ] ) : dtype = dtype_util . common_dtype ( [ response , predicted_linear_response ] ) response = tf . convert_to_tensor ( value = response , dtype = dtype , name = 'response' ) predicted_linear_response = tf . convert_to_tensor ( value = predicted_linear_response , name = 'predicted_linear_response' ) return self . _log_prob ( response , predicted_linear_response )",Computes D ( param = mean ( r )) . log_prob ( response ) for linear response r .
"def _name_scope ( self , name = None , default_name = None , values = None ) : with tf . compat . v1 . name_scope ( self . name ) : with tf . compat . v1 . name_scope ( name , default_name , values = values or [ ] ) as scope : yield scope",Helper function to standardize op scope .
"def mixture_stddev ( mixture_weight_vector , mean_vector , stddev_vector ) : tensorshape_util . assert_has_rank ( mixture_weight_vector . shape , 2 ) if not tensorshape_util . is_compatible_with ( mean_vector . shape , mixture_weight_vector . shape ) : raise ValueError ( ""Expecting means to have same shape as mixture weights."" ) if not tensorshape_util . is_compatible_with ( stddev_vector . shape , mixture_weight_vector . shape ) : raise ValueError ( ""Expecting stddevs to have same shape as mixture weights."" ) pi_for_dot_prod = tf . expand_dims ( mixture_weight_vector , axis = 1 ) mu_for_dot_prod = tf . expand_dims ( mean_vector , axis = 2 ) sigma_for_dot_prod = tf . expand_dims ( stddev_vector , axis = 2 ) mean_wa = tf . matmul ( pi_for_dot_prod , mu_for_dot_prod ) mean_wa = tf . reshape ( mean_wa , ( - 1 , ) ) var_wa = tf . matmul ( pi_for_dot_prod , tf . square ( sigma_for_dot_prod ) ) var_wa = tf . reshape ( var_wa , ( - 1 , ) ) sq_mean_wa = tf . matmul ( pi_for_dot_prod , tf . square ( mu_for_dot_prod ) ) sq_mean_wa = tf . reshape ( sq_mean_wa , ( - 1 , ) ) mixture_variance = var_wa + sq_mean_wa - tf . square ( mean_wa ) return tf . sqrt ( mixture_variance )",Computes the standard deviation of a mixture distribution .
"def make_tril_scale ( loc = None , scale_tril = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None ) : def _maybe_attach_assertion ( x ) : if not validate_args : return x if assert_positive : return with_dependencies ( [ assert_util . assert_positive ( tf . linalg . diag_part ( x ) , message = ""diagonal part must be positive"" ) , ] , x ) return with_dependencies ( [ assert_util . assert_none_equal ( tf . linalg . diag_part ( x ) , tf . zeros ( [ ] , x . dtype ) , message = ""diagonal part must be non-zero"" ) , ] , x ) with tf . name_scope ( name or ""make_tril_scale"" ) : dtype = dtype_util . common_dtype ( [ loc , scale_tril , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) loc = _convert_to_tensor ( loc , name = ""loc"" , dtype = dtype ) scale_tril = _convert_to_tensor ( scale_tril , name = ""scale_tril"" , dtype = dtype ) scale_diag = _convert_to_tensor ( scale_diag , name = ""scale_diag"" , dtype = dtype ) scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = ""scale_identity_multiplier"" , dtype = dtype ) if scale_tril is not None : scale_tril = tf . linalg . band_part ( scale_tril , - 1 , 0 ) tril_diag = tf . linalg . diag_part ( scale_tril ) if scale_diag is not None : tril_diag += scale_diag if scale_identity_multiplier is not None : tril_diag += scale_identity_multiplier [ ... , tf . newaxis ] scale_tril = tf . linalg . set_diag ( scale_tril , tril_diag ) return tf . linalg . LinearOperatorLowerTriangular ( tril = _maybe_attach_assertion ( scale_tril ) , is_non_singular = True , is_self_adjoint = False , is_positive_definite = assert_positive ) return make_diag_scale ( loc = loc , scale_diag = scale_diag , scale_identity_multiplier = scale_identity_multiplier , shape_hint = shape_hint , validate_args = validate_args , assert_positive = assert_positive , name = name )",Creates a LinearOperator representing a lower triangular matrix .
"def make_diag_scale ( loc = None , scale_diag = None , scale_identity_multiplier = None , shape_hint = None , validate_args = False , assert_positive = False , name = None , dtype = None ) : def _maybe_attach_assertion ( x ) : if not validate_args : return x if assert_positive : return with_dependencies ( [ assert_util . assert_positive ( x , message = ""diagonal part must be positive"" ) , ] , x ) return with_dependencies ( [ assert_util . assert_none_equal ( x , tf . zeros ( [ ] , x . dtype ) , message = ""diagonal part must be non-zero"" ) ] , x ) with tf . name_scope ( name or ""make_diag_scale"" ) : if dtype is None : dtype = dtype_util . common_dtype ( [ loc , scale_diag , scale_identity_multiplier ] , preferred_dtype = tf . float32 ) loc = _convert_to_tensor ( loc , name = ""loc"" , dtype = dtype ) scale_diag = _convert_to_tensor ( scale_diag , name = ""scale_diag"" , dtype = dtype ) scale_identity_multiplier = _convert_to_tensor ( scale_identity_multiplier , name = ""scale_identity_multiplier"" , dtype = dtype ) if scale_diag is not None : if scale_identity_multiplier is not None : scale_diag += scale_identity_multiplier [ ... , tf . newaxis ] return tf . linalg . LinearOperatorDiag ( diag = _maybe_attach_assertion ( scale_diag ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive ) if loc is None and shape_hint is None : raise ValueError ( ""Cannot infer `event_shape` unless `loc` or "" ""`shape_hint` is specified."" ) num_rows = shape_hint del shape_hint if num_rows is None : num_rows = tf . compat . dimension_value ( loc . shape [ - 1 ] ) if num_rows is None : num_rows = tf . shape ( input = loc ) [ - 1 ] if scale_identity_multiplier is None : return tf . linalg . LinearOperatorIdentity ( num_rows = num_rows , dtype = dtype , is_self_adjoint = True , is_positive_definite = True , assert_proper_shapes = validate_args ) return tf . linalg . LinearOperatorScaledIdentity ( num_rows = num_rows , multiplier = _maybe_attach_assertion ( scale_identity_multiplier ) , is_non_singular = True , is_self_adjoint = True , is_positive_definite = assert_positive , assert_proper_shapes = validate_args )",Creates a LinearOperator representing a diagonal matrix .
"def shapes_from_loc_and_scale ( loc , scale , name = ""shapes_from_loc_and_scale"" ) : if loc is not None and tensorshape_util . rank ( loc . shape ) == 0 : loc = None with tf . name_scope ( name ) : event_size = scale . range_dimension_tensor ( ) event_size_ = tf . get_static_value ( event_size ) loc_event_size_ = ( None if loc is None else tf . compat . dimension_value ( loc . shape [ - 1 ] ) ) if event_size_ is not None and loc_event_size_ is not None : if loc_event_size_ != 1 and loc_event_size_ != event_size_ : raise ValueError ( ""Event size of 'scale' ({}) could not be broadcast up to that "" ""of 'loc' ({})."" . format ( event_size_ , loc_event_size_ ) ) elif loc_event_size_ is not None and loc_event_size_ != 1 : event_size_ = loc_event_size_ if event_size_ is None : event_shape = event_size [ tf . newaxis ] else : event_shape = tf . convert_to_tensor ( value = np . reshape ( event_size_ , [ 1 ] ) , dtype = tf . int32 , name = ""event_shape"" ) batch_shape = scale . batch_shape_tensor ( ) if loc is not None : loc_batch_shape = tensorshape_util . with_rank_at_least ( loc . shape , 1 ) [ : - 1 ] if tensorshape_util . rank ( loc . shape ) is None or not tensorshape_util . is_fully_defined ( loc_batch_shape ) : loc_batch_shape = tf . shape ( input = loc ) [ : - 1 ] else : loc_batch_shape = tf . convert_to_tensor ( value = loc_batch_shape , dtype = tf . int32 , name = ""loc_batch_shape"" ) batch_shape = prefer_static_broadcast_shape ( batch_shape , loc_batch_shape ) batch_shape = tf . convert_to_tensor ( value = batch_shape , dtype = tf . int32 , name = ""batch_shape"" ) return batch_shape , event_shape",Infer distribution batch and event shapes from a location and scale .
"def get_broadcast_shape ( * tensors ) : s_shape = tensors [ 0 ] . shape for t in tensors [ 1 : ] : s_shape = tf . broadcast_static_shape ( s_shape , t . shape ) if tensorshape_util . is_fully_defined ( s_shape ) : return tensorshape_util . as_list ( s_shape ) d_shape = tf . shape ( input = tensors [ 0 ] ) for t in tensors [ 1 : ] : d_shape = tf . broadcast_dynamic_shape ( d_shape , tf . shape ( input = t ) ) return d_shape",Get broadcast shape as a Python list of integers ( preferred ) or Tensor .
"def is_diagonal_scale ( scale ) : if not isinstance ( scale , tf . linalg . LinearOperator ) : raise TypeError ( ""Expected argument 'scale' to be instance of LinearOperator"" "". Found: %s"" % scale ) return ( isinstance ( scale , tf . linalg . LinearOperatorIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorScaledIdentity ) or isinstance ( scale , tf . linalg . LinearOperatorDiag ) )",Returns True if scale is a LinearOperator that is known to be diag .
"def maybe_check_scalar_distribution ( distribution , expected_base_dtype , validate_args ) : if distribution . dtype != expected_base_dtype : raise TypeError ( ""dtype mismatch; "" ""distribution.dtype=\""{}\"" is not \""{}\"""" . format ( dtype_util . name ( distribution . dtype ) , dtype_util . name ( expected_base_dtype ) ) ) if validate_args and ( distribution . reparameterization_type != reparameterization . FULLY_REPARAMETERIZED ) : raise ValueError ( ""Base distribution should be reparameterized or be "" ""a function of non-trainable variables; "" ""distribution.reparameterization_type = \""{}\"" "" ""!= \""FULLY_REPARAMETERIZED\""."" . format ( distribution . reparameterization_type ) ) with tf . name_scope ( ""check_distribution"" ) : assertions = [ ] def check_is_scalar ( is_scalar , name ) : is_scalar_ = tf . get_static_value ( is_scalar ) if is_scalar_ is not None : if not is_scalar_ : raise ValueError ( ""distribution must be scalar; "" ""distribution.{}=False is not True"" . format ( name ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( is_scalar , True , message = ( ""distribution must be scalar; "" ""distribution.{}=False is not True"" . format ( name ) ) ) ) check_is_scalar ( distribution . is_scalar_event ( ) , ""is_scalar_event"" ) check_is_scalar ( distribution . is_scalar_batch ( ) , ""is_scalar_batch"" ) return assertions",Helper which checks validity of a scalar distribution init arg .
"def pad_mixture_dimensions ( x , mixture_distribution , categorical_distribution , event_ndims ) : with tf . name_scope ( ""pad_mix_dims"" ) : def _get_ndims ( d ) : if tensorshape_util . rank ( d . batch_shape ) is not None : return tensorshape_util . rank ( d . batch_shape ) return tf . shape ( input = d . batch_shape_tensor ( ) ) [ 0 ] dist_batch_ndims = _get_ndims ( mixture_distribution ) cat_batch_ndims = _get_ndims ( categorical_distribution ) pad_ndims = tf . where ( categorical_distribution . is_scalar_batch ( ) , dist_batch_ndims , dist_batch_ndims - cat_batch_ndims ) s = tf . shape ( input = x ) x = tf . reshape ( x , shape = tf . concat ( [ s [ : - 1 ] , tf . ones ( [ pad_ndims ] , dtype = tf . int32 ) , s [ - 1 : ] , tf . ones ( [ event_ndims ] , dtype = tf . int32 ) , ] , axis = 0 ) ) return x",Pad dimensions of event tensors for mixture distributions .
"def pick_scalar_condition ( pred , true_value , false_value , name = None ) : with tf . name_scope ( name or ""pick_scalar_condition"" ) : pred = tf . convert_to_tensor ( value = pred , dtype_hint = tf . bool , name = ""pred"" ) true_value = tf . convert_to_tensor ( value = true_value , name = ""true_value"" ) false_value = tf . convert_to_tensor ( value = false_value , name = ""false_value"" ) pred_ = tf . get_static_value ( pred ) if pred_ is None : return tf . where ( pred , true_value , false_value ) return true_value if pred_ else false_value",Convenience function that chooses one of two values based on the predicate .
"def make_non_negative_axis ( axis , rank ) : axis = tf . convert_to_tensor ( value = axis , name = ""axis"" ) rank = tf . convert_to_tensor ( value = rank , name = ""rank"" ) axis_ = tf . get_static_value ( axis ) rank_ = tf . get_static_value ( rank ) if axis_ is not None and rank_ is not None : is_scalar = axis_ . ndim == 0 if is_scalar : axis_ = [ axis_ ] positive_axis = [ ] for a_ in axis_ : if a_ < 0 : positive_axis . append ( rank_ + a_ ) else : positive_axis . append ( a_ ) if is_scalar : positive_axis = positive_axis [ 0 ] return tf . convert_to_tensor ( value = positive_axis , dtype = axis . dtype ) return tf . where ( axis < 0 , rank + axis , axis )",Make ( possibly negatively indexed ) axis argument non - negative .
"def move_dimension ( x , source_idx , dest_idx ) : ndims = prefer_static_rank ( x ) dtype = dtype_util . common_dtype ( [ source_idx , dest_idx ] , preferred_dtype = tf . int32 ) source_idx = tf . convert_to_tensor ( value = source_idx , dtype = dtype ) dest_idx = tf . convert_to_tensor ( value = dest_idx , dtype = dtype ) source_idx = pick_scalar_condition ( source_idx < 0 , ndims + source_idx , source_idx ) dest_idx = pick_scalar_condition ( dest_idx < 0 , ndims + dest_idx , dest_idx ) def move_left_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , dest_idx , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def move_right_permutation ( ) : return prefer_static_value ( tf . concat ( [ tf . range ( 0 , source_idx , dtype = dtype ) , tf . range ( source_idx + 1 , dest_idx + 1 , dtype = dtype ) , [ source_idx ] , tf . range ( dest_idx + 1 , ndims , dtype = dtype ) ] , axis = 0 ) ) def x_permuted ( ) : return tf . transpose ( a = x , perm = prefer_static . cond ( source_idx < dest_idx , move_right_permutation , move_left_permutation ) ) return prefer_static . cond ( tf . equal ( source_idx , dest_idx ) , lambda : x , x_permuted )",Move a single tensor dimension within its shape .
"def assert_integer_form ( x , data = None , summarize = None , message = None , int_dtype = None , name = ""assert_integer_form"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if dtype_util . is_integer ( x . dtype ) : return tf . no_op ( ) message = message or ""{} has non-integer components"" . format ( x ) if int_dtype is None : try : int_dtype = { tf . float16 : tf . int16 , tf . float32 : tf . int32 , tf . float64 : tf . int64 , } [ dtype_util . base_dtype ( x . dtype ) ] except KeyError : raise TypeError ( ""Unrecognized type {}"" . format ( dtype_util . name ( x . dtype ) ) ) return assert_util . assert_equal ( x , tf . cast ( tf . cast ( x , int_dtype ) , x . dtype ) , data = data , summarize = summarize , message = message , name = name )",Assert that x has integer components ( or floats equal to integers ) .
"def embed_check_nonnegative_integer_form ( x , name = ""embed_check_nonnegative_integer_form"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) assertions = [ assert_util . assert_non_negative ( x , message = ""'{}' must be non-negative."" . format ( x ) ) , ] if not dtype_util . is_integer ( x . dtype ) : assertions += [ assert_integer_form ( x , message = ""'{}' cannot contain fractional components."" . format ( x ) ) , ] return with_dependencies ( assertions , x )",Assert x is a non - negative tensor and optionally of integers .
"def same_dynamic_shape ( a , b ) : a = tf . convert_to_tensor ( value = a , name = ""a"" ) b = tf . convert_to_tensor ( value = b , name = ""b"" ) def all_shapes_equal ( ) : return tf . reduce_all ( input_tensor = tf . equal ( tf . concat ( [ tf . shape ( input = a ) , tf . shape ( input = b ) ] , 0 ) , tf . concat ( [ tf . shape ( input = b ) , tf . shape ( input = a ) ] , 0 ) ) ) return tf . cond ( pred = tf . equal ( tf . rank ( a ) , tf . rank ( b ) ) , true_fn = all_shapes_equal , false_fn = lambda : tf . constant ( False ) )",Returns whether a and b have the same dynamic shape .
"def maybe_get_static_value ( x , dtype = None ) : if x is None : return x try : x_ = tf . get_static_value ( x ) except TypeError : x_ = x if x_ is None or dtype is None : return x_ return np . array ( x_ , dtype )",Helper which tries to return a static value .
"def get_logits_and_probs ( logits = None , probs = None , multidimensional = False , validate_args = False , name = ""get_logits_and_probs"" , dtype = None ) : if dtype is None : dtype = dtype_util . common_dtype ( [ probs , logits ] , preferred_dtype = tf . float32 ) with tf . name_scope ( name ) : if ( probs is None ) == ( logits is None ) : raise ValueError ( ""Must pass probs or logits, but not both."" ) if probs is None : logits = tf . convert_to_tensor ( value = logits , name = ""logits"" , dtype = dtype ) if not dtype_util . is_floating ( logits . dtype ) : raise TypeError ( ""logits must having floating type."" ) if multidimensional : if validate_args : logits = embed_check_categorical_event_shape ( logits ) return logits , tf . nn . softmax ( logits , name = ""probs"" ) return logits , tf . sigmoid ( logits , name = ""probs"" ) probs = tf . convert_to_tensor ( value = probs , name = ""probs"" , dtype = dtype ) if not dtype_util . is_floating ( probs . dtype ) : raise TypeError ( ""probs must having floating type."" ) if validate_args : with tf . name_scope ( ""validate_probs"" ) : one = tf . constant ( 1. , probs . dtype ) dependencies = [ assert_util . assert_non_negative ( probs ) ] if multidimensional : probs = embed_check_categorical_event_shape ( probs ) dependencies += [ assert_util . assert_near ( tf . reduce_sum ( input_tensor = probs , axis = - 1 ) , one , message = ""probs does not sum to 1."" ) ] else : dependencies += [ assert_util . assert_less_equal ( probs , one , message = ""probs has components greater than 1."" ) ] probs = with_dependencies ( dependencies , probs ) with tf . name_scope ( ""logits"" ) : if multidimensional : return tf . math . log ( probs ) , probs return tf . math . log ( probs ) - tf . math . log1p ( - 1. * probs ) , probs",Converts logit to probabilities ( or vice - versa ) and returns both .
"def _is_known_unsigned_by_dtype ( dt ) : return { tf . bool : True , tf . uint8 : True , tf . uint16 : True , } . get ( dt . base_dtype , False )",Helper returning True if dtype is known to be unsigned .
"def _is_known_signed_by_dtype ( dt ) : return { tf . float16 : True , tf . float32 : True , tf . float64 : True , tf . int8 : True , tf . int16 : True , tf . int32 : True , tf . int64 : True , } . get ( dt . base_dtype , False )",Helper returning True if dtype is known to be signed .
"def _largest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( ""Unrecognized dtype: {}"" . format ( dt . name ) ) if dt . is_floating : return int ( 2 ** ( np . finfo ( dt . as_numpy_dtype ) . nmant + 1 ) ) if dt . is_integer : return np . iinfo ( dt . as_numpy_dtype ) . max if dt . base_dtype == tf . bool : return int ( 1 ) raise TypeError ( ""Unrecognized dtype: {}"" . format ( dt . name ) )",Helper returning the largest integer exactly representable by dtype .
"def _smallest_integer_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( ""Unrecognized dtype: {}"" . format ( dt . name ) ) if _is_known_unsigned_by_dtype ( dt ) : return 0 return - 1 * _largest_integer_by_dtype ( dt )",Helper returning the smallest integer exactly representable by dtype .
"def _is_integer_like_by_dtype ( dt ) : if not _is_known_dtype ( dt ) : raise TypeError ( ""Unrecognized dtype: {}"" . format ( dt . name ) ) return dt . is_integer or dt . base_dtype == tf . bool",Helper returning True if dtype . is_integer or is bool .
"def embed_check_categorical_event_shape ( categorical_param , name = ""embed_check_categorical_event_shape"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = categorical_param , name = ""categorical_param"" ) x_dtype = dtype_util . base_dtype ( x . dtype ) max_event_size = ( _largest_integer_by_dtype ( x_dtype ) if dtype_util . is_floating ( x_dtype ) else 0 ) if max_event_size is 0 : raise TypeError ( ""Unable to validate size of unrecognized dtype "" ""({})."" . format ( dtype_util . name ( x_dtype ) ) ) try : x_shape_static = tensorshape_util . with_rank_at_least ( x . shape , 1 ) except ValueError : raise ValueError ( ""A categorical-distribution parameter must have "" ""at least 1 dimension."" ) event_size = tf . compat . dimension_value ( x_shape_static [ - 1 ] ) if event_size is not None : if event_size < 2 : raise ValueError ( ""A categorical-distribution parameter must have at "" ""least 2 events."" ) if event_size > max_event_size : raise ValueError ( ""Number of classes exceeds `dtype` precision, i.e., "" ""{} implies shape ({}) cannot exceed {}."" . format ( dtype_util . name ( x_dtype ) , event_size , max_event_size ) ) return x else : event_size = tf . shape ( input = x , out_type = tf . int64 , name = ""x_shape"" ) [ - 1 ] return with_dependencies ( [ assert_util . assert_rank_at_least ( x , 1 , message = ( ""A categorical-distribution parameter must have "" ""at least 1 dimension."" ) ) , assert_util . assert_greater_equal ( tf . shape ( input = x ) [ - 1 ] , 2 , message = ( ""A categorical-distribution parameter must have at "" ""least 2 events."" ) ) , assert_util . assert_less_equal ( event_size , tf . convert_to_tensor ( max_event_size , dtype = tf . int64 ) , message = ""Number of classes exceeds `dtype` precision, "" ""i.e., {} dtype cannot exceed {} shape."" . format ( dtype_util . name ( x_dtype ) , max_event_size ) ) , ] , x )",Embeds checks that categorical distributions don t have too many classes .
"def embed_check_integer_casting_closed ( x , target_dtype , assert_nonnegative = True , assert_positive = False , name = ""embed_check_casting_closed"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if ( not _is_integer_like_by_dtype ( x . dtype ) and not dtype_util . is_floating ( x . dtype ) ) : raise TypeError ( ""{}.dtype must be floating- or "" ""integer-type."" . format ( dtype_util . name ( x . dtype ) ) ) if ( not _is_integer_like_by_dtype ( target_dtype ) and not dtype_util . is_floating ( target_dtype ) ) : raise TypeError ( ""target_dtype ({}) must be floating- or "" ""integer-type."" . format ( dtype_util . name ( target_dtype ) ) ) if ( not _is_integer_like_by_dtype ( x . dtype ) and not _is_integer_like_by_dtype ( target_dtype ) ) : raise TypeError ( ""At least one of {}.dtype ({}) and target_dtype ({}) "" ""must be integer-type."" . format ( x , dtype_util . name ( x . dtype ) , dtype_util . name ( target_dtype ) ) ) assertions = [ ] if assert_positive : assertions += [ assert_util . assert_positive ( x , message = ""Elements must be positive."" ) , ] elif assert_nonnegative : assertions += [ assert_util . assert_non_negative ( x , message = ""Elements must be non-negative."" ) , ] if dtype_util . is_floating ( x . dtype ) : assertions += [ assert_integer_form ( x , int_dtype = target_dtype , message = ""Elements must be {}-equivalent."" . format ( dtype_util . name ( target_dtype ) ) ) , ] else : if ( _largest_integer_by_dtype ( x . dtype ) > _largest_integer_by_dtype ( target_dtype ) ) : assertions += [ assert_util . assert_less_equal ( x , _largest_integer_by_dtype ( target_dtype ) , message = ( ""Elements cannot exceed {}."" . format ( _largest_integer_by_dtype ( target_dtype ) ) ) ) , ] if ( not assert_nonnegative and ( _smallest_integer_by_dtype ( x . dtype ) < _smallest_integer_by_dtype ( target_dtype ) ) ) : assertions += [ assert_util . assert_greater_equal ( x , _smallest_integer_by_dtype ( target_dtype ) , message = ( ""Elements cannot be smaller than {}."" . format ( _smallest_integer_by_dtype ( target_dtype ) ) ) ) , ] if not assertions : return x return with_dependencies ( assertions , x )",Ensures integers remain unaffected despite casting to / from int / float types .
"def log_combinations ( n , counts , name = ""log_combinations"" ) : with tf . name_scope ( name ) : n = tf . convert_to_tensor ( value = n , name = ""n"" ) counts = tf . convert_to_tensor ( value = counts , name = ""counts"" ) total_permutations = tf . math . lgamma ( n + 1 ) counts_factorial = tf . math . lgamma ( counts + 1 ) redundant_permutations = tf . reduce_sum ( input_tensor = counts_factorial , axis = [ - 1 ] ) return total_permutations - redundant_permutations",Multinomial coefficient .
"def matrix_diag_transform ( matrix , transform = None , name = None ) : with tf . name_scope ( name or ""matrix_diag_transform"" ) : matrix = tf . convert_to_tensor ( value = matrix , name = ""matrix"" ) if transform is None : return matrix diag = tf . linalg . diag_part ( matrix ) transformed_diag = transform ( diag ) transformed_mat = tf . linalg . set_diag ( matrix , transformed_diag ) return transformed_mat",Transform diagonal of [ batch - ] matrix leave rest of matrix unchanged .
"def rotate_transpose ( x , shift , name = ""rotate_transpose"" ) : with tf . name_scope ( name ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) shift = tf . convert_to_tensor ( value = shift , name = ""shift"" ) assert_util . assert_integer ( shift ) shift_value_static = tf . get_static_value ( shift ) ndims = tensorshape_util . rank ( x . shape ) if ndims is not None and shift_value_static is not None : if ndims < 2 : return x shift_value_static = np . sign ( shift_value_static ) * ( abs ( shift_value_static ) % ndims ) if shift_value_static == 0 : return x perm = np . roll ( np . arange ( ndims ) , shift_value_static ) return tf . transpose ( a = x , perm = perm ) else : ndims = tf . rank ( x ) shift = tf . where ( tf . less ( shift , 0 ) , - shift % ndims , ndims - shift % ndims ) first = tf . range ( 0 , shift ) last = tf . range ( shift , ndims ) perm = tf . concat ( [ last , first ] , 0 ) return tf . transpose ( a = x , perm = perm )",Circularly moves dims left or right .
"def pick_vector ( cond , true_vector , false_vector , name = ""pick_vector"" ) : with tf . name_scope ( name ) : cond = tf . convert_to_tensor ( value = cond , dtype_hint = tf . bool , name = ""cond"" ) if cond . dtype != tf . bool : raise TypeError ( ""{}.dtype={} which is not {}"" . format ( cond , cond . dtype , tf . bool ) ) true_vector = tf . convert_to_tensor ( value = true_vector , name = ""true_vector"" ) false_vector = tf . convert_to_tensor ( value = false_vector , name = ""false_vector"" ) if true_vector . dtype != false_vector . dtype : raise TypeError ( ""{}.dtype={} does not match {}.dtype={}"" . format ( true_vector , true_vector . dtype , false_vector , false_vector . dtype ) ) cond_value_static = tf . get_static_value ( cond ) if cond_value_static is not None : return true_vector if cond_value_static else false_vector n = tf . shape ( input = true_vector ) [ 0 ] return tf . slice ( tf . concat ( [ true_vector , false_vector ] , 0 ) , [ tf . where ( cond , 0 , n ) ] , [ tf . where ( cond , n , - 1 ) ] )",Picks possibly different length row Tensor s based on condition .
"def prefer_static_broadcast_shape ( shape1 , shape2 , name = ""prefer_static_broadcast_shape"" ) : with tf . name_scope ( name ) : def make_shape_tensor ( x ) : return tf . convert_to_tensor ( value = x , name = ""shape"" , dtype = tf . int32 ) def get_tensor_shape ( s ) : if isinstance ( s , tf . TensorShape ) : return s s_ = tf . get_static_value ( make_shape_tensor ( s ) ) if s_ is not None : return tf . TensorShape ( s_ ) return None def get_shape_tensor ( s ) : if not isinstance ( s , tf . TensorShape ) : return make_shape_tensor ( s ) if tensorshape_util . is_fully_defined ( s ) : return make_shape_tensor ( tensorshape_util . as_list ( s ) ) raise ValueError ( ""Cannot broadcast from partially "" ""defined `TensorShape`."" ) shape1_ = get_tensor_shape ( shape1 ) shape2_ = get_tensor_shape ( shape2 ) if shape1_ is not None and shape2_ is not None : return tf . broadcast_static_shape ( shape1_ , shape2_ ) shape1_ = get_shape_tensor ( shape1 ) shape2_ = get_shape_tensor ( shape2 ) return tf . broadcast_dynamic_shape ( shape1_ , shape2_ )",Convenience function which statically broadcasts shape when possible .
"def gen_new_seed ( seed , salt ) : if seed is None : return None string = ( str ( seed ) + salt ) . encode ( ""utf-8"" ) return int ( hashlib . md5 ( string ) . hexdigest ( ) [ : 8 ] , 16 ) & 0x7FFFFFFF",Generate a new seed from the given seed and salt .
"def fill_triangular ( x , upper = False , name = None ) : with tf . name_scope ( name or ""fill_triangular"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) m = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) if m is not None : m = np . int32 ( m ) n = np . sqrt ( 0.25 + 2. * m ) - 0.5 if n != np . floor ( n ) : raise ValueError ( ""Input right-most shape ({}) does not "" ""correspond to a triangular matrix."" . format ( m ) ) n = np . int32 ( n ) static_final_shape = x . shape [ : - 1 ] . concatenate ( [ n , n ] ) else : m = tf . shape ( input = x ) [ - 1 ] n = tf . cast ( tf . sqrt ( 0.25 + tf . cast ( 2 * m , dtype = tf . float32 ) ) , dtype = tf . int32 ) static_final_shape = tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ : - 1 ] . concatenate ( [ None , None ] ) ndims = prefer_static_rank ( x ) if upper : x_list = [ x , tf . reverse ( x [ ... , n : ] , axis = [ ndims - 1 ] ) ] else : x_list = [ x [ ... , n : ] , tf . reverse ( x , axis = [ ndims - 1 ] ) ] new_shape = ( tensorshape_util . as_list ( static_final_shape ) if tensorshape_util . is_fully_defined ( static_final_shape ) else tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ n , n ] ] , axis = 0 ) ) x = tf . reshape ( tf . concat ( x_list , axis = - 1 ) , new_shape ) x = tf . linalg . band_part ( x , num_lower = ( 0 if upper else - 1 ) , num_upper = ( - 1 if upper else 0 ) ) tensorshape_util . set_shape ( x , static_final_shape ) return x",r Creates a ( batch of ) triangular matrix from a vector of inputs .
"def fill_triangular_inverse ( x , upper = False , name = None ) : with tf . name_scope ( name or ""fill_triangular_inverse"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) n = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 2 ) [ - 1 ] ) if n is not None : n = np . int32 ( n ) m = np . int32 ( ( n * ( n + 1 ) ) // 2 ) static_final_shape = x . shape [ : - 2 ] . concatenate ( [ m ] ) else : n = tf . shape ( input = x ) [ - 1 ] m = ( n * ( n + 1 ) ) // 2 static_final_shape = tensorshape_util . with_rank_at_least ( x . shape , 2 ) [ : - 2 ] . concatenate ( [ None ] ) ndims = prefer_static_rank ( x ) if upper : initial_elements = x [ ... , 0 , : ] triangular_portion = x [ ... , 1 : , : ] else : initial_elements = tf . reverse ( x [ ... , - 1 , : ] , axis = [ ndims - 2 ] ) triangular_portion = x [ ... , : - 1 , : ] rotated_triangular_portion = tf . reverse ( tf . reverse ( triangular_portion , axis = [ ndims - 1 ] ) , axis = [ ndims - 2 ] ) consolidated_matrix = triangular_portion + rotated_triangular_portion end_sequence = tf . reshape ( consolidated_matrix , tf . concat ( [ tf . shape ( input = x ) [ : - 2 ] , [ n * ( n - 1 ) ] ] , axis = 0 ) ) y = tf . concat ( [ initial_elements , end_sequence [ ... , : m - n ] ] , axis = - 1 ) tensorshape_util . set_shape ( y , static_final_shape ) return y",Creates a vector from a ( batch of ) triangular matrix .
"def tridiag ( below = None , diag = None , above = None , name = None ) : def _pad ( x ) : """"""Prepends and appends a zero to every vector in a batch of vectors."""""" shape = tf . concat ( [ tf . shape ( input = x ) [ : - 1 ] , [ 1 ] ] , axis = 0 ) z = tf . zeros ( shape , dtype = x . dtype ) return tf . concat ( [ z , x , z ] , axis = - 1 ) def _add ( * x ) : """"""Adds list of Tensors, ignoring `None`."""""" s = None for y in x : if y is None : continue elif s is None : s = y else : s += y if s is None : raise ValueError ( ""Must specify at least one of `below`, `diag`, `above`."" ) return s with tf . name_scope ( name or ""tridiag"" ) : if below is not None : below = tf . convert_to_tensor ( value = below , name = ""below"" ) below = tf . linalg . diag ( _pad ( below ) ) [ ... , : - 1 , 1 : ] if diag is not None : diag = tf . convert_to_tensor ( value = diag , name = ""diag"" ) diag = tf . linalg . diag ( diag ) if above is not None : above = tf . convert_to_tensor ( value = above , name = ""above"" ) above = tf . linalg . diag ( _pad ( above ) ) [ ... , 1 : , : - 1 ] return _add ( below , diag , above )",Creates a matrix with values set above below and on the diagonal .
"def reduce_weighted_logsumexp ( logx , w = None , axis = None , keep_dims = False , return_sign = False , name = None ) : with tf . name_scope ( name or ""reduce_weighted_logsumexp"" ) : logx = tf . convert_to_tensor ( value = logx , name = ""logx"" ) if w is None : lswe = tf . reduce_logsumexp ( input_tensor = logx , axis = axis , keepdims = keep_dims ) if return_sign : sgn = tf . ones_like ( lswe ) return lswe , sgn return lswe w = tf . convert_to_tensor ( value = w , dtype = logx . dtype , name = ""w"" ) log_absw_x = logx + tf . math . log ( tf . abs ( w ) ) max_log_absw_x = tf . reduce_max ( input_tensor = log_absw_x , axis = axis , keepdims = True ) max_log_absw_x = tf . where ( tf . math . is_inf ( max_log_absw_x ) , tf . zeros_like ( max_log_absw_x ) , max_log_absw_x ) wx_over_max_absw_x = ( tf . sign ( w ) * tf . exp ( log_absw_x - max_log_absw_x ) ) sum_wx_over_max_absw_x = tf . reduce_sum ( input_tensor = wx_over_max_absw_x , axis = axis , keepdims = keep_dims ) if not keep_dims : max_log_absw_x = tf . squeeze ( max_log_absw_x , axis ) sgn = tf . sign ( sum_wx_over_max_absw_x ) lswe = max_log_absw_x + tf . math . log ( sgn * sum_wx_over_max_absw_x ) if return_sign : return lswe , sgn return lswe",Computes log ( abs ( sum ( weight * exp ( elements across tensor dimensions )))) .
"def softplus_inverse ( x , name = None ) : with tf . name_scope ( name or ""softplus_inverse"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) threshold = np . log ( np . finfo ( dtype_util . as_numpy_dtype ( x . dtype ) ) . eps ) + 2. is_too_small = tf . less ( x , np . exp ( threshold ) ) is_too_large = tf . greater ( x , - threshold ) too_small_value = tf . math . log ( x ) too_large_value = x x = tf . where ( tf . logical_or ( is_too_small , is_too_large ) , tf . ones_like ( x ) , x ) y = x + tf . math . log ( - tf . math . expm1 ( - x ) ) return tf . where ( is_too_small , too_small_value , tf . where ( is_too_large , too_large_value , y ) )",Computes the inverse softplus i . e . x = softplus_inverse ( softplus ( x )) .
"def dimension_size ( x , axis ) : s = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , np . abs ( axis ) ) [ axis ] ) if s is not None : return s return tf . shape ( input = x ) [ axis ]",Returns the size of a specific dimension .
"def process_quadrature_grid_and_probs ( quadrature_grid_and_probs , dtype , validate_args , name = None ) : with tf . name_scope ( name or ""process_quadrature_grid_and_probs"" ) : if quadrature_grid_and_probs is None : grid , probs = np . polynomial . hermite . hermgauss ( deg = 8 ) grid = grid . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs = probs . astype ( dtype_util . as_numpy_dtype ( dtype ) ) probs /= np . linalg . norm ( probs , ord = 1 , keepdims = True ) grid = tf . convert_to_tensor ( value = grid , name = ""grid"" , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = ""probs"" , dtype = dtype ) return grid , probs grid , probs = tuple ( quadrature_grid_and_probs ) grid = tf . convert_to_tensor ( value = grid , name = ""grid"" , dtype = dtype ) probs = tf . convert_to_tensor ( value = probs , name = ""unnormalized_probs"" , dtype = dtype ) probs /= tf . norm ( tensor = probs , ord = 1 , axis = - 1 , keepdims = True , name = ""probs"" ) def _static_event_size ( x ) : """"""Returns the static size of a specific dimension or `None`."""""" return tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) m , n = _static_event_size ( probs ) , _static_event_size ( grid ) if m is not None and n is not None : if m != n : raise ValueError ( ""`quadrature_grid_and_probs` must be a `tuple` of "" ""same-length zero-th-dimension `Tensor`s "" ""(saw lengths {}, {})"" . format ( m , n ) ) elif validate_args : assertions = [ assert_util . assert_equal ( dimension_size ( probs , axis = - 1 ) , dimension_size ( grid , axis = - 1 ) , message = ( ""`quadrature_grid_and_probs` must be a `tuple` of "" ""same-length zero-th-dimension `Tensor`s"" ) ) , ] with tf . control_dependencies ( assertions ) : grid = tf . identity ( grid ) probs = tf . identity ( probs ) return grid , probs",Validates quadrature grid probs or computes them as necessary .
"def pad ( x , axis , front = False , back = False , value = 0 , count = 1 , name = None ) : with tf . name_scope ( name or ""pad"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) value = tf . convert_to_tensor ( value = value , dtype = x . dtype , name = ""value"" ) count = tf . convert_to_tensor ( value = count , name = ""count"" ) if not dtype_util . is_integer ( count . dtype ) : raise TypeError ( ""`count.dtype` (`{}`) must be `int`-like."" . format ( dtype_util . name ( count . dtype ) ) ) if not front and not back : raise ValueError ( ""At least one of `front`, `back` must be `True`."" ) ndims = ( tensorshape_util . rank ( x . shape ) if tensorshape_util . rank ( x . shape ) is not None else tf . rank ( x , name = ""ndims"" ) ) axis = tf . convert_to_tensor ( value = axis , name = ""axis"" ) axis_ = tf . get_static_value ( axis ) if axis_ is not None : axis = axis_ if axis < 0 : axis = ndims + axis count_ = tf . get_static_value ( count ) if axis_ >= 0 or tensorshape_util . rank ( x . shape ) is not None : head = x . shape [ : axis ] mid_dim_value = tf . compat . dimension_value ( x . shape [ axis ] ) if count_ is None or mid_dim_value is None : middle = tf . TensorShape ( None ) else : middle = tf . TensorShape ( mid_dim_value + count_ * ( front + back ) ) tail = x . shape [ axis + 1 : ] final_shape = head . concatenate ( middle . concatenate ( tail ) ) else : final_shape = None else : axis = tf . where ( axis < 0 , ndims + axis , axis ) final_shape = None x = tf . pad ( tensor = x , paddings = tf . one_hot ( indices = tf . stack ( [ axis if front else - 1 , axis if back else - 1 ] ) , depth = ndims , axis = 0 , on_value = count , dtype = tf . int32 ) , constant_values = value ) if final_shape is not None : tensorshape_util . set_shape ( x , final_shape ) return x",Pads value to the front and / or back of a Tensor dim count times .
"def parent_frame_arguments ( ) : arg_names , variable_arg_name , keyword_arg_name , local_vars = ( tf_inspect . _inspect . getargvalues ( tf_inspect . _inspect . stack ( ) [ 1 ] [ 0 ] ) ) local_vars . pop ( variable_arg_name , { } ) keyword_args = local_vars . pop ( keyword_arg_name , { } ) final_args = { } for arg_name in arg_names : final_args [ arg_name ] = local_vars . pop ( arg_name ) final_args . update ( keyword_args ) return final_args",Returns parent frame arguments .
"def expand_to_vector ( x , tensor_name = None , op_name = None , validate_args = False ) : with tf . name_scope ( op_name or ""expand_to_vector"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) ndims = tensorshape_util . rank ( x . shape ) if ndims is None : if validate_args : x = with_dependencies ( [ assert_util . assert_rank_at_most ( x , 1 , message = ""Input is neither scalar nor vector."" ) ] , x ) ndims = tf . rank ( x ) expanded_shape = pick_vector ( tf . equal ( ndims , 0 ) , np . array ( [ 1 ] , dtype = np . int32 ) , tf . shape ( input = x ) ) return tf . reshape ( x , expanded_shape ) elif ndims == 0 : x_const = tf . get_static_value ( x ) if x_const is not None : return tf . convert_to_tensor ( value = dtype_util . as_numpy_dtype ( x . dtype ) ( [ x_const ] ) , name = tensor_name ) else : return tf . reshape ( x , [ 1 ] ) elif ndims != 1 : raise ValueError ( ""Input is neither scalar nor vector."" ) return x",Transform a 0 - D or 1 - D Tensor to be 1 - D .
"def with_dependencies ( dependencies , output_tensor , name = None ) : if tf . executing_eagerly ( ) : return output_tensor with tf . name_scope ( name or ""control_dependency"" ) as name : with tf . control_dependencies ( d for d in dependencies if d is not None ) : output_tensor = tf . convert_to_tensor ( value = output_tensor ) if isinstance ( output_tensor , tf . Tensor ) : return tf . identity ( output_tensor , name = name ) else : return tf . IndexedSlices ( tf . identity ( output_tensor . values , name = name ) , output_tensor . indices , output_tensor . dense_shape )",Produces the content of output_tensor only after dependencies .
"def _maybe_validate_rightmost_transposed_ndims ( rightmost_transposed_ndims , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_rightmost_transposed_ndims' ) : assertions = [ ] if not dtype_util . is_integer ( rightmost_transposed_ndims . dtype ) : raise TypeError ( '`rightmost_transposed_ndims` must be integer type.' ) if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) is not None : if tensorshape_util . rank ( rightmost_transposed_ndims . shape ) != 0 : raise ValueError ( '`rightmost_transposed_ndims` must be a scalar, ' 'saw rank: {}.' . format ( tensorshape_util . rank ( rightmost_transposed_ndims . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( rightmost_transposed_ndims , 0 ) ] rightmost_transposed_ndims_ = tf . get_static_value ( rightmost_transposed_ndims ) msg = '`rightmost_transposed_ndims` must be non-negative.' if rightmost_transposed_ndims_ is not None : if rightmost_transposed_ndims_ < 0 : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( rightmost_transposed_ndims_ ) ) elif validate_args : assertions += [ assert_util . assert_non_negative ( rightmost_transposed_ndims , message = msg ) ] return assertions",Checks that rightmost_transposed_ndims is valid .
"def _maybe_validate_perm ( perm , validate_args , name = None ) : with tf . name_scope ( name or 'maybe_validate_perm' ) : assertions = [ ] if not dtype_util . is_integer ( perm . dtype ) : raise TypeError ( '`perm` must be integer type' ) msg = '`perm` must be a vector.' if tensorshape_util . rank ( perm . shape ) is not None : if tensorshape_util . rank ( perm . shape ) != 1 : raise ValueError ( msg [ : - 1 ] + ', saw rank: {}.' . format ( tensorshape_util . rank ( perm . shape ) ) ) elif validate_args : assertions += [ assert_util . assert_rank ( perm , 1 , message = msg ) ] perm_ = tf . get_static_value ( perm ) msg = '`perm` must be a valid permutation vector.' if perm_ is not None : if not np . all ( np . arange ( np . size ( perm_ ) ) == np . sort ( perm_ ) ) : raise ValueError ( msg [ : - 1 ] + ', saw: {}.' . format ( perm_ ) ) elif validate_args : assertions += [ assert_util . assert_equal ( tf . sort ( perm ) , tf . range ( tf . size ( input = perm ) ) , message = msg ) ] return assertions",Checks that perm is valid .
"def _event_shape ( self , shape , static_perm_to_shape ) : rightmost_ = tf . get_static_value ( self . rightmost_transposed_ndims ) if tensorshape_util . rank ( shape ) is None or rightmost_ is None : return tf . TensorShape ( None ) if tensorshape_util . rank ( shape ) < rightmost_ : raise ValueError ( 'Invalid shape: min event ndims={} but got {}' . format ( rightmost_ , shape ) ) perm_ = tf . get_static_value ( self . perm , partial = True ) if perm_ is None : return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( [ None ] * int ( rightmost_ ) ) if sum ( p is None for p in perm_ ) == 1 : present = np . argsort ( [ - 1 if p is None else p for p in perm_ ] ) for i , p in enumerate ( present [ 1 : ] ) : if i != p : perm_ = [ i if p is None else p for p in perm_ ] break return shape [ : tensorshape_util . rank ( shape ) - rightmost_ ] . concatenate ( static_perm_to_shape ( shape [ tensorshape_util . rank ( shape ) - rightmost_ : ] , perm_ ) )",Helper for _forward and _inverse_event_shape .
"def concatenate ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . concatenate ( other ) )",Returns the concatenation of the dimension in x and other .
def constant_value_as_shape ( tensor ) : shape = tf . get_static_value ( tensor ) if shape is not None : return [ None if dim == - 1 else dim for dim in shape ] return tensor_util . constant_value_as_shape ( tensor ),A version of constant_value () that returns a TensorShape .
"def dims ( x ) : if isinstance ( x , tf . TensorShape ) : return x . dims r = tf . TensorShape ( x ) . dims return None if r is None else list ( map ( tf . compat . dimension_value , r ) )",Returns a list of dimension sizes or None if rank is unknown .
"def merge_with ( x , other ) : return type ( x ) ( tf . TensorShape ( x ) . merge_with ( other ) )",Returns a shape combining the information in x and other .
"def with_rank_at_least ( x , rank ) : return type ( x ) ( tf . TensorShape ( x ) . with_rank_at_least ( rank ) )",Returns a shape based on x with at least the given rank .
"def _check_equal_shape ( name , static_shape , dynamic_shape , static_target_shape , dynamic_target_shape = None ) : static_target_shape = tf . TensorShape ( static_target_shape ) if tensorshape_util . is_fully_defined ( static_shape ) and tensorshape_util . is_fully_defined ( static_target_shape ) : if static_shape != static_target_shape : raise ValueError ( ""{}: required shape {} but found {}"" . format ( name , static_target_shape , static_shape ) ) return None else : if dynamic_target_shape is None : if tensorshape_util . is_fully_defined ( static_target_shape ) : dynamic_target_shape = tensorshape_util . as_list ( static_target_shape ) else : raise ValueError ( ""{}: cannot infer target shape: no dynamic shape "" ""specified and static shape {} is not fully defined"" . format ( name , static_target_shape ) ) return assert_util . assert_equal ( dynamic_shape , dynamic_target_shape , message = ( ""{}: required shape {}"" . format ( name , static_target_shape ) ) )",Check that source and target shape match statically if possible .
"def _augment_sample_shape ( partial_batch_dist , full_sample_and_batch_shape , validate_args = False ) : full_ndims = distribution_util . prefer_static_shape ( full_sample_and_batch_shape ) [ 0 ] partial_batch_ndims = ( tensorshape_util . rank ( partial_batch_dist . batch_shape ) if tensorshape_util . rank ( partial_batch_dist . batch_shape ) is not None else distribution_util . prefer_static_shape ( partial_batch_dist . batch_shape_tensor ( ) ) [ 0 ] ) num_broadcast_dims = full_ndims - partial_batch_ndims expected_partial_batch_shape = ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) expected_partial_batch_shape_static = tf . get_static_value ( full_sample_and_batch_shape [ num_broadcast_dims : ] ) num_broadcast_dims_static = tf . get_static_value ( num_broadcast_dims ) if num_broadcast_dims_static is not None : if num_broadcast_dims_static < 0 : raise ValueError ( ""Cannot broadcast distribution {} batch shape to "" ""target batch shape with fewer dimensions"" . format ( partial_batch_dist ) ) if ( expected_partial_batch_shape_static is not None and tensorshape_util . is_fully_defined ( partial_batch_dist . batch_shape ) ) : if ( partial_batch_dist . batch_shape and any ( expected_partial_batch_shape_static != tensorshape_util . as_list ( partial_batch_dist . batch_shape ) ) ) : raise NotImplementedError ( ""Broadcasting is not supported; "" ""unexpected batch shape "" ""(expected {}, saw {})."" . format ( expected_partial_batch_shape_static , partial_batch_dist . batch_shape ) ) runtime_assertions = [ ] if validate_args : runtime_assertions . append ( assert_util . assert_greater_equal ( tf . convert_to_tensor ( value = num_broadcast_dims , dtype = tf . int32 ) , tf . zeros ( ( ) , dtype = tf . int32 ) , message = ( ""Cannot broadcast distribution {} batch shape to "" ""target batch shape with fewer dimensions."" . format ( partial_batch_dist ) ) ) ) runtime_assertions . append ( assert_util . assert_equal ( expected_partial_batch_shape , partial_batch_dist . batch_shape_tensor ( ) , message = ( ""Broadcasting is not supported; "" ""unexpected batch shape."" ) , name = ""assert_batch_shape_same"" ) ) with tf . control_dependencies ( runtime_assertions ) : return full_sample_and_batch_shape [ : num_broadcast_dims ]",Augment a sample shape to broadcast batch dimensions .
"def build_backward_pass_step ( get_transition_matrix_for_timestep ) : def backward_pass_step ( state , filtered_parameters ) : """"""Run a single step of backward smoothing."""""" ( filtered_mean , filtered_cov , predicted_mean , predicted_cov ) = filtered_parameters transition_matrix = get_transition_matrix_for_timestep ( state . timestep ) next_posterior_mean = state . backward_mean next_posterior_cov = state . backward_cov posterior_mean , posterior_cov = backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) return BackwardPassState ( backward_mean = posterior_mean , backward_cov = posterior_cov , timestep = state . timestep - 1 ) return backward_pass_step",Build a callable that perform one step for backward smoothing .
"def backward_smoothing_update ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , next_posterior_mean , next_posterior_cov , transition_matrix ) : tmp_gain_cov = transition_matrix . matmul ( filtered_cov ) predicted_cov_chol = tf . linalg . cholesky ( predicted_cov ) gain_transpose = tf . linalg . cholesky_solve ( predicted_cov_chol , tmp_gain_cov ) posterior_mean = ( filtered_mean + tf . linalg . matmul ( gain_transpose , next_posterior_mean - predicted_mean , adjoint_a = True ) ) posterior_cov = ( filtered_cov + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( next_posterior_cov - predicted_cov , gain_transpose ) , adjoint_a = True ) ) return ( posterior_mean , posterior_cov )",Backward update for a Kalman smoother .
"def build_kalman_filter_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def kalman_filter_step ( state , elems_t ) : """"""Run a single step of Kalman filtering.

    Args:
      state: A `KalmanFilterState` object representing the previous
        filter state at time `t-1`.
      elems_t: A tuple of Tensors `(x_t, mask_t)`, or a `Tensor` `x_t`.
        `x_t` is a `Tensor` with rightmost shape dimensions
        `[observation_size, 1]` representing the vector observed at time `t`,
        and `mask_t` is a `Tensor` with rightmost dimensions`[1, 1]`
        representing the observation mask at time `t`. Both `x_t` and `mask_t`
        may have batch dimensions, which must be compatible with the batch
        dimensions of `state.predicted_mean` and `state.predictived_cov`
        respectively. If `mask_t` is not provided, it is assumed to be `None`.

    Returns:
      new_state: A `KalmanFilterState` object representing the new
        filter state at time `t`.
    """""" if isinstance ( elems_t , tuple ) : x_t , mask_t = elems_t else : x_t = elems_t mask_t = None observation_matrix = get_observation_matrix_for_timestep ( state . timestep ) observation_noise = get_observation_noise_for_timestep ( state . timestep ) if mask_t is not None : x_expected = _propagate_mean ( state . predicted_mean , observation_matrix , observation_noise ) * tf . ones_like ( x_t ) x_t = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = x_expected ) ) , x_expected , tf . broadcast_to ( x_t , tf . shape ( input = x_expected ) ) ) ( filtered_mean , filtered_cov , observation_dist ) = linear_gaussian_update ( state . predicted_mean , state . predicted_cov , observation_matrix , observation_noise , x_t ) log_marginal_likelihood = observation_dist . log_prob ( x_t [ ... , 0 ] ) if mask_t is not None : filtered_mean = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_mean ) ) , state . predicted_mean , filtered_mean ) filtered_cov = tf . where ( tf . broadcast_to ( mask_t , tf . shape ( input = filtered_cov ) ) , state . predicted_cov , filtered_cov ) log_marginal_likelihood = tf . where ( tf . broadcast_to ( mask_t [ ... , 0 , 0 ] , tf . shape ( input = log_marginal_likelihood ) ) , tf . zeros_like ( log_marginal_likelihood ) , log_marginal_likelihood ) predicted_mean , predicted_cov = kalman_transition ( filtered_mean , filtered_cov , get_transition_matrix_for_timestep ( state . timestep ) , get_transition_noise_for_timestep ( state . timestep ) ) return KalmanFilterState ( filtered_mean , filtered_cov , predicted_mean , predicted_cov , observation_dist . mean ( ) [ ... , tf . newaxis ] , observation_dist . covariance ( ) , log_marginal_likelihood , state . timestep + 1 ) return kalman_filter_step",Build a callable that performs one step of Kalman filtering .
"def linear_gaussian_update ( prior_mean , prior_cov , observation_matrix , observation_noise , x_observed ) : observation_size_is_static_and_scalar = ( tf . compat . dimension_value ( observation_matrix . shape [ - 2 ] ) == 1 ) x_expected = _propagate_mean ( prior_mean , observation_matrix , observation_noise ) tmp_obs_cov = observation_matrix . matmul ( prior_cov ) predicted_obs_cov = ( observation_matrix . matmul ( tmp_obs_cov , adjoint_arg = True ) + observation_noise . covariance ( ) ) if observation_size_is_static_and_scalar : gain_transpose = tmp_obs_cov / predicted_obs_cov else : predicted_obs_cov_chol = tf . linalg . cholesky ( predicted_obs_cov ) gain_transpose = tf . linalg . cholesky_solve ( predicted_obs_cov_chol , tmp_obs_cov ) posterior_mean = ( prior_mean + tf . linalg . matmul ( gain_transpose , x_observed - x_expected , adjoint_a = True ) ) tmp_term = - observation_matrix . matmul ( gain_transpose , adjoint = True ) tmp_term = tf . linalg . set_diag ( tmp_term , tf . linalg . diag_part ( tmp_term ) + 1 ) posterior_cov = ( tf . linalg . matmul ( tmp_term , tf . linalg . matmul ( prior_cov , tmp_term ) , adjoint_a = True ) + tf . linalg . matmul ( gain_transpose , tf . linalg . matmul ( observation_noise . covariance ( ) , gain_transpose ) , adjoint_a = True ) ) if observation_size_is_static_and_scalar : predictive_dist = independent . Independent ( normal . Normal ( loc = x_expected [ ... , 0 ] , scale = tf . sqrt ( predicted_obs_cov [ ... , 0 ] ) ) , reinterpreted_batch_ndims = 1 ) predictive_dist . covariance = lambda : predicted_obs_cov else : predictive_dist = mvn_tril . MultivariateNormalTriL ( loc = x_expected [ ... , 0 ] , scale_tril = predicted_obs_cov_chol ) return posterior_mean , posterior_cov , predictive_dist",Conjugate update for a linear Gaussian model .
"def kalman_transition ( filtered_mean , filtered_cov , transition_matrix , transition_noise ) : predicted_mean = _propagate_mean ( filtered_mean , transition_matrix , transition_noise ) predicted_cov = _propagate_cov ( filtered_cov , transition_matrix , transition_noise ) return predicted_mean , predicted_cov",Propagate a filtered distribution through a transition model .
"def build_kalman_mean_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def mean_step ( previous_means , t ) : """"""Single step of prior mean recursion."""""" previous_latent_mean , _ = previous_means latent_mean = _propagate_mean ( previous_latent_mean , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_mean = _propagate_mean ( latent_mean , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_mean , observation_mean ) return mean_step",Build a callable that performs one step of Kalman mean recursion .
"def build_kalman_cov_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def cov_step ( previous_covs , t ) : """"""Single step of prior covariance recursion."""""" previous_latent_cov , _ = previous_covs latent_cov = _propagate_cov ( previous_latent_cov , get_transition_matrix_for_timestep ( t - 1 ) , get_transition_noise_for_timestep ( t - 1 ) ) observation_cov = _propagate_cov ( latent_cov , get_observation_matrix_for_timestep ( t ) , get_observation_noise_for_timestep ( t ) ) return ( latent_cov , observation_cov ) return cov_step",Build a callable for one step of Kalman covariance recursion .
"def build_kalman_sample_step ( get_transition_matrix_for_timestep , get_transition_noise_for_timestep , get_observation_matrix_for_timestep , get_observation_noise_for_timestep , full_sample_and_batch_shape , stream , validate_args = False ) : def sample_step ( sampled_prev , t ) : """"""Sample values for a single timestep."""""" latent_prev , _ = sampled_prev transition_matrix = get_transition_matrix_for_timestep ( t - 1 ) transition_noise = get_transition_noise_for_timestep ( t - 1 ) latent_pred = transition_matrix . matmul ( latent_prev ) latent_sampled = latent_pred + transition_noise . sample ( sample_shape = _augment_sample_shape ( transition_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] observation_matrix = get_observation_matrix_for_timestep ( t ) observation_noise = get_observation_noise_for_timestep ( t ) observation_pred = observation_matrix . matmul ( latent_sampled ) observation_sampled = observation_pred + observation_noise . sample ( sample_shape = _augment_sample_shape ( observation_noise , full_sample_and_batch_shape , validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] return ( latent_sampled , observation_sampled ) return sample_step",Build a callable for one step of Kalman sampling recursion .
"def build_pushforward_latents_step ( get_observation_matrix_for_timestep , get_observation_noise_for_timestep ) : def pushforward_latents_step ( _ , latent_t_mean_cov ) : """"""Loop body fn to pushforward latents to observations at a time step."""""" t , latent_mean , latent_cov = latent_t_mean_cov observation_matrix = get_observation_matrix_for_timestep ( t ) observation_noise = get_observation_noise_for_timestep ( t ) observation_mean = _propagate_mean ( latent_mean , observation_matrix , observation_noise ) observation_cov = _propagate_cov ( latent_cov , observation_matrix , observation_noise ) return ( observation_mean , observation_cov ) return pushforward_latents_step",Build a callable to push latent means / covs to observed means / covs .
"def _propagate_mean ( mean , linop , dist ) : return linop . matmul ( mean ) + dist . mean ( ) [ ... , tf . newaxis ]",Propagate a mean through linear Gaussian transformation .
"def _propagate_cov ( cov , linop , dist ) : return linop . matmul ( linop . matmul ( cov ) , adjoint_arg = True ) + dist . covariance ( )",Propagate covariance through linear Gaussian transformation .
"def backward_smoothing_pass ( self , filtered_means , filtered_covs , predicted_means , predicted_covs ) : with tf . name_scope ( ""backward_pass"" ) : filtered_means = tf . convert_to_tensor ( value = filtered_means , name = ""filtered_means"" ) filtered_covs = tf . convert_to_tensor ( value = filtered_covs , name = ""filtered_covs"" ) predicted_means = tf . convert_to_tensor ( value = predicted_means , name = ""predicted_means"" ) predicted_covs = tf . convert_to_tensor ( value = predicted_covs , name = ""predicted_covs"" ) filtered_means = distribution_util . move_dimension ( filtered_means , - 2 , 0 ) filtered_covs = distribution_util . move_dimension ( filtered_covs , - 3 , 0 ) predicted_means = distribution_util . move_dimension ( predicted_means , - 2 , 0 ) predicted_covs = distribution_util . move_dimension ( predicted_covs , - 3 , 0 ) filtered_means = filtered_means [ ... , tf . newaxis ] predicted_means = predicted_means [ ... , tf . newaxis ] initial_backward_mean = predicted_means [ - 1 , ... ] initial_backward_cov = predicted_covs [ - 1 , ... ] num_timesteps = tf . shape ( input = filtered_means ) [ 0 ] initial_state = BackwardPassState ( backward_mean = initial_backward_mean , backward_cov = initial_backward_cov , timestep = self . initial_step + num_timesteps - 1 ) update_step_fn = build_backward_pass_step ( self . get_transition_matrix_for_timestep ) posterior_states = tf . scan ( update_step_fn , elems = ( filtered_means , filtered_covs , predicted_means , predicted_covs ) , initializer = initial_state , reverse = True ) posterior_means = distribution_util . move_dimension ( posterior_states . backward_mean [ ... , 0 ] , 0 , - 2 ) posterior_covs = distribution_util . move_dimension ( posterior_states . backward_cov , 0 , - 3 ) return ( posterior_means , posterior_covs )",Run the backward pass in Kalman smoother .
"def _joint_sample_n ( self , n , seed = None ) : with tf . name_scope ( ""sample_n_joint"" ) : stream = seed_stream . SeedStream ( seed , salt = ""LinearGaussianStateSpaceModel_sample_n_joint"" ) sample_and_batch_shape = distribution_util . prefer_static_value ( tf . concat ( [ [ n ] , self . batch_shape_tensor ( ) ] , axis = 0 ) ) with tf . control_dependencies ( self . runtime_assertions ) : initial_latent = self . initial_state_prior . sample ( sample_shape = _augment_sample_shape ( self . initial_state_prior , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) initial_latent = initial_latent [ ... , tf . newaxis ] initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) initial_observation_pred = initial_observation_matrix . matmul ( initial_latent ) initial_observation = ( initial_observation_pred + initial_observation_noise . sample ( sample_shape = _augment_sample_shape ( initial_observation_noise , sample_and_batch_shape , self . validate_args ) , seed = stream ( ) ) [ ... , tf . newaxis ] ) sample_step = build_kalman_sample_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep , full_sample_and_batch_shape = sample_and_batch_shape , stream = stream , validate_args = self . validate_args ) ( latents , observations ) = tf . scan ( sample_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent , initial_observation ) ) latents = tf . concat ( [ initial_latent [ tf . newaxis , ... ] , latents ] , axis = 0 ) observations = tf . concat ( [ initial_observation [ tf . newaxis , ... ] , observations ] , axis = 0 ) latents = tf . squeeze ( latents , - 1 ) latents = distribution_util . move_dimension ( latents , 0 , - 2 ) observations = tf . squeeze ( observations , - 1 ) observations = distribution_util . move_dimension ( observations , 0 , - 2 ) return latents , observations",Draw a joint sample from the prior over latents and observations .
"def forward_filter ( self , x , mask = None ) : with tf . name_scope ( ""forward_filter"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) if mask is not None : mask = tf . convert_to_tensor ( value = mask , name = ""mask"" , dtype_hint = tf . bool ) check_x_shape_op = _check_equal_shape ( ""x"" , x . shape [ - 2 : ] , tf . shape ( input = x ) [ - 2 : ] , self . event_shape , self . event_shape_tensor ( ) ) check_mask_dims_op = None check_mask_shape_op = None if mask is not None : if ( tensorshape_util . rank ( mask . shape ) is None or tensorshape_util . rank ( x . shape ) is None ) : check_mask_dims_op = assert_util . assert_greater_equal ( tf . rank ( x ) , tf . rank ( mask ) , message = ( ""mask cannot have higher rank than x!"" ) ) elif tensorshape_util . rank ( mask . shape ) > tensorshape_util . rank ( x . shape ) : raise ValueError ( ""mask cannot have higher rank than x! ({} vs {})"" . format ( tensorshape_util . rank ( mask . shape ) , tensorshape_util . rank ( x . shape ) ) ) check_mask_shape_op = _check_equal_shape ( ""mask"" , mask . shape [ - 1 : ] , tf . shape ( input = mask ) [ - 1 : ] , self . event_shape [ - 2 : - 1 ] , self . event_shape_tensor ( ) [ - 2 : - 1 ] ) if self . validate_args : runtime_assertions = self . runtime_assertions if check_x_shape_op is not None : runtime_assertions += [ check_x_shape_op ] if check_mask_shape_op is not None : runtime_assertions += [ check_mask_shape_op ] if check_mask_dims_op is not None : runtime_assertions += [ check_mask_dims_op ] with tf . control_dependencies ( runtime_assertions ) : x = tf . identity ( x ) if tensorshape_util . is_fully_defined ( self . batch_shape ) and tensorshape_util . is_fully_defined ( x . shape ) : sample_and_batch_shape = tf . broadcast_static_shape ( x . shape [ : - 2 ] , self . batch_shape ) else : sample_and_batch_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = x ) [ : - 2 ] , self . batch_shape_tensor ( ) ) if mask is None : mask_sample_and_batch_shape = self . batch_shape_tensor ( ) else : if ( tensorshape_util . is_fully_defined ( self . batch_shape ) and tensorshape_util . is_fully_defined ( mask . shape ) ) : mask_sample_and_batch_shape = tf . broadcast_static_shape ( mask . shape [ : - 1 ] , self . batch_shape ) else : mask_sample_and_batch_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = mask ) [ : - 1 ] , self . batch_shape_tensor ( ) ) x = distribution_util . move_dimension ( x , - 2 , 0 ) if mask is not None : mask = distribution_util . move_dimension ( mask , - 1 , 0 ) x = x [ ... , tf . newaxis ] if mask is not None : mask = mask [ ... , tf . newaxis , tf . newaxis ] prior_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ sample_and_batch_shape , [ self . latent_size , 1 ] ] , axis = 0 ) ) prior_cov = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ mask_sample_and_batch_shape , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) initial_observation_matrix = ( self . get_observation_matrix_for_timestep ( self . initial_step ) ) initial_observation_noise = ( self . get_observation_noise_for_timestep ( self . initial_step ) ) initial_observation_mean = _propagate_mean ( prior_mean , initial_observation_matrix , initial_observation_noise ) initial_observation_cov = _propagate_cov ( prior_cov , initial_observation_matrix , initial_observation_noise ) initial_state = KalmanFilterState ( predicted_mean = prior_mean , predicted_cov = prior_cov , filtered_mean = prior_mean , filtered_cov = prior_cov , observation_mean = initial_observation_mean , observation_cov = initial_observation_cov , log_marginal_likelihood = tf . zeros ( shape = sample_and_batch_shape , dtype = self . dtype ) , timestep = tf . convert_to_tensor ( value = self . initial_step , dtype = tf . int32 , name = ""initial_step"" ) ) update_step_fn = build_kalman_filter_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) filter_states = tf . scan ( update_step_fn , elems = x if mask is None else ( x , mask ) , initializer = initial_state ) log_likelihoods = distribution_util . move_dimension ( filter_states . log_marginal_likelihood , 0 , - 1 ) filtered_means = distribution_util . move_dimension ( filter_states . filtered_mean [ ... , 0 ] , 0 , - 2 ) filtered_covs = distribution_util . move_dimension ( filter_states . filtered_cov , 0 , - 3 ) predicted_means = distribution_util . move_dimension ( filter_states . predicted_mean [ ... , 0 ] , 0 , - 2 ) predicted_covs = distribution_util . move_dimension ( filter_states . predicted_cov , 0 , - 3 ) observation_means = distribution_util . move_dimension ( filter_states . observation_mean [ ... , 0 ] , 0 , - 2 ) observation_covs = distribution_util . move_dimension ( filter_states . observation_cov , 0 , - 3 ) return ( log_likelihoods , filtered_means , filtered_covs , predicted_means , predicted_covs , observation_means , observation_covs )",Run a Kalman filter over a provided sequence of outputs .
"def posterior_marginals ( self , x , mask = None ) : with tf . name_scope ( ""smooth"" ) : x = tf . convert_to_tensor ( value = x , name = ""x"" ) ( _ , filtered_means , filtered_covs , predicted_means , predicted_covs , _ , _ ) = self . forward_filter ( x , mask = mask ) ( smoothed_means , smoothed_covs ) = self . backward_smoothing_pass ( filtered_means , filtered_covs , predicted_means , predicted_covs ) return ( smoothed_means , smoothed_covs )",Run a Kalman smoother to return posterior mean and cov .
"def _joint_mean ( self ) : with tf . name_scope ( ""mean_joint"" ) : with tf . control_dependencies ( self . runtime_assertions ) : initial_latent_mean = _broadcast_to_shape ( self . initial_state_prior . mean ( ) [ ... , tf . newaxis ] , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , 1 ] ] , axis = 0 ) ) initial_observation_mean = _propagate_mean ( initial_latent_mean , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) mean_step = build_kalman_mean_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( latent_means , observation_means ) = tf . scan ( mean_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_mean , initial_observation_mean ) ) latent_means = tf . concat ( [ initial_latent_mean [ tf . newaxis , ... ] , latent_means ] , axis = 0 ) observation_means = tf . concat ( [ initial_observation_mean [ tf . newaxis , ... ] , observation_means ] , axis = 0 ) latent_means = tf . squeeze ( latent_means , - 1 ) latent_means = distribution_util . move_dimension ( latent_means , 0 , - 2 ) observation_means = tf . squeeze ( observation_means , - 1 ) observation_means = distribution_util . move_dimension ( observation_means , 0 , - 2 ) return latent_means , observation_means",Compute prior means for all variables via dynamic programming .
"def _joint_covariances ( self ) : with tf . name_scope ( ""covariance_joint"" ) : with tf . control_dependencies ( self . runtime_assertions ) : initial_latent_cov = _broadcast_to_shape ( self . initial_state_prior . covariance ( ) , tf . concat ( [ self . batch_shape_tensor ( ) , [ self . latent_size , self . latent_size ] ] , axis = 0 ) ) initial_observation_cov = _propagate_cov ( initial_latent_cov , self . get_observation_matrix_for_timestep ( self . initial_step ) , self . get_observation_noise_for_timestep ( self . initial_step ) ) cov_step = build_kalman_cov_step ( self . get_transition_matrix_for_timestep , self . get_transition_noise_for_timestep , self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) ( latent_covs , observation_covs ) = tf . scan ( cov_step , elems = tf . range ( self . initial_step + 1 , self . final_step ) , initializer = ( initial_latent_cov , initial_observation_cov ) ) latent_covs = tf . concat ( [ initial_latent_cov [ tf . newaxis , ... ] , latent_covs ] , axis = 0 ) observation_covs = tf . concat ( [ initial_observation_cov [ tf . newaxis , ... ] , observation_covs ] , axis = 0 ) latent_covs = distribution_util . move_dimension ( latent_covs , 0 , - 3 ) observation_covs = distribution_util . move_dimension ( observation_covs , 0 , - 3 ) return latent_covs , observation_covs",Compute prior covariances for all variables via dynamic programming .
"def latents_to_observations ( self , latent_means , latent_covs ) : with tf . name_scope ( ""latents_to_observations"" ) : pushforward_latents_step = build_pushforward_latents_step ( self . get_observation_matrix_for_timestep , self . get_observation_noise_for_timestep ) latent_means = distribution_util . move_dimension ( latent_means , source_idx = - 2 , dest_idx = 0 ) latent_means = latent_means [ ... , tf . newaxis ] latent_covs = distribution_util . move_dimension ( latent_covs , source_idx = - 3 , dest_idx = 0 ) ( initial_observation_mean , initial_observation_cov ) = pushforward_latents_step ( _ = None , latent_t_mean_cov = ( self . initial_step , latent_means [ self . initial_step ] , latent_covs [ self . initial_step ] ) ) timesteps = tf . range ( self . initial_step , self . initial_step + self . num_timesteps ) observation_means , observation_covs = tf . scan ( pushforward_latents_step , elems = ( timesteps , latent_means , latent_covs ) , initializer = ( initial_observation_mean , initial_observation_cov ) , parallel_iterations = 10000 ) observation_means = distribution_util . move_dimension ( observation_means [ ... , 0 ] , source_idx = 0 , dest_idx = - 2 ) observation_covs = distribution_util . move_dimension ( observation_covs , source_idx = 0 , dest_idx = - 3 ) return observation_means , observation_covs",Push latent means and covariances forward through the observation model .
"def _bessel_ive ( v , z , cache = None ) : z = tf . convert_to_tensor ( value = z ) wrap = lambda result : tf . debugging . check_numerics ( result , 'besseli{}' . format ( v ) ) if float ( v ) >= 2 : raise ValueError ( 'Evaluating bessel_i by recurrence becomes imprecise for large v' ) cache = cache or { } safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) if v in cache : return wrap ( cache [ v ] ) if v == 0 : cache [ v ] = tf . math . bessel_i0e ( z ) elif v == 1 : cache [ v ] = tf . math . bessel_i1e ( z ) elif v == 0.5 : sinhe = lambda x : ( tf . exp ( x - tf . abs ( x ) ) - tf . exp ( - x - tf . abs ( x ) ) ) / 2 cache [ v ] = ( np . sqrt ( 2 / np . pi ) * sinhe ( z ) * tf . where ( z > 0 , tf . math . rsqrt ( safe_z ) , tf . ones_like ( safe_z ) ) ) elif v == - 0.5 : coshe = lambda x : ( tf . exp ( x - tf . abs ( x ) ) + tf . exp ( - x - tf . abs ( x ) ) ) / 2 cache [ v ] = ( np . sqrt ( 2 / np . pi ) * coshe ( z ) * tf . where ( z > 0 , tf . math . rsqrt ( safe_z ) , tf . ones_like ( safe_z ) ) ) if v <= 1 : return wrap ( cache [ v ] ) cache [ v ] = ( _bessel_ive ( v - 2 , z , cache ) - ( 2 * ( v - 1 ) ) * _bessel_ive ( v - 1 , z , cache ) / z ) return wrap ( cache [ v ] )",Computes I_v ( z ) * exp ( - abs ( z )) using a recurrence relation where z > 0 .
"def _log_normalization ( self ) : event_dim = tf . compat . dimension_value ( self . event_shape [ 0 ] ) if event_dim is None : raise ValueError ( 'vMF _log_normalizer currently only supports ' 'statically known event shape' ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_lognorm = ( ( event_dim / 2 - 1 ) * tf . math . log ( safe_conc ) - ( event_dim / 2 ) * np . log ( 2 * np . pi ) - tf . math . log ( _bessel_ive ( event_dim / 2 - 1 , safe_conc ) ) - tf . abs ( safe_conc ) ) log_nsphere_surface_area = ( np . log ( 2. ) + ( event_dim / 2 ) * np . log ( np . pi ) - tf . math . lgamma ( tf . cast ( event_dim / 2 , self . dtype ) ) ) return tf . where ( self . concentration > 0 , - safe_lognorm , log_nsphere_surface_area * tf . ones_like ( safe_lognorm ) )",Computes the log - normalizer of the distribution .
"def _maybe_assert_valid_sample ( self , samples ) : if not self . validate_args : return samples with tf . control_dependencies ( [ assert_util . assert_near ( 1. , tf . linalg . norm ( tensor = samples , axis = - 1 ) , message = 'samples must be unit length' ) , assert_util . assert_equal ( tf . shape ( input = samples ) [ - 1 : ] , self . event_shape_tensor ( ) , message = ( 'samples must have innermost dimension matching that of ' '`self.mean_direction`' ) ) , ] ) : return tf . identity ( samples )",Check counts for proper shape values then return tensor version .
"def _mode ( self ) : return ( self . mean_direction + tf . zeros_like ( self . concentration ) [ ... , tf . newaxis ] )",The mode of the von Mises - Fisher distribution is the mean direction .
"def _rotate ( self , samples ) : event_dim = ( tf . compat . dimension_value ( self . event_shape [ 0 ] ) or self . _event_shape_tensor ( ) [ 0 ] ) basis = tf . concat ( [ [ 1. ] , tf . zeros ( [ event_dim - 1 ] , dtype = self . dtype ) ] , axis = 0 ) , u = tf . nn . l2_normalize ( basis - self . mean_direction , axis = - 1 ) return samples - 2 * tf . reduce_sum ( input_tensor = samples * u , axis = - 1 , keepdims = True ) * u",Applies a Householder rotation to samples .
"def _sample_3d ( self , n , seed = None ) : seed = seed_stream . SeedStream ( seed , salt = 'von_mises_fisher_3d' ) u_shape = tf . concat ( [ [ n ] , self . _batch_shape_tensor ( ) ] , axis = 0 ) z = tf . random . uniform ( u_shape , seed = seed ( ) , dtype = self . dtype ) safe_conc = tf . where ( self . concentration > 0 , self . concentration , tf . ones_like ( self . concentration ) ) safe_z = tf . where ( z > 0 , z , tf . ones_like ( z ) ) safe_u = 1 + tf . reduce_logsumexp ( input_tensor = [ tf . math . log ( safe_z ) , tf . math . log1p ( - safe_z ) - 2 * safe_conc ] , axis = 0 ) / safe_conc u = tf . where ( self . concentration > tf . zeros_like ( safe_u ) , safe_u , 2 * z - 1 ) u = tf . where ( tf . equal ( z , 0 ) , - tf . ones_like ( u ) , u ) if not self . _allow_nan_stats : u = tf . debugging . check_numerics ( u , 'u in _sample_3d' ) return u [ ... , tf . newaxis ]",Specialized inversion sampler for 3D .
"def _copy_fn ( fn ) : if not callable ( fn ) : raise TypeError ( ""fn is not callable: {}"" . format ( fn ) ) return types . FunctionType ( code = fn . __code__ , globals = fn . __globals__ , name = fn . __name__ , argdefs = fn . __defaults__ , closure = fn . __closure__ )",Create a deep copy of fn .
"def _update_docstring ( old_str , append_str ) : old_str = old_str or """" old_str_lines = old_str . split ( ""\n"" ) append_str = ""\n"" . join ( ""    %s"" % line for line in append_str . split ( ""\n"" ) ) has_args_ix = [ ix for ix , line in enumerate ( old_str_lines ) if line . strip ( ) . lower ( ) == ""args:"" ] if has_args_ix : final_args_ix = has_args_ix [ - 1 ] return ( ""\n"" . join ( old_str_lines [ : final_args_ix ] ) + ""\n\n"" + append_str + ""\n\n"" + ""\n"" . join ( old_str_lines [ final_args_ix : ] ) ) else : return old_str + ""\n\n"" + append_str",Update old_str by inserting append_str just before the Args : section .
"def _convert_to_tensor ( value , dtype = None , dtype_hint = None , name = None ) : if ( tf . nest . is_nested ( dtype ) or tf . nest . is_nested ( dtype_hint ) ) : if dtype is None : fn = lambda v , pd : tf . convert_to_tensor ( v , dtype_hint = pd , name = name ) return tf . nest . map_structure ( fn , value , dtype_hint ) elif dtype_hint is None : fn = lambda v , d : tf . convert_to_tensor ( v , dtype = d , name = name ) return tf . nest . map_structure ( fn , value , dtype_hint ) else : fn = lambda v , d , pd : tf . convert_to_tensor ( v , dtype = d , dtype_hint = pd , name = name ) return tf . nest . map_structure ( fn , value , dtype , dtype_hint ) return tf . convert_to_tensor ( value = value , dtype = dtype , dtype_hint = dtype_hint , name = name )",Converts the given value to a ( structure of ) Tensor .
"def _remove_dict_keys_with_value ( dict_ , val ) : return { k : v for k , v in dict_ . items ( ) if v is not val }",Removes dict keys which have have self as value .
"def _recursively_replace_dict_for_pretty_dict ( x ) : if isinstance ( x , dict ) : return _PrettyDict ( { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) if ( isinstance ( x , collections . Sequence ) and not isinstance ( x , six . string_types ) ) : args = ( _recursively_replace_dict_for_pretty_dict ( x_ ) for x_ in x ) is_named_tuple = ( isinstance ( x , tuple ) and hasattr ( x , ""_asdict"" ) and hasattr ( x , ""_fields"" ) ) return type ( x ) ( * args ) if is_named_tuple else type ( x ) ( args ) if isinstance ( x , collections . Mapping ) : return type ( x ) ( * * { k : _recursively_replace_dict_for_pretty_dict ( v ) for k , v in x . items ( ) } ) return x",Recursively replace dict s with _PrettyDict .
"def expectation ( f , samples , log_prob = None , use_reparametrization = True , axis = 0 , keep_dims = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'expectation' , [ samples ] ) : if not callable ( f ) : raise ValueError ( '`f` must be a callable function.' ) if use_reparametrization : return tf . reduce_mean ( input_tensor = f ( samples ) , axis = axis , keepdims = keep_dims ) else : if not callable ( log_prob ) : raise ValueError ( '`log_prob` must be a callable function.' ) stop = tf . stop_gradient x = stop ( samples ) logpx = log_prob ( x ) fx = f ( x ) dice = fx * tf . exp ( logpx - stop ( logpx ) ) return tf . reduce_mean ( input_tensor = dice , axis = axis , keepdims = keep_dims )",Computes the Monte - Carlo approximation of E_p [ f ( X ) ] .
"def _get_samples ( dist , z , n , seed ) : with tf . compat . v1 . name_scope ( 'get_samples' , values = [ z , n ] ) : if ( n is None ) == ( z is None ) : raise ValueError ( 'Must specify exactly one of arguments ""n"" and ""z"".  Found: ' 'n = %s, z = %s' % ( n , z ) ) if n is not None : return dist . sample ( n , seed = seed ) else : return tf . convert_to_tensor ( value = z , name = 'z' )",Check args and return samples .
"def is_namedtuple_like ( x ) : try : for fn in x . _fields : _ = getattr ( x , fn ) return True except AttributeError : return False",Helper which returns True if input is collections . namedtuple - like .
"def make_name ( super_name , default_super_name , sub_name ) : name = super_name if super_name is not None else default_super_name if sub_name is not None : name += '_' + sub_name return name",Helper which makes a str name ; useful for tf . compat . v1 . name_scope .
"def _choose_base_case ( is_accepted , accepted , rejected , name = None ) : def _expand_is_accepted_like ( x ) : """"""Helper to expand `is_accepted` like the shape of some input arg."""""" with tf . compat . v1 . name_scope ( 'expand_is_accepted_like' ) : expand_shape = tf . concat ( [ tf . shape ( input = is_accepted ) , tf . ones ( [ tf . rank ( x ) - tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , ] , axis = 0 ) multiples = tf . concat ( [ tf . ones ( [ tf . rank ( is_accepted ) ] , dtype = tf . int32 ) , tf . shape ( input = x ) [ tf . rank ( is_accepted ) : ] , ] , axis = 0 ) m = tf . tile ( tf . reshape ( is_accepted , expand_shape ) , multiples ) m . set_shape ( m . shape . merge_with ( x . shape ) ) return m def _where ( accepted , rejected ) : if accepted is rejected : return accepted accepted = tf . convert_to_tensor ( value = accepted , name = 'accepted' ) rejected = tf . convert_to_tensor ( value = rejected , name = 'rejected' ) r = tf . where ( _expand_is_accepted_like ( accepted ) , accepted , rejected ) r . set_shape ( r . shape . merge_with ( accepted . shape . merge_with ( rejected . shape ) ) ) return r with tf . compat . v1 . name_scope ( name , 'choose' , values = [ is_accepted , accepted , rejected ] ) : if not is_list_like ( accepted ) : return _where ( accepted , rejected ) return [ ( choose ( is_accepted , a , r , name = name ) if is_namedtuple_like ( a ) else _where ( a , r ) ) for a , r in zip ( accepted , rejected ) ]",Helper to choose which expand_dims is_accepted and applies tf . where .
"def choose ( is_accepted , accepted , rejected , name = None ) : if not is_namedtuple_like ( accepted ) : return _choose_base_case ( is_accepted , accepted , rejected , name = name ) if not isinstance ( accepted , type ( rejected ) ) : raise TypeError ( 'Type of `accepted` ({}) must be identical to ' 'type of `rejected` ({})' . format ( type ( accepted ) . __name__ , type ( rejected ) . __name__ ) ) return type ( accepted ) ( * * dict ( [ ( fn , choose ( is_accepted , getattr ( accepted , fn ) , getattr ( rejected , fn ) , name = name ) ) for fn in accepted . _fields ] ) )",Helper which expand_dims is_accepted then applies tf . where .
"def safe_sum ( x , alt_value = - np . inf , name = None ) : with tf . compat . v1 . name_scope ( name , 'safe_sum' , [ x , alt_value ] ) : if not is_list_like ( x ) : raise TypeError ( 'Expected list input.' ) if not x : raise ValueError ( 'Input should not be empty.' ) in_shape = x [ 0 ] . shape x = tf . stack ( x , axis = - 1 ) x = tf . reduce_sum ( input_tensor = x , axis = - 1 ) alt_value = np . array ( alt_value , x . dtype . as_numpy_dtype ) alt_fill = tf . fill ( tf . shape ( input = x ) , value = alt_value ) x = tf . where ( tf . math . is_finite ( x ) , x , alt_fill ) x . set_shape ( x . shape . merge_with ( in_shape ) ) return x",Elementwise adds list members replacing non - finite results with alt_value .
"def _value_and_gradients ( fn , fn_arg_list , result = None , grads = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'value_and_gradients' , [ fn_arg_list , result , grads ] ) : def _convert_to_tensor ( x , name ) : ctt = lambda x_ : x_ if x_ is None else tf . convert_to_tensor ( value = x_ , name = name ) return [ ctt ( x_ ) for x_ in x ] if is_list_like ( x ) else ctt ( x ) fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) fn_arg_list = _convert_to_tensor ( fn_arg_list , 'fn_arg' ) if result is None : result = fn ( * fn_arg_list ) if grads is None and tf . executing_eagerly ( ) : fn_arg_list = [ 0 + x for x in fn_arg_list ] result = _convert_to_tensor ( result , 'fn_result' ) if grads is not None : grads = _convert_to_tensor ( grads , 'fn_grad' ) return result , grads if is_list_like ( result ) and len ( result ) == len ( fn_arg_list ) : def fn_slice ( i ) : """"""Needed to prevent `cell-var-from-loop` pylint warning."""""" return lambda x : fn ( * ( fn_arg_list [ : i ] + [ x ] + fn_arg_list [ i + 1 : ] ) ) grads = [ tfp_math_value_and_gradients ( fn_slice ( i ) , fn_arg_list [ i ] ) [ 1 ] for i in range ( len ( result ) ) ] else : _ , grads = tfp_math_value_and_gradients ( fn , fn_arg_list ) return result , grads",Helper to maybe_call_fn_and_grads .
"def maybe_call_fn_and_grads ( fn , fn_arg_list , result = None , grads = None , check_non_none_grads = True , name = None ) : with tf . compat . v1 . name_scope ( name , 'maybe_call_fn_and_grads' , [ fn_arg_list , result , grads ] ) : fn_arg_list = ( list ( fn_arg_list ) if is_list_like ( fn_arg_list ) else [ fn_arg_list ] ) result , grads = _value_and_gradients ( fn , fn_arg_list , result , grads ) if not all ( r . dtype . is_floating for r in ( result if is_list_like ( result ) else [ result ] ) ) : raise TypeError ( 'Function result must be a `Tensor` with `float` ' '`dtype`.' ) if len ( fn_arg_list ) != len ( grads ) : raise ValueError ( 'Function args must be in one-to-one correspondence ' 'with grads.' ) if check_non_none_grads and any ( g is None for g in grads ) : raise ValueError ( 'Encountered `None` gradient.\n' '  fn_arg_list: {}\n' '  grads: {}' . format ( fn_arg_list , grads ) ) return result , grads",Calls fn and computes the gradient of the result wrt args_list .
"def smart_for_loop ( loop_num_iter , body_fn , initial_loop_vars , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , 'smart_for_loop' , [ loop_num_iter , initial_loop_vars ] ) : loop_num_iter_ = tf . get_static_value ( loop_num_iter ) if ( loop_num_iter_ is None or tf . executing_eagerly ( ) or control_flow_util . GraphOrParentsInXlaContext ( tf . compat . v1 . get_default_graph ( ) ) ) : loop_num_iter = tf . cast ( loop_num_iter , dtype = tf . int32 ) return tf . while_loop ( cond = lambda i , * args : i < loop_num_iter , body = lambda i , * args : [ i + 1 ] + list ( body_fn ( * args ) ) , loop_vars = [ np . int32 ( 0 ) ] + initial_loop_vars , parallel_iterations = parallel_iterations ) [ 1 : ] result = initial_loop_vars for _ in range ( loop_num_iter_ ) : result = body_fn ( * result ) return result",Construct a for loop preferring a python loop if n is staticaly known .
"def trace_scan ( loop_fn , initial_state , elems , trace_fn , parallel_iterations = 10 , name = None ) : with tf . compat . v1 . name_scope ( name , 'trace_scan' , [ initial_state , elems ] ) , tf . compat . v1 . variable_scope ( tf . compat . v1 . get_variable_scope ( ) ) as vs : if vs . caching_device is None and not tf . executing_eagerly ( ) : vs . set_caching_device ( lambda op : op . device ) initial_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = 'initial_state' ) , initial_state ) elems = tf . convert_to_tensor ( value = elems , name = 'elems' ) static_length = elems . shape [ 0 ] if tf . compat . dimension_value ( static_length ) is None : length = tf . shape ( input = elems ) [ 0 ] else : length = tf . convert_to_tensor ( value = static_length , dtype = tf . int32 , name = 'length' ) elems_array = tf . TensorArray ( elems . dtype , size = length , element_shape = elems . shape [ 1 : ] ) elems_array = elems_array . unstack ( elems ) trace_arrays = tf . nest . map_structure ( lambda x : tf . TensorArray ( x . dtype , size = length , element_shape = x . shape ) , trace_fn ( initial_state ) ) def _body ( i , state , trace_arrays ) : state = loop_fn ( state , elems_array . read ( i ) ) trace_arrays = tf . nest . pack_sequence_as ( trace_arrays , [ a . write ( i , v ) for a , v in zip ( tf . nest . flatten ( trace_arrays ) , tf . nest . flatten ( trace_fn ( state ) ) ) ] ) return i + 1 , state , trace_arrays _ , final_state , trace_arrays = tf . while_loop ( cond = lambda i , * args : i < length , body = _body , loop_vars = ( 0 , initial_state , trace_arrays ) , parallel_iterations = parallel_iterations ) stacked_trace = tf . nest . map_structure ( lambda x : x . stack ( ) , trace_arrays ) def _merge_static_length ( x ) : x . set_shape ( tf . TensorShape ( static_length ) . concatenate ( x . shape [ 1 : ] ) ) return x stacked_trace = tf . nest . map_structure ( _merge_static_length , stacked_trace ) return final_state , stacked_trace",A simplified version of tf . scan that has configurable tracing .
"def make_innermost_setter ( setter ) : @ functools . wraps ( setter ) def _new_setter ( kernel_results , * args , * * kwargs ) : """"""Wrapped setter."""""" results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results new_kernel_results = setter ( kernel_results , * args , * * kwargs ) for outer_results in reversed ( results_stack ) : new_kernel_results = outer_results . _replace ( inner_results = new_kernel_results ) return new_kernel_results return _new_setter",Wraps a setter so it applies to the inner - most results in kernel_results .
"def make_innermost_getter ( getter ) : @ functools . wraps ( getter ) def _new_getter ( kernel_results , * args , * * kwargs ) : """"""Wrapped getter."""""" results_stack = [ ] while hasattr ( kernel_results , 'inner_results' ) : results_stack . append ( kernel_results ) kernel_results = kernel_results . inner_results return getter ( kernel_results , * args , * * kwargs ) return _new_getter",Wraps a getter so it applies to the inner - most results in kernel_results .
"def enable_store_parameters_in_results ( kernel ) : kernel_stack = [ ] while hasattr ( kernel , 'parameters' ) and 'inner_kernel' in kernel . parameters : kernel_stack . append ( kernel ) kernel = kernel . parameters [ 'inner_kernel' ] def _recreate_kernel ( kernel , parameters ) : new_parameters = kernel . parameters . copy ( ) new_parameters . update ( parameters ) if 'store_parameters_in_results' in new_parameters : new_parameters [ 'store_parameters_in_results' ] = True with deprecation . silence ( ) : return type ( kernel ) ( * * new_parameters ) if hasattr ( kernel , 'parameters' ) : kernel = _recreate_kernel ( kernel , { } ) for outer_kernel in reversed ( kernel_stack ) : outer_kernel = _recreate_kernel ( outer_kernel , { 'inner_kernel' : kernel } ) kernel = outer_kernel return kernel",Enables the store_parameters_in_results parameter in a chain of kernels .
"def _replace_event_shape_in_shape_tensor ( input_shape , event_shape_in , event_shape_out , validate_args ) : output_tensorshape , is_validated = _replace_event_shape_in_tensorshape ( tensorshape_util . constant_value_as_shape ( input_shape ) , event_shape_in , event_shape_out ) validation_dependencies = ( map ( tf . identity , ( event_shape_in , event_shape_out ) ) if validate_args else ( ) ) if ( tensorshape_util . is_fully_defined ( output_tensorshape ) and ( is_validated or not validate_args ) ) : with tf . control_dependencies ( validation_dependencies ) : output_shape = tf . convert_to_tensor ( value = output_tensorshape , name = 'output_shape' , dtype_hint = tf . int32 ) return output_shape , output_tensorshape with tf . control_dependencies ( validation_dependencies ) : event_shape_in_ndims = ( tf . size ( input = event_shape_in ) if tensorshape_util . num_elements ( event_shape_in . shape ) is None else tensorshape_util . num_elements ( event_shape_in . shape ) ) input_non_event_shape , input_event_shape = tf . split ( input_shape , num_or_size_splits = [ - 1 , event_shape_in_ndims ] ) additional_assertions = [ ] if is_validated : pass elif validate_args : mask = event_shape_in >= 0 explicit_input_event_shape = tf . boolean_mask ( tensor = input_event_shape , mask = mask ) explicit_event_shape_in = tf . boolean_mask ( tensor = event_shape_in , mask = mask ) additional_assertions . append ( assert_util . assert_equal ( explicit_input_event_shape , explicit_event_shape_in , message = 'Input `event_shape` does not match `event_shape_in`.' ) ) with tf . control_dependencies ( additional_assertions ) : output_shape = tf . concat ( [ input_non_event_shape , event_shape_out ] , axis = 0 , name = 'output_shape' ) return output_shape , output_tensorshape",Replaces the rightmost dims in a Tensor representing a shape .
"def _replace_event_shape_in_tensorshape ( input_tensorshape , event_shape_in , event_shape_out ) : event_shape_in_ndims = tensorshape_util . num_elements ( event_shape_in . shape ) if tensorshape_util . rank ( input_tensorshape ) is None or event_shape_in_ndims is None : return tf . TensorShape ( None ) , False input_non_event_ndims = tensorshape_util . rank ( input_tensorshape ) - event_shape_in_ndims if input_non_event_ndims < 0 : raise ValueError ( 'Input has fewer ndims ({}) than event shape ndims ({}).' . format ( tensorshape_util . rank ( input_tensorshape ) , event_shape_in_ndims ) ) input_non_event_tensorshape = input_tensorshape [ : input_non_event_ndims ] input_event_tensorshape = input_tensorshape [ input_non_event_ndims : ] event_shape_in_ = tf . get_static_value ( event_shape_in ) is_validated = ( tensorshape_util . is_fully_defined ( input_event_tensorshape ) and event_shape_in_ is not None ) if is_validated : input_event_shape_ = np . int32 ( input_event_tensorshape ) mask = event_shape_in_ >= 0 explicit_input_event_shape_ = input_event_shape_ [ mask ] explicit_event_shape_in_ = event_shape_in_ [ mask ] if not all ( explicit_input_event_shape_ == explicit_event_shape_in_ ) : raise ValueError ( 'Input `event_shape` does not match `event_shape_in`. ' '({} vs {}).' . format ( input_event_shape_ , event_shape_in_ ) ) event_tensorshape_out = tensorshape_util . constant_value_as_shape ( event_shape_out ) if tensorshape_util . rank ( event_tensorshape_out ) is None : output_tensorshape = tf . TensorShape ( None ) else : output_tensorshape = tensorshape_util . concatenate ( input_non_event_tensorshape , event_tensorshape_out ) return output_tensorshape , is_validated",Replaces the event shape dims of a TensorShape .
"def _maybe_check_valid_shape ( shape , validate_args ) : if not dtype_util . is_integer ( shape . dtype ) : raise TypeError ( '{} dtype ({}) should be `int`-like.' . format ( shape , dtype_util . name ( shape . dtype ) ) ) assertions = [ ] message = '`{}` rank should be <= 1.' if tensorshape_util . rank ( shape . shape ) is not None : if tensorshape_util . rank ( shape . shape ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . rank ( shape ) , 2 , message = message . format ( shape ) ) ) shape_ = tf . get_static_value ( shape ) message = '`{}` elements must have at most one `-1`.' if shape_ is not None : if sum ( shape_ == - 1 ) > 1 : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_less ( tf . reduce_sum ( input_tensor = tf . cast ( tf . equal ( shape , - 1 ) , tf . int32 ) ) , 2 , message = message . format ( shape ) ) ) message = '`{}` elements must be either positive integers or `-1`.' if shape_ is not None : if np . any ( shape_ < - 1 ) : raise ValueError ( message . format ( shape ) ) elif validate_args : assertions . append ( assert_util . assert_greater ( shape , - 2 , message = message . format ( shape ) ) ) return assertions",Check that a shape Tensor is int - type and otherwise sane .
"def _kl_beta_beta ( d1 , d2 , name = None ) : def delta ( fn , is_property = True ) : fn1 = getattr ( d1 , fn ) fn2 = getattr ( d2 , fn ) return ( fn2 - fn1 ) if is_property else ( fn2 ( ) - fn1 ( ) ) with tf . name_scope ( name or ""kl_beta_beta"" ) : return ( delta ( ""_log_normalization"" , is_property = False ) - tf . math . digamma ( d1 . concentration1 ) * delta ( ""concentration1"" ) - tf . math . digamma ( d1 . concentration0 ) * delta ( ""concentration0"" ) + ( tf . math . digamma ( d1 . total_concentration ) * delta ( ""total_concentration"" ) ) )",Calculate the batchwise KL divergence KL ( d1 || d2 ) with d1 and d2 Beta .
"def _maybe_assert_valid_sample ( self , x ) : if not self . validate_args : return x return distribution_util . with_dependencies ( [ assert_util . assert_positive ( x , message = ""sample must be positive"" ) , assert_util . assert_less ( x , 1. , message = ""sample must be less than `1`."" ) , ] , x )",Checks the validity of a sample .
"def converged_any ( converged , failed ) : return ( tf . reduce_any ( input_tensor = converged ) | tf . reduce_all ( input_tensor = failed ) )",Condition to stop when any batch member converges or all have failed .
"def get_initial_state_args ( value_and_gradients_function , initial_position , grad_tolerance , control_inputs = None ) : if control_inputs : with tf . control_dependencies ( control_inputs ) : f0 , df0 = value_and_gradients_function ( initial_position ) else : f0 , df0 = value_and_gradients_function ( initial_position ) converged = norm ( df0 , dims = 1 ) < grad_tolerance return dict ( converged = converged , failed = tf . zeros_like ( converged ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , num_objective_evaluations = tf . convert_to_tensor ( value = 1 ) , position = initial_position , objective_value = f0 , objective_gradient = df0 )",Returns a dictionary to populate the initial state of the search procedure .
"def line_search_step ( state , value_and_gradients_function , search_direction , grad_tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) : line_search_value_grad_func = _restrict_along_direction ( value_and_gradients_function , state . position , search_direction ) derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) val_0 = ValueAndGradient ( x = _broadcast ( 0 , state . position ) , f = state . objective_value , df = derivative_at_start_pt , full_gradient = state . objective_gradient ) inactive = state . failed | state . converged ls_result = linesearch . hager_zhang ( line_search_value_grad_func , initial_step_size = _broadcast ( 1 , state . position ) , value_at_zero = val_0 , converged = inactive ) state_after_ls = update_fields ( state , failed = state . failed | ~ ls_result . converged , num_iterations = state . num_iterations + 1 , num_objective_evaluations = ( state . num_objective_evaluations + ls_result . func_evals ) ) def _do_update_position ( ) : position_delta = tf . where ( inactive , tf . zeros_like ( search_direction ) , search_direction * tf . expand_dims ( ls_result . left . x , axis = - 1 ) ) return _update_position ( state_after_ls , position_delta , ls_result . left . f , ls_result . left . full_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) return prefer_static . cond ( stopping_condition ( state . converged , state . failed ) , true_fn = lambda : state_after_ls , false_fn = _do_update_position )",Performs the line search step of the BFGS search procedure .
"def _restrict_along_direction ( value_and_gradients_function , position , direction ) : def _restricted_func ( t ) : t = _broadcast ( t , position ) pt = position + tf . expand_dims ( t , axis = - 1 ) * direction objective_value , gradient = value_and_gradients_function ( pt ) return ValueAndGradient ( x = t , f = objective_value , df = tf . reduce_sum ( input_tensor = gradient * direction , axis = - 1 ) , full_gradient = gradient ) return _restricted_func",Restricts a function in n - dimensions to a given direction .
"def _update_position ( state , position_delta , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : failed = state . failed | ~ tf . math . is_finite ( next_objective ) | ~ tf . reduce_all ( input_tensor = tf . math . is_finite ( next_gradient ) , axis = - 1 ) next_position = state . position + position_delta converged = ~ failed & _check_convergence ( state . position , next_position , state . objective_value , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) return update_fields ( state , converged = state . converged | converged , failed = failed , position = next_position , objective_value = next_objective , objective_gradient = next_gradient )",Updates the state advancing its position by a given position_delta .
"def norm ( value , dims , order = None ) : if dims == 0 : return tf . math . abs ( value ) elif dims == 1 : axis = - 1 elif dims == 2 : axis = [ - 1 , - 2 ] else : ValueError ( dims ) if order is None : order = np . inf return tf . norm ( tensor = value , axis = axis , ord = order )",Compute the norm of the given ( possibly batched ) value .
"def _check_convergence ( current_position , next_position , current_objective , next_objective , next_gradient , grad_tolerance , f_relative_tolerance , x_tolerance ) : grad_converged = norm ( next_gradient , dims = 1 ) <= grad_tolerance x_converged = norm ( next_position - current_position , dims = 1 ) <= x_tolerance f_converged = ( norm ( next_objective - current_objective , dims = 0 ) <= f_relative_tolerance * current_objective ) return grad_converged | x_converged | f_converged",Checks if the algorithm satisfies the convergence criteria .
"def _broadcast ( value , target ) : return tf . broadcast_to ( tf . convert_to_tensor ( value = value , dtype = target . dtype ) , distribution_util . prefer_static_shape ( target ) [ : - 1 ] )",Broadcast a value to match the batching dimensions of a target .
"def _harmonic_number ( x ) : one = tf . ones ( [ ] , dtype = x . dtype ) return tf . math . digamma ( x + one ) - tf . math . digamma ( one )",Compute the harmonic number from its analytic continuation .
"def _moment ( self , n ) : total_concentration = self . concentration1 + self . concentration0 expanded_concentration1 = tf . ones_like ( total_concentration , dtype = self . dtype ) * self . concentration1 expanded_concentration0 = tf . ones_like ( total_concentration , dtype = self . dtype ) * self . concentration0 beta_arg0 = 1 + n / expanded_concentration1 beta_arg = tf . stack ( [ beta_arg0 , expanded_concentration0 ] , - 1 ) log_moment = tf . math . log ( expanded_concentration0 ) + tf . math . lbeta ( beta_arg ) return tf . exp ( log_moment )",Compute the n th ( uncentered ) moment .
"def _maybe_validate_target_accept_prob ( target_accept_prob , validate_args ) : if not validate_args : return target_accept_prob with tf . control_dependencies ( [ tf . compat . v1 . assert_positive ( target_accept_prob , message = '`target_accept_prob` must be > 0.' ) , tf . compat . v1 . assert_less ( target_accept_prob , tf . ones_like ( target_accept_prob ) , message = '`target_accept_prob` must be < 1.' ) ] ) : return tf . identity ( target_accept_prob )",Validates that target_accept_prob is in ( 0 1 ) .
"def default_exchange_proposed_fn ( prob_exchange ) : def default_exchange_proposed_fn_ ( num_replica , seed = None ) : """"""Default function for `exchange_proposed_fn` of `kernel`."""""" seed_stream = distributions . SeedStream ( seed , 'default_exchange_proposed_fn' ) zero_start = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) > 0.5 if num_replica % 2 == 0 : def _exchange ( ) : flat_exchange = tf . range ( num_replica ) if num_replica > 2 : start = tf . cast ( ~ zero_start , dtype = tf . int32 ) end = num_replica - start flat_exchange = flat_exchange [ start : end ] return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) else : def _exchange ( ) : start = tf . cast ( zero_start , dtype = tf . int32 ) end = num_replica - tf . cast ( ~ zero_start , dtype = tf . int32 ) flat_exchange = tf . range ( num_replica ) [ start : end ] return tf . reshape ( flat_exchange , [ tf . size ( input = flat_exchange ) // 2 , 2 ] ) def _null_exchange ( ) : return tf . reshape ( tf . cast ( [ ] , dtype = tf . int32 ) , shape = [ 0 , 2 ] ) return tf . cond ( pred = tf . random . uniform ( [ ] , seed = seed_stream ( ) ) < prob_exchange , true_fn = _exchange , false_fn = _null_exchange ) return default_exchange_proposed_fn_",Default exchange proposal function for replica exchange MC .
"def _get_field ( kernel_results , field_name ) : if hasattr ( kernel_results , field_name ) : return getattr ( kernel_results , field_name ) if hasattr ( kernel_results , 'accepted_results' ) : return getattr ( kernel_results . accepted_results , field_name ) raise TypeError ( 'Cannot extract %s from %s' % ( field_name , kernel_results ) )",field_name from kernel_results or kernel_results . accepted_results .
"def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'remc' , 'one_step' ) , values = [ current_state , previous_kernel_results ] ) : sampled_replica_states , sampled_replica_results = zip ( * [ rk . one_step ( previous_kernel_results . replica_states [ i ] , previous_kernel_results . replica_results [ i ] ) for i , rk in enumerate ( self . replica_kernels ) ] ) sampled_replica_states = list ( sampled_replica_states ) sampled_replica_results = list ( sampled_replica_results ) states_are_lists = mcmc_util . is_list_like ( sampled_replica_states [ 0 ] ) if not states_are_lists : sampled_replica_states = [ [ s ] for s in sampled_replica_states ] num_state_parts = len ( sampled_replica_states [ 0 ] ) dtype = sampled_replica_states [ 0 ] [ 0 ] . dtype old_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , clear_after_read = False , tensor_array_name = 'old_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] for k in range ( num_state_parts ) : for i in range ( self . num_replica ) : old_states [ k ] = old_states [ k ] . write ( i , sampled_replica_states [ i ] [ k ] ) exchange_proposed = self . exchange_proposed_fn ( self . num_replica , seed = self . _seed_stream ( ) ) exchange_proposed_n = tf . shape ( input = exchange_proposed ) [ 0 ] exchanged_states = self . _get_exchanged_states ( old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) no_exchange_proposed , _ = tf . compat . v1 . setdiff1d ( tf . range ( self . num_replica ) , tf . reshape ( exchange_proposed , [ - 1 ] ) ) exchanged_states = self . _insert_old_states_where_no_exchange_was_proposed ( no_exchange_proposed , old_states , exchanged_states ) next_replica_states = [ ] for i in range ( self . num_replica ) : next_replica_states_i = [ ] for k in range ( num_state_parts ) : next_replica_states_i . append ( exchanged_states [ k ] . read ( i ) ) next_replica_states . append ( next_replica_states_i ) if not states_are_lists : next_replica_states = [ s [ 0 ] for s in next_replica_states ] sampled_replica_states = [ s [ 0 ] for s in sampled_replica_states ] next_replica_results = [ rk . bootstrap_results ( state ) for rk , state in zip ( self . replica_kernels , next_replica_states ) ] next_state = next_replica_states [ 0 ] kernel_results = ReplicaExchangeMCKernelResults ( replica_states = next_replica_states , replica_results = next_replica_results , sampled_replica_states = sampled_replica_states , sampled_replica_results = sampled_replica_results , ) return next_state , kernel_results",Takes one step of the TransitionKernel .
"def _get_exchanged_states ( self , old_states , exchange_proposed , exchange_proposed_n , sampled_replica_states , sampled_replica_results ) : with tf . compat . v1 . name_scope ( 'get_exchanged_states' ) : target_log_probs = [ ] for replica in range ( self . num_replica ) : replica_log_prob = _get_field ( sampled_replica_results [ replica ] , 'target_log_prob' ) inverse_temp = self . inverse_temperatures [ replica ] target_log_probs . append ( replica_log_prob / inverse_temp ) target_log_probs = tf . stack ( target_log_probs , axis = 0 ) dtype = target_log_probs . dtype num_state_parts = len ( sampled_replica_states [ 0 ] ) exchanged_states = [ tf . TensorArray ( dtype , size = self . num_replica , dynamic_size = False , tensor_array_name = 'exchanged_states' , element_shape = sampled_replica_states [ 0 ] [ k ] . shape ) for k in range ( num_state_parts ) ] sample_shape = tf . concat ( ( [ self . num_replica // 2 ] , tf . shape ( input = target_log_probs ) [ 1 : ] ) , axis = 0 ) log_uniforms = tf . math . log ( tf . random . uniform ( shape = sample_shape , dtype = dtype , seed = self . _seed_stream ( ) ) ) def _swap ( is_exchange_accepted , x , y ) : """"""Swap batches of x, y where accepted."""""" with tf . compat . v1 . name_scope ( 'swap_where_exchange_accepted' ) : new_x = mcmc_util . choose ( is_exchange_accepted , y , x ) new_y = mcmc_util . choose ( is_exchange_accepted , x , y ) return new_x , new_y def cond ( i , unused_exchanged_states ) : return i < exchange_proposed_n def body ( i , exchanged_states ) : """"""Body of while loop for exchanging states."""""" m , n = tf . unstack ( exchange_proposed [ i ] ) temp_diff = self . inverse_temperatures [ m ] - self . inverse_temperatures [ n ] log_accept_ratio = mcmc_util . safe_sum ( [ - temp_diff * target_log_probs [ m ] , temp_diff * target_log_probs [ n ] ] ) is_exchange_accepted = log_uniforms [ i ] < log_accept_ratio for k in range ( num_state_parts ) : new_m , new_n = _swap ( is_exchange_accepted , old_states [ k ] . read ( m ) , old_states [ k ] . read ( n ) ) exchanged_states [ k ] = exchanged_states [ k ] . write ( m , new_m ) exchanged_states [ k ] = exchanged_states [ k ] . write ( n , new_n ) return i + 1 , exchanged_states return tf . while_loop ( cond = cond , body = body , loop_vars = [ tf . constant ( 0 ) , exchanged_states ] ) [ 1 ]",Get list of TensorArrays holding exchanged states and zeros .
"def bootstrap_results ( self , init_state ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'remc' , 'bootstrap_results' ) , values = [ init_state ] ) : replica_results = [ self . replica_kernels [ i ] . bootstrap_results ( init_state ) for i in range ( self . num_replica ) ] init_state_parts = ( list ( init_state ) if mcmc_util . is_list_like ( init_state ) else [ init_state ] ) replica_states = [ [ tf . convert_to_tensor ( value = s ) for s in init_state_parts ] for i in range ( self . num_replica ) ] if not mcmc_util . is_list_like ( init_state ) : replica_states = [ s [ 0 ] for s in replica_states ] return ReplicaExchangeMCKernelResults ( replica_states = replica_states , replica_results = replica_results , sampled_replica_states = replica_states , sampled_replica_results = replica_results , )",Returns an object with the same type as returned by one_step .
"def _variance_scale_term ( self ) : c0 = self . total_concentration [ ... , tf . newaxis ] return tf . sqrt ( ( 1. + c0 / self . total_count [ ... , tf . newaxis ] ) / ( 1. + c0 ) )",Helper to _covariance and _variance which computes a shared scale .
"def _maybe_assert_valid_concentration ( self , concentration , validate_args ) : if not validate_args : return concentration concentration = distribution_util . embed_check_categorical_event_shape ( concentration ) return distribution_util . with_dependencies ( [ assert_util . assert_positive ( concentration , message = ""Concentration parameter must be positive."" ) , ] , concentration )",Checks the validity of the concentration parameter .
"def _maybe_assert_valid_sample ( self , counts ) : if not self . validate_args : return counts counts = distribution_util . embed_check_nonnegative_integer_form ( counts ) return distribution_util . with_dependencies ( [ assert_util . assert_equal ( self . total_count , tf . reduce_sum ( input_tensor = counts , axis = - 1 ) , message = ""counts last-dimension must sum to `self.total_count`"" ) , ] , counts )",Check counts for proper shape values then return tensor version .
"def forward_log_det_jacobian_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts , event_ndims ) : return sum ( [ b . forward_log_det_jacobian ( sp , event_ndims = e ) for b , e , sp in zip ( bijector , event_ndims , transformed_state_parts ) ] ) return fn",Makes a function which applies a list of Bijectors log_det_jacobian s .
"def forward_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( transformed_state_parts ) : return [ b . forward ( sp ) for b , sp in zip ( bijector , transformed_state_parts ) ] return fn",Makes a function which applies a list of Bijectors forward s .
"def inverse_transform_fn ( bijector ) : if not mcmc_util . is_list_like ( bijector ) : bijector = [ bijector ] def fn ( state_parts ) : return [ b . inverse ( sp ) for b , sp in zip ( bijector , state_parts ) ] return fn",Makes a function which applies a list of Bijectors inverse s .
"def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'transformed_kernel' , 'one_step' ) , values = [ previous_kernel_results ] ) : transformed_next_state , kernel_results = self . _inner_kernel . one_step ( previous_kernel_results . transformed_state , previous_kernel_results . inner_results ) transformed_next_state_parts = ( transformed_next_state if mcmc_util . is_list_like ( transformed_next_state ) else [ transformed_next_state ] ) next_state_parts = self . _forward_transform ( transformed_next_state_parts ) next_state = ( next_state_parts if mcmc_util . is_list_like ( transformed_next_state ) else next_state_parts [ 0 ] ) kernel_results = TransformedTransitionKernelResults ( transformed_state = transformed_next_state , inner_results = kernel_results ) return next_state , kernel_results",Runs one iteration of the Transformed Kernel .
"def bootstrap_results ( self , init_state = None , transformed_init_state = None ) : if ( init_state is None ) == ( transformed_init_state is None ) : raise ValueError ( 'Must specify exactly one of `init_state` ' 'or `transformed_init_state`.' ) with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'transformed_kernel' , 'bootstrap_results' ) , values = [ init_state , transformed_init_state ] ) : if transformed_init_state is None : init_state_parts = ( init_state if mcmc_util . is_list_like ( init_state ) else [ init_state ] ) transformed_init_state_parts = self . _inverse_transform ( init_state_parts ) transformed_init_state = ( transformed_init_state_parts if mcmc_util . is_list_like ( init_state ) else transformed_init_state_parts [ 0 ] ) else : if mcmc_util . is_list_like ( transformed_init_state ) : transformed_init_state = [ tf . convert_to_tensor ( value = s , name = 'transformed_init_state' ) for s in transformed_init_state ] else : transformed_init_state = tf . convert_to_tensor ( value = transformed_init_state , name = 'transformed_init_state' ) kernel_results = TransformedTransitionKernelResults ( transformed_state = transformed_init_state , inner_results = self . _inner_kernel . bootstrap_results ( transformed_init_state ) ) return kernel_results",Returns an object with the same type as returned by one_step .
"def val_where ( cond , tval , fval ) : if isinstance ( tval , tf . Tensor ) : return tf . where ( cond , tval , fval ) elif isinstance ( tval , tuple ) : cls = type ( tval ) return cls ( * ( val_where ( cond , t , f ) for t , f in zip ( tval , fval ) ) ) else : raise Exception ( TypeError )",Like tf . where but works on namedtuples .
"def secant2 ( value_and_gradients_function , val_0 , search_interval , f_lim , sufficient_decrease_param = 0.1 , curvature_param = 0.9 , name = None ) : with tf . compat . v1 . name_scope ( name , 'secant2' , [ val_0 , search_interval , f_lim , sufficient_decrease_param , curvature_param ] ) : val_c = value_and_gradients_function ( _secant ( search_interval . left , search_interval . right ) ) failed = search_interval . failed | ~ is_finite ( val_c ) converged = search_interval . converged | ( ~ failed & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) ) new_converged = converged & ~ search_interval . converged val_left = val_where ( new_converged , val_c , search_interval . left ) val_right = val_where ( new_converged , val_c , search_interval . right ) initial_args = _Secant2Result ( active = ~ failed & ~ converged , converged = converged , failed = failed , num_evals = search_interval . func_evals + 1 , left = val_left , right = val_right ) def _apply_secant2_inner ( ) : return _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) return prefer_static . cond ( tf . reduce_any ( input_tensor = initial_args . active ) , _apply_secant2_inner , lambda : initial_args )",Performs the secant square procedure of Hager Zhang .
"def _secant2_inner ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : update_result = update ( value_and_gradients_function , initial_args . left , initial_args . right , val_c , f_lim , active = initial_args . active ) active = initial_args . active & ~ update_result . failed failed = initial_args . failed | update_result . failed val_left = val_where ( active , update_result . left , initial_args . left ) val_right = val_where ( active , update_result . right , initial_args . right ) updated_left = active & tf . equal ( val_left . x , val_c . x ) updated_right = active & tf . equal ( val_right . x , val_c . x ) is_new = updated_left | updated_right next_c = tf . where ( updated_left , _secant ( initial_args . left , val_left ) , val_c . x ) next_c = tf . where ( updated_right , _secant ( initial_args . right , val_right ) , next_c ) in_range = ( val_left . x <= next_c ) & ( next_c <= val_right . x ) needs_extra_eval = tf . reduce_any ( input_tensor = in_range & is_new ) num_evals = initial_args . num_evals + update_result . num_evals num_evals = num_evals + tf . cast ( needs_extra_eval , num_evals . dtype ) next_args = _Secant2Result ( active = active & in_range , converged = initial_args . converged , failed = failed , num_evals = num_evals , left = val_left , right = val_right ) def _apply_inner_update ( ) : next_val_c = prefer_static . cond ( needs_extra_eval , ( lambda : value_and_gradients_function ( next_c ) ) , ( lambda : val_c ) ) return _secant2_inner_update ( value_and_gradients_function , next_args , val_0 , next_val_c , f_lim , sufficient_decrease_param , curvature_param ) return prefer_static . cond ( tf . reduce_any ( input_tensor = next_args . active ) , _apply_inner_update , lambda : next_args )",Helper function for secant square .
"def _secant2_inner_update ( value_and_gradients_function , initial_args , val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : new_failed = initial_args . active & ~ is_finite ( val_c ) active = initial_args . active & ~ new_failed failed = initial_args . failed | new_failed found_wolfe = active & _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) val_left = val_where ( found_wolfe , val_c , initial_args . left ) val_right = val_where ( found_wolfe , val_c , initial_args . right ) converged = initial_args . converged | found_wolfe active = active & ~ found_wolfe def _apply_update ( ) : update_result = update ( value_and_gradients_function , val_left , val_right , val_c , f_lim , active = active ) return _Secant2Result ( active = tf . zeros_like ( active ) , converged = converged , failed = failed | update_result . failed , num_evals = initial_args . num_evals + update_result . num_evals , left = update_result . left , right = update_result . right ) def _default ( ) : return _Secant2Result ( active = active , converged = converged , failed = failed , num_evals = initial_args . num_evals , left = val_left , right = val_right ) return prefer_static . cond ( tf . reduce_any ( input_tensor = active ) , _apply_update , _default )",Helper function for secant - square step .
"def update ( value_and_gradients_function , val_left , val_right , val_trial , f_lim , active = None ) : within_range = ( val_left . x < val_trial . x ) & ( val_trial . x < val_right . x ) if active is not None : within_range = within_range & active valid_left = ( val_trial . df < 0 ) & ( val_trial . f <= f_lim ) needs_bisect = within_range & ( val_trial . df < 0 ) & ( val_trial . f > f_lim ) left = val_where ( within_range & valid_left , val_trial , val_left ) right = val_where ( within_range & ~ valid_left , val_trial , val_right ) bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = ~ needs_bisect , failed = tf . zeros_like ( within_range ) , num_evals = tf . convert_to_tensor ( value = 0 ) , left = left , right = right ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )",Squeezes a bracketing interval containing the minimum .
"def bracket ( value_and_gradients_function , search_interval , f_lim , max_iterations , expansion_param = 5.0 ) : already_stopped = search_interval . failed | search_interval . converged bracketed = search_interval . right . df >= 0 needs_bisect = ( search_interval . right . df < 0 ) & ( search_interval . right . f > f_lim ) initial_args = _IntermediateResult ( iteration = search_interval . iterations , stopped = already_stopped | bracketed | needs_bisect , failed = search_interval . failed , num_evals = search_interval . func_evals , left = search_interval . left , right = search_interval . right ) def _loop_cond ( curr ) : return ( curr . iteration < max_iterations ) & ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : """"""Main body of bracketing loop."""""" new_right = value_and_gradients_function ( expansion_param * curr . right . x ) left = val_where ( curr . stopped , curr . left , curr . right ) right = val_where ( curr . stopped , curr . right , new_right ) failed = curr . failed | ~ is_finite ( right ) bracketed = right . df >= 0 needs_bisect = ( right . df < 0 ) & ( right . f > f_lim ) return [ _IntermediateResult ( iteration = curr . iteration + 1 , stopped = curr . stopped | failed | bracketed | needs_bisect , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] bracket_result = tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ] needs_bisect = ( ( bracket_result . right . df < 0 ) & ( bracket_result . right . f > f_lim ) ) stopped = already_stopped | bracket_result . failed | ~ needs_bisect left = val_where ( stopped , bracket_result . left , search_interval . left ) bisect_args = bracket_result . _replace ( stopped = stopped , left = left ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )",Brackets the minimum given an initial starting point .
"def bisect ( value_and_gradients_function , initial_left , initial_right , f_lim ) : failed = ~ is_finite ( initial_left , initial_right ) needs_bisect = ( initial_right . df < 0 ) & ( initial_right . f > f_lim ) bisect_args = _IntermediateResult ( iteration = tf . convert_to_tensor ( value = 0 ) , stopped = failed | ~ needs_bisect , failed = failed , num_evals = tf . convert_to_tensor ( value = 0 ) , left = initial_left , right = initial_right ) return _bisect ( value_and_gradients_function , bisect_args , f_lim )",Bisects an interval and updates to satisfy opposite slope conditions .
"def _bisect ( value_and_gradients_function , initial_args , f_lim ) : def _loop_cond ( curr ) : return ~ tf . reduce_all ( input_tensor = curr . stopped ) def _loop_body ( curr ) : """"""Narrow down interval to satisfy opposite slope conditions."""""" mid = value_and_gradients_function ( ( curr . left . x + curr . right . x ) / 2 ) failed = ( curr . failed | ~ is_finite ( mid ) | tf . equal ( mid . x , curr . left . x ) | tf . equal ( mid . x , curr . right . x ) ) to_update = ~ ( curr . stopped | failed ) update_left = ( mid . df < 0 ) & ( mid . f <= f_lim ) left = val_where ( to_update & update_left , mid , curr . left ) right = val_where ( to_update & ~ update_left , mid , curr . right ) stopped = curr . stopped | failed | ( right . df >= 0 ) return [ _IntermediateResult ( iteration = curr . iteration , stopped = stopped , failed = failed , num_evals = curr . num_evals + 1 , left = left , right = right ) ] return tf . while_loop ( cond = _loop_cond , body = _loop_body , loop_vars = [ initial_args ] ) [ 0 ]",Actual implementation of bisect given initial_args in a _BracketResult .
"def is_finite ( val_1 , val_2 = None ) : val_1_finite = tf . math . is_finite ( val_1 . f ) & tf . math . is_finite ( val_1 . df ) if val_2 is not None : return val_1_finite & tf . math . is_finite ( val_2 . f ) & tf . math . is_finite ( val_2 . df ) return val_1_finite",Checks if the supplied values are finite .
"def _satisfies_wolfe ( val_0 , val_c , f_lim , sufficient_decrease_param , curvature_param ) : exact_wolfe_suff_dec = ( sufficient_decrease_param * val_0 . df >= ( val_c . f - val_0 . f ) / val_c . x ) wolfe_curvature = val_c . df >= curvature_param * val_0 . df exact_wolfe = exact_wolfe_suff_dec & wolfe_curvature approx_wolfe_applies = val_c . f <= f_lim approx_wolfe_suff_dec = ( ( 2 * sufficient_decrease_param - 1 ) * val_0 . df >= val_c . df ) approx_wolfe = approx_wolfe_applies & approx_wolfe_suff_dec & wolfe_curvature is_satisfied = exact_wolfe | approx_wolfe return is_satisfied",Checks whether the Wolfe or approx Wolfe conditions are satisfied .
"def _secant ( val_a , val_b ) : return ( val_a . x * val_b . df - val_b . x * val_a . df ) / ( val_b . df - val_a . df )",Returns the secant interpolation for the minimum .
"def make_simple_step_size_update_policy ( num_adaptation_steps , target_rate = 0.75 , decrement_multiplier = 0.01 , increment_multiplier = 0.01 , step_counter = None ) : if step_counter is None and num_adaptation_steps is not None : step_counter = tf . compat . v1 . get_variable ( name = 'step_size_adaptation_step_counter' , initializer = np . array ( - 1 , dtype = np . int32 ) , dtype = tf . int32 , trainable = False , use_resource = True ) def step_size_simple_update_fn ( step_size_var , kernel_results ) : """"""Updates (list of) `step_size` using a standard adaptive MCMC procedure.

    Args:
      step_size_var: (List of) `tf.Variable`s representing the per `state_part`
        HMC `step_size`.
      kernel_results: `collections.namedtuple` containing `Tensor`s
        representing values from most recent call to `one_step`.

    Returns:
      step_size_assign: (List of) `Tensor`(s) representing updated
        `step_size_var`(s).
    """""" if kernel_results is None : if mcmc_util . is_list_like ( step_size_var ) : return [ tf . identity ( ss ) for ss in step_size_var ] return tf . identity ( step_size_var ) log_n = tf . math . log ( tf . cast ( tf . size ( input = kernel_results . log_accept_ratio ) , kernel_results . log_accept_ratio . dtype ) ) log_mean_accept_ratio = tf . reduce_logsumexp ( input_tensor = tf . minimum ( kernel_results . log_accept_ratio , 0. ) ) - log_n adjustment = tf . where ( log_mean_accept_ratio < tf . cast ( tf . math . log ( target_rate ) , log_mean_accept_ratio . dtype ) , - decrement_multiplier / ( 1. + decrement_multiplier ) , increment_multiplier ) def build_assign_op ( ) : if mcmc_util . is_list_like ( step_size_var ) : return [ ss . assign_add ( ss * tf . cast ( adjustment , ss . dtype ) ) for ss in step_size_var ] return step_size_var . assign_add ( step_size_var * tf . cast ( adjustment , step_size_var . dtype ) ) if num_adaptation_steps is None : return build_assign_op ( ) else : with tf . control_dependencies ( [ step_counter . assign_add ( 1 ) ] ) : return tf . cond ( pred = step_counter < num_adaptation_steps , true_fn = build_assign_op , false_fn = lambda : step_size_var ) return step_size_simple_update_fn",Create a function implementing a step - size update policy .
"def _leapfrog_integrator_one_step ( target_log_prob_fn , independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts , state_gradients_are_stopped = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'hmc_leapfrog_integrator_one_step' , [ independent_chain_ndims , step_sizes , current_momentum_parts , current_state_parts , current_target_log_prob , current_target_log_prob_grad_parts ] ) : proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( current_momentum_parts , step_sizes , current_target_log_prob_grad_parts ) ] proposed_state_parts = [ x + tf . cast ( eps , v . dtype ) * v for x , eps , v in zip ( current_state_parts , step_sizes , proposed_momentum_parts ) ] if state_gradients_are_stopped : proposed_state_parts = [ tf . stop_gradient ( x ) for x in proposed_state_parts ] [ proposed_target_log_prob , proposed_target_log_prob_grad_parts , ] = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , proposed_state_parts ) if not proposed_target_log_prob . dtype . is_floating : raise TypeError ( '`target_log_prob_fn` must produce a `Tensor` ' 'with `float` `dtype`.' ) if any ( g is None for g in proposed_target_log_prob_grad_parts ) : raise ValueError ( 'Encountered `None` gradient. Does your target `target_log_prob_fn` ' 'access all `tf.Variable`s via `tf.get_variable`?\n' '  current_state_parts: {}\n' '  proposed_state_parts: {}\n' '  proposed_target_log_prob_grad_parts: {}' . format ( current_state_parts , proposed_state_parts , proposed_target_log_prob_grad_parts ) ) proposed_momentum_parts = [ v + 0.5 * tf . cast ( eps , v . dtype ) * g for v , eps , g in zip ( proposed_momentum_parts , step_sizes , proposed_target_log_prob_grad_parts ) ] return [ proposed_momentum_parts , proposed_state_parts , proposed_target_log_prob , proposed_target_log_prob_grad_parts , ]",Applies num_leapfrog_steps of the leapfrog integrator .
"def _compute_log_acceptance_correction ( current_momentums , proposed_momentums , independent_chain_ndims , name = None ) : with tf . compat . v1 . name_scope ( name , 'compute_log_acceptance_correction' , [ independent_chain_ndims , current_momentums , proposed_momentums ] ) : log_current_kinetic , log_proposed_kinetic = [ ] , [ ] for current_momentum , proposed_momentum in zip ( current_momentums , proposed_momentums ) : axis = tf . range ( independent_chain_ndims , tf . rank ( current_momentum ) ) log_current_kinetic . append ( _log_sum_sq ( current_momentum , axis ) ) log_proposed_kinetic . append ( _log_sum_sq ( proposed_momentum , axis ) ) current_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_current_kinetic , axis = - 1 ) , axis = - 1 ) ) proposed_kinetic = 0.5 * tf . exp ( tf . reduce_logsumexp ( input_tensor = tf . stack ( log_proposed_kinetic , axis = - 1 ) , axis = - 1 ) ) return mcmc_util . safe_sum ( [ current_kinetic , - proposed_kinetic ] )",Helper to kernel which computes the log acceptance - correction .
"def _prepare_args ( target_log_prob_fn , state , step_size , target_log_prob = None , grads_target_log_prob = None , maybe_expand = False , state_gradients_are_stopped = False ) : state_parts = list ( state ) if mcmc_util . is_list_like ( state ) else [ state ] state_parts = [ tf . convert_to_tensor ( value = s , name = 'current_state' ) for s in state_parts ] if state_gradients_are_stopped : state_parts = [ tf . stop_gradient ( x ) for x in state_parts ] target_log_prob , grads_target_log_prob = mcmc_util . maybe_call_fn_and_grads ( target_log_prob_fn , state_parts , target_log_prob , grads_target_log_prob ) step_sizes = ( list ( step_size ) if mcmc_util . is_list_like ( step_size ) else [ step_size ] ) step_sizes = [ tf . convert_to_tensor ( value = s , name = 'step_size' , dtype = target_log_prob . dtype ) for s in step_sizes ] if len ( step_sizes ) == 1 : step_sizes *= len ( state_parts ) if len ( state_parts ) != len ( step_sizes ) : raise ValueError ( 'There should be exactly one `step_size` or it should ' 'have same length as `current_state`.' ) def maybe_flatten ( x ) : return x if maybe_expand or mcmc_util . is_list_like ( state ) else x [ 0 ] return [ maybe_flatten ( state_parts ) , maybe_flatten ( step_sizes ) , target_log_prob , grads_target_log_prob , ]",Helper which processes input args to meet list - like assumptions .
"def _log_sum_sq ( x , axis = None ) : return tf . reduce_logsumexp ( input_tensor = 2. * tf . math . log ( tf . abs ( x ) ) , axis = axis )",Computes log ( sum ( x ** 2 )) .
"def one_step ( self , current_state , previous_kernel_results ) : previous_step_size_assign = ( [ ] if self . step_size_update_fn is None else ( previous_kernel_results . extra . step_size_assign if mcmc_util . is_list_like ( previous_kernel_results . extra . step_size_assign ) else [ previous_kernel_results . extra . step_size_assign ] ) ) with tf . control_dependencies ( previous_step_size_assign ) : next_state , kernel_results = self . _impl . one_step ( current_state , previous_kernel_results ) if self . step_size_update_fn is not None : step_size_assign = self . step_size_update_fn ( self . step_size , kernel_results ) kernel_results = kernel_results . _replace ( extra = HamiltonianMonteCarloExtraKernelResults ( step_size_assign = step_size_assign ) ) return next_state , kernel_results",Runs one iteration of Hamiltonian Monte Carlo .
"def bootstrap_results ( self , init_state ) : kernel_results = self . _impl . bootstrap_results ( init_state ) if self . step_size_update_fn is not None : step_size_assign = self . step_size_update_fn ( self . step_size , None ) kernel_results = kernel_results . _replace ( extra = HamiltonianMonteCarloExtraKernelResults ( step_size_assign = step_size_assign ) ) return kernel_results",Creates initial previous_kernel_results using a supplied state .
"def bayesian_resnet ( input_shape , num_classes = 10 , kernel_posterior_scale_mean = - 9.0 , kernel_posterior_scale_stddev = 0.1 , kernel_posterior_scale_constraint = 0.2 ) : filters = [ 64 , 128 , 256 , 512 ] kernels = [ 3 , 3 , 3 , 3 ] strides = [ 1 , 2 , 2 , 2 ] def _untransformed_scale_constraint ( t ) : return tf . clip_by_value ( t , - 1000 , tf . math . log ( kernel_posterior_scale_constraint ) ) kernel_posterior_fn = tfp . layers . default_mean_field_normal_fn ( untransformed_scale_initializer = tf . compat . v1 . initializers . random_normal ( mean = kernel_posterior_scale_mean , stddev = kernel_posterior_scale_stddev ) , untransformed_scale_constraint = _untransformed_scale_constraint ) image = tf . keras . layers . Input ( shape = input_shape , dtype = 'float32' ) x = tfp . layers . Convolution2DFlipout ( 64 , 3 , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( image ) for i in range ( len ( kernels ) ) : x = _resnet_block ( x , filters [ i ] , kernels [ i ] , strides [ i ] , kernel_posterior_fn ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tf . keras . layers . AveragePooling2D ( 4 , 1 ) ( x ) x = tf . keras . layers . Flatten ( ) ( x ) x = tfp . layers . DenseFlipout ( num_classes , kernel_posterior_fn = kernel_posterior_fn ) ( x ) model = tf . keras . Model ( inputs = image , outputs = x , name = 'resnet18' ) return model",Constructs a ResNet18 model .
"def _resnet_block ( x , filters , kernel , stride , kernel_posterior_fn ) : x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) if stride != 1 or filters != x . shape [ 1 ] : shortcut = _projection_shortcut ( x , filters , stride , kernel_posterior_fn ) else : shortcut = x x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = stride , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . BatchNormalization ( ) ( x ) x = tf . keras . layers . Activation ( 'relu' ) ( x ) x = tfp . layers . Convolution2DFlipout ( filters , kernel , strides = 1 , padding = 'same' , kernel_posterior_fn = kernel_posterior_fn ) ( x ) x = tf . keras . layers . add ( [ x , shortcut ] ) return x",Network block for ResNet .
"def make_encoder ( activation , num_topics , layer_sizes ) : encoder_net = tf . keras . Sequential ( ) for num_hidden_units in layer_sizes : encoder_net . add ( tf . keras . layers . Dense ( num_hidden_units , activation = activation , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) encoder_net . add ( tf . keras . layers . Dense ( num_topics , activation = tf . nn . softplus , kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) ) def encoder ( bag_of_words ) : net = _clip_dirichlet_parameters ( encoder_net ( bag_of_words ) ) return tfd . Dirichlet ( concentration = net , name = ""topics_posterior"" ) return encoder",Create the encoder function .
"def make_decoder ( num_topics , num_words ) : topics_words_logits = tf . compat . v1 . get_variable ( ""topics_words_logits"" , shape = [ num_topics , num_words ] , initializer = tf . compat . v1 . glorot_normal_initializer ( ) ) topics_words = tf . nn . softmax ( topics_words_logits , axis = - 1 ) def decoder ( topics ) : word_probs = tf . matmul ( topics , topics_words ) return tfd . OneHotCategorical ( probs = word_probs , name = ""bag_of_words"" ) return decoder , topics_words",Create the decoder function .
"def make_prior ( num_topics , initial_value ) : def _softplus_inverse ( x ) : return np . log ( np . expm1 ( x ) ) logit_concentration = tf . compat . v1 . get_variable ( ""logit_concentration"" , shape = [ 1 , num_topics ] , initializer = tf . compat . v1 . initializers . constant ( _softplus_inverse ( initial_value ) ) ) concentration = _clip_dirichlet_parameters ( tf . nn . softplus ( logit_concentration ) ) def prior ( ) : return tfd . Dirichlet ( concentration = concentration , name = ""topics_prior"" ) prior_variables = [ logit_concentration ] return prior , prior_variables",Create the prior distribution .
"def model_fn ( features , labels , mode , params , config ) : del labels , config encoder = make_encoder ( params [ ""activation"" ] , params [ ""num_topics"" ] , params [ ""layer_sizes"" ] ) decoder , topics_words = make_decoder ( params [ ""num_topics"" ] , features . shape [ 1 ] ) prior , prior_variables = make_prior ( params [ ""num_topics"" ] , params [ ""prior_initial_value"" ] ) topics_prior = prior ( ) alpha = topics_prior . concentration topics_posterior = encoder ( features ) topics = topics_posterior . sample ( ) random_reconstruction = decoder ( topics ) reconstruction = random_reconstruction . log_prob ( features ) tf . compat . v1 . summary . scalar ( ""reconstruction"" , tf . reduce_mean ( input_tensor = reconstruction ) ) kl = tfd . kl_divergence ( topics_posterior , topics_prior ) tf . compat . v1 . summary . scalar ( ""kl"" , tf . reduce_mean ( input_tensor = kl ) ) with tf . control_dependencies ( [ tf . compat . v1 . assert_greater ( kl , - 1e-3 , message = ""kl"" ) ] ) : kl = tf . identity ( kl ) elbo = reconstruction - kl avg_elbo = tf . reduce_mean ( input_tensor = elbo ) tf . compat . v1 . summary . scalar ( ""elbo"" , avg_elbo ) loss = - avg_elbo global_step = tf . compat . v1 . train . get_or_create_global_step ( ) optimizer = tf . compat . v1 . train . AdamOptimizer ( params [ ""learning_rate"" ] ) grads_and_vars = optimizer . compute_gradients ( loss ) grads_and_vars_except_prior = [ x for x in grads_and_vars if x [ 1 ] not in prior_variables ] def train_op_except_prior ( ) : return optimizer . apply_gradients ( grads_and_vars_except_prior , global_step = global_step ) def train_op_all ( ) : return optimizer . apply_gradients ( grads_and_vars , global_step = global_step ) train_op = tf . cond ( pred = global_step < params [ ""prior_burn_in_steps"" ] , true_fn = train_op_except_prior , false_fn = train_op_all ) words_per_document = tf . reduce_sum ( input_tensor = features , axis = 1 ) log_perplexity = - elbo / words_per_document tf . compat . v1 . summary . scalar ( ""perplexity"" , tf . exp ( tf . reduce_mean ( input_tensor = log_perplexity ) ) ) ( log_perplexity_tensor , log_perplexity_update ) = tf . compat . v1 . metrics . mean ( log_perplexity ) perplexity_tensor = tf . exp ( log_perplexity_tensor ) topics = tf . compat . v1 . py_func ( functools . partial ( get_topics_strings , vocabulary = params [ ""vocabulary"" ] ) , [ topics_words , alpha ] , tf . string , stateful = False ) tf . compat . v1 . summary . text ( ""topics"" , topics ) return tf . estimator . EstimatorSpec ( mode = mode , loss = loss , train_op = train_op , eval_metric_ops = { ""elbo"" : tf . compat . v1 . metrics . mean ( elbo ) , ""reconstruction"" : tf . compat . v1 . metrics . mean ( reconstruction ) , ""kl"" : tf . compat . v1 . metrics . mean ( kl ) , ""perplexity"" : ( perplexity_tensor , log_perplexity_update ) , ""topics"" : ( topics , tf . no_op ( ) ) , } , )",Build the model function for use in an estimator .
"def sample_chain ( num_results , current_state , previous_kernel_results = None , kernel = None , num_burnin_steps = 0 , num_steps_between_results = 0 , trace_fn = lambda current_state , kernel_results : kernel_results , return_final_kernel_results = False , parallel_iterations = 10 , name = None , ) : if not kernel . is_calibrated : warnings . warn ( ""supplied `TransitionKernel` is not calibrated. Markov "" ""chain may not converge to intended target distribution."" ) with tf . compat . v1 . name_scope ( name , ""mcmc_sample_chain"" , [ num_results , num_burnin_steps , num_steps_between_results ] ) : num_results = tf . convert_to_tensor ( value = num_results , dtype = tf . int32 , name = ""num_results"" ) num_burnin_steps = tf . convert_to_tensor ( value = num_burnin_steps , dtype = tf . int32 , name = ""num_burnin_steps"" ) num_steps_between_results = tf . convert_to_tensor ( value = num_steps_between_results , dtype = tf . int32 , name = ""num_steps_between_results"" ) current_state = tf . nest . map_structure ( lambda x : tf . convert_to_tensor ( value = x , name = ""current_state"" ) , current_state ) if previous_kernel_results is None : previous_kernel_results = kernel . bootstrap_results ( current_state ) if trace_fn is None : trace_fn = lambda * args : ( ) no_trace = True else : no_trace = False if trace_fn is sample_chain . __defaults__ [ 4 ] : warnings . warn ( ""Tracing all kernel results by default is deprecated. Set "" ""the `trace_fn` argument to None (the future default "" ""value) or an explicit callback that traces the values "" ""you are interested in."" ) def _trace_scan_fn ( state_and_results , num_steps ) : next_state , current_kernel_results = mcmc_util . smart_for_loop ( loop_num_iter = num_steps , body_fn = kernel . one_step , initial_loop_vars = list ( state_and_results ) , parallel_iterations = parallel_iterations ) return next_state , current_kernel_results ( _ , final_kernel_results ) , ( all_states , trace ) = mcmc_util . trace_scan ( loop_fn = _trace_scan_fn , initial_state = ( current_state , previous_kernel_results ) , elems = tf . one_hot ( indices = 0 , depth = num_results , on_value = 1 + num_burnin_steps , off_value = 1 + num_steps_between_results , dtype = tf . int32 ) , trace_fn = lambda state_and_results : ( state_and_results [ 0 ] , trace_fn ( * state_and_results ) ) , parallel_iterations = parallel_iterations ) if return_final_kernel_results : return CheckpointableStatesAndTrace ( all_states = all_states , trace = trace , final_kernel_results = final_kernel_results ) else : if no_trace : return all_states else : return StatesAndTrace ( all_states = all_states , trace = trace )",Implements Markov chain Monte Carlo via repeated TransitionKernel steps .
"def deep_exponential_family ( data_size , feature_size , units , shape ) : w2 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 2 ] , units [ 1 ] ] , name = ""w2"" ) w1 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 1 ] , units [ 0 ] ] , name = ""w1"" ) w0 = ed . Gamma ( 0.1 , 0.3 , sample_shape = [ units [ 0 ] , feature_size ] , name = ""w0"" ) z2 = ed . Gamma ( 0.1 , 0.1 , sample_shape = [ data_size , units [ 2 ] ] , name = ""z2"" ) z1 = ed . Gamma ( shape , shape / tf . matmul ( z2 , w2 ) , name = ""z1"" ) z0 = ed . Gamma ( shape , shape / tf . matmul ( z1 , w1 ) , name = ""z0"" ) x = ed . Poisson ( tf . matmul ( z0 , w0 ) , name = ""x"" ) return x",A multi - layered topic model over a documents - by - terms matrix .
"def trainable_positive_deterministic ( shape , min_loc = 1e-3 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = ""trainable_positive_deterministic"" ) : unconstrained_loc = tf . compat . v1 . get_variable ( ""unconstrained_loc"" , shape ) loc = tf . maximum ( tf . nn . softplus ( unconstrained_loc ) , min_loc ) rv = ed . Deterministic ( loc = loc , name = name ) return rv",Learnable Deterministic distribution over positive reals .
"def trainable_gamma ( shape , min_concentration = 1e-3 , min_scale = 1e-5 , name = None ) : with tf . compat . v1 . variable_scope ( None , default_name = ""trainable_gamma"" ) : unconstrained_concentration = tf . compat . v1 . get_variable ( ""unconstrained_concentration"" , shape , initializer = tf . compat . v1 . initializers . random_normal ( mean = 0.5 , stddev = 0.1 ) ) unconstrained_scale = tf . compat . v1 . get_variable ( ""unconstrained_scale"" , shape , initializer = tf . compat . v1 . initializers . random_normal ( stddev = 0.1 ) ) concentration = tf . maximum ( tf . nn . softplus ( unconstrained_concentration ) , min_concentration ) rate = tf . maximum ( 1. / tf . nn . softplus ( unconstrained_scale ) , 1. / min_scale ) rv = ed . Gamma ( concentration = concentration , rate = rate , name = name ) return rv",Learnable Gamma via concentration and scale parameterization .
"def deep_exponential_family_variational ( data_size , feature_size , units ) : qw2 = trainable_positive_deterministic ( [ units [ 2 ] , units [ 1 ] ] , name = ""qw2"" ) qw1 = trainable_positive_deterministic ( [ units [ 1 ] , units [ 0 ] ] , name = ""qw1"" ) qw0 = trainable_positive_deterministic ( [ units [ 0 ] , feature_size ] , name = ""qw0"" ) qz2 = trainable_gamma ( [ data_size , units [ 2 ] ] , name = ""qz2"" ) qz1 = trainable_gamma ( [ data_size , units [ 1 ] ] , name = ""qz1"" ) qz0 = trainable_gamma ( [ data_size , units [ 0 ] ] , name = ""qz0"" ) return qw2 , qw1 , qw0 , qz2 , qz1 , qz0",Posterior approx . for deep exponential family p ( w { 0 1 2 } z { 1 2 3 } | x ) .
"def load_nips2011_papers ( path ) : path = os . path . expanduser ( path ) filename = ""NIPS_1987-2015.csv"" filepath = os . path . join ( path , filename ) if not os . path . exists ( filepath ) : url = ( ""https://archive.ics.uci.edu/ml/machine-learning-databases/"" ""00371/NIPS_1987-2015.csv"" ) if not tf . io . gfile . exists ( path ) : tf . io . gfile . makedirs ( path ) print ( ""Downloading %s to %s"" % ( url , filepath ) ) urllib . request . urlretrieve ( url , filepath ) with open ( filepath ) as f : iterator = csv . reader ( f ) documents = next ( iterator ) [ 1 : ] words = [ ] x_train = [ ] for row in iterator : words . append ( row [ 0 ] ) x_train . append ( row [ 1 : ] ) x_train = np . array ( x_train , dtype = np . int ) doc_idx = [ i for i , document in enumerate ( documents ) if document . startswith ( ""2011"" ) ] documents = [ documents [ doc ] for doc in doc_idx ] x_train = x_train [ : , doc_idx ] word_idx = np . logical_and ( np . sum ( x_train != 0 , 1 ) >= 2 , np . sum ( x_train , 1 ) >= 10 ) words = [ word for word , idx in zip ( words , word_idx ) if idx ] bag_of_words = x_train [ word_idx , : ] . T return bag_of_words , words",Loads NIPS 2011 conference papers .
"def _init_params ( self , amplitude , length_scale , validate_args ) : dtype = util . maybe_get_common_dtype ( [ amplitude , length_scale ] ) if amplitude is not None : amplitude = tf . convert_to_tensor ( value = amplitude , name = 'amplitude' , dtype = dtype ) self . _amplitude = _validate_arg_if_not_none ( amplitude , tf . compat . v1 . assert_positive , validate_args ) if length_scale is not None : length_scale = tf . convert_to_tensor ( value = length_scale , name = 'length_scale' , dtype = dtype ) self . _length_scale = _validate_arg_if_not_none ( length_scale , tf . compat . v1 . assert_positive , validate_args ) return dtype",Shared init logic for amplitude and length_scale params .
"def _registered_kl ( type_a , type_b ) : hierarchy_a = tf_inspect . getmro ( type_a ) hierarchy_b = tf_inspect . getmro ( type_b ) dist_to_children = None kl_fn = None for mro_to_a , parent_a in enumerate ( hierarchy_a ) : for mro_to_b , parent_b in enumerate ( hierarchy_b ) : candidate_dist = mro_to_a + mro_to_b candidate_kl_fn = _DIVERGENCES . get ( ( parent_a , parent_b ) , None ) if not kl_fn or ( candidate_kl_fn and candidate_dist < dist_to_children ) : dist_to_children = candidate_dist kl_fn = candidate_kl_fn return kl_fn",Get the KL function registered for classes a and b .
"def kl_divergence ( distribution_a , distribution_b , allow_nan_stats = True , name = None ) : kl_fn = _registered_kl ( type ( distribution_a ) , type ( distribution_b ) ) if kl_fn is None : raise NotImplementedError ( ""No KL(distribution_a || distribution_b) registered for distribution_a "" ""type {} and distribution_b type {}"" . format ( type ( distribution_a ) . __name__ , type ( distribution_b ) . __name__ ) ) with tf . name_scope ( ""KullbackLeibler"" ) : kl_t = kl_fn ( distribution_a , distribution_b , name = name ) if allow_nan_stats : return kl_t kl_t = tf . identity ( kl_t , name = ""kl"" ) with tf . control_dependencies ( [ tf . Assert ( tf . logical_not ( tf . reduce_any ( input_tensor = tf . math . is_nan ( kl_t ) ) ) , [ ( ""KL calculation between {} and {} returned NaN values "" ""(and was called with allow_nan_stats=False). Values:"" . format ( distribution_a . name , distribution_b . name ) ) , kl_t ] ) ] ) : return tf . identity ( kl_t , name = ""checked_kl"" )",Get the KL - divergence KL ( distribution_a || distribution_b ) .
"def cross_entropy ( ref , other , allow_nan_stats = True , name = None ) : with tf . name_scope ( name or ""cross_entropy"" ) : return ref . entropy ( ) + kl_divergence ( ref , other , allow_nan_stats = allow_nan_stats )",Computes the ( Shannon ) cross entropy .
"def read_image ( filepath ) : im_bytes = tf . io . read_file ( filepath ) im = tf . image . decode_image ( im_bytes , channels = CHANNELS ) im = tf . image . convert_image_dtype ( im , tf . float32 ) return im",Returns an image tensor .
"def download_sprites ( ) : filepath = os . path . join ( FLAGS . data_dir , DATA_SPRITES_DIR ) if not tf . io . gfile . exists ( filepath ) : if not tf . io . gfile . exists ( FLAGS . data_dir ) : tf . io . gfile . makedirs ( FLAGS . data_dir ) zip_name = ""{}.zip"" . format ( filepath ) urllib . request . urlretrieve ( DATA_SPRITES_URL , zip_name ) with zipfile . ZipFile ( zip_name , ""r"" ) as zip_file : zip_file . extractall ( FLAGS . data_dir ) tf . io . gfile . remove ( zip_name ) return filepath",Downloads the sprites data and returns the saved filepath .
"def create_character ( skin , hair , top , pants ) : dtype = skin . dtype hair_mask = tf . cast ( hair [ ... , - 1 : ] <= 0 , dtype ) top_mask = tf . cast ( top [ ... , - 1 : ] <= 0 , dtype ) pants_mask = tf . cast ( pants [ ... , - 1 : ] <= 0 , dtype ) char = ( skin * hair_mask ) + hair char = ( char * top_mask ) + top char = ( char * pants_mask ) + pants return char",Creates a character sprite from a set of attribute sprites .
"def create_seq ( character , action_metadata , direction , length = 8 , start = 0 ) : sprite_start = ( action_metadata [ 0 ] + direction ) * FRAME_SIZE sprite_end = ( action_metadata [ 0 ] + direction + 1 ) * FRAME_SIZE sprite_line = character [ sprite_start : sprite_end , ... ] frames = tf . stack ( tf . split ( sprite_line , 13 , axis = 1 ) ) frames = frames [ 0 : action_metadata [ 1 ] ] frames = tf . roll ( frames , shift = - start , axis = 0 ) frames = tf . tile ( frames , [ 2 , 1 , 1 , 1 ] ) frames = frames [ : length ] frames = tf . cast ( frames , dtype = tf . float32 ) frames . set_shape ( [ length , FRAME_SIZE , FRAME_SIZE , CHANNELS ] ) return frames",Creates a sequence .
"def create_random_seq ( character , action_metadata , direction , length = 8 ) : start = tf . random . uniform ( [ ] , maxval = action_metadata [ 1 ] , dtype = tf . int32 ) return create_seq ( character , action_metadata , direction , length , start )",Creates a random sequence .
"def create_sprites_dataset ( characters , actions , directions , channels = 3 , length = 8 , shuffle = False , fake_data = False ) : if fake_data : dummy_image = tf . random . normal ( [ HEIGHT , WIDTH , CHANNELS ] ) else : basedir = download_sprites ( ) action_names = [ action . name for action in actions ] action_metadata = [ ( action . start_row , action . frames ) for action in actions ] direction_rows = [ direction . row_offset for direction in directions ] chars = tf . data . Dataset . from_tensor_slices ( characters ) act_names = tf . data . Dataset . from_tensor_slices ( action_names ) . repeat ( ) acts_metadata = tf . data . Dataset . from_tensor_slices ( action_metadata ) . repeat ( ) dir_rows = tf . data . Dataset . from_tensor_slices ( direction_rows ) . repeat ( ) if shuffle : chars = chars . shuffle ( len ( characters ) ) dataset = tf . data . Dataset . zip ( ( chars , act_names , acts_metadata , dir_rows ) ) skin_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( SKIN_COLORS ) ) hair_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( HAIRSTYLES ) ) top_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( TOPS ) ) pants_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( PANTS ) ) action_table = tf . contrib . lookup . index_table_from_tensor ( sorted ( action_names ) ) def process_example ( attrs , act_name , act_metadata , dir_row_offset ) : """"""Processes a dataset row."""""" skin_name = attrs [ 0 ] hair_name = attrs [ 1 ] top_name = attrs [ 2 ] pants_name = attrs [ 3 ] if fake_data : char = dummy_image else : skin = read_image ( basedir + os . sep + skin_name ) hair = read_image ( basedir + os . sep + hair_name ) top = read_image ( basedir + os . sep + top_name ) pants = read_image ( basedir + os . sep + pants_name ) char = create_character ( skin , hair , top , pants ) if shuffle : seq = create_random_seq ( char , act_metadata , dir_row_offset , length ) else : seq = create_seq ( char , act_metadata , dir_row_offset , length ) seq = seq [ ... , : channels ] skin_idx = skin_table . lookup ( skin_name ) hair_idx = hair_table . lookup ( hair_name ) top_idx = top_table . lookup ( top_name ) pants_idx = pants_table . lookup ( pants_name ) act_idx = action_table . lookup ( act_name ) return ( seq , skin_idx , hair_idx , top_idx , pants_idx , act_idx , skin_name , hair_name , top_name , pants_name , act_name ) dataset = dataset . map ( process_example ) return dataset",Creates a tf . data pipeline for the sprites dataset .
"def _maybe_validate_distributions ( distributions , dtype_override , validate_args ) : assertions = [ ] if not _is_iterable ( distributions ) or not distributions : raise ValueError ( '`distributions` must be a list of one or more ' 'distributions.' ) if dtype_override is None : dts = [ dtype_util . base_dtype ( d . dtype ) for d in distributions if d . dtype is not None ] if dts [ 1 : ] != dts [ : - 1 ] : raise TypeError ( 'Distributions must have same dtype; found: {}.' . format ( set ( dtype_util . name ( dt ) for dt in dts ) ) ) for d in distributions : if tensorshape_util . rank ( d . event_shape ) is not None : if tensorshape_util . rank ( d . event_shape ) != 1 : raise ValueError ( '`Distribution` must be vector variate, ' 'found event nimds: {}.' . format ( tensorshape_util . rank ( d . event_shape ) ) ) elif validate_args : assertions . append ( assert_util . assert_equal ( 1 , tf . size ( input = d . event_shape_tensor ( ) ) , message = '`Distribution` must be vector variate.' ) ) batch_shapes = [ d . batch_shape for d in distributions ] if all ( tensorshape_util . is_fully_defined ( b ) for b in batch_shapes ) : if batch_shapes [ 1 : ] != batch_shapes [ : - 1 ] : raise ValueError ( 'Distributions must have the same `batch_shape`; ' 'found: {}.' . format ( batch_shapes ) ) elif validate_args : batch_shapes = [ tensorshape_util . as_list ( d . batch_shape ) if tensorshape_util . is_fully_defined ( d . batch_shape ) else d . batch_shape_tensor ( ) for d in distributions ] assertions . extend ( assert_util . assert_equal ( b1 , b2 , message = 'Distribution `batch_shape`s must be identical.' ) for b1 , b2 in zip ( batch_shapes [ 1 : ] , batch_shapes [ : - 1 ] ) ) return assertions",Checks that distributions satisfies all assumptions .
"def _kl_blockwise_blockwise ( b0 , b1 , name = None ) : if len ( b0 . distributions ) != len ( b1 . distributions ) : raise ValueError ( 'Can only compute KL divergence between Blockwise distributions with ' 'the same number of component distributions.' ) b0_event_sizes = [ _event_size ( d ) for d in b0 . distributions ] b1_event_sizes = [ _event_size ( d ) for d in b1 . distributions ] assertions = [ ] message = ( 'Can only compute KL divergence between Blockwise distributions ' 'with the same pairwise event shapes.' ) if ( all ( isinstance ( event_size , int ) for event_size in b0_event_sizes ) and all ( isinstance ( event_size , int ) for event_size in b1_event_sizes ) ) : if b0_event_sizes != b1_event_sizes : raise ValueError ( message ) else : if b0 . validate_args or b1 . validate_args : assertions . extend ( assert_util . assert_equal ( e1 , e2 , message = message ) for e1 , e2 in zip ( b0_event_sizes , b1_event_sizes ) ) with tf . name_scope ( name or 'kl_blockwise_blockwise' ) : with tf . control_dependencies ( assertions ) : return sum ( [ kullback_leibler . kl_divergence ( d1 , d2 ) for d1 , d2 in zip ( b0 . distributions , b1 . distributions ) ] )",Calculate the batched KL divergence KL ( b0 || b1 ) with b0 and b1 Blockwise distributions .
"def _kl_half_normal_half_normal ( a , b , name = None ) : with tf . name_scope ( name or ""kl_half_normal_half_normal"" ) : return ( tf . math . log ( b . scale ) - tf . math . log ( a . scale ) + ( a . scale ** 2 - b . scale ** 2 ) / ( 2 * b . scale ** 2 ) )",Calculate the batched KL divergence KL ( a || b ) with a and b HalfNormal .
"def _flatten_summand_list ( kernels ) : flattened = [ ] for k in kernels : if isinstance ( k , _SumKernel ) : flattened += k . kernels else : flattened . append ( k ) return flattened",Flatten a list of kernels which may contain _SumKernel instances .
"def _flatten_multiplicand_list ( kernels ) : flattened = [ ] for k in kernels : if isinstance ( k , _ProductKernel ) : flattened += k . kernels else : flattened . append ( k ) return flattened",Flatten a list of kernels which may contain _ProductKernel instances .
"def build_input_pipeline ( x_train , x_test , y_train , y_test , batch_size , valid_size ) : x_train = x_train . astype ( ""float32"" ) x_test = x_test . astype ( ""float32"" ) x_train /= 255 x_test /= 255 y_train = y_train . flatten ( ) y_test = y_test . flatten ( ) if FLAGS . subtract_pixel_mean : x_train_mean = np . mean ( x_train , axis = 0 ) x_train -= x_train_mean x_test -= x_train_mean print ( ""x_train shape:"" + str ( x_train . shape ) ) print ( str ( x_train . shape [ 0 ] ) + "" train samples"" ) print ( str ( x_test . shape [ 0 ] ) + "" test samples"" ) training_dataset = tf . data . Dataset . from_tensor_slices ( ( x_train , np . int32 ( y_train ) ) ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) heldout_dataset = tf . data . Dataset . from_tensor_slices ( ( x_test , np . int32 ( y_test ) ) ) heldout_batches = heldout_dataset . repeat ( ) . batch ( valid_size ) heldout_iterator = tf . compat . v1 . data . make_one_shot_iterator ( heldout_batches ) handle = tf . compat . v1 . placeholder ( tf . string , shape = [ ] ) feedable_iterator = tf . compat . v1 . data . Iterator . from_string_handle ( handle , training_batches . output_types , training_batches . output_shapes ) images , labels = feedable_iterator . get_next ( ) return images , labels , handle , training_iterator , heldout_iterator",Build an Iterator switching between train and heldout data .
"def build_fake_data ( ) : num_examples = 10 x_train = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_train = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) x_test = np . random . rand ( num_examples , * IMAGE_SHAPE ) . astype ( np . float32 ) y_test = np . random . permutation ( np . arange ( num_examples ) ) . astype ( np . int32 ) return ( x_train , y_train ) , ( x_test , y_test )",Build fake CIFAR10 - style data for unit testing .
"def count_integers ( arr , weights = None , minlength = None , maxlength = None , axis = None , dtype = tf . int32 , name = None ) : with tf . compat . v1 . name_scope ( name , 'count_integers' , values = [ arr , weights , minlength , maxlength , axis ] ) : if axis is None : return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) arr = tf . convert_to_tensor ( value = arr , dtype = tf . int32 , name = 'arr' ) arr_ndims = _get_static_ndims ( arr , expect_static = True ) axis = _make_static_axis_non_negative_list ( axis , arr_ndims ) not_axis = sorted ( set ( range ( arr_ndims ) ) . difference ( axis ) ) if not not_axis : return tf . math . bincount ( arr , weights = weights , minlength = minlength , maxlength = maxlength , dtype = dtype ) flat_arr = _move_dims_to_flat_end ( arr , not_axis , arr_ndims , right_end = False ) if weights is None : def one_bincount ( arr_slice ) : return tf . math . bincount ( arr_slice , weights = None , minlength = minlength , maxlength = maxlength , dtype = dtype ) flat_counts = tf . map_fn ( one_bincount , elems = flat_arr , dtype = dtype ) else : weights = tf . convert_to_tensor ( value = weights , name = 'weights' ) _get_static_ndims ( weights , expect_static = True , expect_ndims = arr_ndims ) flat_weights = _move_dims_to_flat_end ( weights , not_axis , arr_ndims , right_end = False ) def one_bincount ( arr_and_weights_slices ) : arr_slice , weights_slice = arr_and_weights_slices return tf . math . bincount ( arr_slice , weights = weights_slice , minlength = minlength , maxlength = maxlength , dtype = dtype ) flat_counts = tf . map_fn ( one_bincount , elems = [ flat_arr , flat_weights ] , dtype = weights . dtype ) flat_counts_t = tf . transpose ( a = flat_counts , perm = [ 1 , 0 ] ) _get_static_ndims ( flat_counts_t , expect_ndims = 2 , expect_static = True ) not_axis_shape = tf . gather ( tf . shape ( input = arr ) , indices = not_axis ) out_shape = tf . concat ( [ [ - 1 ] , not_axis_shape ] , axis = 0 ) return tf . reshape ( flat_counts_t , out_shape )",Counts the number of occurrences of each value in an integer array arr .
"def find_bins ( x , edges , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : with tf . compat . v1 . name_scope ( name , default_name = 'find_bins' , values = [ x , edges ] ) : in_type = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_type ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_type ) if ( tf . compat . dimension_value ( edges . shape [ 0 ] ) is not None and tf . compat . dimension_value ( edges . shape [ 0 ] ) < 2 ) : raise ValueError ( 'First dimension of `edges` must have length > 1 to index 1 or ' 'more bin. Found: {}' . format ( edges . shape ) ) flattening_x = edges . shape . ndims == 1 and x . shape . ndims > 1 if flattening_x : x_orig_shape = tf . shape ( input = x ) x = tf . reshape ( x , [ - 1 ] ) if dtype is None : dtype = in_type dtype = tf . as_dtype ( dtype ) x_permed = distribution_util . rotate_transpose ( x , shift = - 1 ) edges_permed = distribution_util . rotate_transpose ( edges , shift = - 1 ) searchsorted_type = dtype if dtype in [ tf . int32 , tf . int64 ] else None almost_output_permed = tf . searchsorted ( sorted_sequence = edges_permed , values = x_permed , side = 'right' , out_type = searchsorted_type ) almost_output = tf . cast ( distribution_util . rotate_transpose ( almost_output_permed , shift = 1 ) , dtype ) bins = tf . clip_by_value ( almost_output - 1 , tf . cast ( 0 , dtype ) , tf . cast ( tf . shape ( input = edges ) [ 0 ] - 2 , dtype ) ) if not extend_lower_interval : low_fill = np . nan if dtype . is_floating else - 1 bins = tf . where ( x < tf . expand_dims ( edges [ 0 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( low_fill , dtype ) ) , bins ) if not extend_upper_interval : up_fill = np . nan if dtype . is_floating else tf . shape ( input = edges ) [ 0 ] - 1 bins = tf . where ( x > tf . expand_dims ( edges [ - 1 ] , 0 ) , tf . fill ( tf . shape ( input = x ) , tf . cast ( up_fill , dtype ) ) , bins ) if flattening_x : bins = tf . reshape ( bins , x_orig_shape ) return bins",Bin values into discrete intervals .
"def histogram ( x , edges , axis = None , extend_lower_interval = False , extend_upper_interval = False , dtype = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'histogram' , values = [ x , edges , axis ] ) : in_dtype = dtype_util . common_dtype ( [ x , edges ] , preferred_dtype = tf . float32 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = in_dtype ) edges = tf . convert_to_tensor ( value = edges , name = 'edges' , dtype = in_dtype ) if axis is None : x = tf . reshape ( x , shape = [ - 1 ] ) else : x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) axis = _make_static_axis_non_negative_list ( axis , x_ndims ) if not axis : raise ValueError ( '`axis` cannot be empty.  Found: {}' . format ( axis ) ) x = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = False ) bins = find_bins ( x , edges = edges , extend_lower_interval = extend_lower_interval , extend_upper_interval = extend_upper_interval , dtype = tf . int32 ) counts = count_integers ( bins , minlength = tf . shape ( input = edges ) [ 0 ] - 1 , maxlength = tf . shape ( input = edges ) [ 0 ] - 1 , axis = 0 , dtype = dtype or in_dtype ) n_edges = tf . compat . dimension_value ( edges . shape [ 0 ] ) if n_edges is not None : counts . set_shape ( tf . TensorShape ( [ n_edges - 1 ] ) . concatenate ( counts . shape [ 1 : ] ) ) return counts",Count how often x falls in intervals defined by edges .
"def percentile ( x , q , axis = None , interpolation = None , keep_dims = False , validate_args = False , preserve_gradients = True , name = None ) : name = name or 'percentile' allowed_interpolations = { 'linear' , 'lower' , 'higher' , 'nearest' , 'midpoint' } if interpolation is None : interpolation = 'nearest' else : if interpolation not in allowed_interpolations : raise ValueError ( 'Argument `interpolation` must be in %s.  Found %s' % ( allowed_interpolations , interpolation ) ) with tf . compat . v1 . name_scope ( name , values = [ x , q ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if interpolation in { 'linear' , 'midpoint' } and x . dtype . is_integer : raise TypeError ( '{} interpolation not allowed with dtype {}' . format ( interpolation , x . dtype ) ) q = tf . cast ( q , tf . float64 ) _get_static_ndims ( q , expect_ndims_no_more_than = 1 ) if validate_args : q = distribution_util . with_dependencies ( [ tf . compat . v1 . assert_rank_in ( q , [ 0 , 1 ] ) , tf . compat . v1 . assert_greater_equal ( q , tf . cast ( 0. , tf . float64 ) ) , tf . compat . v1 . assert_less_equal ( q , tf . cast ( 100. , tf . float64 ) ) ] , q ) if axis is None : y = tf . reshape ( x , [ - 1 ] ) else : x_ndims = _get_static_ndims ( x , expect_static = True , expect_ndims_at_least = 1 ) axis = _make_static_axis_non_negative_list ( axis , x_ndims ) y = _move_dims_to_flat_end ( x , axis , x_ndims , right_end = True ) frac_at_q_or_above = 1. - q / 100. sorted_y = _sort_tensor ( y ) d = tf . cast ( tf . shape ( input = y ) [ - 1 ] , tf . float64 ) def _get_indices ( interp_type ) : """"""Get values of y at the indices implied by interp_type."""""" if interp_type == 'lower' : indices = tf . math . ceil ( ( d - 1 ) * frac_at_q_or_above ) elif interp_type == 'higher' : indices = tf . floor ( ( d - 1 ) * frac_at_q_or_above ) elif interp_type == 'nearest' : indices = tf . round ( ( d - 1 ) * frac_at_q_or_above ) return tf . clip_by_value ( tf . cast ( indices , tf . int32 ) , 0 , tf . shape ( input = y ) [ - 1 ] - 1 ) if interpolation in [ 'nearest' , 'lower' , 'higher' ] : gathered_y = tf . gather ( sorted_y , _get_indices ( interpolation ) , axis = - 1 ) elif interpolation == 'midpoint' : gathered_y = 0.5 * ( tf . gather ( sorted_y , _get_indices ( 'lower' ) , axis = - 1 ) + tf . gather ( sorted_y , _get_indices ( 'higher' ) , axis = - 1 ) ) elif interpolation == 'linear' : larger_y_idx = _get_indices ( 'lower' ) exact_idx = ( d - 1 ) * frac_at_q_or_above if preserve_gradients : smaller_y_idx = tf . maximum ( larger_y_idx - 1 , 0 ) larger_y_idx = tf . minimum ( smaller_y_idx + 1 , tf . shape ( input = y ) [ - 1 ] - 1 ) fraction = tf . cast ( larger_y_idx , tf . float64 ) - exact_idx else : smaller_y_idx = _get_indices ( 'higher' ) fraction = tf . math . ceil ( ( d - 1 ) * frac_at_q_or_above ) - exact_idx fraction = tf . cast ( fraction , y . dtype ) gathered_y = ( tf . gather ( sorted_y , larger_y_idx , axis = - 1 ) * ( 1 - fraction ) + tf . gather ( sorted_y , smaller_y_idx , axis = - 1 ) * fraction ) if x . dtype in ( tf . bfloat16 , tf . float16 , tf . float32 , tf . float64 ) : nan_batch_members = tf . reduce_any ( input_tensor = tf . math . is_nan ( x ) , axis = axis ) right_rank_matched_shape = tf . pad ( tensor = tf . shape ( input = nan_batch_members ) , paddings = [ [ 0 , tf . rank ( input = q ) ] ] , constant_values = 1 ) nan_batch_members = tf . reshape ( nan_batch_members , shape = right_rank_matched_shape ) shape_gathered_y = tf . shape ( input = gathered_y ) nan = np . array ( np . nan , gathered_y . dtype . as_numpy_dtype ) gathered_y = tf . where ( tf . broadcast_to ( nan_batch_members , shape_gathered_y ) , tf . fill ( shape_gathered_y , nan ) , gathered_y ) if keep_dims : if axis is None : ones_vec = tf . ones ( shape = [ _get_best_effort_ndims ( x ) + _get_best_effort_ndims ( q ) ] , dtype = tf . int32 ) gathered_y *= tf . ones ( ones_vec , dtype = x . dtype ) else : gathered_y = _insert_back_keep_dims ( gathered_y , axis ) return distribution_util . rotate_transpose ( gathered_y , tf . rank ( q ) )",Compute the q - th percentile ( s ) of x .
"def quantiles ( x , num_quantiles , axis = None , interpolation = None , keep_dims = False , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'quantiles' , values = [ x , num_quantiles , axis ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) return percentile ( x , q = tf . linspace ( tf . convert_to_tensor ( value = 0 , dtype = tf . float64 ) , tf . convert_to_tensor ( value = 100 , dtype = tf . float64 ) , num = num_quantiles + 1 ) , axis = axis , interpolation = interpolation , keep_dims = keep_dims , validate_args = validate_args , preserve_gradients = False )",Compute quantiles of x along axis .
"def _get_static_ndims ( x , expect_static = False , expect_ndims = None , expect_ndims_no_more_than = None , expect_ndims_at_least = None ) : ndims = x . shape . ndims if ndims is None : shape_const = tf . get_static_value ( tf . shape ( input = x ) ) if shape_const is not None : ndims = shape_const . ndim if ndims is None : if expect_static : raise ValueError ( 'Expected argument `x` to have statically defined `ndims`.  Found: ' % x ) return if expect_ndims is not None : ndims_message = ( 'Expected argument `x` to have ndims %s.  Found tensor %s' % ( expect_ndims , x ) ) if ndims != expect_ndims : raise ValueError ( ndims_message ) if expect_ndims_at_least is not None : ndims_at_least_message = ( 'Expected argument `x` to have ndims >= %d.  Found tensor %s' % ( expect_ndims_at_least , x ) ) if ndims < expect_ndims_at_least : raise ValueError ( ndims_at_least_message ) if expect_ndims_no_more_than is not None : ndims_no_more_than_message = ( 'Expected argument `x` to have ndims <= %d.  Found tensor %s' % ( expect_ndims_no_more_than , x ) ) if ndims > expect_ndims_no_more_than : raise ValueError ( ndims_no_more_than_message ) return ndims",Get static number of dimensions and assert that some expectations are met .
"def _get_best_effort_ndims ( x , expect_ndims = None , expect_ndims_at_least = None , expect_ndims_no_more_than = None ) : ndims_static = _get_static_ndims ( x , expect_ndims = expect_ndims , expect_ndims_at_least = expect_ndims_at_least , expect_ndims_no_more_than = expect_ndims_no_more_than ) if ndims_static is not None : return ndims_static return tf . rank ( x )",Get static ndims if possible . Fallback on tf . rank ( x ) .
"def _insert_back_keep_dims ( x , axis ) : for i in sorted ( axis ) : x = tf . expand_dims ( x , axis = i ) return x",Insert the dims in axis back as singletons after being removed .
"def _make_static_axis_non_negative_list ( axis , ndims ) : axis = distribution_util . make_non_negative_axis ( axis , ndims ) axis_const = tf . get_static_value ( axis ) if axis_const is None : raise ValueError ( 'Expected argument `axis` to be statically available.  Found: %s' % axis ) axis = axis_const + np . zeros ( [ 1 ] , dtype = axis_const . dtype ) return list ( int ( dim ) for dim in axis )",Convert possibly negatively indexed axis to non - negative list of ints .
"def _move_dims_to_flat_end ( x , axis , x_ndims , right_end = True ) : if not axis : return x other_dims = sorted ( set ( range ( x_ndims ) ) . difference ( axis ) ) perm = other_dims + list ( axis ) if right_end else list ( axis ) + other_dims x_permed = tf . transpose ( a = x , perm = perm ) if x . shape . is_fully_defined ( ) : x_shape = x . shape . as_list ( ) other_shape = [ x_shape [ i ] for i in other_dims ] end_shape = [ np . prod ( [ x_shape [ i ] for i in axis ] ) ] full_shape = ( other_shape + end_shape if right_end else end_shape + other_shape ) else : other_shape = tf . gather ( tf . shape ( input = x ) , other_dims ) full_shape = tf . concat ( [ other_shape , [ - 1 ] ] if right_end else [ [ - 1 ] , other_shape ] , axis = 0 ) return tf . reshape ( x_permed , shape = full_shape )",Move dims corresponding to axis in x to the end then flatten .
"def _sort_tensor ( tensor ) : sorted_ , _ = tf . nn . top_k ( tensor , k = tf . shape ( input = tensor ) [ - 1 ] ) sorted_ . set_shape ( tensor . shape ) return sorted_",Use top_k to sort a Tensor along the last dimension .
"def make_component_state_space_models ( self , num_timesteps , param_vals , initial_step = 0 ) : with tf . compat . v1 . name_scope ( 'make_component_state_space_models' ) : param_map = self . _canonicalize_param_vals_as_map ( param_vals ) param_vals_list = [ param_map [ p . name ] for p in self . parameters ] remaining_param_vals = param_vals_list [ 1 : ] component_ssms = [ ] for component in self . components : num_parameters = len ( component . parameters ) component_param_vals = remaining_param_vals [ : num_parameters ] remaining_param_vals = remaining_param_vals [ num_parameters : ] component_ssms . append ( component . make_state_space_model ( num_timesteps , param_vals = component_param_vals , initial_step = initial_step ) ) return component_ssms",Build an ordered list of Distribution instances for component models .
"def amari_alpha ( logu , alpha = 1. , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , ""amari_alpha"" , [ logu ] ) : if alpha is None or tf . is_tensor ( alpha ) : raise TypeError ( ""`alpha` cannot be `None` or `Tensor` type."" ) if ( self_normalized is None or tf . is_tensor ( self_normalized ) ) : raise TypeError ( ""`self_normalized` cannot be `None` or `Tensor` type."" ) logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) if alpha == 0. : f = - logu elif alpha == 1. : f = tf . exp ( logu ) * logu else : f = tf . math . expm1 ( alpha * logu ) / ( alpha * ( alpha - 1. ) ) if not self_normalized : return f if alpha == 0. : return f + tf . math . expm1 ( logu ) elif alpha == 1. : return f - tf . math . expm1 ( logu ) else : return f - tf . math . expm1 ( logu ) / ( alpha - 1. )",The Amari - alpha Csiszar - function in log - space .
"def kl_reverse ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , ""kl_reverse"" , [ logu ] ) : return amari_alpha ( logu , alpha = 0. , self_normalized = self_normalized )",The reverse Kullback - Leibler Csiszar - function in log - space .
"def jensen_shannon ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , ""jensen_shannon"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) npdt = logu . dtype . as_numpy_dtype y = tf . nn . softplus ( logu ) if self_normalized : y -= np . log ( 2 ) . astype ( npdt ) return tf . exp ( logu ) * logu - ( 1. + tf . exp ( logu ) ) * y",The Jensen - Shannon Csiszar - function in log - space .
"def pearson ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""pearson"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return tf . square ( tf . math . expm1 ( logu ) )",The Pearson Csiszar - function in log - space .
"def squared_hellinger ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""squared_hellinger"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return pearson ( 0.5 * logu )",The Squared - Hellinger Csiszar - function in log - space .
"def triangular ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""triangular"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return pearson ( logu ) / ( 1. + tf . exp ( logu ) )",The Triangular Csiszar - function in log - space .
"def t_power ( logu , t , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , ""t_power"" , [ logu , t ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) t = tf . convert_to_tensor ( value = t , dtype = logu . dtype . base_dtype , name = ""t"" ) fu = tf . math . expm1 ( t * logu ) if self_normalized : fu -= t * tf . math . expm1 ( logu ) fu *= tf . where ( tf . logical_and ( 0. < t , t < 1. ) , - tf . ones_like ( t ) , tf . ones_like ( t ) ) return fu",The T - Power Csiszar - function in log - space .
"def log1p_abs ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""log1p_abs"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return tf . math . expm1 ( tf . abs ( logu ) )",The log1p - abs Csiszar - function in log - space .
"def jeffreys ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""jeffreys"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return 0.5 * tf . math . expm1 ( logu ) * logu",The Jeffreys Csiszar - function in log - space .
"def modified_gan ( logu , self_normalized = False , name = None ) : with tf . compat . v1 . name_scope ( name , ""chi_square"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) y = tf . nn . softplus ( logu ) - logu if self_normalized : y += 0.5 * tf . math . expm1 ( logu ) return y",The Modified - GAN Csiszar - function in log - space .
"def dual_csiszar_function ( logu , csiszar_function , name = None ) : with tf . compat . v1 . name_scope ( name , ""dual_csiszar_function"" , [ logu ] ) : return tf . exp ( logu ) * csiszar_function ( - logu )",Calculates the dual Csiszar - function in log - space .
"def symmetrized_csiszar_function ( logu , csiszar_function , name = None ) : with tf . compat . v1 . name_scope ( name , ""symmetrized_csiszar_function"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) return 0.5 * ( csiszar_function ( logu ) + dual_csiszar_function ( logu , csiszar_function ) )",Symmetrizes a Csiszar - function in log - space .
"def monte_carlo_csiszar_f_divergence ( f , p_log_prob , q , num_draws , use_reparametrization = None , seed = None , name = None ) : reparameterization_types = tf . nest . flatten ( q . reparameterization_type ) with tf . compat . v1 . name_scope ( name , ""monte_carlo_csiszar_f_divergence"" , [ num_draws ] ) : if use_reparametrization is None : use_reparametrization = all ( reparameterization_type == tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) elif ( use_reparametrization and any ( reparameterization_type != tfd . FULLY_REPARAMETERIZED for reparameterization_type in reparameterization_types ) ) : raise ValueError ( ""Distribution `q` must be reparameterized, i.e., a diffeomorphic "" ""transformation of a parameterless distribution. (Otherwise this "" ""function has a biased gradient.)"" ) if not callable ( p_log_prob ) : raise TypeError ( ""`p_log_prob` must be a Python `callable` function."" ) return monte_carlo . expectation ( f = lambda q_samples : f ( p_log_prob ( q_samples ) - q . log_prob ( q_samples ) ) , samples = q . sample ( num_draws , seed = seed ) , log_prob = q . log_prob , use_reparametrization = use_reparametrization )",Monte - Carlo approximation of the Csiszar f - Divergence .
"def csiszar_vimco ( f , p_log_prob , q , num_draws , num_batch_draws = 1 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , ""csiszar_vimco"" , [ num_draws , num_batch_draws ] ) : if num_draws < 2 : raise ValueError ( ""Must specify num_draws > 1."" ) stop = tf . stop_gradient x = stop ( q . sample ( sample_shape = [ num_draws , num_batch_draws ] , seed = seed ) ) logqx = q . log_prob ( x ) logu = p_log_prob ( x ) - logqx f_log_avg_u , f_log_sooavg_u = [ f ( r ) for r in csiszar_vimco_helper ( logu ) ] dotprod = tf . reduce_sum ( input_tensor = logqx * stop ( f_log_avg_u - f_log_sooavg_u ) , axis = 0 ) f_log_avg_u += dotprod - stop ( dotprod ) return tf . reduce_mean ( input_tensor = f_log_avg_u , axis = 0 )",Use VIMCO to lower the variance of gradient [ csiszar_function ( Avg ( logu )) ] .
"def csiszar_vimco_helper ( logu , name = None ) : with tf . compat . v1 . name_scope ( name , ""csiszar_vimco_helper"" , [ logu ] ) : logu = tf . convert_to_tensor ( value = logu , name = ""logu"" ) n = tf . compat . dimension_value ( logu . shape . with_rank_at_least ( 1 ) [ 0 ] ) if n is None : n = tf . shape ( input = logu ) [ 0 ] log_n = tf . math . log ( tf . cast ( n , dtype = logu . dtype ) ) nm1 = tf . cast ( n - 1 , dtype = logu . dtype ) else : log_n = np . log ( n ) . astype ( logu . dtype . as_numpy_dtype ) nm1 = np . asarray ( n - 1 , dtype = logu . dtype . as_numpy_dtype ) log_max_u = tf . reduce_max ( input_tensor = logu , axis = 0 ) log_sum_u_minus_log_max_u = tf . reduce_logsumexp ( input_tensor = logu - log_max_u , axis = 0 ) d = log_sum_u_minus_log_max_u + ( log_max_u - logu ) d_ok = tf . not_equal ( d , 0. ) safe_d = tf . where ( d_ok , d , tf . ones_like ( d ) ) d_ok_result = logu + tfd . softplus_inverse ( safe_d ) inf = np . array ( np . inf , dtype = logu . dtype . as_numpy_dtype ) is_positive_and_largest = tf . logical_and ( logu > 0. , tf . equal ( logu , log_max_u [ tf . newaxis , ... ] ) ) log_lomsum_u = tf . reduce_logsumexp ( input_tensor = tf . where ( is_positive_and_largest , tf . fill ( tf . shape ( input = logu ) , - inf ) , logu ) , axis = 0 , keepdims = True ) log_lomsum_u = tf . tile ( log_lomsum_u , multiples = 1 + tf . pad ( tensor = [ n - 1 ] , paddings = [ [ 0 , tf . rank ( logu ) - 1 ] ] ) ) d_not_ok_result = tf . where ( is_positive_and_largest , log_lomsum_u , tf . fill ( tf . shape ( input = d ) , - inf ) ) log_loosum_u = tf . where ( d_ok , d_ok_result , d_not_ok_result ) looavg_logu = ( tf . reduce_sum ( input_tensor = logu , axis = 0 ) - logu ) / nm1 log_soosum_u = tf . reduce_logsumexp ( input_tensor = tf . stack ( [ log_loosum_u , looavg_logu ] ) , axis = 0 ) log_avg_u = log_sum_u_minus_log_max_u + log_max_u - log_n log_sooavg_u = log_soosum_u - log_n log_avg_u . set_shape ( logu . shape . with_rank_at_least ( 1 ) [ 1 : ] ) log_sooavg_u . set_shape ( logu . shape ) return log_avg_u , log_sooavg_u",Helper to csiszar_vimco ; computes log_avg_u log_sooavg_u .
"def _interp_regular_1d_grid_impl ( x , x_ref_min , x_ref_max , y_ref , axis = - 1 , batch_y_ref = False , fill_value = 'constant_extension' , fill_value_below = None , fill_value_above = None , grid_regularizing_transform = None , name = None ) : with tf . compat . v1 . name_scope ( name , values = [ x , x_ref_min , x_ref_max , y_ref , axis , fill_value , fill_value_below , fill_value_above ] ) : allowed_fv_st = ( 'constant_extension' , 'extrapolate' ) for fv in ( fill_value , fill_value_below , fill_value_above ) : if isinstance ( fv , str ) and fv not in allowed_fv_st : raise ValueError ( 'A fill value ({}) was not an allowed string ({})' . format ( fv , allowed_fv_st ) ) need_separate_fills = ( fill_value_above is not None or fill_value_below is not None or fill_value == 'extrapolate' ) if need_separate_fills and fill_value_above is None : fill_value_above = fill_value if need_separate_fills and fill_value_below is None : fill_value_below = fill_value dtype = dtype_util . common_dtype ( [ x , x_ref_min , x_ref_max , y_ref ] , preferred_dtype = tf . float32 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = dtype ) x_ref_min = tf . convert_to_tensor ( value = x_ref_min , name = 'x_ref_min' , dtype = dtype ) x_ref_max = tf . convert_to_tensor ( value = x_ref_max , name = 'x_ref_max' , dtype = dtype ) if not batch_y_ref : _assert_ndims_statically ( x_ref_min , expect_ndims = 0 ) _assert_ndims_statically ( x_ref_max , expect_ndims = 0 ) y_ref = tf . convert_to_tensor ( value = y_ref , name = 'y_ref' , dtype = dtype ) if batch_y_ref : x_ref_min = x_ref_min [ ... , tf . newaxis ] x_ref_max = x_ref_max [ ... , tf . newaxis ] axis = tf . convert_to_tensor ( value = axis , name = 'axis' , dtype = tf . int32 ) axis = distribution_util . make_non_negative_axis ( axis , tf . rank ( y_ref ) ) _assert_ndims_statically ( axis , expect_ndims = 0 ) ny = tf . cast ( tf . shape ( input = y_ref ) [ axis ] , dtype ) if grid_regularizing_transform is None : g = lambda x : x else : g = grid_regularizing_transform fractional_idx = ( ( g ( x ) - g ( x_ref_min ) ) / ( g ( x_ref_max ) - g ( x_ref_min ) ) ) x_idx_unclipped = fractional_idx * ( ny - 1 ) nan_idx = tf . math . is_nan ( x_idx_unclipped ) x_idx_unclipped = tf . where ( nan_idx , tf . zeros_like ( x_idx_unclipped ) , x_idx_unclipped ) x_idx = tf . clip_by_value ( x_idx_unclipped , tf . zeros ( ( ) , dtype = dtype ) , ny - 1 ) idx_below = tf . floor ( x_idx ) idx_above = tf . minimum ( idx_below + 1 , ny - 1 ) idx_below = tf . maximum ( idx_above - 1 , 0 ) idx_below_int32 = tf . cast ( idx_below , dtype = tf . int32 ) idx_above_int32 = tf . cast ( idx_above , dtype = tf . int32 ) if batch_y_ref : y_ref_below = _batch_gather_with_broadcast ( y_ref , idx_below_int32 , axis ) y_ref_above = _batch_gather_with_broadcast ( y_ref , idx_above_int32 , axis ) else : y_ref_below = tf . gather ( y_ref , idx_below_int32 , axis = axis ) y_ref_above = tf . gather ( y_ref , idx_above_int32 , axis = axis ) t = x_idx - idx_below if batch_y_ref : expand_x_fn = _make_expand_x_fn_for_batch_interpolation ( y_ref , axis ) else : expand_x_fn = _make_expand_x_fn_for_non_batch_interpolation ( y_ref , axis ) t = expand_x_fn ( t ) nan_idx = expand_x_fn ( nan_idx , broadcast = True ) x_idx_unclipped = expand_x_fn ( x_idx_unclipped , broadcast = True ) y = t * y_ref_above + ( 1 - t ) * y_ref_below y = tf . where ( nan_idx , tf . fill ( tf . shape ( input = y ) , tf . constant ( np . nan , y . dtype ) ) , y ) if not need_separate_fills : if fill_value == 'constant_extension' : pass else : y = tf . where ( ( x_idx_unclipped < 0 ) | ( x_idx_unclipped > ny - 1 ) , fill_value + tf . zeros_like ( y ) , y ) else : if fill_value_below == 'constant_extension' : pass elif fill_value_below == 'extrapolate' : if batch_y_ref : y_0 = tf . gather ( y_ref , [ 0 ] , axis = axis ) y_1 = tf . gather ( y_ref , [ 1 ] , axis = axis ) else : y_0 = tf . gather ( y_ref , tf . zeros ( tf . shape ( input = x ) , dtype = tf . int32 ) , axis = axis ) y_1 = tf . gather ( y_ref , tf . ones ( tf . shape ( input = x ) , dtype = tf . int32 ) , axis = axis ) x_delta = ( x_ref_max - x_ref_min ) / ( ny - 1 ) x_factor = expand_x_fn ( ( x - x_ref_min ) / x_delta , broadcast = True ) y = tf . where ( x_idx_unclipped < 0 , y_0 + x_factor * ( y_1 - y_0 ) , y ) else : y = tf . where ( x_idx_unclipped < 0 , fill_value_below + tf . zeros_like ( y ) , y ) if fill_value_above == 'constant_extension' : pass elif fill_value_above == 'extrapolate' : ny_int32 = tf . shape ( input = y_ref ) [ axis ] if batch_y_ref : y_n1 = tf . gather ( y_ref , [ tf . shape ( input = y_ref ) [ axis ] - 1 ] , axis = axis ) y_n2 = tf . gather ( y_ref , [ tf . shape ( input = y_ref ) [ axis ] - 2 ] , axis = axis ) else : y_n1 = tf . gather ( y_ref , tf . fill ( tf . shape ( input = x ) , ny_int32 - 1 ) , axis = axis ) y_n2 = tf . gather ( y_ref , tf . fill ( tf . shape ( input = x ) , ny_int32 - 2 ) , axis = axis ) x_delta = ( x_ref_max - x_ref_min ) / ( ny - 1 ) x_factor = expand_x_fn ( ( x - x_ref_max ) / x_delta , broadcast = True ) y = tf . where ( x_idx_unclipped > ny - 1 , y_n1 + x_factor * ( y_n1 - y_n2 ) , y ) else : y = tf . where ( x_idx_unclipped > ny - 1 , fill_value_above + tf . zeros_like ( y ) , y ) return y",1 - D interpolation that works with / without batching .
"def interp_regular_1d_grid ( x , x_ref_min , x_ref_max , y_ref , axis = - 1 , fill_value = 'constant_extension' , fill_value_below = None , fill_value_above = None , grid_regularizing_transform = None , name = None ) : return _interp_regular_1d_grid_impl ( x , x_ref_min , x_ref_max , y_ref , axis = axis , batch_y_ref = False , fill_value = fill_value , fill_value_below = fill_value_below , fill_value_above = fill_value_above , grid_regularizing_transform = grid_regularizing_transform , name = name or 'interp_regular_1d_grid' )",Linear 1 - D interpolation on a regular ( constant spacing ) grid .
"def batch_interp_regular_1d_grid ( x , x_ref_min , x_ref_max , y_ref , axis = - 1 , fill_value = 'constant_extension' , fill_value_below = None , fill_value_above = None , grid_regularizing_transform = None , name = None ) : return _interp_regular_1d_grid_impl ( x , x_ref_min , x_ref_max , y_ref , axis = axis , batch_y_ref = True , fill_value = fill_value , fill_value_below = fill_value_below , fill_value_above = fill_value_above , grid_regularizing_transform = grid_regularizing_transform , name = name or 'batch_interp_regular_1d_grid' )",Linear 1 - D interpolation on a regular ( constant spacing ) grid .
"def batch_interp_regular_nd_grid ( x , x_ref_min , x_ref_max , y_ref , axis , fill_value = 'constant_extension' , name = None ) : with tf . compat . v1 . name_scope ( name , default_name = 'interp_regular_nd_grid' , values = [ x , x_ref_min , x_ref_max , y_ref , fill_value ] ) : dtype = dtype_util . common_dtype ( [ x , x_ref_min , x_ref_max , y_ref ] , preferred_dtype = tf . float32 ) if isinstance ( fill_value , str ) : if fill_value != 'constant_extension' : raise ValueError ( 'A fill value ({}) was not an allowed string ({})' . format ( fill_value , 'constant_extension' ) ) else : fill_value = tf . convert_to_tensor ( value = fill_value , name = 'fill_value' , dtype = dtype ) _assert_ndims_statically ( fill_value , expect_ndims = 0 ) x = tf . convert_to_tensor ( value = x , name = 'x' , dtype = dtype ) _assert_ndims_statically ( x , expect_ndims_at_least = 2 ) y_ref = tf . convert_to_tensor ( value = y_ref , name = 'y_ref' , dtype = dtype ) x_ref_min = tf . convert_to_tensor ( value = x_ref_min , name = 'x_ref_min' , dtype = dtype ) x_ref_max = tf . convert_to_tensor ( value = x_ref_max , name = 'x_ref_max' , dtype = dtype ) _assert_ndims_statically ( x_ref_min , expect_ndims_at_least = 1 , expect_static = True ) _assert_ndims_statically ( x_ref_max , expect_ndims_at_least = 1 , expect_static = True ) nd = tf . compat . dimension_value ( x_ref_min . shape [ - 1 ] ) if nd is None : raise ValueError ( '`x_ref_min.shape[-1]` must be known statically.' ) x_ref_max . shape [ - 1 : ] . assert_is_compatible_with ( x_ref_min . shape [ - 1 : ] ) axis = tf . convert_to_tensor ( value = axis , dtype = tf . int32 , name = 'axis' ) axis = distribution_util . make_non_negative_axis ( axis , tf . rank ( y_ref ) ) axis . shape . assert_has_rank ( 0 ) axis_ = tf . get_static_value ( axis ) y_ref_rank_ = tf . get_static_value ( tf . rank ( y_ref ) ) if axis_ is not None and y_ref_rank_ is not None : if axis_ + nd > y_ref_rank_ : raise ValueError ( 'Since dims `[axis, axis + nd)` index the interpolation table, we ' 'must have `axis + nd <= rank(y_ref)`.  Found: ' '`axis`: {},  rank(y_ref): {}, and inferred `nd` from trailing ' 'dimensions of `x_ref_min` to be {}.' . format ( axis_ , y_ref_rank_ , nd ) ) x_batch_shape = tf . shape ( input = x ) [ : - 2 ] x_ref_min_batch_shape = tf . shape ( input = x_ref_min ) [ : - 1 ] x_ref_max_batch_shape = tf . shape ( input = x_ref_max ) [ : - 1 ] y_ref_batch_shape = tf . shape ( input = y_ref ) [ : axis ] batch_shape = y_ref_batch_shape for tensor in [ x_batch_shape , x_ref_min_batch_shape , x_ref_max_batch_shape ] : batch_shape = tf . broadcast_dynamic_shape ( batch_shape , tensor ) def _batch_of_zeros_with_rightmost_singletons ( n_singletons ) : """"""Return Tensor of zeros with some singletons on the rightmost dims."""""" ones = tf . ones ( shape = [ n_singletons ] , dtype = tf . int32 ) return tf . zeros ( shape = tf . concat ( [ batch_shape , ones ] , axis = 0 ) , dtype = dtype ) x += _batch_of_zeros_with_rightmost_singletons ( n_singletons = 2 ) x_ref_min += _batch_of_zeros_with_rightmost_singletons ( n_singletons = 1 ) x_ref_max += _batch_of_zeros_with_rightmost_singletons ( n_singletons = 1 ) y_ref += _batch_of_zeros_with_rightmost_singletons ( n_singletons = tf . rank ( y_ref ) - axis ) return _batch_interp_with_gather_nd ( x = x , x_ref_min = x_ref_min , x_ref_max = x_ref_max , y_ref = y_ref , nd = nd , fill_value = fill_value , batch_dims = tf . get_static_value ( tf . rank ( x ) ) - 2 )",Multi - linear interpolation on a regular ( constant spacing ) grid .
"def _batch_interp_with_gather_nd ( x , x_ref_min , x_ref_max , y_ref , nd , fill_value , batch_dims ) : dtype = x . dtype ny = tf . cast ( tf . shape ( input = y_ref ) [ batch_dims : batch_dims + nd ] , dtype ) x_ref_min_expanded = tf . expand_dims ( x_ref_min , axis = - 2 ) x_ref_max_expanded = tf . expand_dims ( x_ref_max , axis = - 2 ) x_idx_unclipped = ( ny - 1 ) * ( x - x_ref_min_expanded ) / ( x_ref_max_expanded - x_ref_min_expanded ) nan_idx = tf . math . is_nan ( x_idx_unclipped ) x_idx_unclipped = tf . where ( nan_idx , tf . zeros_like ( x_idx_unclipped ) , x_idx_unclipped ) x_idx = tf . clip_by_value ( x_idx_unclipped , tf . zeros ( ( ) , dtype = dtype ) , ny - 1 ) idx_below = tf . floor ( x_idx ) idx_above = tf . minimum ( idx_below + 1 , ny - 1 ) idx_below = tf . maximum ( idx_above - 1 , 0 ) idx_below_int32 = tf . cast ( idx_below , dtype = tf . int32 ) idx_above_int32 = tf . cast ( idx_above , dtype = tf . int32 ) idx_below_list = tf . unstack ( idx_below_int32 , axis = - 1 ) idx_above_list = tf . unstack ( idx_above_int32 , axis = - 1 ) t = x_idx - idx_below def _expand_x_fn ( tensor ) : extended_shape = tf . concat ( [ tf . shape ( input = tensor ) , tf . ones_like ( tf . shape ( input = y_ref ) [ batch_dims + nd : ] ) ] , axis = 0 ) return tf . reshape ( tensor , extended_shape ) t = _expand_x_fn ( t ) s = 1 - t nan_idx = _expand_x_fn ( nan_idx ) t = tf . where ( nan_idx , tf . fill ( tf . shape ( input = t ) , tf . constant ( np . nan , dtype ) ) , t ) terms = [ ] for zero_ones_list in _binary_count ( nd ) : gather_from_y_ref_idx = [ ] opposite_volume_t_idx = [ ] opposite_volume_s_idx = [ ] for k , zero_or_one in enumerate ( zero_ones_list ) : if zero_or_one == 0 : gather_from_y_ref_idx . append ( idx_below_list [ k ] ) opposite_volume_s_idx . append ( k ) else : gather_from_y_ref_idx . append ( idx_above_list [ k ] ) opposite_volume_t_idx . append ( k ) ov_axis = tf . rank ( x ) - 1 opposite_volume = ( tf . reduce_prod ( input_tensor = tf . gather ( t , indices = tf . cast ( opposite_volume_t_idx , dtype = tf . int32 ) , axis = ov_axis ) , axis = ov_axis ) * tf . reduce_prod ( input_tensor = tf . gather ( s , indices = tf . cast ( opposite_volume_s_idx , dtype = tf . int32 ) , axis = ov_axis ) , axis = ov_axis ) ) y_ref_pt = tf . gather_nd ( y_ref , tf . stack ( gather_from_y_ref_idx , axis = - 1 ) , batch_dims = batch_dims ) terms . append ( y_ref_pt * opposite_volume ) y = tf . math . add_n ( terms ) if tf . debugging . is_numeric_tensor ( fill_value ) : oob_idx = tf . reduce_any ( input_tensor = ( x_idx_unclipped < 0 ) | ( x_idx_unclipped > ny - 1 ) , axis = - 1 ) oob_idx = _expand_x_fn ( oob_idx ) oob_idx |= tf . fill ( tf . shape ( input = y ) , False ) y = tf . where ( oob_idx , tf . fill ( tf . shape ( input = y ) , fill_value ) , y ) return y",N - D interpolation that works with leading batch dims .
"def _assert_ndims_statically ( x , expect_ndims = None , expect_ndims_at_least = None , expect_static = False ) : ndims = x . shape . ndims if ndims is None : if expect_static : raise ValueError ( 'Expected static ndims. Found: {}' . format ( x ) ) return if expect_ndims is not None and ndims != expect_ndims : raise ValueError ( 'ndims must be {}.  Found: {}' . format ( expect_ndims , ndims ) ) if expect_ndims_at_least is not None and ndims < expect_ndims_at_least : raise ValueError ( 'ndims must be at least {}. Found {}' . format ( expect_ndims_at_least , ndims ) )",Assert that Tensor x has expected number of dimensions .
"def _make_expand_x_fn_for_non_batch_interpolation ( y_ref , axis ) : y_ref_shape = tf . shape ( input = y_ref ) y_ref_shape_left = y_ref_shape [ : axis ] y_ref_shape_right = y_ref_shape [ axis + 1 : ] def expand_ends ( x , broadcast = False ) : """"""Expand x so it can bcast w/ tensors of output shape."""""" expanded_shape = tf . pad ( tensor = tf . shape ( input = x ) , paddings = [ [ axis , tf . size ( input = y_ref_shape_right ) ] ] , constant_values = 1 ) x_expanded = tf . reshape ( x , expanded_shape ) if broadcast : out_shape = tf . concat ( ( y_ref_shape_left , tf . shape ( input = x ) , y_ref_shape_right , ) , axis = 0 ) if x . dtype . is_bool : x_expanded = x_expanded | tf . cast ( tf . zeros ( out_shape ) , tf . bool ) else : x_expanded += tf . zeros ( out_shape , dtype = x . dtype ) return x_expanded return expand_ends",Make func to expand left / right ( of axis ) dims of tensors shaped like x .
"def _make_expand_x_fn_for_batch_interpolation ( y_ref , axis ) : y_ref_shape = tf . shape ( input = y_ref ) y_ref_shape_left = y_ref_shape [ : axis ] y_ref_shape_right = y_ref_shape [ axis + 1 : ] def expand_right_dims ( x , broadcast = False ) : """"""Expand x so it can bcast w/ tensors of output shape."""""" expanded_shape_left = tf . broadcast_dynamic_shape ( tf . shape ( input = x ) [ : - 1 ] , tf . ones ( [ tf . size ( input = y_ref_shape_left ) ] , dtype = tf . int32 ) ) expanded_shape = tf . concat ( ( expanded_shape_left , tf . shape ( input = x ) [ - 1 : ] , tf . ones ( [ tf . size ( input = y_ref_shape_right ) ] , dtype = tf . int32 ) ) , axis = 0 ) x_expanded = tf . reshape ( x , expanded_shape ) if broadcast : broadcast_shape_left = tf . broadcast_dynamic_shape ( tf . shape ( input = x ) [ : - 1 ] , y_ref_shape_left ) broadcast_shape = tf . concat ( ( broadcast_shape_left , tf . shape ( input = x ) [ - 1 : ] , y_ref_shape_right ) , axis = 0 ) if x . dtype . is_bool : x_expanded = x_expanded | tf . cast ( tf . zeros ( broadcast_shape ) , tf . bool ) else : x_expanded += tf . zeros ( broadcast_shape , dtype = x . dtype ) return x_expanded return expand_right_dims",Make func to expand left / right ( of axis ) dims of tensors shaped like x .
"def _batch_gather_with_broadcast ( params , indices , axis ) : leading_bcast_shape = tf . broadcast_dynamic_shape ( tf . shape ( input = params ) [ : axis ] , tf . shape ( input = indices ) [ : - 1 ] ) params += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = params ) [ axis : ] ) , axis = 0 ) , dtype = params . dtype ) indices += tf . zeros ( tf . concat ( ( leading_bcast_shape , tf . shape ( input = indices ) [ - 1 : ] ) , axis = 0 ) , dtype = indices . dtype ) return tf . compat . v1 . batch_gather ( params , indices )",Like batch_gather but broadcasts to the left of axis .
"def _broadcast_cat_event_and_params ( event , params , base_dtype ) : if dtype_util . is_integer ( event . dtype ) : pass elif dtype_util . is_floating ( event . dtype ) : event = tf . cast ( event , dtype = tf . int32 ) else : raise TypeError ( ""`value` should have integer `dtype` or "" ""`self.dtype` ({})"" . format ( base_dtype ) ) shape_known_statically = ( tensorshape_util . rank ( params . shape ) is not None and tensorshape_util . is_fully_defined ( params . shape [ : - 1 ] ) and tensorshape_util . is_fully_defined ( event . shape ) ) if not shape_known_statically or params . shape [ : - 1 ] != event . shape : params *= tf . ones_like ( event [ ... , tf . newaxis ] , dtype = params . dtype ) params_shape = tf . shape ( input = params ) [ : - 1 ] event *= tf . ones ( params_shape , dtype = event . dtype ) if tensorshape_util . rank ( params . shape ) is not None : tensorshape_util . set_shape ( event , params . shape [ : - 1 ] ) return event , params",Broadcasts the event or distribution parameters .
"def expectation_importance_sampler ( f , log_p , sampling_dist_q , z = None , n = None , seed = None , name = 'expectation_importance_sampler' ) : q = sampling_dist_q with tf . name_scope ( name ) : z = _get_samples ( q , z , n , seed ) log_p_z = log_p ( z ) q_log_prob_z = q . log_prob ( z ) def _importance_sampler_positive_f ( log_f_z ) : log_values = log_f_z + log_p_z - q_log_prob_z return _logspace_mean ( log_values ) f_z = f ( z ) log_f_plus_z = tf . math . log1p ( tf . nn . relu ( f_z ) ) log_f_minus_z = tf . math . log1p ( tf . nn . relu ( - 1. * f_z ) ) log_f_plus_integral = _importance_sampler_positive_f ( log_f_plus_z ) log_f_minus_integral = _importance_sampler_positive_f ( log_f_minus_z ) return tf . math . exp ( log_f_plus_integral ) - tf . math . exp ( log_f_minus_integral )",r Monte Carlo estimate of \\ ( E_p [ f ( Z ) ] = E_q [ f ( Z ) p ( Z ) / q ( Z ) ] \\ ) .
"def expectation_importance_sampler_logspace ( log_f , log_p , sampling_dist_q , z = None , n = None , seed = None , name = 'expectation_importance_sampler_logspace' ) : q = sampling_dist_q with tf . name_scope ( name ) : z = _get_samples ( q , z , n , seed ) log_values = log_f ( z ) + log_p ( z ) - q . log_prob ( z ) return _logspace_mean ( log_values )",r Importance sampling with a positive function in log - space .
def _logspace_mean ( log_values ) : center = tf . stop_gradient ( _sample_max ( log_values ) ) centered_values = tf . math . exp ( log_values - center ) log_mean_of_values = tf . math . log ( _sample_mean ( centered_values ) ) + center return log_mean_of_values,Evaluate Log [ E [ values ]] in a stable manner .
"def _broadcast_event_and_samples ( event , samples , event_ndims ) : samples_shape = tf . concat ( [ tf . shape ( input = samples ) [ : - event_ndims - 1 ] , tf . shape ( input = samples ) [ tf . rank ( samples ) - event_ndims : ] ] , axis = 0 ) event *= tf . ones ( samples_shape , dtype = event . dtype ) event = tf . expand_dims ( event , axis = - event_ndims - 1 ) samples *= tf . ones_like ( event , dtype = samples . dtype ) return event , samples",Broadcasts the event or samples .
"def one_step ( self , current_state , previous_kernel_results ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'mh' , 'one_step' ) , values = [ current_state , previous_kernel_results ] ) : [ proposed_state , proposed_results , ] = self . inner_kernel . one_step ( current_state , previous_kernel_results . accepted_results ) if ( not has_target_log_prob ( proposed_results ) or not has_target_log_prob ( previous_kernel_results . accepted_results ) ) : raise ValueError ( '""target_log_prob"" must be a member of ' '`inner_kernel` results.' ) to_sum = [ proposed_results . target_log_prob , - previous_kernel_results . accepted_results . target_log_prob ] try : if ( not mcmc_util . is_list_like ( proposed_results . log_acceptance_correction ) or proposed_results . log_acceptance_correction ) : to_sum . append ( proposed_results . log_acceptance_correction ) except AttributeError : warnings . warn ( 'Supplied inner `TransitionKernel` does not have a ' '`log_acceptance_correction`. Assuming its value is `0.`' ) log_accept_ratio = mcmc_util . safe_sum ( to_sum , name = 'compute_log_accept_ratio' ) log_uniform = tf . math . log ( tf . random . uniform ( shape = tf . shape ( input = proposed_results . target_log_prob ) , dtype = proposed_results . target_log_prob . dtype . base_dtype , seed = self . _seed_stream ( ) ) ) is_accepted = log_uniform < log_accept_ratio next_state = mcmc_util . choose ( is_accepted , proposed_state , current_state , name = 'choose_next_state' ) kernel_results = MetropolisHastingsKernelResults ( accepted_results = mcmc_util . choose ( is_accepted , proposed_results , previous_kernel_results . accepted_results , name = 'choose_inner_results' ) , is_accepted = is_accepted , log_accept_ratio = log_accept_ratio , proposed_state = proposed_state , proposed_results = proposed_results , extra = [ ] , ) return next_state , kernel_results",Takes one step of the TransitionKernel .
"def bootstrap_results ( self , init_state ) : with tf . compat . v1 . name_scope ( name = mcmc_util . make_name ( self . name , 'mh' , 'bootstrap_results' ) , values = [ init_state ] ) : pkr = self . inner_kernel . bootstrap_results ( init_state ) if not has_target_log_prob ( pkr ) : raise ValueError ( '""target_log_prob"" must be a member of `inner_kernel` results.' ) x = pkr . target_log_prob return MetropolisHastingsKernelResults ( accepted_results = pkr , is_accepted = tf . ones_like ( x , dtype = tf . bool ) , log_accept_ratio = tf . zeros_like ( x ) , proposed_state = init_state , proposed_results = pkr , extra = [ ] , )",Returns an object with the same type as returned by one_step .
"def minimize ( value_and_gradients_function , initial_position , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance , initial_inverse_hessian_estimate ] ) : initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) dtype = initial_position . dtype . base_dtype tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) input_shape = distribution_util . prefer_static_shape ( initial_position ) batch_shape , domain_size = input_shape [ : - 1 ] , input_shape [ - 1 ] if stopping_condition is None : stopping_condition = bfgs_utils . converged_all control_inputs = None if initial_inverse_hessian_estimate is None : initial_inv_hessian = tf . eye ( domain_size , batch_shape = batch_shape , dtype = dtype , name = 'initial_inv_hessian' ) else : initial_inv_hessian = tf . convert_to_tensor ( value = initial_inverse_hessian_estimate , dtype = dtype , name = 'initial_inv_hessian' ) control_inputs = _inv_hessian_control_inputs ( initial_inv_hessian ) hessian_shape = tf . concat ( [ batch_shape , [ domain_size , domain_size ] ] , 0 ) initial_inv_hessian = tf . broadcast_to ( initial_inv_hessian , hessian_shape ) def _cond ( state ) : """"""Continue if iterations remain and stopping condition is not met."""""" return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) def _body ( state ) : """"""Main optimization loop."""""" search_direction = _get_search_direction ( state . inverse_hessian_estimate , state . objective_gradient ) derivative_at_start_pt = tf . reduce_sum ( input_tensor = state . objective_gradient * search_direction , axis = - 1 ) needs_reset = ( ~ state . failed & ~ state . converged & ( derivative_at_start_pt >= 0 ) ) search_direction_reset = _get_search_direction ( initial_inv_hessian , state . objective_gradient ) actual_serch_direction = tf . where ( needs_reset , search_direction_reset , search_direction ) actual_inv_hessian = tf . where ( needs_reset , initial_inv_hessian , state . inverse_hessian_estimate ) current_state = bfgs_utils . update_fields ( state , inverse_hessian_estimate = actual_inv_hessian ) next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , actual_serch_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) return [ _update_inv_hessian ( current_state , next_state ) ] kwargs = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance , control_inputs ) kwargs [ 'inverse_hessian_estimate' ] = initial_inv_hessian initial_state = BfgsOptimizerResults ( * * kwargs ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ]",Applies the BFGS algorithm to minimize a differentiable function .
"def _inv_hessian_control_inputs ( inv_hessian ) : is_positive_definite = tf . reduce_all ( input_tensor = tf . math . is_finite ( tf . linalg . cholesky ( inv_hessian ) ) , axis = [ - 1 , - 2 ] ) is_symmetric = tf . equal ( bfgs_utils . norm ( inv_hessian - _batch_transpose ( inv_hessian ) , dims = 2 ) , 0 ) return [ tf . Assert ( is_positive_definite , [ 'Initial inverse Hessian is not positive definite.' , inv_hessian ] ) , tf . Assert ( is_symmetric , [ 'Initial inverse Hessian is not symmetric' , inv_hessian ] ) ]",Computes control inputs to validate a provided inverse Hessian .
"def _update_inv_hessian ( prev_state , next_state ) : should_update = ~ next_state . converged & ~ next_state . failed gradient_delta = next_state . objective_gradient - prev_state . objective_gradient position_delta = next_state . position - prev_state . position normalization_factor = tf . reduce_sum ( input_tensor = gradient_delta * position_delta , axis = - 1 ) should_update = should_update & ~ tf . equal ( normalization_factor , 0 ) def _do_update_inv_hessian ( ) : next_inv_hessian = _bfgs_inv_hessian_update ( gradient_delta , position_delta , normalization_factor , prev_state . inverse_hessian_estimate ) return bfgs_utils . update_fields ( next_state , inverse_hessian_estimate = tf . where ( should_update , next_inv_hessian , prev_state . inverse_hessian_estimate ) ) return prefer_static . cond ( tf . reduce_any ( input_tensor = should_update ) , _do_update_inv_hessian , lambda : next_state )",Update the BGFS state by computing the next inverse hessian estimate .
"def _bfgs_inv_hessian_update ( grad_delta , position_delta , normalization_factor , inv_hessian_estimate ) : conditioned_grad_delta = _mul_right ( inv_hessian_estimate , grad_delta ) conditioned_grad_delta_norm = tf . reduce_sum ( input_tensor = conditioned_grad_delta * grad_delta , axis = - 1 ) cross_term = _tensor_product ( position_delta , conditioned_grad_delta ) def _expand_scalar ( s ) : return s [ ... , tf . newaxis , tf . newaxis ] cross_term += _tensor_product ( conditioned_grad_delta , position_delta ) position_term = _tensor_product ( position_delta , position_delta ) with tf . control_dependencies ( [ position_term ] ) : position_term *= _expand_scalar ( 1 + conditioned_grad_delta_norm / normalization_factor ) return ( inv_hessian_estimate + ( position_term - cross_term ) / _expand_scalar ( normalization_factor ) )",Applies the BFGS update to the inverse Hessian estimate .
"def _mul_right ( mat , vec ) : return tf . squeeze ( tf . matmul ( mat , tf . expand_dims ( vec , axis = - 1 ) ) , axis = - 1 )",Computes the product of a matrix with a vector on the right .
"def _tensor_product ( t1 , t2 ) : return tf . matmul ( tf . expand_dims ( t1 , axis = - 1 ) , tf . expand_dims ( t2 , axis = - 2 ) )",Computes the outer product of two possibly batched vectors .
"def _batch_transpose ( mat ) : n = distribution_util . prefer_static_rank ( mat ) perm = tf . range ( n ) perm = tf . concat ( [ perm [ : - 2 ] , [ perm [ - 1 ] , perm [ - 2 ] ] ] , axis = 0 ) return tf . transpose ( a = mat , perm = perm )",Transpose a possibly batched matrix .
"def pad_shape_right_with_ones ( x , ndims ) : if not ( isinstance ( ndims , int ) and ndims >= 0 ) : raise ValueError ( '`ndims` must be a Python `integer` greater than zero. Got: {}' . format ( ndims ) ) if ndims == 0 : return x x = tf . convert_to_tensor ( value = x ) original_shape = x . shape new_shape = distribution_util . pad ( tf . shape ( input = x ) , axis = 0 , back = True , value = 1 , count = ndims ) x = tf . reshape ( x , new_shape ) x . set_shape ( original_shape . concatenate ( [ 1 ] * ndims ) ) return x",Maybe add ndims ones to x . shape on the right .
"def sum_rightmost_ndims_preserving_shape ( x , ndims ) : x = tf . convert_to_tensor ( value = x ) if x . shape . ndims is not None : axes = tf . range ( x . shape . ndims - ndims , x . shape . ndims ) else : axes = tf . range ( tf . rank ( x ) - ndims , tf . rank ( x ) ) return tf . reduce_sum ( input_tensor = x , axis = axes )",Return Tensor with right - most ndims summed .
"def sqrt_with_finite_grads ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'sqrt_with_finite_grads' , [ x ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) if not x . dtype . is_floating : raise TypeError ( 'Input `x` must be floating type.' ) def grad ( grad_ys ) : large_float_like_x = np . sqrt ( np . finfo ( x . dtype . as_numpy_dtype ( ) ) . max ) safe_grads = tf . where ( tf . equal ( x , 0 ) , tf . fill ( tf . shape ( input = x ) , large_float_like_x ) , 0.5 * tf . math . rsqrt ( x ) ) return grad_ys * safe_grads return tf . sqrt ( x ) , grad",A sqrt function whose gradient at zero is very large but finite .
"def maybe_get_common_dtype ( arg_list ) : if all ( a is None for a in arg_list ) : return None return dtype_util . common_dtype ( arg_list , tf . float32 )",Return common dtype of arg_list or None .
"def minimize ( value_and_gradients_function , initial_position , num_correction_pairs = 10 , tolerance = 1e-8 , x_tolerance = 0 , f_relative_tolerance = 0 , initial_inverse_hessian_estimate = None , max_iterations = 50 , parallel_iterations = 1 , stopping_condition = None , name = None ) : if initial_inverse_hessian_estimate is not None : raise NotImplementedError ( 'Support of initial_inverse_hessian_estimate arg not yet implemented' ) if stopping_condition is None : stopping_condition = bfgs_utils . converged_all with tf . compat . v1 . name_scope ( name , 'minimize' , [ initial_position , tolerance ] ) : initial_position = tf . convert_to_tensor ( value = initial_position , name = 'initial_position' ) dtype = initial_position . dtype . base_dtype tolerance = tf . convert_to_tensor ( value = tolerance , dtype = dtype , name = 'grad_tolerance' ) f_relative_tolerance = tf . convert_to_tensor ( value = f_relative_tolerance , dtype = dtype , name = 'f_relative_tolerance' ) x_tolerance = tf . convert_to_tensor ( value = x_tolerance , dtype = dtype , name = 'x_tolerance' ) max_iterations = tf . convert_to_tensor ( value = max_iterations , name = 'max_iterations' ) def _cond ( state ) : """"""Continue if iterations remain and stopping condition is not met."""""" return ( ( state . num_iterations < max_iterations ) & tf . logical_not ( stopping_condition ( state . converged , state . failed ) ) ) def _body ( current_state ) : """"""Main optimization loop."""""" search_direction = _get_search_direction ( current_state ) next_state = bfgs_utils . line_search_step ( current_state , value_and_gradients_function , search_direction , tolerance , f_relative_tolerance , x_tolerance , stopping_condition ) should_update = ~ ( next_state . converged | next_state . failed ) state_after_inv_hessian_update = bfgs_utils . update_fields ( next_state , position_deltas = _queue_push ( current_state . position_deltas , should_update , next_state . position - current_state . position ) , gradient_deltas = _queue_push ( current_state . gradient_deltas , should_update , next_state . objective_gradient - current_state . objective_gradient ) ) return [ state_after_inv_hessian_update ] initial_state = _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) return tf . while_loop ( cond = _cond , body = _body , loop_vars = [ initial_state ] , parallel_iterations = parallel_iterations ) [ 0 ]",Applies the L - BFGS algorithm to minimize a differentiable function .
"def _get_initial_state ( value_and_gradients_function , initial_position , num_correction_pairs , tolerance ) : init_args = bfgs_utils . get_initial_state_args ( value_and_gradients_function , initial_position , tolerance ) empty_queue = _make_empty_queue_for ( num_correction_pairs , initial_position ) init_args . update ( position_deltas = empty_queue , gradient_deltas = empty_queue ) return LBfgsOptimizerResults ( * * init_args )",Create LBfgsOptimizerResults with initial state of search procedure .
"def _get_search_direction ( state ) : num_elements = tf . minimum ( state . num_iterations , distribution_util . prefer_static_shape ( state . position_deltas ) [ 0 ] ) def _two_loop_algorithm ( ) : """"""L-BFGS two-loop algorithm."""""" position_deltas = state . position_deltas [ - num_elements : ] gradient_deltas = state . gradient_deltas [ - num_elements : ] inv_rhos = tf . reduce_sum ( input_tensor = gradient_deltas * position_deltas , axis = - 1 ) def first_loop ( acc , args ) : _ , q_direction = acc position_delta , gradient_delta , inv_rho = args alpha = tf . reduce_sum ( input_tensor = position_delta * q_direction , axis = - 1 ) / inv_rho direction_delta = tf . expand_dims ( alpha , axis = - 1 ) * gradient_delta return ( alpha , q_direction - direction_delta ) zero = tf . zeros_like ( inv_rhos [ 0 ] ) alphas , q_directions = tf . scan ( first_loop , [ position_deltas , gradient_deltas , inv_rhos ] , initializer = ( zero , state . objective_gradient ) , reverse = True ) gamma_k = inv_rhos [ - 1 ] / tf . reduce_sum ( input_tensor = gradient_deltas [ - 1 ] * gradient_deltas [ - 1 ] , axis = - 1 ) r_direction = tf . expand_dims ( gamma_k , axis = - 1 ) * q_directions [ 0 ] def second_loop ( r_direction , args ) : alpha , position_delta , gradient_delta , inv_rho = args beta = tf . reduce_sum ( input_tensor = gradient_delta * r_direction , axis = - 1 ) / inv_rho direction_delta = tf . expand_dims ( alpha - beta , axis = - 1 ) * position_delta return r_direction + direction_delta r_directions = tf . scan ( second_loop , [ alphas , position_deltas , gradient_deltas , inv_rhos ] , initializer = r_direction ) return - r_directions [ - 1 ] return prefer_static . cond ( tf . equal ( num_elements , 0 ) , ( lambda : - state . objective_gradient ) , _two_loop_algorithm )",Computes the search direction to follow at the current state .
"def _make_empty_queue_for ( k , element ) : queue_shape = tf . concat ( [ [ k ] , distribution_util . prefer_static_shape ( element ) ] , axis = 0 ) return tf . zeros ( queue_shape , dtype = element . dtype . base_dtype )",Creates a tf . Tensor suitable to hold k element - shaped tensors .
"def _queue_push ( queue , should_update , new_vecs ) : new_queue = tf . concat ( [ queue [ 1 : ] , [ new_vecs ] ] , axis = 0 ) update_pattern = tf . broadcast_to ( should_update [ tf . newaxis , ... , tf . newaxis ] , distribution_util . prefer_static_shape ( queue ) ) return tf . where ( update_pattern , new_queue , queue )",Conditionally push new vectors into a batch of first - in - first - out queues .
"def _psd_mask ( x ) : eigenvalues , _ = tf . linalg . eigh ( x ) return tf . cast ( tf . reduce_min ( input_tensor = eigenvalues , axis = - 1 ) >= 0 , dtype = x . dtype )",Computes whether each square matrix in the input is positive semi - definite .
"def _det_large_enough_mask ( x , det_bounds ) : return tf . cast ( tf . linalg . det ( x ) > det_bounds , dtype = x . dtype )",Returns whether the input matches the given determinant limit .
"def _uniform_correlation_like_matrix ( num_rows , batch_shape , dtype , seed ) : num_entries = num_rows * ( num_rows + 1 ) / 2 ones = tf . ones ( shape = [ num_entries ] , dtype = dtype ) unifs = uniform . Uniform ( - ones , ones ) . sample ( batch_shape , seed = seed ) tril = util . fill_triangular ( unifs ) symmetric = tril + tf . linalg . matrix_transpose ( tril ) diagonal_ones = tf . ones ( shape = util . pad ( batch_shape , axis = 0 , back = True , value = num_rows ) , dtype = dtype ) return tf . linalg . set_diag ( symmetric , diagonal_ones )",Returns a uniformly random Tensor of correlation - like matrices .
"def correlation_matrix_volume_rejection_samples ( det_bounds , dim , sample_shape , dtype , seed ) : with tf . compat . v1 . name_scope ( ""rejection_sampler"" ) : rej_proposals = _uniform_correlation_like_matrix ( dim , sample_shape , dtype , seed = seed ) rej_proposal_volume = 2. ** ( dim * ( dim - 1 ) / 2. ) rej_weights = rej_proposal_volume * _psd_mask ( rej_proposals ) * _det_large_enough_mask ( rej_proposals , det_bounds ) return rej_weights , rej_proposal_volume",Returns rejection samples from trying to get good correlation matrices .
"def _clopper_pearson_confidence_interval ( samples , error_rate ) : if optimize is None or stats is None : raise ValueError ( ""Scipy is required for computing Clopper-Pearson confidence intervals"" ) if len ( samples . shape ) != 1 : raise ValueError ( ""Batch semantics not implemented"" ) n = len ( samples ) low = np . amin ( samples ) high = np . amax ( samples ) successes = np . count_nonzero ( samples - low ) failures = np . count_nonzero ( samples - high ) if successes + failures != n : uniques = np . unique ( samples ) msg = ( ""Purportedly Bernoulli distribution had distinct samples"" "" {}, {}, and {}"" . format ( uniques [ 0 ] , uniques [ 1 ] , uniques [ 2 ] ) ) raise ValueError ( msg ) def p_small_enough ( p ) : prob = stats . binom . logcdf ( successes , n , p ) return prob - np . log ( error_rate / 2. ) def p_big_enough ( p ) : prob = stats . binom . logsf ( successes , n , p ) return prob - np . log ( error_rate / 2. ) high_p = optimize . brentq ( p_small_enough , float ( successes ) / n , 1. , rtol = 1e-9 ) low_p = optimize . brentq ( p_big_enough , 0. , float ( successes ) / n , rtol = 1e-9 ) low_interval = low + ( high - low ) * low_p high_interval = low + ( high - low ) * high_p return ( low_interval , high_interval )",Computes a confidence interval for the mean of the given 1 - D distribution .
"def compute_true_volumes ( det_bounds , dim , num_samples , error_rate = 1e-6 , seed = 42 ) : bounds = { } with tf . compat . v1 . Session ( ) as sess : rej_weights , _ = correlation_matrix_volume_rejection_samples ( det_bounds , dim , [ num_samples , len ( det_bounds ) ] , np . float32 , seed = seed ) rej_weights = sess . run ( rej_weights ) for rw , det in zip ( np . rollaxis ( rej_weights , 1 ) , det_bounds ) : template = ( ""Estimating volume of {}x{} correlation "" ""matrices with determinant >= {}."" ) print ( template . format ( dim , dim , det ) ) sys . stdout . flush ( ) bounds [ det ] = _clopper_pearson_confidence_interval ( rw , error_rate = error_rate ) return bounds",Returns confidence intervals for the desired correlation matrix volumes .
"def _kl_von_mises_von_mises ( d1 , d2 , name = None ) : with tf . name_scope ( name or ""kl_von_mises_von_mises"" ) : i0e_concentration1 = tf . math . bessel_i0e ( d1 . concentration ) i1e_concentration1 = tf . math . bessel_i1e ( d1 . concentration ) i0e_concentration2 = tf . math . bessel_i0e ( d2 . concentration ) return ( ( d2 . concentration - d1 . concentration ) + tf . math . log ( i0e_concentration2 / i0e_concentration1 ) + ( d1 . concentration - d2 . concentration * tf . cos ( d1 . loc - d2 . loc ) ) * ( i1e_concentration1 / i0e_concentration1 ) )",Batchwise KL divergence KL ( d1 || d2 ) with d1 and d2 von Mises .
"def von_mises_cdf ( x , concentration ) : x = tf . convert_to_tensor ( value = x ) concentration = tf . convert_to_tensor ( value = concentration ) dtype = x . dtype num_periods = tf . round ( x / ( 2. * np . pi ) ) x -= ( 2. * np . pi ) * num_periods ck = 10.5 num_terms = 20 cdf_series , dcdf_dconcentration_series = _von_mises_cdf_series ( x , concentration , num_terms , dtype ) cdf_normal , dcdf_dconcentration_normal = _von_mises_cdf_normal ( x , concentration , dtype ) use_series = concentration < ck cdf = tf . where ( use_series , cdf_series , cdf_normal ) cdf += num_periods dcdf_dconcentration = tf . where ( use_series , dcdf_dconcentration_series , dcdf_dconcentration_normal ) def grad ( dy ) : prob = tf . exp ( concentration * ( tf . cos ( x ) - 1. ) ) / ( ( 2. * np . pi ) * tf . math . bessel_i0e ( concentration ) ) return dy * prob , dy * dcdf_dconcentration return cdf , grad",Computes the cumulative density function ( CDF ) of von Mises distribution .
"def _von_mises_cdf_series ( x , concentration , num_terms , dtype ) : num_terms = tf . cast ( num_terms , dtype = dtype ) def loop_body ( n , rn , drn_dconcentration , vn , dvn_dconcentration ) : """"""One iteration of the series loop."""""" denominator = 2. * n / concentration + rn ddenominator_dk = - 2. * n / concentration ** 2 + drn_dconcentration rn = 1. / denominator drn_dconcentration = - ddenominator_dk / denominator ** 2 multiplier = tf . sin ( n * x ) / n + vn vn = rn * multiplier dvn_dconcentration = ( drn_dconcentration * multiplier + rn * dvn_dconcentration ) n -= 1. return n , rn , drn_dconcentration , vn , dvn_dconcentration ( _ , _ , _ , vn , dvn_dconcentration ) = tf . while_loop ( cond = lambda n , * _ : n > 0. , body = loop_body , loop_vars = ( num_terms , tf . zeros_like ( x , name = ""rn"" ) , tf . zeros_like ( x , name = ""drn_dconcentration"" ) , tf . zeros_like ( x , name = ""vn"" ) , tf . zeros_like ( x , name = ""dvn_dconcentration"" ) , ) , ) cdf = .5 + x / ( 2. * np . pi ) + vn / np . pi dcdf_dconcentration = dvn_dconcentration / np . pi cdf_clipped = tf . clip_by_value ( cdf , 0. , 1. ) dcdf_dconcentration *= tf . cast ( ( cdf >= 0. ) & ( cdf <= 1. ) , dtype ) return cdf_clipped , dcdf_dconcentration",Computes the von Mises CDF and its derivative via series expansion .
"def _von_mises_cdf_normal ( x , concentration , dtype ) : def cdf_func ( concentration ) : """"""A helper function that is passed to value_and_gradient."""""" z = ( ( np . sqrt ( 2. / np . pi ) / tf . math . bessel_i0e ( concentration ) ) * tf . sin ( .5 * x ) ) z2 = z ** 2 z3 = z2 * z z4 = z2 ** 2 c = 24. * concentration c1 = 56. xi = z - z3 / ( ( c - 2. * z2 - 16. ) / 3. - ( z4 + ( 7. / 4. ) * z2 + 167. / 2. ) / ( c - c1 - z2 + 3. ) ) ** 2 distrib = normal . Normal ( tf . cast ( 0. , dtype ) , tf . cast ( 1. , dtype ) ) return distrib . cdf ( xi ) return value_and_gradient ( cdf_func , concentration )",Computes the von Mises CDF and its derivative via Normal approximation .
"def random_von_mises ( shape , concentration , dtype = tf . float32 , seed = None ) : seed = SeedStream ( seed , salt = ""von_mises"" ) concentration = tf . convert_to_tensor ( value = concentration , dtype = dtype , name = ""concentration"" ) @ tf . custom_gradient def rejection_sample_with_gradient ( concentration ) : """"""Performs rejection sampling for standardized von Mises.

    A nested function is required because @tf.custom_gradient does not handle
    non-tensor inputs such as dtype. Instead, they are captured by the outer
    scope.

    Arguments:
      concentration: The concentration parameter of the distribution.

    Returns:
      Differentiable samples of standardized von Mises.
    """""" r = 1. + tf . sqrt ( 1. + 4. * concentration ** 2 ) rho = ( r - tf . sqrt ( 2. * r ) ) / ( 2. * concentration ) s_exact = ( 1. + rho ** 2 ) / ( 2. * rho ) s_approximate = 1. / concentration s_concentration_cutoff_dict = { tf . float16 : 1.8e-1 , tf . float32 : 2e-2 , tf . float64 : 1.2e-4 , } s_concentration_cutoff = s_concentration_cutoff_dict [ dtype ] s = tf . where ( concentration > s_concentration_cutoff , s_exact , s_approximate ) def loop_body ( done , u , w ) : """"""Resample the non-accepted points."""""" u = tf . random . uniform ( shape , minval = - 1. , maxval = 1. , dtype = dtype , seed = seed ( ) ) z = tf . cos ( np . pi * u ) w = tf . where ( done , w , ( 1. + s * z ) / ( s + z ) ) y = concentration * ( s - w ) v = tf . random . uniform ( shape , minval = 0. , maxval = 1. , dtype = dtype , seed = seed ( ) ) accept = ( y * ( 2. - y ) >= v ) | ( tf . math . log ( y / v ) + 1. >= y ) return done | accept , u , w _ , u , w = tf . while_loop ( cond = lambda done , * _ : ~ tf . reduce_all ( input_tensor = done ) , body = loop_body , loop_vars = ( tf . zeros ( shape , dtype = tf . bool , name = ""done"" ) , tf . zeros ( shape , dtype = dtype , name = ""u"" ) , tf . zeros ( shape , dtype = dtype , name = ""w"" ) , ) , maximum_iterations = 100 , parallel_iterations = 1 if seed . original_seed is None else 10 , ) x = tf . sign ( u ) * tf . math . acos ( w ) def grad ( dy ) : """"""The gradient of the von Mises samples w.r.t. concentration."""""" broadcast_concentration = concentration + tf . zeros_like ( x ) _ , dcdf_dconcentration = value_and_gradient ( lambda conc : von_mises_cdf ( x , conc ) , broadcast_concentration ) inv_prob = tf . exp ( - broadcast_concentration * ( tf . cos ( x ) - 1. ) ) * ( ( 2. * np . pi ) * tf . math . bessel_i0e ( broadcast_concentration ) ) ret = dy * ( - inv_prob * dcdf_dconcentration ) num_sample_dimensions = ( tf . rank ( broadcast_concentration ) - tf . rank ( concentration ) ) return tf . reduce_sum ( input_tensor = ret , axis = tf . range ( num_sample_dimensions ) ) return x , grad return rejection_sample_with_gradient ( concentration )",Samples from the standardized von Mises distribution .
"def one_step ( objective_function , population , population_values = None , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , 'one_step' , [ population , population_values , differential_weight , crossover_prob ] ) : population , _ = _ensure_list ( population ) if population_values is None : population_values = objective_function ( * population ) population_size = tf . shape ( input = population [ 0 ] ) [ 0 ] seed_stream = distributions . SeedStream ( seed , salt = 'one_step' ) mixing_indices = _get_mixing_indices ( population_size , seed = seed_stream ( ) ) mutants = _get_mutants ( population , population_size , mixing_indices , differential_weight ) candidates = _binary_crossover ( population , population_size , mutants , crossover_prob , seed = seed_stream ( ) ) candidate_values = objective_function ( * candidates ) if population_values is None : population_values = objective_function ( * population ) infinity = tf . zeros_like ( population_values ) + np . inf population_values = tf . where ( tf . math . is_nan ( population_values ) , x = infinity , y = population_values ) to_replace = candidate_values < population_values next_population = [ tf . where ( to_replace , x = candidates_part , y = population_part ) for candidates_part , population_part in zip ( candidates , population ) ] next_values = tf . where ( to_replace , x = candidate_values , y = population_values ) return next_population , next_values",Performs one step of the differential evolution algorithm .
"def minimize ( objective_function , initial_population = None , initial_position = None , population_size = 50 , population_stddev = 1. , max_iterations = 100 , func_tolerance = 0 , position_tolerance = 1e-8 , differential_weight = 0.5 , crossover_prob = 0.9 , seed = None , name = None ) : if initial_population is None and initial_position is None : raise ValueError ( 'Either the initial population or the initial position ' 'must be specified.' ) if initial_population is not None and initial_position is not None : raise ValueError ( 'Only one of initial population or initial position ' 'should be specified' ) with tf . compat . v1 . name_scope ( name , default_name = 'minimize' , values = [ initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ] ) : ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob ) = _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) def evolve_body ( loop_vars ) : """"""Performs one step of the evolution."""""" next_population , next_population_values = one_step ( objective_function , loop_vars . population , population_values = loop_vars . population_values , differential_weight = differential_weight , crossover_prob = crossover_prob , seed = seed ) converged = _check_convergence ( next_population , next_population_values , func_tolerance , position_tolerance ) failed = _check_failure ( next_population_values ) return [ _MinimizeLoopVars ( converged = converged , failed = failed , num_iterations = loop_vars . num_iterations + 1 , population = next_population , population_values = next_population_values ) ] def evolve_cond ( loop_vars ) : should_stop = ( loop_vars . failed | loop_vars . converged | ( max_iterations is not None and loop_vars . num_iterations >= max_iterations ) ) return ~ should_stop initial_vars = _MinimizeLoopVars ( converged = tf . convert_to_tensor ( value = False ) , failed = tf . convert_to_tensor ( value = False ) , num_iterations = tf . convert_to_tensor ( value = 0 ) , population = population , population_values = population_values ) final_state = tf . while_loop ( cond = evolve_cond , body = evolve_body , loop_vars = ( initial_vars , ) ) [ 0 ] best_position , best_values = _find_best_in_population ( final_state . population , final_state . population_values ) final_population = final_state . population if not was_iterable : final_population = final_population [ 0 ] best_position = best_position [ 0 ] return DifferentialEvolutionOptimizerResults ( converged = final_state . converged , failed = final_state . failed , position = best_position , objective_value = best_values , final_population = final_population , final_objective_values = final_state . population_values , initial_population = population , initial_objective_values = population_values , num_iterations = final_state . num_iterations )",Applies the Differential evolution algorithm to minimize a function .
"def _get_initial_args ( objective_function , initial_population , initial_position , population_size , population_stddev , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob , seed ) : was_iterable = False if initial_position is not None : initial_position , was_iterable = _ensure_list ( initial_position ) if initial_population is not None : initial_population , was_iterable = _ensure_list ( initial_population ) population = _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed = seed ) differential_weight = tf . convert_to_tensor ( value = differential_weight , dtype = population [ 0 ] . dtype . base_dtype ) crossover_prob = tf . convert_to_tensor ( value = crossover_prob ) population_values = objective_function ( * population ) if max_iterations is not None : max_iterations = tf . convert_to_tensor ( value = max_iterations ) func_tolerance = tf . convert_to_tensor ( value = func_tolerance , dtype = population_values . dtype . base_dtype ) position_tolerance = tf . convert_to_tensor ( value = position_tolerance , dtype = population [ 0 ] . dtype . base_dtype ) return ( was_iterable , population , population_values , max_iterations , func_tolerance , position_tolerance , differential_weight , crossover_prob )",Processes initial args .
def _check_failure ( population_values ) : return tf . math . reduce_all ( input_tensor = tf . math . is_inf ( population_values ) ),Checks if all the population values are NaN / infinite .
"def _find_best_in_population ( population , values ) : best_value = tf . math . reduce_min ( input_tensor = values ) best_index = tf . where ( tf . math . equal ( values , best_value ) ) [ 0 , 0 ] return ( [ population_part [ best_index ] for population_part in population ] , best_value )",Finds the population member with the lowest value .
"def _check_convergence ( population , population_values , func_tolerance , position_tolerance ) : value_range = tf . math . abs ( tf . math . reduce_max ( input_tensor = population_values ) - tf . math . reduce_min ( input_tensor = population_values ) ) value_converged = value_range <= func_tolerance half_tol = position_tolerance / 2 def part_converged ( part ) : return tf . math . reduce_max ( input_tensor = tf . math . abs ( part - part [ 0 ] ) ) <= half_tol x_converged = tf . math . reduce_all ( input_tensor = [ part_converged ( part ) for part in population ] ) return value_converged | x_converged",Checks whether the convergence criteria have been met .
"def _get_starting_population ( initial_population , initial_position , population_size , population_stddev , seed ) : if initial_population is not None : return [ tf . convert_to_tensor ( value = part ) for part in initial_population ] seed_stream = distributions . SeedStream ( seed , salt = 'get_starting_population' ) population = [ ] for part in initial_position : part = tf . convert_to_tensor ( value = part ) part_event_shape = tf . shape ( input = part ) population_part_shape = tf . concat ( [ [ population_size - 1 ] , part_event_shape ] , axis = 0 ) population_part = tf . random . normal ( population_part_shape , stddev = population_stddev , dtype = part . dtype . base_dtype , seed = seed_stream ( ) ) population_part += part population_part = tf . concat ( [ [ part ] , population_part ] , axis = 0 ) population . append ( population_part ) return population",Constructs the initial population .
"def _binary_crossover ( population , population_size , mutants , crossover_prob , seed ) : sizes = [ tf . cast ( tf . size ( input = x ) , dtype = tf . float64 ) for x in population ] seed_stream = distributions . SeedStream ( seed , salt = 'binary_crossover' ) force_crossover_group = distributions . Categorical ( sizes ) . sample ( [ population_size , 1 ] , seed = seed_stream ( ) ) recombinants = [ ] for i , population_part in enumerate ( population ) : pop_part_flat = tf . reshape ( population_part , [ population_size , - 1 ] ) mutant_part_flat = tf . reshape ( mutants [ i ] , [ population_size , - 1 ] ) part_size = tf . size ( input = population_part ) // population_size force_crossovers = tf . one_hot ( tf . random . uniform ( [ population_size ] , minval = 0 , maxval = part_size , dtype = tf . int32 , seed = seed_stream ( ) ) , part_size , on_value = True , off_value = False , dtype = tf . bool ) group_mask = tf . math . equal ( force_crossover_group , i ) force_crossovers &= group_mask do_binary_crossover = tf . random . uniform ( [ population_size , part_size ] , dtype = crossover_prob . dtype . base_dtype , seed = seed_stream ( ) ) < crossover_prob do_binary_crossover |= force_crossovers recombinant_flat = tf . where ( do_binary_crossover , x = mutant_part_flat , y = pop_part_flat ) recombinant = tf . reshape ( recombinant_flat , tf . shape ( input = population_part ) ) recombinants . append ( recombinant ) return recombinants",Performs recombination by binary crossover for the current population .
"def _get_mutants ( population , population_size , mixing_indices , differential_weight ) : mixing_indices = tf . reshape ( mixing_indices , [ - 1 ] ) weights = tf . stack ( [ 1.0 , differential_weight , - differential_weight ] ) def _mutant_part ( population_part ) : donors = tf . gather ( population_part , mixing_indices ) donors = tf . transpose ( a = tf . reshape ( donors , [ population_size , 3 , - 1 ] ) , perm = [ 0 , 2 , 1 ] ) return tf . math . reduce_sum ( input_tensor = donors * weights , axis = - 1 ) return [ _mutant_part ( population_part ) for population_part in population ]",Computes the mutatated vectors for each population member .
"def _get_mixing_indices ( size , seed = None , name = None ) : with tf . compat . v1 . name_scope ( name , default_name = 'get_mixing_indices' , values = [ size ] ) : size = tf . convert_to_tensor ( value = size ) dtype = size . dtype seed_stream = distributions . SeedStream ( seed , salt = 'get_mixing_indices' ) first = tf . random . uniform ( [ size ] , maxval = size - 1 , dtype = dtype , seed = seed_stream ( ) ) second = tf . random . uniform ( [ size ] , maxval = size - 2 , dtype = dtype , seed = seed_stream ( ) ) third = tf . random . uniform ( [ size ] , maxval = size - 3 , dtype = dtype , seed = seed_stream ( ) ) second = tf . where ( first < second , x = second , y = second + 1 ) smaller = tf . math . minimum ( first , second ) larger = tf . math . maximum ( first , second ) third = tf . where ( third < smaller , x = third , y = third + 1 ) third = tf . where ( third < larger , x = third , y = third + 1 ) sample = tf . stack ( [ first , second , third ] , axis = 1 ) to_avoid = tf . expand_dims ( tf . range ( size ) , axis = - 1 ) sample = tf . where ( sample < to_avoid , x = sample , y = sample + 1 ) return sample",Generates an array of indices suitable for mutation operation .
"def _ensure_list ( tensor_or_list ) : if isinstance ( tensor_or_list , ( list , tuple ) ) : return list ( tensor_or_list ) , True return [ tensor_or_list ] , False",Converts the input arg to a list if it is not a list already .
"def _get_tol ( tol , dtype , validate_args ) : if tol is None : return tf . convert_to_tensor ( value = 0 , dtype = dtype ) tol = tf . convert_to_tensor ( value = tol , dtype = dtype ) if validate_args : tol = distribution_util . with_dependencies ( [ assert_util . assert_non_negative ( tol , message = ""Argument 'tol' must be non-negative"" ) ] , tol ) return tol",Gets a Tensor of type dtype 0 if tol is None validation optional .
"def _kl_deterministic_distribution ( a , b , name = None ) : with tf . name_scope ( name or ""kl_deterministic_distribution"" ) : return - b . log_prob ( a . loc )",Calculate the batched KL divergence KL ( a || b ) with a Deterministic .
"def _sqrtx2p1 ( x ) : sqrt_eps = np . sqrt ( np . finfo ( dtype_util . as_numpy_dtype ( x . dtype ) ) . eps ) return tf . where ( tf . abs ( x ) * sqrt_eps <= 1. , tf . sqrt ( x ** 2. + 1. ) , tf . abs ( x ) )",Implementation of sqrt ( 1 + x ** 2 ) which is stable despite large x .
"def log1psquare ( x , name = None ) : with tf . compat . v1 . name_scope ( name , 'log1psquare' , [ x ] ) : x = tf . convert_to_tensor ( value = x , dtype_hint = tf . float32 , name = 'x' ) dtype = x . dtype . as_numpy_dtype eps = np . finfo ( dtype ) . eps . astype ( np . float64 ) is_large = tf . abs ( x ) > ( eps ** - 0.5 ) . astype ( dtype ) abs_large_x = tf . where ( is_large , tf . abs ( x ) , tf . ones_like ( x ) ) return tf . where ( is_large , 2. * tf . math . log ( abs_large_x ) , tf . math . log1p ( tf . square ( x ) ) )",Numerically stable calculation of log ( 1 + x ** 2 ) for small or large |x| .
"def soft_threshold ( x , threshold , name = None ) : with tf . compat . v1 . name_scope ( name , 'soft_threshold' , [ x , threshold ] ) : x = tf . convert_to_tensor ( value = x , name = 'x' ) threshold = tf . convert_to_tensor ( value = threshold , dtype = x . dtype , name = 'threshold' ) return tf . sign ( x ) * tf . maximum ( tf . abs ( x ) - threshold , 0. )",Soft Thresholding operator .
"def clip_by_value_preserve_gradient ( t , clip_value_min , clip_value_max , name = None ) : with tf . compat . v1 . name_scope ( name , 'clip_by_value_preserve_gradient' , [ t , clip_value_min , clip_value_max ] ) : t = tf . convert_to_tensor ( value = t , name = 't' ) clip_t = tf . clip_by_value ( t , clip_value_min , clip_value_max ) return t + tf . stop_gradient ( clip_t - t )",Clips values to a specified min and max while leaving gradient unaltered .
"def build_input_pipeline ( train_images , batch_size ) : training_dataset = tf . data . Dataset . from_tensor_slices ( train_images ) training_batches = training_dataset . shuffle ( 50000 , reshuffle_each_iteration = True ) . repeat ( ) . batch ( batch_size ) training_iterator = tf . compat . v1 . data . make_one_shot_iterator ( training_batches ) images = training_iterator . get_next ( ) return images",Build an iterator over training batches .
"def plot_generated_images ( images , fname ) : fig = plt . figure ( figsize = ( 4 , 4 ) ) canvas = backend_agg . FigureCanvasAgg ( fig ) for i , image in enumerate ( images ) : ax = fig . add_subplot ( 4 , 4 , i + 1 ) plt . axis ( 'off' ) ax . set_xticklabels ( [ ] ) ax . set_yticklabels ( [ ] ) ax . imshow ( image . reshape ( IMAGE_SHAPE [ : - 1 ] ) , cmap = 'Greys_r' ) fig . tight_layout ( ) plt . subplots_adjust ( wspace = 0.05 , hspace = 0.05 ) canvas . print_figure ( fname , format = 'png' )",Save a synthetic image as a PNG file .
"def convert_to_string ( self , productions ) : symbols = [ ] for production in tf . unstack ( productions , axis = 1 ) : lhs , rhs = self . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] if not symbols : if lhs != self . start_symbol : raise ValueError ( ""`productions` must begin with `self.start_symbol`."" ) symbols = rhs else : index = symbols . index ( lhs ) symbols = symbols [ : index ] + rhs + symbols [ index + 1 : ] string = """" . join ( symbols ) return string",Converts a sequence of productions into a string of terminal symbols .
"def mask ( self , symbol , on_value , off_value ) : mask_values = [ on_value if lhs == symbol else off_value for lhs , _ in self . production_rules ] mask_values = tf . reshape ( mask_values , [ 1 , len ( self . production_rules ) ] ) return mask_values",Produces a masking tensor for ( in ) valid production rules .
"def call ( self , inputs ) : del inputs latent_code = ed . MultivariateNormalDiag ( loc = tf . zeros ( self . latent_size ) , sample_shape = 1 , name = ""latent_code"" ) state = self . lstm . zero_state ( 1 , dtype = tf . float32 ) t = 0 productions = [ ] stack = [ self . grammar . start_symbol ] while stack : symbol = stack . pop ( ) net , state = self . lstm ( latent_code , state ) logits = ( self . output_layer ( net ) + self . grammar . mask ( symbol , on_value = 0. , off_value = - 1e9 ) ) production = ed . OneHotCategorical ( logits = logits , name = ""production_"" + str ( t ) ) _ , rhs = self . grammar . production_rules [ tf . argmax ( input = production , axis = - 1 ) ] for symbol in rhs : if symbol in self . grammar . nonterminal_symbols : stack . append ( symbol ) productions . append ( production ) t += 1 return tf . stack ( productions , axis = 1 )",Runs the model forward to generate a sequence of productions .
"def call ( self , inputs ) : net = self . encoder_net ( tf . cast ( inputs , tf . float32 ) ) return ed . MultivariateNormalDiag ( loc = net [ ... , : self . latent_size ] , scale_diag = tf . nn . softplus ( net [ ... , self . latent_size : ] ) , name = ""latent_code_posterior"" )",Runs the model forward to return a stochastic encoding .
"def _hat_integral ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . exp ( ( - t ) * tf . math . log1p ( x ) - tf . math . log ( t ) )",Integral of the hat function used for sampling .
"def _hat_integral_inverse ( self , x ) : x = tf . cast ( x , self . power . dtype ) t = self . power - 1. return tf . math . expm1 ( - ( tf . math . log ( t ) + tf . math . log ( x ) ) / t )",Inverse function of _hat_integral .
"def matrix_rank ( a , tol = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'matrix_rank' , [ a , tol ] ) : a = tf . convert_to_tensor ( value = a , dtype_hint = tf . float32 , name = 'a' ) assertions = _maybe_validate_matrix ( a , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : a = tf . identity ( a ) s = tf . linalg . svd ( a , compute_uv = False ) if tol is None : if a . shape [ - 2 : ] . is_fully_defined ( ) : m = np . max ( a . shape [ - 2 : ] . as_list ( ) ) else : m = tf . reduce_max ( input_tensor = tf . shape ( input = a ) [ - 2 : ] ) eps = np . finfo ( a . dtype . as_numpy_dtype ) . eps tol = ( eps * tf . cast ( m , a . dtype ) * tf . reduce_max ( input_tensor = s , axis = - 1 , keepdims = True ) ) return tf . reduce_sum ( input_tensor = tf . cast ( s > tol , tf . int32 ) , axis = - 1 )",Compute the matrix rank ; the number of non - zero SVD singular values .
"def cholesky_concat ( chol , cols , name = None ) : with tf . compat . v2 . name_scope ( name or 'cholesky_extend' ) : dtype = dtype_util . common_dtype ( [ chol , cols ] , preferred_dtype = tf . float32 ) chol = tf . convert_to_tensor ( value = chol , name = 'chol' , dtype = dtype ) cols = tf . convert_to_tensor ( value = cols , name = 'cols' , dtype = dtype ) n = prefer_static . shape ( chol ) [ - 1 ] mat_nm , mat_mm = cols [ ... , : n , : ] , cols [ ... , n : , : ] solved_nm = linear_operator_util . matrix_triangular_solve_with_broadcast ( chol , mat_nm ) lower_right_mm = tf . linalg . cholesky ( mat_mm - tf . matmul ( solved_nm , solved_nm , adjoint_a = True ) ) lower_left_mn = tf . math . conj ( tf . linalg . matrix_transpose ( solved_nm ) ) out_batch = prefer_static . shape ( solved_nm ) [ : - 2 ] chol = tf . broadcast_to ( chol , tf . concat ( [ out_batch , prefer_static . shape ( chol ) [ - 2 : ] ] , axis = 0 ) ) top_right_zeros_nm = tf . zeros_like ( solved_nm ) return tf . concat ( [ tf . concat ( [ chol , top_right_zeros_nm ] , axis = - 1 ) , tf . concat ( [ lower_left_mn , lower_right_mm ] , axis = - 1 ) ] , axis = - 2 )",Concatenates chol @ chol . T with additional rows and columns .
"def _swap_m_with_i ( vecs , m , i ) : vecs = tf . convert_to_tensor ( value = vecs , dtype = tf . int64 , name = 'vecs' ) m = tf . convert_to_tensor ( value = m , dtype = tf . int64 , name = 'm' ) i = tf . convert_to_tensor ( value = i , dtype = tf . int64 , name = 'i' ) trailing_elts = tf . broadcast_to ( tf . range ( m + 1 , prefer_static . shape ( vecs , out_type = tf . int64 ) [ - 1 ] ) , prefer_static . shape ( vecs [ ... , m + 1 : ] ) ) shp = prefer_static . shape ( trailing_elts ) trailing_elts = tf . where ( tf . equal ( trailing_elts , tf . broadcast_to ( i , shp ) ) , tf . broadcast_to ( tf . gather ( vecs , [ m ] , axis = - 1 ) , shp ) , tf . broadcast_to ( vecs [ ... , m + 1 : ] , shp ) ) vecs_shape = vecs . shape vecs = tf . concat ( [ vecs [ ... , : m ] , tf . gather ( vecs , i , batch_dims = prefer_static . rank ( vecs ) - 1 ) , trailing_elts ] , axis = - 1 ) tensorshape_util . set_shape ( vecs , vecs_shape ) return vecs",Swaps m and i on axis - 1 . ( Helper for pivoted_cholesky . )
"def pivoted_cholesky ( matrix , max_rank , diag_rtol = 1e-3 , name = None ) : with tf . compat . v2 . name_scope ( name or 'pivoted_cholesky' ) : dtype = dtype_util . common_dtype ( [ matrix , diag_rtol ] , preferred_dtype = tf . float32 ) matrix = tf . convert_to_tensor ( value = matrix , name = 'matrix' , dtype = dtype ) if tensorshape_util . rank ( matrix . shape ) is None : raise NotImplementedError ( 'Rank of `matrix` must be known statically' ) max_rank = tf . convert_to_tensor ( value = max_rank , name = 'max_rank' , dtype = tf . int64 ) max_rank = tf . minimum ( max_rank , prefer_static . shape ( matrix , out_type = tf . int64 ) [ - 1 ] ) diag_rtol = tf . convert_to_tensor ( value = diag_rtol , dtype = dtype , name = 'diag_rtol' ) matrix_diag = tf . linalg . diag_part ( matrix ) orig_error = tf . reduce_max ( input_tensor = matrix_diag , axis = - 1 ) def cond ( m , pchol , perm , matrix_diag ) : """"""Condition for `tf.while_loop` continuation."""""" del pchol del perm error = tf . linalg . norm ( tensor = matrix_diag , ord = 1 , axis = - 1 ) max_err = tf . reduce_max ( input_tensor = error / orig_error ) return ( m < max_rank ) & ( tf . equal ( m , 0 ) | ( max_err > diag_rtol ) ) batch_dims = tensorshape_util . rank ( matrix . shape ) - 2 def batch_gather ( params , indices , axis = - 1 ) : return tf . gather ( params , indices , axis = axis , batch_dims = batch_dims ) def body ( m , pchol , perm , matrix_diag ) : """"""Body of a single `tf.while_loop` iteration."""""" permuted_diag = batch_gather ( matrix_diag , perm [ ... , m : ] ) maxi = tf . argmax ( input = permuted_diag , axis = - 1 , output_type = tf . int64 ) [ ... , tf . newaxis ] maxval = batch_gather ( permuted_diag , maxi ) maxi = maxi + m maxval = maxval [ ... , 0 ] perm = _swap_m_with_i ( perm , m , maxi ) row = batch_gather ( matrix , perm [ ... , m : m + 1 ] , axis = - 2 ) row = batch_gather ( row , perm [ ... , m + 1 : ] ) prev_rows = pchol [ ... , : m , : ] prev_rows_perm_m_onward = batch_gather ( prev_rows , perm [ ... , m + 1 : ] ) prev_rows_pivot_col = batch_gather ( prev_rows , perm [ ... , m : m + 1 ] ) row -= tf . reduce_sum ( input_tensor = prev_rows_perm_m_onward * prev_rows_pivot_col , axis = - 2 ) [ ... , tf . newaxis , : ] pivot = tf . sqrt ( maxval ) [ ... , tf . newaxis , tf . newaxis ] row = tf . concat ( [ pivot , row / pivot ] , axis = - 1 ) paddings = tf . concat ( [ tf . zeros ( [ prefer_static . rank ( pchol ) - 1 , 2 ] , dtype = tf . int32 ) , [ [ tf . cast ( m , tf . int32 ) , 0 ] ] ] , axis = 0 ) diag_update = tf . pad ( tensor = row ** 2 , paddings = paddings ) [ ... , 0 , : ] reverse_perm = _invert_permutation ( perm ) matrix_diag -= batch_gather ( diag_update , reverse_perm ) row = tf . pad ( tensor = row , paddings = paddings ) row = batch_gather ( row , reverse_perm ) pchol_shape = pchol . shape pchol = tf . concat ( [ pchol [ ... , : m , : ] , row , pchol [ ... , m + 1 : , : ] ] , axis = - 2 ) tensorshape_util . set_shape ( pchol , pchol_shape ) return m + 1 , pchol , perm , matrix_diag m = np . int64 ( 0 ) pchol = tf . zeros_like ( matrix [ ... , : max_rank , : ] ) matrix_shape = prefer_static . shape ( matrix , out_type = tf . int64 ) perm = tf . broadcast_to ( prefer_static . range ( matrix_shape [ - 1 ] ) , matrix_shape [ : - 1 ] ) _ , pchol , _ , _ = tf . while_loop ( cond = cond , body = body , loop_vars = ( m , pchol , perm , matrix_diag ) ) pchol = tf . linalg . matrix_transpose ( pchol ) tensorshape_util . set_shape ( pchol , tensorshape_util . concatenate ( matrix_diag . shape , [ None ] ) ) return pchol",Computes the ( partial ) pivoted cholesky decomposition of matrix .
"def pinv ( a , rcond = None , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'pinv' , [ a , rcond ] ) : a = tf . convert_to_tensor ( value = a , name = 'a' ) assertions = _maybe_validate_matrix ( a , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : a = tf . identity ( a ) dtype = a . dtype . as_numpy_dtype if rcond is None : def get_dim_size ( dim ) : if tf . compat . dimension_value ( a . shape [ dim ] ) is not None : return tf . compat . dimension_value ( a . shape [ dim ] ) return tf . shape ( input = a ) [ dim ] num_rows = get_dim_size ( - 2 ) num_cols = get_dim_size ( - 1 ) if isinstance ( num_rows , int ) and isinstance ( num_cols , int ) : max_rows_cols = float ( max ( num_rows , num_cols ) ) else : max_rows_cols = tf . cast ( tf . maximum ( num_rows , num_cols ) , dtype ) rcond = 10. * max_rows_cols * np . finfo ( dtype ) . eps rcond = tf . convert_to_tensor ( value = rcond , dtype = dtype , name = 'rcond' ) [ singular_values , left_singular_vectors , right_singular_vectors , ] = tf . linalg . svd ( a , full_matrices = False , compute_uv = True ) cutoff = rcond * tf . reduce_max ( input_tensor = singular_values , axis = - 1 ) singular_values = tf . where ( singular_values > cutoff [ ... , tf . newaxis ] , singular_values , tf . fill ( tf . shape ( input = singular_values ) , np . array ( np . inf , dtype ) ) ) a_pinv = tf . matmul ( right_singular_vectors / singular_values [ ... , tf . newaxis , : ] , left_singular_vectors , adjoint_b = True ) if a . shape . ndims is not None : a_pinv . set_shape ( a . shape [ : - 2 ] . concatenate ( [ a . shape [ - 1 ] , a . shape [ - 2 ] ] ) ) return a_pinv",Compute the Moore - Penrose pseudo - inverse of a matrix .
"def lu_solve ( lower_upper , perm , rhs , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'lu_solve' , [ lower_upper , perm , rhs ] ) : lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) rhs = tf . convert_to_tensor ( value = rhs , dtype_hint = lower_upper . dtype , name = 'rhs' ) assertions = _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : lower_upper = tf . identity ( lower_upper ) perm = tf . identity ( perm ) rhs = tf . identity ( rhs ) if rhs . shape . ndims == 2 and perm . shape . ndims == 1 : permuted_rhs = tf . gather ( rhs , perm , axis = - 2 ) else : rhs_shape = tf . shape ( input = rhs ) broadcast_batch_shape = tf . broadcast_dynamic_shape ( rhs_shape [ : - 2 ] , tf . shape ( input = perm ) [ : - 1 ] ) d , m = rhs_shape [ - 2 ] , rhs_shape [ - 1 ] rhs_broadcast_shape = tf . concat ( [ broadcast_batch_shape , [ d , m ] ] , axis = 0 ) broadcast_rhs = tf . broadcast_to ( rhs , rhs_broadcast_shape ) broadcast_rhs = tf . reshape ( broadcast_rhs , [ - 1 , d , m ] ) broadcast_perm = tf . broadcast_to ( perm , rhs_broadcast_shape [ : - 1 ] ) broadcast_perm = tf . reshape ( broadcast_perm , [ - 1 , d ] ) broadcast_batch_size = tf . reduce_prod ( input_tensor = broadcast_batch_shape ) broadcast_batch_indices = tf . broadcast_to ( tf . range ( broadcast_batch_size ) [ : , tf . newaxis ] , [ broadcast_batch_size , d ] ) broadcast_perm = tf . stack ( [ broadcast_batch_indices , broadcast_perm ] , axis = - 1 ) permuted_rhs = tf . gather_nd ( broadcast_rhs , broadcast_perm ) permuted_rhs = tf . reshape ( permuted_rhs , rhs_broadcast_shape ) lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( tf . shape ( input = lower_upper ) [ : - 1 ] , dtype = lower_upper . dtype ) ) return linear_operator_util . matrix_triangular_solve_with_broadcast ( lower_upper , linear_operator_util . matrix_triangular_solve_with_broadcast ( lower , permuted_rhs ) , lower = False )",Solves systems of linear eqns A X = RHS given LU factorizations .
"def lu_matrix_inverse ( lower_upper , perm , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'lu_matrix_inverse' , [ lower_upper , perm ] ) : lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : lower_upper = tf . identity ( lower_upper ) perm = tf . identity ( perm ) shape = tf . shape ( input = lower_upper ) return lu_solve ( lower_upper , perm , rhs = tf . eye ( shape [ - 1 ] , batch_shape = shape [ : - 2 ] , dtype = lower_upper . dtype ) , validate_args = False )",Computes a matrix inverse given the matrix s LU decomposition .
"def lu_reconstruct ( lower_upper , perm , validate_args = False , name = None ) : with tf . compat . v1 . name_scope ( name , 'lu_reconstruct' , [ lower_upper , perm ] ) : lower_upper = tf . convert_to_tensor ( value = lower_upper , dtype_hint = tf . float32 , name = 'lower_upper' ) perm = tf . convert_to_tensor ( value = perm , dtype_hint = tf . int32 , name = 'perm' ) assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) if assertions : with tf . control_dependencies ( assertions ) : lower_upper = tf . identity ( lower_upper ) perm = tf . identity ( perm ) shape = tf . shape ( input = lower_upper ) lower = tf . linalg . set_diag ( tf . linalg . band_part ( lower_upper , num_lower = - 1 , num_upper = 0 ) , tf . ones ( shape [ : - 1 ] , dtype = lower_upper . dtype ) ) upper = tf . linalg . band_part ( lower_upper , num_lower = 0 , num_upper = - 1 ) x = tf . matmul ( lower , upper ) if lower_upper . shape . ndims is None or lower_upper . shape . ndims != 2 : batch_size = tf . reduce_prod ( input_tensor = shape [ : - 2 ] ) d = shape [ - 1 ] x = tf . reshape ( x , [ batch_size , d , d ] ) perm = tf . reshape ( perm , [ batch_size , d ] ) perm = tf . map_fn ( tf . math . invert_permutation , perm ) batch_indices = tf . broadcast_to ( tf . range ( batch_size ) [ : , tf . newaxis ] , [ batch_size , d ] ) x = tf . gather_nd ( x , tf . stack ( [ batch_indices , perm ] , axis = - 1 ) ) x = tf . reshape ( x , shape ) else : x = tf . gather ( x , tf . math . invert_permutation ( perm ) ) x . set_shape ( lower_upper . shape ) return x",The inverse LU decomposition X == lu_reconstruct ( * tf . linalg . lu ( X )) .
"def _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) : assertions = [ ] message = 'Input `lower_upper` must have at least 2 dimensions.' if lower_upper . shape . ndims is not None : if lower_upper . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( lower_upper , rank = 2 , message = message ) ) message = '`rank(lower_upper)` must equal `rank(perm) + 1`' if lower_upper . shape . ndims is not None and perm . shape . ndims is not None : if lower_upper . shape . ndims != perm . shape . ndims + 1 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank ( lower_upper , rank = tf . rank ( perm ) + 1 , message = message ) ) message = '`lower_upper` must be square.' if lower_upper . shape [ : - 2 ] . is_fully_defined ( ) : if lower_upper . shape [ - 2 ] != lower_upper . shape [ - 1 ] : raise ValueError ( message ) elif validate_args : m , n = tf . split ( tf . shape ( input = lower_upper ) [ - 2 : ] , num_or_size_splits = 2 ) assertions . append ( tf . compat . v1 . assert_equal ( m , n , message = message ) ) return assertions",Returns list of assertions related to lu_reconstruct assumptions .
"def _lu_solve_assertions ( lower_upper , perm , rhs , validate_args ) : assertions = _lu_reconstruct_assertions ( lower_upper , perm , validate_args ) message = 'Input `rhs` must have at least 2 dimensions.' if rhs . shape . ndims is not None : if rhs . shape . ndims < 2 : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( rhs , rank = 2 , message = message ) ) message = '`lower_upper.shape[-1]` must equal `rhs.shape[-1]`.' if ( tf . compat . dimension_value ( lower_upper . shape [ - 1 ] ) is not None and tf . compat . dimension_value ( rhs . shape [ - 2 ] ) is not None ) : if lower_upper . shape [ - 1 ] != rhs . shape [ - 2 ] : raise ValueError ( message ) elif validate_args : assertions . append ( tf . compat . v1 . assert_equal ( tf . shape ( input = lower_upper ) [ - 1 ] , tf . shape ( input = rhs ) [ - 2 ] , message = message ) ) return assertions",Returns list of assertions related to lu_solve assumptions .
"def sparse_or_dense_matmul ( sparse_or_dense_a , dense_b , validate_args = False , name = None , * * kwargs ) : with tf . compat . v1 . name_scope ( name , 'sparse_or_dense_matmul' , [ sparse_or_dense_a , dense_b ] ) : dense_b = tf . convert_to_tensor ( value = dense_b , dtype_hint = tf . float32 , name = 'dense_b' ) if validate_args : assert_a_rank_at_least_2 = tf . compat . v1 . assert_rank_at_least ( sparse_or_dense_a , rank = 2 , message = 'Input `sparse_or_dense_a` must have at least 2 dimensions.' ) assert_b_rank_at_least_2 = tf . compat . v1 . assert_rank_at_least ( dense_b , rank = 2 , message = 'Input `dense_b` must have at least 2 dimensions.' ) with tf . control_dependencies ( [ assert_a_rank_at_least_2 , assert_b_rank_at_least_2 ] ) : sparse_or_dense_a = tf . identity ( sparse_or_dense_a ) dense_b = tf . identity ( dense_b ) if isinstance ( sparse_or_dense_a , ( tf . SparseTensor , tf . compat . v1 . SparseTensorValue ) ) : return _sparse_tensor_dense_matmul ( sparse_or_dense_a , dense_b , * * kwargs ) else : return tf . matmul ( sparse_or_dense_a , dense_b , * * kwargs )",Returns ( batched ) matmul of a SparseTensor ( or Tensor ) with a Tensor .
"def sparse_or_dense_matvecmul ( sparse_or_dense_matrix , dense_vector , validate_args = False , name = None , * * kwargs ) : with tf . compat . v1 . name_scope ( name , 'sparse_or_dense_matvecmul' , [ sparse_or_dense_matrix , dense_vector ] ) : dense_vector = tf . convert_to_tensor ( value = dense_vector , dtype_hint = tf . float32 , name = 'dense_vector' ) return tf . squeeze ( sparse_or_dense_matmul ( sparse_or_dense_matrix , dense_vector [ ... , tf . newaxis ] , validate_args = validate_args , * * kwargs ) , axis = [ - 1 ] )",Returns ( batched ) matmul of a ( sparse ) matrix with a column vector .
"def _sparse_tensor_dense_matmul ( sp_a , b , * * kwargs ) : batch_shape = _get_shape ( sp_a ) [ : - 2 ] sp_a = tf . sparse . reshape ( sp_a , tf . concat ( [ [ - 1 ] , _get_shape ( sp_a ) [ - 2 : ] ] , axis = 0 ) ) b = tf . reshape ( b , tf . concat ( [ [ - 1 ] , _get_shape ( b ) [ - 1 : ] ] , axis = 0 ) ) out = tf . sparse . sparse_dense_matmul ( _sparse_block_diag ( sp_a ) , b , * * kwargs ) return tf . reshape ( out , tf . concat ( [ batch_shape , [ - 1 ] , _get_shape ( out ) [ - 1 : ] ] , axis = 0 ) )",Returns ( batched ) matmul of a SparseTensor with a Tensor .
"def _sparse_block_diag ( sp_a ) : sp_a_shape = tf . convert_to_tensor ( value = _get_shape ( sp_a , tf . int64 ) ) ind_mat = tf . concat ( [ [ sp_a_shape [ - 2 : ] ] , tf . eye ( 2 , dtype = tf . int64 ) ] , axis = 0 ) indices = tf . matmul ( sp_a . indices , ind_mat ) dense_shape = sp_a_shape [ 0 ] * sp_a_shape [ 1 : ] return tf . SparseTensor ( indices = indices , values = sp_a . values , dense_shape = dense_shape )",Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors .
"def _maybe_validate_matrix ( a , validate_args ) : assertions = [ ] if not a . dtype . is_floating : raise TypeError ( 'Input `a` must have `float`-like `dtype` ' '(saw {}).' . format ( a . dtype . name ) ) if a . shape . ndims is not None : if a . shape . ndims < 2 : raise ValueError ( 'Input `a` must have at least 2 dimensions ' '(saw: {}).' . format ( a . shape . ndims ) ) elif validate_args : assertions . append ( tf . compat . v1 . assert_rank_at_least ( a , rank = 2 , message = 'Input `a` must have at least 2 dimensions.' ) ) return assertions",Checks that input is a float matrix .
"def _grad_neg_log_likelihood_and_fim ( model_matrix , linear_response , response , model ) : mean , variance , grad_mean = model ( linear_response ) is_valid = ( tf . math . is_finite ( grad_mean ) & tf . not_equal ( grad_mean , 0. ) & tf . math . is_finite ( variance ) & ( variance > 0. ) ) def _mask_if_invalid ( x , mask ) : mask = tf . fill ( tf . shape ( input = x ) , value = np . array ( mask , x . dtype . as_numpy_dtype ) ) return tf . where ( is_valid , x , mask ) v = ( response - mean ) * _mask_if_invalid ( grad_mean , 1 ) / _mask_if_invalid ( variance , np . inf ) grad_log_likelihood = sparse_or_dense_matvecmul ( model_matrix , v , adjoint_a = True ) fim_middle = _mask_if_invalid ( grad_mean , 0. ) ** 2 / _mask_if_invalid ( variance , np . inf ) return - grad_log_likelihood , fim_middle",Computes the neg - log - likelihood gradient and Fisher information for a GLM .
"def fit_sparse_one_step ( model_matrix , response , model , model_coefficients_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_full_sweeps = None , learning_rate = None , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , l1_regularizer , l2_regularizer , maximum_full_sweeps , tolerance , learning_rate , ] with tf . compat . v1 . name_scope ( name , 'fit_sparse_one_step' , graph_deps ) : predicted_linear_response = sparse_or_dense_matvecmul ( model_matrix , model_coefficients_start ) g , h_middle = _grad_neg_log_likelihood_and_fim ( model_matrix , predicted_linear_response , response , model ) return tfp . optimizer . proximal_hessian_sparse_one_step ( gradient_unregularized_loss = g , hessian_unregularized_loss_outer = model_matrix , hessian_unregularized_loss_middle = h_middle , x_start = model_coefficients_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_full_sweeps = maximum_full_sweeps , tolerance = tolerance , learning_rate = learning_rate , name = name )",One step of ( the outer loop of ) the GLM fitting algorithm .
"def fit_sparse ( model_matrix , response , model , model_coefficients_start , tolerance , l1_regularizer , l2_regularizer = None , maximum_iterations = None , maximum_full_sweeps_per_iteration = 1 , learning_rate = None , name = None ) : graph_deps = [ model_matrix , response , model_coefficients_start , l1_regularizer , l2_regularizer , maximum_iterations , maximum_full_sweeps_per_iteration , tolerance , learning_rate , ] with tf . compat . v1 . name_scope ( name , 'fit_sparse' , graph_deps ) : def _grad_neg_log_likelihood_and_fim_fn ( x ) : predicted_linear_response = sparse_or_dense_matvecmul ( model_matrix , x ) g , h_middle = _grad_neg_log_likelihood_and_fim ( model_matrix , predicted_linear_response , response , model ) return g , model_matrix , h_middle return tfp . optimizer . proximal_hessian_sparse_minimize ( _grad_neg_log_likelihood_and_fim_fn , x_start = model_coefficients_start , l1_regularizer = l1_regularizer , l2_regularizer = l2_regularizer , maximum_iterations = maximum_iterations , maximum_full_sweeps_per_iteration = maximum_full_sweeps_per_iteration , learning_rate = learning_rate , tolerance = tolerance , name = name )",r Fits a GLM using coordinate - wise FIM - informed proximal gradient descent .
"def _gen_slices ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE ) : slices = [ ] col = 0 d_in = n_in // num_blocks d_out = n_out // num_blocks row = d_out if mask_type == MASK_EXCLUSIVE else 0 for _ in range ( num_blocks ) : row_slice = slice ( row , None ) col_slice = slice ( col , col + d_in ) slices . append ( [ row_slice , col_slice ] ) col += d_in row += d_out return slices",Generate the slices for building an autoregressive mask .
"def _gen_mask ( num_blocks , n_in , n_out , mask_type = MASK_EXCLUSIVE , dtype = tf . float32 ) : mask = np . zeros ( [ n_out , n_in ] , dtype = dtype . as_numpy_dtype ( ) ) slices = _gen_slices ( num_blocks , n_in , n_out , mask_type = mask_type ) for [ row_slice , col_slice ] in slices : mask [ row_slice , col_slice ] = 1 return mask",Generate the mask for building an autoregressive dense layer .
"def masked_dense ( inputs , units , num_blocks = None , exclusive = False , kernel_initializer = None , reuse = None , name = None , * args , * * kwargs ) : input_depth = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( inputs . shape , 1 ) [ - 1 ] ) if input_depth is None : raise NotImplementedError ( ""Rightmost dimension must be known prior to graph execution."" ) mask = _gen_mask ( num_blocks , input_depth , units , MASK_EXCLUSIVE if exclusive else MASK_INCLUSIVE ) . T if kernel_initializer is None : kernel_initializer = tf . compat . v1 . glorot_normal_initializer ( ) def masked_initializer ( shape , dtype = None , partition_info = None ) : return mask * kernel_initializer ( shape , dtype , partition_info ) with tf . compat . v2 . name_scope ( name or ""masked_dense"" ) : layer = tf . compat . v1 . layers . Dense ( units , kernel_initializer = masked_initializer , kernel_constraint = lambda x : mask * x , name = name , dtype = dtype_util . base_dtype ( inputs . dtype ) , _scope = name , _reuse = reuse , * args , * * kwargs ) return layer . apply ( inputs )",A autoregressively masked dense layer . Analogous to tf . layers . dense .
"def masked_autoregressive_default_template ( hidden_layers , shift_only = False , activation = tf . nn . relu , log_scale_min_clip = - 5. , log_scale_max_clip = 3. , log_scale_clip_gradient = False , name = None , * args , * * kwargs ) : name = name or ""masked_autoregressive_default_template"" with tf . compat . v2 . name_scope ( name ) : def _fn ( x ) : """"""MADE parameterized via `masked_autoregressive_default_template`."""""" input_depth = tf . compat . dimension_value ( tensorshape_util . with_rank_at_least ( x . shape , 1 ) [ - 1 ] ) if input_depth is None : raise NotImplementedError ( ""Rightmost dimension must be known prior to graph execution."" ) input_shape = ( np . int32 ( tensorshape_util . as_list ( x . shape ) ) if tensorshape_util . is_fully_defined ( x . shape ) else tf . shape ( input = x ) ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] for i , units in enumerate ( hidden_layers ) : x = masked_dense ( inputs = x , units = units , num_blocks = input_depth , exclusive = True if i == 0 else False , activation = activation , * args , * * kwargs ) x = masked_dense ( inputs = x , units = ( 1 if shift_only else 2 ) * input_depth , num_blocks = input_depth , activation = None , * args , * * kwargs ) if shift_only : x = tf . reshape ( x , shape = input_shape ) return x , None x = tf . reshape ( x , shape = tf . concat ( [ input_shape , [ 2 ] ] , axis = 0 ) ) shift , log_scale = tf . unstack ( x , num = 2 , axis = - 1 ) which_clip = ( tf . clip_by_value if log_scale_clip_gradient else clip_by_value_preserve_gradient ) log_scale = which_clip ( log_scale , log_scale_min_clip , log_scale_max_clip ) return shift , log_scale return tf . compat . v1 . make_template ( name , _fn )",Build the Masked Autoregressive Density Estimator ( Germain et al . 2015 ) .
"def _create_input_order ( input_size , input_order = ""left-to-right"" ) : if isinstance ( input_order , six . string_types ) : if input_order == ""left-to-right"" : return np . arange ( start = 1 , stop = input_size + 1 ) elif input_order == ""right-to-left"" : return np . arange ( start = input_size , stop = 0 , step = - 1 ) elif input_order == ""random"" : ret = np . arange ( start = 1 , stop = input_size + 1 ) np . random . shuffle ( ret ) return ret elif np . all ( np . sort ( input_order ) == np . arange ( 1 , input_size + 1 ) ) : return np . array ( input_order ) raise ValueError ( ""Invalid input order: '{}'."" . format ( input_order ) )",Returns a degree vectors for the input .
"def _create_degrees ( input_size , hidden_units = None , input_order = ""left-to-right"" , hidden_degrees = ""equal"" ) : input_order = _create_input_order ( input_size , input_order ) degrees = [ input_order ] if hidden_units is None : hidden_units = [ ] for units in hidden_units : if isinstance ( hidden_degrees , six . string_types ) : if hidden_degrees == ""random"" : degrees . append ( np . random . randint ( low = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) , high = input_size , size = units ) ) elif hidden_degrees == ""equal"" : min_degree = min ( np . min ( degrees [ - 1 ] ) , input_size - 1 ) degrees . append ( np . maximum ( min_degree , np . ceil ( np . arange ( 1 , units + 1 ) * ( input_size - 1 ) / float ( units + 1 ) ) . astype ( np . int32 ) ) ) else : raise ValueError ( 'Invalid hidden order: ""{}"".' . format ( hidden_degrees ) ) return degrees",Returns a list of degree vectors one for each input and hidden layer .
"def _create_masks ( degrees ) : return [ inp [ : , np . newaxis ] <= out for inp , out in zip ( degrees [ : - 1 ] , degrees [ 1 : ] ) ] + [ degrees [ - 1 ] [ : , np . newaxis ] < degrees [ 0 ] ]",Returns a list of binary mask matrices enforcing autoregressivity .
"def _make_masked_initializer ( mask , initializer ) : initializer = tf . keras . initializers . get ( initializer ) def masked_initializer ( shape , dtype = None , partition_info = None ) : if partition_info is None : x = initializer ( shape , dtype ) else : x = initializer ( shape , dtype , partition_info ) return tf . cast ( mask , x . dtype ) * x return masked_initializer",Returns a masked version of the given initializer .
"def build ( self , input_shape ) : if self . _event_shape is None : self . _event_shape = [ tf . compat . dimension_value ( input_shape [ - 1 ] ) ] self . _event_size = self . _event_shape [ - 1 ] self . _event_ndims = len ( self . _event_shape ) if input_shape [ - 1 ] != self . _event_shape [ - 1 ] : raise ValueError ( ""Invalid final dimension of `input_shape`. "" ""Expected `{!r}`, but got `{!r}`"" . format ( self . _event_shape [ - 1 ] , input_shape [ - 1 ] ) ) self . _input_order = _create_input_order ( self . _event_size , self . _input_order_param ) self . _masks = _create_masks ( _create_degrees ( input_size = self . _event_size , hidden_units = self . _hidden_units , input_order = self . _input_order , hidden_degrees = self . _hidden_degrees ) ) self . _masks [ - 1 ] = np . reshape ( np . tile ( self . _masks [ - 1 ] [ ... , tf . newaxis ] , [ 1 , 1 , self . _params ] ) , [ self . _masks [ - 1 ] . shape [ 0 ] , self . _event_size * self . _params ] ) self . _network = tf . keras . Sequential ( [ tf . keras . layers . InputLayer ( ( self . _event_size , ) , dtype = self . dtype ) ] ) layer_output_sizes = self . _hidden_units + [ self . _event_size * self . _params ] for k in range ( len ( self . _masks ) ) : self . _network . add ( tf . keras . layers . Dense ( layer_output_sizes [ k ] , kernel_initializer = _make_masked_initializer ( self . _masks [ k ] , self . _kernel_initializer ) , kernel_constraint = _make_masked_constraint ( self . _masks [ k ] ) , activation = self . _activation if k + 1 < len ( self . _masks ) else None , use_bias = self . _use_bias , * * self . _kwargs ) ) super ( AutoregressiveLayer , self ) . build ( input_shape )",See tfkl . Layer . build .
"def call ( self , x ) : with tf . compat . v2 . name_scope ( self . name or ""AutoregressiveLayer_call"" ) : x = tf . convert_to_tensor ( value = x , dtype = self . dtype , name = ""x"" ) input_shape = tf . shape ( input = x ) if tensorshape_util . rank ( x . shape ) == 1 : x = x [ tf . newaxis , ... ] return tf . reshape ( self . _network ( x ) , tf . concat ( [ input_shape , [ self . _params ] ] , axis = 0 ) )",See tfkl . Layer . call .
"def draw_sample ( num_samples , num_classes , logits , num_trials , dtype , seed ) : with tf . name_scope ( ""multinomial.draw_sample"" ) : num_trials = tf . ones_like ( logits [ ... , 0 ] , dtype = num_trials . dtype ) * num_trials logits = tf . ones_like ( num_trials [ ... , tf . newaxis ] , dtype = logits . dtype ) * logits flat_logits = tf . reshape ( logits , [ - 1 , num_classes ] ) flat_num_trials = num_samples * tf . reshape ( num_trials , [ - 1 ] ) def _sample_one_batch_member ( args ) : logits , num_cat_samples = args [ 0 ] , args [ 1 ] x = tf . random . categorical ( logits [ tf . newaxis , ... ] , num_cat_samples , seed = seed ) x = tf . reshape ( x , shape = [ num_samples , - 1 ] ) x = tf . one_hot ( x , depth = num_classes ) x = tf . reduce_sum ( input_tensor = x , axis = - 2 ) return tf . cast ( x , dtype = dtype ) x = tf . map_fn ( _sample_one_batch_member , [ flat_logits , flat_num_trials ] , dtype = dtype ) x = tf . transpose ( a = x , perm = [ 1 , 0 , 2 ] ) final_shape = tf . concat ( [ [ num_samples ] , tf . shape ( input = num_trials ) , [ num_classes ] ] , axis = 0 ) x = tf . reshape ( x , final_shape ) return x",Sample a multinomial .
"def _zero_dimensional_mvndiag ( dtype ) : dummy_mvndiag = tfd . MultivariateNormalDiag ( scale_diag = tf . ones ( [ 0 ] , dtype = dtype ) ) dummy_mvndiag . covariance = lambda : dummy_mvndiag . variance ( ) [ ... , tf . newaxis ] return dummy_mvndiag",Build a zero - dimensional MVNDiag object .
"def _observe_timeseries_fn ( timeseries ) : def observation_noise_fn ( t ) : current_slice = timeseries [ ... , t , : ] return tfd . MultivariateNormalDiag ( loc = current_slice , scale_diag = tf . zeros_like ( current_slice ) ) return observation_noise_fn",Build an observation_noise_fn that observes a Tensor timeseries .
"def params_to_weights ( self , global_scale_variance , global_scale_noncentered , local_scale_variances , local_scales_noncentered , weights_noncentered ) : global_scale = ( global_scale_noncentered * tf . sqrt ( global_scale_variance ) * self . weights_prior_scale ) local_scales = local_scales_noncentered * tf . sqrt ( local_scale_variances ) return weights_noncentered * local_scales * global_scale [ ... , tf . newaxis ]",Build regression weights from model parameters .
"def _depth ( g ) : def _explore ( v ) : if v . depth < 0 : v . depth = ( ( 1 + max ( [ - 1 ] + [ _explore ( annotated_graph [ u ] ) for u in v . parents ] ) ) if v . parents else 0 ) return v . depth annotated_graph = { k : _Node ( k , v ) for k , v in g . items ( ) } for v in annotated_graph . values ( ) : _explore ( v ) return annotated_graph",Computes the number of edges on longest path from node to root .
"def _best_order ( g ) : def _explore ( u ) : """"""Recursive function to ascend up through unvisited dependencies."""""" if u . depth < 0 : return if not u . parents : result . append ( ( u . name , u . parents ) ) u . depth = - 1 return b = ( u . name , [ ] ) result . append ( b ) u . depth = - 1 d = 0 for v in sorted ( ( g . get ( p ) for p in u . parents ) , key = lambda v : v . depth ) : n0 = len ( result ) _explore ( v ) n1 = len ( result ) b [ 1 ] . extend ( [ '_' ] * d + [ v . name ] ) d = n1 - n0 - 1 g = _depth ( g ) result = [ ] for u in sorted ( g . values ( ) , key = lambda v : v . depth , reverse = True ) : _explore ( u ) return tuple ( reversed ( result ) )",Creates tuple of str tuple - str pairs representing resolved & sorted DAG .
"def _prob_chain_rule_flatten ( named_makers ) : def _make ( dist_fn , args ) : if args is None : return lambda * _ : dist_fn if not args : return lambda * _ : dist_fn ( ) def _fn ( * xs ) : kwargs = dict ( zip ( args , reversed ( xs [ - len ( args ) : ] ) ) ) kwargs . pop ( '_' , None ) return dist_fn ( * * kwargs ) return _fn named_makers = _convert_to_dict ( named_makers ) g = { k : ( None if distribution_util . is_distribution_instance ( v ) else joint_distribution_sequential . _get_required_args ( v ) ) for k , v in named_makers . items ( ) } g = _best_order ( g ) dist_fn_name , dist_fn_args = zip ( * g ) dist_fn_args = tuple ( None if a is None else tuple ( a ) for a in dist_fn_args ) dist_fn_wrapped = tuple ( _make ( named_makers [ name ] , parents ) for ( name , parents ) in g ) dist_fn = tuple ( named_makers . get ( n ) for n in dist_fn_name ) return dist_fn , dist_fn_wrapped , dist_fn_args , dist_fn_name",Creates lists of callables suitable for JDSeq .
"def _build ( self , model ) : if not _is_dict_like ( model ) : raise TypeError ( '`model` must be convertible to `dict` (saw: {}).' . format ( type ( model ) . __name__ ) ) [ self . _dist_fn , self . _dist_fn_wrapped , self . _dist_fn_args , self . _dist_fn_name , ] = _prob_chain_rule_flatten ( model )",Creates dist_fn dist_fn_wrapped dist_fn_args dist_fn_name .
"def variational_loss ( self , observations , observation_index_points = None , kl_weight = 1. , name = 'variational_loss' ) : with tf . name_scope ( name or 'variational_gp_loss' ) : if observation_index_points is None : observation_index_points = self . _index_points observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = self . _dtype , name = 'observation_index_points' ) observations = tf . convert_to_tensor ( value = observations , dtype = self . _dtype , name = 'observations' ) kl_weight = tf . convert_to_tensor ( value = kl_weight , dtype = self . _dtype , name = 'kl_weight' ) kzx = self . kernel . matrix ( self . _inducing_index_points , observation_index_points ) kzx_linop = tf . linalg . LinearOperatorFullMatrix ( kzx ) loc = ( self . _mean_fn ( observation_index_points ) + kzx_linop . matvec ( self . _kzz_inv_varloc , adjoint = True ) ) likelihood = independent . Independent ( normal . Normal ( loc = loc , scale = tf . sqrt ( self . _observation_noise_variance + self . _jitter ) , name = 'NormalLikelihood' ) , reinterpreted_batch_ndims = 1 ) obs_ll = likelihood . log_prob ( observations ) chol_kzz_linop = tf . linalg . LinearOperatorLowerTriangular ( self . _chol_kzz ) chol_kzz_inv_kzx = chol_kzz_linop . solve ( kzx ) kzz_inv_kzx = chol_kzz_linop . solve ( chol_kzz_inv_kzx , adjoint = True ) kxx_diag = tf . linalg . diag_part ( self . kernel . matrix ( observation_index_points , observation_index_points ) ) ktilde_trace_term = ( tf . reduce_sum ( input_tensor = kxx_diag , axis = - 1 ) - tf . reduce_sum ( input_tensor = chol_kzz_inv_kzx ** 2 , axis = [ - 2 , - 1 ] ) ) other_trace_term = tf . reduce_sum ( input_tensor = ( self . _variational_inducing_observations_posterior . scale . matmul ( kzz_inv_kzx ) ** 2 ) , axis = [ - 2 , - 1 ] ) trace_term = ( .5 * ( ktilde_trace_term + other_trace_term ) / self . _observation_noise_variance ) inducing_prior = gaussian_process . GaussianProcess ( kernel = self . _kernel , mean_fn = self . _mean_fn , index_points = self . _inducing_index_points , observation_noise_variance = self . _observation_noise_variance ) kl_term = kl_weight * kullback_leibler . kl_divergence ( self . _variational_inducing_observations_posterior , inducing_prior ) lower_bound = ( obs_ll - trace_term - kl_term ) return - tf . reduce_mean ( input_tensor = lower_bound )",Variational loss for the VGP .
"def optimal_variational_posterior ( kernel , inducing_index_points , observation_index_points , observations , observation_noise_variance , mean_fn = None , jitter = 1e-6 , name = None ) : with tf . name_scope ( name or 'optimal_variational_posterior' ) : dtype = dtype_util . common_dtype ( [ inducing_index_points , observation_index_points , observations , observation_noise_variance , jitter ] , tf . float32 ) inducing_index_points = tf . convert_to_tensor ( value = inducing_index_points , dtype = dtype , name = 'inducing_index_points' ) observation_index_points = tf . convert_to_tensor ( value = observation_index_points , dtype = dtype , name = 'observation_index_points' ) observations = tf . convert_to_tensor ( value = observations , dtype = dtype , name = 'observations' ) observation_noise_variance = tf . convert_to_tensor ( value = observation_noise_variance , dtype = dtype , name = 'observation_noise_variance' ) jitter = tf . convert_to_tensor ( value = jitter , dtype = dtype , name = 'jitter' ) if mean_fn is None : mean_fn = lambda x : tf . zeros ( [ 1 ] , dtype = dtype ) else : if not callable ( mean_fn ) : raise ValueError ( '`mean_fn` must be a Python callable' ) kzz = kernel . matrix ( inducing_index_points , inducing_index_points ) kzx = kernel . matrix ( inducing_index_points , observation_index_points ) noise_var_inv = tf . math . reciprocal ( observation_noise_variance ) sigma_inv = _add_diagonal_shift ( kzz + noise_var_inv * tf . matmul ( kzx , kzx , adjoint_b = True ) , jitter ) chol_sigma_inv = tf . linalg . cholesky ( sigma_inv ) kzx_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzx ) kzx_obs = kzx_lin_op . matvec ( observations - mean_fn ( observation_index_points ) ) kzz_lin_op = tf . linalg . LinearOperatorFullMatrix ( kzz ) loc = ( mean_fn ( inducing_index_points ) + noise_var_inv * kzz_lin_op . matvec ( _solve_cholesky_factored_system_vec ( chol_sigma_inv , kzx_obs ) ) ) chol_sigma_inv_lin_op = tf . linalg . LinearOperatorLowerTriangular ( chol_sigma_inv ) scale = chol_sigma_inv_lin_op . solve ( kzz ) return loc , scale",Model selection for optimal variational hyperparameters .
"def build_is_last_day_of_season ( num_steps_per_season ) : num_steps_per_cycle = np . sum ( num_steps_per_season ) changepoints = np . cumsum ( np . ravel ( num_steps_per_season ) ) - 1 def is_last_day_of_season ( t ) : t_ = dist_util . maybe_get_static_value ( t ) if t_ is not None : step_in_cycle = t_ % num_steps_per_cycle return any ( step_in_cycle == changepoints ) else : step_in_cycle = tf . math . floormod ( t , num_steps_per_cycle ) return tf . reduce_any ( input_tensor = tf . equal ( step_in_cycle , changepoints ) ) return is_last_day_of_season",Build utility method to compute whether the season is changing .
"def build_effects_to_residuals_matrix ( num_seasons , dtype ) : effects_to_residuals_fullrank = np . eye ( num_seasons ) - 1. / num_seasons effects_to_residuals_fullrank [ - 1 , : ] = 1. / num_seasons residuals_to_effects_fullrank = np . linalg . inv ( effects_to_residuals_fullrank ) effects_to_residuals = effects_to_residuals_fullrank [ : - 1 , : ] residuals_to_effects = residuals_to_effects_fullrank [ : , : - 1 ] effects_to_residuals = tf . cast ( effects_to_residuals , dtype = dtype , name = 'effects_to_residuals' ) residuals_to_effects = tf . cast ( residuals_to_effects , dtype = dtype , name = 'residuals_to_effects' ) return effects_to_residuals , residuals_to_effects",Build change - of - basis matrices for constrained seasonal effects .
"def build_seasonal_transition_matrix ( num_seasons , is_last_day_of_season , dtype , basis_change_matrix = None , basis_change_matrix_inv = None ) : with tf . compat . v1 . name_scope ( 'build_seasonal_transition_matrix' ) : seasonal_permutation = np . concatenate ( [ np . arange ( 1 , num_seasons ) , [ 0 ] ] , axis = 0 ) seasonal_permutation_matrix = tf . constant ( np . eye ( num_seasons ) [ seasonal_permutation ] , dtype = dtype ) if basis_change_matrix is not None : seasonal_permutation_matrix = tf . matmul ( basis_change_matrix , tf . matmul ( seasonal_permutation_matrix , basis_change_matrix_inv ) ) identity_matrix = tf . eye ( tf . shape ( input = seasonal_permutation_matrix ) [ - 1 ] , dtype = dtype ) def seasonal_transition_matrix ( t ) : return tf . linalg . LinearOperatorFullMatrix ( matrix = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , seasonal_permutation_matrix , identity_matrix ) ) return seasonal_transition_matrix",Build a function computing transitions for a seasonal effect model .
"def build_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : drift_scale_diag = tf . stack ( [ tf . zeros_like ( drift_scale ) ] * ( num_seasons - 1 ) + [ drift_scale ] , axis = - 1 ) def seasonal_transition_noise ( t ) : noise_scale_diag = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_diag , tf . zeros_like ( drift_scale_diag ) ) return tfd . MultivariateNormalDiag ( loc = tf . zeros ( num_seasons , dtype = drift_scale . dtype ) , scale_diag = noise_scale_diag ) return seasonal_transition_noise",Build the transition noise model for a SeasonalStateSpaceModel .
"def build_constrained_seasonal_transition_noise ( drift_scale , num_seasons , is_last_day_of_season ) : drift_scale_tril_nonzeros = tf . concat ( [ tf . ones ( [ num_seasons - 1 , 1 ] , dtype = drift_scale . dtype ) , tf . zeros ( [ num_seasons - 1 , num_seasons - 2 ] , dtype = drift_scale . dtype ) ] , axis = - 1 ) drift_scale_tril = ( drift_scale_tril_nonzeros * drift_scale [ ... , tf . newaxis , tf . newaxis ] / num_seasons ) def seasonal_transition_noise ( t ) : noise_scale_tril = dist_util . pick_scalar_condition ( is_last_day_of_season ( t ) , drift_scale_tril , tf . zeros_like ( drift_scale_tril ) ) return tfd . MultivariateNormalTriL ( loc = tf . zeros ( num_seasons - 1 , dtype = drift_scale . dtype ) , scale_tril = noise_scale_tril ) return seasonal_transition_noise",Build transition noise distribution for a ConstrainedSeasonalSSM .
"def _is_empty_observation_data ( feature_ndims , observation_index_points , observations ) : if observation_index_points is None and observations is None : return True num_obs = tf . compat . dimension_value ( observation_index_points . shape [ - ( feature_ndims + 1 ) ] ) if num_obs is not None and num_obs == 0 : return True return False",Returns True if given observation data is empty .
"def _validate_observation_data ( kernel , observation_index_points , observations ) : ndims = kernel . feature_ndims if ( tensorshape_util . is_fully_defined ( observation_index_points . shape [ : - ndims ] ) and tensorshape_util . is_fully_defined ( observations . shape ) ) : index_point_count = observation_index_points . shape [ : - ndims ] observation_count = observations . shape try : tf . broadcast_static_shape ( index_point_count , observation_count ) except ValueError : raise ValueError ( 'Observation index point and observation counts are not ' 'broadcastable: {} and {}, respectively.' . format ( index_point_count , observation_count ) )",Ensure that observation data and locations have consistent shapes .
"def _kl_gamma_gamma ( g0 , g1 , name = None ) : with tf . name_scope ( name or ""kl_gamma_gamma"" ) : return ( ( ( g0 . concentration - g1 . concentration ) * tf . math . digamma ( g0 . concentration ) ) + tf . math . lgamma ( g1 . concentration ) - tf . math . lgamma ( g0 . concentration ) + g1 . concentration * tf . math . log ( g0 . rate ) - g1 . concentration * tf . math . log ( g1 . rate ) + g0 . concentration * ( g1 . rate / g0 . rate - 1. ) )",Calculate the batched KL divergence KL ( g0 || g1 ) with g0 and g1 Gamma .
"def add ( self , scheduler , max_iteration , bigdl_type = ""float"" ) : return callBigDlFunc ( bigdl_type , ""addScheduler"" , self . value , scheduler , max_iteration )",Add a learning rate scheduler to the contained schedules
"def save ( self , path , overWrite ) : method = self . value return callBigDlFunc ( self . bigdl_type , ""saveOptimMethod"" , method , path , overWrite )",save OptimMethod : param path path : param overWrite whether to overwrite
"def set_checkpoint ( self , checkpoint_trigger , checkpoint_path , isOverWrite = True ) : if not os . path . exists ( checkpoint_path ) : mkpath ( checkpoint_path ) callBigDlFunc ( self . bigdl_type , ""setCheckPoint"" , self . value , checkpoint_trigger , checkpoint_path , isOverWrite )",Configure checkpoint settings .
"def set_gradclip_const ( self , min_value , max_value ) : callBigDlFunc ( self . bigdl_type , ""setConstantClip"" , self . value , min_value , max_value )",Configure constant clipping settings .
def optimize ( self ) : jmodel = callJavaFunc ( self . value . optimize ) from bigdl . nn . layer import Layer return Layer . of ( jmodel ),Do an optimization .
"def set_train_summary ( self , summary ) : callBigDlFunc ( self . bigdl_type , ""setTrainSummary"" , self . value , summary ) return self",Set train summary . A TrainSummary object contains information necessary for the optimizer to know how often the logs are recorded where to store the logs and how to retrieve them etc . For details refer to the docs of TrainSummary .
"def set_val_summary ( self , summary ) : callBigDlFunc ( self . bigdl_type , ""setValSummary"" , self . value , summary ) return self",Set validation summary . A ValidationSummary object contains information necessary for the optimizer to know how often the logs are recorded where to store the logs and how to retrieve them etc . For details refer to the docs of ValidationSummary .
"def create ( model , training_set , criterion , end_trigger = None , batch_size = 32 , optim_method = None , cores = None , bigdl_type = ""float"" ) : if not end_trigger : end_trigger = MaxEpoch ( 1 ) if not optim_method : optim_method = SGD ( ) if isinstance ( training_set , RDD ) or isinstance ( training_set , DataSet ) : return DistriOptimizer ( model = model , training_rdd = training_set , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , bigdl_type = bigdl_type ) elif isinstance ( training_set , tuple ) and len ( training_set ) == 2 : x , y = training_set return LocalOptimizer ( X = x , Y = y , model = model , criterion = criterion , end_trigger = end_trigger , batch_size = batch_size , optim_method = optim_method , cores = cores , bigdl_type = ""float"" ) else : raise Exception ( ""Not supported training set: %s"" % type ( training_set ) )",Create an optimizer . Depend on the input type the returning optimizer can be a local optimizer \ or a distributed optimizer .
"def set_validation ( self , batch_size , val_rdd , trigger , val_method = None ) : if val_method is None : val_method = [ Top1Accuracy ( ) ] func_name = ""setValidation"" if isinstance ( val_rdd , DataSet ) : func_name = ""setValidationFromDataSet"" callBigDlFunc ( self . bigdl_type , func_name , self . value , batch_size , trigger , val_rdd , to_list ( val_method ) )",Configure validation settings .
"def set_traindata ( self , training_rdd , batch_size ) : callBigDlFunc ( self . bigdl_type , ""setTrainData"" , self . value , training_rdd , batch_size )",Set new training dataset for optimizer reuse
"def set_validation ( self , batch_size , X_val , Y_val , trigger , val_method = None ) : if val_method is None : val_method = [ Top1Accuracy ( ) ] callBigDlFunc ( self . bigdl_type , ""setValidation"" , self . value , batch_size , trigger , [ JTensor . from_ndarray ( X ) for X in to_list ( X_val ) ] , JTensor . from_ndarray ( Y_val ) , to_list ( val_method ) )",Configure validation settings .
"def set_summary_trigger ( self , name , trigger ) : return callBigDlFunc ( self . bigdl_type , ""summarySetTrigger"" , self . value , name , trigger )",Set the interval of recording for each indicator .
"def read_data_sets ( train_dir , data_type = ""train"" ) : TRAIN_IMAGES = 'train-images-idx3-ubyte.gz' TRAIN_LABELS = 'train-labels-idx1-ubyte.gz' TEST_IMAGES = 't10k-images-idx3-ubyte.gz' TEST_LABELS = 't10k-labels-idx1-ubyte.gz' if data_type == ""train"" : local_file = base . maybe_download ( TRAIN_IMAGES , train_dir , SOURCE_URL + TRAIN_IMAGES ) with open ( local_file , 'rb' ) as f : train_images = extract_images ( f ) local_file = base . maybe_download ( TRAIN_LABELS , train_dir , SOURCE_URL + TRAIN_LABELS ) with open ( local_file , 'rb' ) as f : train_labels = extract_labels ( f ) return train_images , train_labels else : local_file = base . maybe_download ( TEST_IMAGES , train_dir , SOURCE_URL + TEST_IMAGES ) with open ( local_file , 'rb' ) as f : test_images = extract_images ( f ) local_file = base . maybe_download ( TEST_LABELS , train_dir , SOURCE_URL + TEST_LABELS ) with open ( local_file , 'rb' ) as f : test_labels = extract_labels ( f ) return test_images , test_labels",Parse or download mnist data if train_dir is empty .
"def get_news20 ( source_dir = ""./data/news20/"" ) : news_dir = download_news20 ( source_dir ) texts = [ ] label_id = 0 for name in sorted ( os . listdir ( news_dir ) ) : path = os . path . join ( news_dir , name ) label_id += 1 if os . path . isdir ( path ) : for fname in sorted ( os . listdir ( path ) ) : if fname . isdigit ( ) : fpath = os . path . join ( path , fname ) if sys . version_info < ( 3 , ) : f = open ( fpath ) else : f = open ( fpath , encoding = 'latin-1' ) content = f . read ( ) texts . append ( ( content , label_id ) ) f . close ( ) print ( 'Found %s texts.' % len ( texts ) ) return texts",Parse or download news20 if source_dir is empty .
"def get_glove_w2v ( source_dir = ""./data/news20/"" , dim = 100 ) : w2v_dir = download_glove_w2v ( source_dir ) w2v_path = os . path . join ( w2v_dir , ""glove.6B.%sd.txt"" % dim ) if sys . version_info < ( 3 , ) : w2v_f = open ( w2v_path ) else : w2v_f = open ( w2v_path , encoding = 'latin-1' ) pre_w2v = { } for line in w2v_f . readlines ( ) : items = line . split ( "" "" ) pre_w2v [ items [ 0 ] ] = [ float ( i ) for i in items [ 1 : ] ] w2v_f . close ( ) return pre_w2v",Parse or download the pre - trained glove word2vec if source_dir is empty .
"def compile ( self , optimizer , loss , metrics = None ) : if isinstance ( optimizer , six . string_types ) : optimizer = self . __convert_optim_method ( optimizer ) if isinstance ( loss , six . string_types ) : loss = self . __convert_criterion ( loss ) if all ( isinstance ( metric , six . string_types ) for metric in metrics ) : metrics = self . __convert_metrics ( metrics ) callBigDlFunc ( self . bigdl_type , ""compile"" , self . value , optimizer , loss , metrics )",Configures the learning process . Must be called before fit or evaluate .
"def fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , validation_data = None , distributed = True ) : if distributed : if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : training_data = to_sample_rdd ( x , y ) if validation_data : validation_data = to_sample_rdd ( * validation_data ) elif ( isinstance ( x , RDD ) or isinstance ( x , DataSet ) ) and not y : training_data = x else : raise TypeError ( ""Unsupported training data type: %s"" % type ( x ) ) callBigDlFunc ( self . bigdl_type , ""fit"" , self . value , training_data , batch_size , nb_epoch , validation_data ) else : if validation_data : val_x = [ JTensor . from_ndarray ( x ) for x in to_list ( validation_data [ 0 ] ) ] val_y = JTensor . from_ndarray ( validation_data [ 1 ] ) else : val_x , val_y = None , None callBigDlFunc ( self . bigdl_type , ""fit"" , self . value , [ JTensor . from_ndarray ( x ) for x in to_list ( x ) ] , JTensor . from_ndarray ( y ) , batch_size , nb_epoch , val_x , val_y , multiprocessing . cpu_count ( ) )",Train a model for a fixed number of epochs on a dataset .
"def evaluate ( self , x , y = None , batch_size = 32 ) : if isinstance ( x , np . ndarray ) and isinstance ( y , np . ndarray ) : evaluation_data = to_sample_rdd ( x , y ) elif isinstance ( x , RDD ) and not y : evaluation_data = x else : raise TypeError ( ""Unsupported evaluation data type: %s"" % type ( x ) ) return callBigDlFunc ( self . bigdl_type , ""evaluate"" , self . value , evaluation_data , batch_size )",Evaluate a model on a given dataset in distributed mode .
"def predict ( self , x , distributed = True ) : if is_distributed : if isinstance ( x , np . ndarray ) : features = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) elif isinstance ( x , RDD ) : features = x else : raise TypeError ( ""Unsupported prediction data type: %s"" % type ( x ) ) return self . predict_distributed ( features ) else : if isinstance ( x , np . ndarray ) : return self . predict_local ( x ) else : raise TypeError ( ""Unsupported prediction data type: %s"" % type ( x ) )",Use a model to do prediction .
"def from_jvalue ( jvalue , bigdl_type = ""float"" ) : model = Sequential ( jvalue = jvalue ) model . value = jvalue return model",Create a Python Model base on the given java value : param jvalue : Java object create by Py4j : return : A Python Model
"def from_jvalue ( jvalue , bigdl_type = ""float"" ) : model = Model ( [ ] , [ ] , jvalue = jvalue ) model . value = jvalue return model",Create a Python Model base on the given java value : param jvalue : Java object create by Py4j : return : A Python Model
"def get_mnist ( sc , data_type = ""train"" , location = ""/tmp/mnist"" ) : ( images , labels ) = mnist . read_data_sets ( location , data_type ) images = sc . parallelize ( images ) labels = sc . parallelize ( labels + 1 ) record = images . zip ( labels ) return record",Get mnist dataset and parallelize into RDDs . Data would be downloaded automatically if it doesn t present at the specific location .
"def preprocess_mnist ( sc , options ) : train_data = get_mnist ( sc , ""train"" , options . dataPath ) . map ( lambda rec_tuple : ( normalizer ( rec_tuple [ 0 ] , mnist . TRAIN_MEAN , mnist . TRAIN_STD ) , rec_tuple [ 1 ] ) ) . map ( lambda t : Sample . from_ndarray ( t [ 0 ] , t [ 1 ] ) ) test_data = get_mnist ( sc , ""test"" , options . dataPath ) . map ( lambda rec_tuple : ( normalizer ( rec_tuple [ 0 ] , mnist . TEST_MEAN , mnist . TEST_STD ) , rec_tuple [ 1 ] ) ) . map ( lambda t : Sample . from_ndarray ( t [ 0 ] , t [ 1 ] ) ) return train_data , test_data",Preprocess mnist dataset . Normalize and transform into Sample of RDDs .
"def get_end_trigger ( options ) : if options . endTriggerType . lower ( ) == ""epoch"" : return MaxEpoch ( options . endTriggerNum ) else : return MaxIteration ( options . endTriggerNum )",When to end the optimization based on input option .
"def validate_optimizer ( optimizer , test_data , options ) : optimizer . set_validation ( batch_size = options . batchSize , val_rdd = test_data , trigger = EveryEpoch ( ) , val_method = [ Top1Accuracy ( ) ] ) optimizer . set_checkpoint ( EveryEpoch ( ) , options . checkpointPath )",Set validation and checkpoint for distributed optimizer .
"def setBatchSize ( self , val ) : self . _paramMap [ self . batchSize ] = val pythonBigDL_method_name = ""setBatchSize"" + self . __class__ . __name__ callBigDlFunc ( self . bigdl_type , pythonBigDL_method_name , self . value , val ) return self",Sets the value of : py : attr : batchSize .
"def value ( self ) : if not hasattr ( self , ""_value"" ) and self . _path is not None : self . _value = self . _load ( self . _path ) return self . _value",Return the broadcasted value
"def to_sample_rdd ( x , y , numSlices = None ) : sc = get_spark_context ( ) from bigdl . util . common import Sample x_rdd = sc . parallelize ( x , numSlices ) y_rdd = sc . parallelize ( y , numSlices ) return x_rdd . zip ( y_rdd ) . map ( lambda item : Sample . from_ndarray ( item [ 0 ] , item [ 1 ] ) )",Conver x and y into RDD [ Sample ] : param x : ndarray and the first dimension should be batch : param y : ndarray and the first dimension should be batch : param numSlices : : return :
"def get_spark_context ( conf = None ) : if hasattr ( SparkContext , ""getOrCreate"" ) : with SparkContext . _lock : if SparkContext . _active_spark_context is None : spark_conf = create_spark_conf ( ) if conf is None else conf return SparkContext . getOrCreate ( spark_conf ) else : return SparkContext . getOrCreate ( ) else : if SparkContext . _active_spark_context is None : spark_conf = create_spark_conf ( ) if conf is None else conf return SparkContext ( conf = spark_conf ) else : return SparkContext . _active_spark_context",Get the current active spark context and create one if no active instance : param conf : combining bigdl configs into spark conf : return : SparkContext
"def callBigDlFunc ( bigdl_type , name , * args ) : gateway = _get_gateway ( ) error = Exception ( ""Cannot find function: %s"" % name ) for jinvoker in JavaCreator . instance ( bigdl_type , gateway ) . value : try : api = getattr ( jinvoker , name ) result = callJavaFunc ( api , * args ) except Exception as e : error = e if ""does not exist"" not in str ( e ) : raise e else : return result raise error",Call API in PythonBigDL
"def callJavaFunc ( func , * args ) : gateway = _get_gateway ( ) args = [ _py2java ( gateway , a ) for a in args ] result = func ( * args ) return _java2py ( gateway , result )",Call Java Function
"def _to_java_object_rdd ( rdd ) : rdd = rdd . _reserialize ( AutoBatchedSerializer ( PickleSerializer ( ) ) ) return rdd . ctx . _jvm . org . apache . spark . bigdl . api . python . BigDLSerDe . pythonToJava ( rdd . _jrdd , True )",Return a JavaRDD of Object by unpickling
"def _py2java ( gateway , obj ) : if isinstance ( obj , RDD ) : obj = _to_java_object_rdd ( obj ) elif isinstance ( obj , DataFrame ) : obj = obj . _jdf elif isinstance ( obj , SparkContext ) : obj = obj . _jsc elif isinstance ( obj , ( list , tuple ) ) : obj = ListConverter ( ) . convert ( [ _py2java ( gateway , x ) for x in obj ] , gateway . _gateway_client ) elif isinstance ( obj , dict ) : result = { } for ( key , value ) in obj . items ( ) : result [ key ] = _py2java ( gateway , value ) obj = MapConverter ( ) . convert ( result , gateway . _gateway_client ) elif isinstance ( obj , JavaValue ) : obj = obj . value elif isinstance ( obj , JavaObject ) : pass elif isinstance ( obj , ( int , long , float , bool , bytes , unicode ) ) : pass else : data = bytearray ( PickleSerializer ( ) . dumps ( obj ) ) obj = gateway . jvm . org . apache . spark . bigdl . api . python . BigDLSerDe . loads ( data ) return obj",Convert Python object into Java
"def get_activation_by_name ( activation_name , activation_id = None ) : import bigdl . nn . layer as BLayer activation = None activation_name = activation_name . lower ( ) if activation_name == ""tanh"" : activation = BLayer . Tanh ( ) elif activation_name == ""sigmoid"" : activation = BLayer . Sigmoid ( ) elif activation_name == ""hard_sigmoid"" : activation = BLayer . HardSigmoid ( ) elif activation_name == ""relu"" : activation = BLayer . ReLU ( ) elif activation_name == ""softmax"" : activation = BLayer . SoftMax ( ) elif activation_name == ""softplus"" : activation = BLayer . SoftPlus ( beta = 1.0 ) elif activation_name == ""softsign"" : activation = BLayer . SoftSign ( ) elif activation_name == ""linear"" : activation = BLayer . Identity ( ) else : raise Exception ( ""Unsupported activation type: %s"" % activation_name ) if not activation_id : activation . set_name ( activation_id ) return activation",Convert to a bigdl activation layer given the name of the activation as a string
"def from_ndarray ( cls , a_ndarray , bigdl_type = ""float"" ) : if a_ndarray is None : return None assert isinstance ( a_ndarray , np . ndarray ) , ""input should be a np.ndarray, not %s"" % type ( a_ndarray ) return cls ( a_ndarray , a_ndarray . shape if a_ndarray . shape else ( a_ndarray . size ) , bigdl_type )",Convert a ndarray to a DenseTensor which would be used in Java side .
"def sparse ( cls , a_ndarray , i_ndarray , shape , bigdl_type = ""float"" ) : if a_ndarray is None : return None assert isinstance ( a_ndarray , np . ndarray ) , ""values array should be a np.ndarray, not %s"" % type ( a_ndarray ) assert isinstance ( i_ndarray , np . ndarray ) , ""indices array should be a np.ndarray, not %s"" % type ( a_ndarray ) assert i_ndarray . size == a_ndarray . size * shape . size , ""size of values and indices should match."" return cls ( a_ndarray , shape , bigdl_type , i_ndarray )",Convert a three ndarray to SparseTensor which would be used in Java side . For example : a_ndarray = [ 1 3 2 4 ] i_ndarray = [[ 0 0 1 2 ] [ 0 3 2 1 ]] shape = [ 3 4 ] Present a dense tensor [[ 1 0 0 3 ] [ 0 0 2 0 ] [ 0 4 0 0 ]]
"def to_ndarray ( self ) : assert self . indices is None , ""sparseTensor to ndarray is not supported"" return np . array ( self . storage , dtype = get_dtype ( self . bigdl_type ) ) . reshape ( self . shape )",Transfer JTensor to ndarray . As SparseTensor may generate an very big ndarray so we don t support this function for SparseTensor . : return : a ndarray
"def from_ndarray ( cls , features , labels , bigdl_type = ""float"" ) : if isinstance ( features , np . ndarray ) : features = [ features ] else : assert all ( isinstance ( feature , np . ndarray ) for feature in features ) , ""features should be a list of np.ndarray, not %s"" % type ( features ) if np . isscalar ( labels ) : labels = [ np . array ( labels ) ] elif isinstance ( labels , np . ndarray ) : labels = [ labels ] else : assert all ( isinstance ( label , np . ndarray ) for label in labels ) , ""labels should be a list of np.ndarray, not %s"" % type ( labels ) return cls ( features = [ JTensor . from_ndarray ( feature ) for feature in features ] , labels = [ JTensor . from_ndarray ( label ) for label in labels ] , bigdl_type = bigdl_type )",Convert a ndarray of features and labels to Sample which would be used in Java side . : param features : an ndarray or a list of ndarrays : param labels : an ndarray or a list of ndarrays or a scalar : param bigdl_type : double or float
"def transform ( self , image_feature , bigdl_type = ""float"" ) : callBigDlFunc ( bigdl_type , ""transformImageFeature"" , self . value , image_feature ) return image_feature",transform ImageFeature
"def get_label ( self ) : label = callBigDlFunc ( self . bigdl_type , ""imageFeatureToLabelTensor"" , self . value ) return label . to_ndarray ( )",get label as ndarray from ImageFeature
"def read ( cls , path , sc = None , min_partitions = 1 , bigdl_type = ""float"" ) : return ImageFrame ( jvalue = callBigDlFunc ( bigdl_type , ""read"" , path , sc , min_partitions ) )",Read images as Image Frame if sc is defined Read image as DistributedImageFrame from local file system or HDFS if sc is null Read image as LocalImageFrame from local file system : param path path to read images if sc is defined path can be local or HDFS . Wildcard character are supported . if sc is null path is local directory / image file / image file with wildcard character : param sc SparkContext : param min_partitions A suggestion value of the minimal splitting number for input data . : return ImageFrame
"def read_parquet ( cls , path , sc , bigdl_type = ""float"" ) : return DistributedImageFrame ( jvalue = callBigDlFunc ( bigdl_type , ""readParquet"" , path , sc ) )",Read parquet file as DistributedImageFrame
"def write_parquet ( cls , path , output , sc , partition_num = 1 , bigdl_type = ""float"" ) : return callBigDlFunc ( bigdl_type , ""writeParquet"" , path , output , sc , partition_num )",write ImageFrame as parquet file
"def transform ( self , transformer , bigdl_type = ""float"" ) : self . value = callBigDlFunc ( bigdl_type , ""transformImageFrame"" , transformer , self . value ) return self",transformImageFrame
"def get_image ( self , float_key = ""floats"" , to_chw = True ) : return self . image_frame . get_image ( float_key , to_chw )",get image from ImageFrame
"def random_split ( self , weights ) : jvalues = self . image_frame . random_split ( weights ) return [ ImageFrame ( jvalue ) for jvalue in jvalues ]",Random split imageframes according to weights : param weights : weights for each ImageFrame : return :
"def get_image ( self , float_key = ""floats"" , to_chw = True ) : tensors = callBigDlFunc ( self . bigdl_type , ""localImageFrameToImageTensor"" , self . value , float_key , to_chw ) return map ( lambda tensor : tensor . to_ndarray ( ) , tensors )",get image list from ImageFrame
"def get_label ( self ) : tensor_rdd = callBigDlFunc ( self . bigdl_type , ""distributedImageFrameToLabelTensorRdd"" , self . value ) return tensor_rdd . map ( lambda tensor : tensor . to_ndarray ( ) )",get label rdd from ImageFrame
"def get_predict ( self , key = ""predict"" ) : predicts = callBigDlFunc ( self . bigdl_type , ""distributedImageFrameToPredict"" , self . value , key ) return predicts . map ( lambda predict : ( predict [ 0 ] , predict [ 1 ] . to_ndarray ( ) ) if predict [ 1 ] else ( predict [ 0 ] , None ) )",get prediction rdd from ImageFrame
"def files_to_image_frame ( cls , url , sc , class_num , partition_num = - 1 , bigdl_type = ""float"" ) : jvalue = callBigDlFunc ( bigdl_type , ""seqFilesToImageFrame"" , url , sc , class_num , partition_num ) return ImageFrame ( jvalue = jvalue )",Extract hadoop sequence files from an HDFS path as ImageFrame : param url : sequence files folder path : param sc : spark context : param class_num : class number of data : param partition_num : partition number default : Engine . nodeNumber () * Engine . coreNumber ()
"def evaluate ( self , x , y , batch_size = 32 , sample_weight = None , is_distributed = False ) : if sample_weight : unsupport_exp ( ""sample_weight"" ) if is_distributed : if isinstance ( x , np . ndarray ) : input = to_sample_rdd ( x , y ) elif isinstance ( x , RDD ) : input = x if self . metrics : sc = get_spark_context ( ) return [ r . result for r in self . bmodel . evaluate ( input , batch_size , self . metrics ) ] else : raise Exception ( ""No Metrics found."" ) else : raise Exception ( ""We only support evaluation in distributed mode"" )",Evaluate a model by the given metrics . : param x : ndarray or list of ndarray for local mode . RDD [ Sample ] for distributed mode : param y : ndarray or list of ndarray for local mode and would be None for cluster mode . : param batch_size : param is_distributed : run in local mode or distributed mode . NB : if is_distributed = true x should be RDD [ Sample ] and y should be None : return :
"def predict ( self , x , batch_size = None , verbose = None , is_distributed = False ) : if batch_size or verbose : raise Exception ( ""we don't support batch_size or verbose for now"" ) if is_distributed : if isinstance ( x , np . ndarray ) : input = to_sample_rdd ( x , np . zeros ( [ x . shape [ 0 ] ] ) ) elif isinstance ( x , RDD ) : input = x return self . bmodel . predict ( input ) else : if isinstance ( x , np . ndarray ) : return self . bmodel . predict_local ( x ) raise Exception ( ""not supported type: %s"" % x )",Generates output predictions for the input samples processing the samples in a batched way .
"def fit ( self , x , y = None , batch_size = 32 , nb_epoch = 10 , verbose = 1 , callbacks = None , validation_split = 0. , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , is_distributed = False ) : if callbacks : raise Exception ( ""We don't support callbacks in fit for now"" ) if class_weight : unsupport_exp ( ""class_weight"" ) if sample_weight : unsupport_exp ( ""sample_weight"" ) if initial_epoch != 0 : unsupport_exp ( ""initial_epoch"" ) if shuffle != True : unsupport_exp ( ""shuffle"" ) if validation_split != 0. : unsupport_exp ( ""validation_split"" ) bopt = self . __create_optimizer ( x = x , y = y , batch_size = batch_size , nb_epoch = nb_epoch , validation_data = validation_data , is_distributed = is_distributed ) bopt . optimize ( )",Optimize the model by the given options
"def transform ( self , dataset ) : self . _transfer_params_to_java ( ) return callBigDlFunc ( self . bigdl_type , ""dlImageTransform"" , self . value , dataset )",Apply the transformer to the images in inputCol and store the transformed result into outputCols
"def save_keras_definition ( keras_model , path ) : model_json = keras_model . to_json ( ) with open ( path , ""w"" ) as json_file : json_file . write ( model_json )",Save a Keras model definition to JSON with given path
"def get_mnist ( sc , data_type = ""train"" , location = ""/tmp/mnist"" ) : from bigdl . dataset import mnist from bigdl . dataset . transformer import normalizer ( images , labels ) = mnist . read_data_sets ( location , data_type ) images = images . reshape ( ( images . shape [ 0 ] , ) + input_shape ) images = sc . parallelize ( images ) labels = sc . parallelize ( labels + 1 ) record = images . zip ( labels ) . map ( lambda rec_tuple : ( normalizer ( rec_tuple [ 0 ] , mnist . TRAIN_MEAN , mnist . TRAIN_STD ) , rec_tuple [ 1 ] ) ) . map ( lambda t : Sample . from_ndarray ( t [ 0 ] , t [ 1 ] ) ) return record",Download or load MNIST dataset to / from the specified path . Normalize and transform input data into an RDD of Sample
"def build_keras_model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation , Flatten from keras . layers import Convolution2D , MaxPooling2D keras_model = Sequential ( ) keras_model . add ( Convolution2D ( 32 , 3 , 3 , border_mode = 'valid' , input_shape = input_shape ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( Convolution2D ( 32 , 3 , 3 ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( MaxPooling2D ( pool_size = ( 2 , 2 ) ) ) keras_model . add ( Dropout ( 0.25 ) ) keras_model . add ( Flatten ( ) ) keras_model . add ( Dense ( 128 ) ) keras_model . add ( Activation ( 'relu' ) ) keras_model . add ( Dropout ( 0.5 ) ) keras_model . add ( Dense ( 10 ) ) keras_model . add ( Activation ( 'softmax' ) ) return keras_model",Define a convnet model in Keras 1 . 2 . 2
"def load ( path , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadBigDL"" , path ) return Layer . of ( jmodel )",Load a pre - trained Bigdl model .
"def of ( jvalue , bigdl_type = ""float"" ) : def get_py_name ( jclass_name ) : if jclass_name == ""StaticGraph"" or jclass_name == ""DynamicGraph"" : return ""Model"" elif jclass_name == ""Input"" : return ""Layer"" else : return jclass_name jname = callBigDlFunc ( bigdl_type , ""getRealClassNameOfJValue"" , jvalue ) jpackage_name = ""."" . join ( jname . split ( ""."" ) [ : - 1 ] ) pclass_name = get_py_name ( jname . split ( ""."" ) [ - 1 ] ) if ""com.intel.analytics.bigdl.nn.keras.Model"" == jname or ""com.intel.analytics.bigdl.nn.keras.Sequential"" == jname : base_module = importlib . import_module ( 'bigdl.nn.keras.topology' ) elif ""com.intel.analytics.bigdl.nn.keras"" == jpackage_name : base_module = importlib . import_module ( 'bigdl.nn.keras.layer' ) else : base_module = importlib . import_module ( 'bigdl.nn.layer' ) realClassName = ""Layer"" if pclass_name in dir ( base_module ) : realClassName = pclass_name module = getattr ( base_module , realClassName ) jvalue_creator = getattr ( module , ""from_jvalue"" ) model = jvalue_creator ( jvalue , bigdl_type ) return model",Create a Python Layer base on the given java value and the real type . : param jvalue : Java object create by Py4j : return : A Python Layer
"def set_running_mean ( self , running_mean ) : callBigDlFunc ( self . bigdl_type , ""setRunningMean"" , self . value , JTensor . from_ndarray ( running_mean ) ) return self",Set the running mean of the layer . Only use this method for a BatchNormalization layer . : param running_mean : a Numpy array .
"def set_running_std ( self , running_std ) : callBigDlFunc ( self . bigdl_type , ""setRunningStd"" , self . value , JTensor . from_ndarray ( running_std ) ) return self",Set the running variance of the layer . Only use this method for a BatchNormalization layer . : param running_std : a Numpy array .
"def from_jvalue ( jvalue , bigdl_type = ""float"" ) : model = Layer ( jvalue = jvalue , bigdl_type = bigdl_type ) model . value = jvalue return model",Create a Python Model base on the given java value : param jvalue : Java object create by Py4j : return : A Python Model
"def check_input ( input ) : def to_jtensor ( i ) : if isinstance ( i , np . ndarray ) : return JTensor . from_ndarray ( i ) elif isinstance ( i , JTensor ) : return i else : raise Exception ( ""Error unknown input type %s"" % type ( i ) ) if type ( input ) is list : if len ( input ) == 0 : raise Exception ( 'Error when checking: empty input' ) return list ( map ( lambda i : to_jtensor ( i ) , input ) ) , True else : return [ to_jtensor ( input ) ] , False",: param input : ndarray or list of ndarray or JTensor or list of JTensor . : return : ( list of JTensor isTable )
"def forward ( self , input ) : jinput , input_is_table = self . check_input ( input ) output = callBigDlFunc ( self . bigdl_type , ""modelForward"" , self . value , jinput , input_is_table ) return self . convert_output ( output )",NB : It s for debug only please use optimizer . optimize () in production . Takes an input object and computes the corresponding output of the module
"def backward ( self , input , grad_output ) : jinput , input_is_table = self . check_input ( input ) jgrad_output , grad_output_is_table = self . check_input ( grad_output ) output = callBigDlFunc ( self . bigdl_type , ""modelBackward"" , self . value , jinput , input_is_table , jgrad_output , grad_output_is_table ) return self . convert_output ( output )",NB : It s for debug only please use optimizer . optimize () in production . Performs a back - propagation step through the module with respect to the given input . In general this method makes the assumption forward ( input ) has been called before with the same input . This is necessary for optimization reasons . If you do not respect this rule backward () will compute incorrect gradients .
"def parameters ( self ) : name_to_params = callBigDlFunc ( self . bigdl_type , ""modelGetParameters"" , self . value ) def to_ndarray ( params ) : return dict ( ( param_name , np . array ( values [ 0 ] , dtype = self . get_dtype ( ) ) . reshape ( values [ 1 ] ) ) for param_name , values in params . items ( ) ) return dict ( ( layer_name , to_ndarray ( params ) ) for layer_name , params in name_to_params . items ( ) )",Get the model parameters which containing : weight bias gradBias gradWeight
"def evaluate ( self , * args ) : if len ( args ) == 0 : callBigDlFunc ( self . bigdl_type , ""evaluate"" , self . value ) return self elif len ( args ) == 3 : dataset , batch_size , val_methods = args if ( isinstance ( dataset , ImageFrame ) ) : return callBigDlFunc ( self . bigdl_type , ""modelEvaluateImageFrame"" , self . value , dataset , batch_size , val_methods ) else : return callBigDlFunc ( self . bigdl_type , ""modelEvaluate"" , self . value , dataset , batch_size , val_methods ) else : raise Exception ( ""Error when calling evaluate(): it takes no argument or exactly three arguments only"" )",No argument passed in : Evaluate the model to set train = false useful when doing test / forward : return : layer itself
"def predict_local ( self , X , batch_size = - 1 ) : jresults = callBigDlFunc ( self . bigdl_type , ""predictLocal"" , self . value , self . _to_jtensors ( X ) , batch_size ) return np . stack ( [ j . to_ndarray ( ) for j in jresults ] )",: param X : X can be a ndarray or list of ndarray if the model has multiple inputs . The first dimension of X should be batch . : param batch_size : total batch size of prediction . : return : a ndarray as the prediction result .
"def predict ( self , features , batch_size = - 1 ) : if isinstance ( features , RDD ) : return self . predict_distributed ( features , batch_size ) else : return self . predict_local ( features , batch_size )",Model inference base on the given data . : param features : it can be a ndarray or list of ndarray for locally inference or RDD [ Sample ] for running in distributed fashion : param batch_size : total batch size of prediction . : return : ndarray or RDD [ Sample ] depend on the the type of features .
"def predict_class ( self , features ) : if isinstance ( features , RDD ) : return self . predict_class_distributed ( features ) else : return self . predict_class_local ( features )",Model inference base on the given data which returning label : param features : it can be a ndarray or list of ndarray for locally inference or RDD [ Sample ] for running in distributed fashion : return : ndarray or RDD [ Sample ] depend on the the type of features .
"def predict_distributed ( self , data_rdd , batch_size = - 1 ) : result = callBigDlFunc ( self . bigdl_type , ""modelPredictRDD"" , self . value , data_rdd , batch_size ) return result . map ( lambda data : data . to_ndarray ( ) )",Model inference base on the given data . You need to invoke collect () to trigger those action \ as the returning result is an RDD .
"def predict_class_distributed ( self , data_rdd ) : result = callBigDlFunc ( self . bigdl_type , ""modelPredictClass"" , self . value , data_rdd ) return result",module predict return the predict label
"def predict_image ( self , image_frame , output_layer = None , share_buffer = False , batch_per_partition = 4 , predict_key = ""predict"" ) : image_frame = callBigDlFunc ( self . bigdl_type , ""modelPredictImage"" , self . value , image_frame , output_layer , share_buffer , batch_per_partition , predict_key ) return ImageFrame ( image_frame )",model predict images return imageFrame with predicted tensor : param image_frame imageFrame that contains images : param output_layer if output_layer is not null the output of layer that matches output_layer will be used as predicted output : param share_buffer whether to share same memory for each batch predict results : param batch_per_partition batch size per partition default is 4 : param predict_key key to store predicted results
"def set_weights ( self , weights ) : tensors = [ JTensor . from_ndarray ( param , self . bigdl_type ) for param in to_list ( weights ) ] callBigDlFunc ( self . bigdl_type , ""setWeights"" , self . value , tensors )",Set weights for this layer
"def get_weights ( self ) : tensorWeights = callBigDlFunc ( self . bigdl_type , ""getWeights"" , self . value ) if tensorWeights is not None : return [ tensor . to_ndarray ( ) for tensor in tensorWeights ] else : print ( ""The layer does not have weight/bias"" ) return None",Get weights for this layer
"def save_tensorflow ( self , inputs , path , byte_order = ""little_endian"" , data_format = ""nhwc"" ) : callBigDlFunc ( self . bigdl_type , ""saveTF"" , self . value , inputs , path , byte_order , data_format )",Save a model to protobuf files so that it can be used in tensorflow inference .
"def freeze ( self , names = None ) : callBigDlFunc ( self . bigdl_type , ""freeze"" , self . value , names ) return self",freeze module if names is not None set an array of layers that match given names to be freezed : param names : an array of layer names : return :
"def unfreeze ( self , names = None ) : callBigDlFunc ( self . bigdl_type , ""unFreeze"" , self . value , names ) return self",unfreeze module if names is not None unfreeze layers that match given names : param names : an array of layer names : return :
"def training ( self , is_training = True ) : if is_training : callJavaFunc ( self . value . training ) else : callJavaFunc ( self . value . evaluate ) return self",Set this layer in the training mode or in predition mode if is_training = False
"def quantize ( self ) : quantized_model = callBigDlFunc ( self . bigdl_type , ""quantize"" , self . value ) return Layer . of ( quantized_model )",Clone self and quantize it at last return a new quantized model . : return : A new quantized model .
"def loadModel ( modelPath , weightPath = None , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadBigDLModule"" , modelPath , weightPath ) return Layer . of ( jmodel )",Load a pre - trained Bigdl model .
"def load_torch ( path , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadTorch"" , path ) return Layer . of ( jmodel )",Load a pre - trained Torch model .
"def load_keras ( json_path = None , hdf5_path = None , by_name = False ) : import os try : import tensorflow except ImportError : os . environ [ 'KERAS_BACKEND' ] = ""theano"" try : from theano import ifelse except ImportError : raise Exception ( ""No backend is found for Keras. "" ""Please install either tensorflow or theano."" ) from bigdl . keras . converter import DefinitionLoader , WeightLoader if json_path and not hdf5_path : return DefinitionLoader . from_json_path ( json_path ) elif json_path and hdf5_path : return WeightLoader . load_weights_from_json_hdf5 ( json_path , hdf5_path , by_name = by_name ) elif hdf5_path and not json_path : kmodel , bmodel = DefinitionLoader . from_hdf5_path ( hdf5_path ) WeightLoader . load_weights_from_kmodel ( bmodel , kmodel ) return bmodel",Load a pre - trained Keras model .
"def load_caffe ( model , defPath , modelPath , match_all = True , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadCaffe"" , model , defPath , modelPath , match_all ) return Layer . of ( jmodel )",Load a pre - trained Caffe model .
"def load_caffe_model ( defPath , modelPath , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadCaffeModel"" , defPath , modelPath ) return Layer . of ( jmodel )",Load a pre - trained Caffe model .
"def load_tensorflow ( path , inputs , outputs , byte_order = ""little_endian"" , bin_file = None , generated_backward = True , bigdl_type = ""float"" ) : jmodel = callBigDlFunc ( bigdl_type , ""loadTF"" , path , inputs , outputs , byte_order , bin_file , generated_backward ) return Model . of ( jmodel )",Load a pre - trained Tensorflow model . : param path : The path containing the pre - trained model . : param inputs : The input node of this graph : param outputs : The output node of this graph : param byte_order : byte_order of the file little_endian or big_endian : param bin_file : the optional bin file produced by bigdl dump_model util function to store the weights : param generated_backward : if generate backward graph : return : A pre - trained model .
"def stop_gradient ( self , stop_layers , bigdl_type = ""float"" ) : callBigDlFunc ( bigdl_type , ""setStopGradient"" , self . value , stop_layers ) return self",stop the input gradient of layers that match the given names their input gradient are not computed . And they will not contributed to the input gradient computation of layers that depend on them . : param stop_layers : an array of layer names : param bigdl_type : : return :
"def node ( self , name , bigdl_type = ""float"" ) : jnode = callBigDlFunc ( bigdl_type , ""findGraphNode"" , self . value , name ) return Node . of ( jnode )",Return the corresponding node has the given name . If the given name doesn t match any node an exception will be thrown : param name : node name : param bigdl_type : : return :
"def save_graph_topology ( self , log_path , bigdl_type = ""float"" ) : callBigDlFunc ( bigdl_type , ""saveGraphTopology"" , self . value , log_path ) return self",save current model graph to a folder which can be display in tensorboard by running tensorboard -- logdir logPath : param log_path : path to save the model graph : param bigdl_type : : return :
"def forward ( self , input , target ) : jinput , input_is_table = Layer . check_input ( input ) jtarget , target_is_table = Layer . check_input ( target ) output = callBigDlFunc ( self . bigdl_type , ""criterionForward"" , self . value , jinput , input_is_table , jtarget , target_is_table ) return output",NB : It s for debug only please use optimizer . optimize () in production . Takes an input object and computes the corresponding loss of the criterion compared with target
"def of ( cls , jcriterion , bigdl_type = ""float"" ) : criterion = Criterion ( bigdl_type , jcriterion ) criterion . value = jcriterion criterion . bigdl_type = bigdl_type return criterion",Create a python Criterion by a java criterion object
"def readImages ( path , sc = None , minParitions = 1 , bigdl_type = ""float"" ) : df = callBigDlFunc ( bigdl_type , ""dlReadImage"" , path , sc , minParitions ) df . _sc . _jsc = sc . _jsc return df",Read the directory of images into DataFrame from the local or remote source . : param path Directory to the input data files the path can be comma separated paths as the list of inputs . Wildcards path are supported similarly to sc . binaryFiles ( path ) . : param min_partitions A suggestion value of the minimal splitting number for input data . : return DataFrame with a single column image ; Each record in the column represents one image record : Row ( uri height width channels CvType bytes )
"def load_weights_from_json_hdf5 ( def_json , weights_hdf5 , by_name = False ) : bmodel = DefinitionLoader . from_json_path ( def_json ) def_value = BCommon . text_from_path ( def_json ) kmodel = model_from_json ( def_value ) WeightLoader . load_weights_from_hdf5 ( bmodel , kmodel , weights_hdf5 , by_name ) return bmodel",The file path can be stored in a local file system HDFS S3 or any Hadoop - supported file system .
"def load_weights_from_hdf5 ( bmodel , kmodel , filepath , by_name = False ) : local_file_path = BCommon . get_local_file ( filepath ) kmodel . load_weights ( filepath = local_file_path , by_name = by_name ) WeightLoader . load_weights_from_kmodel ( bmodel , kmodel )",Loads all layer weights from a HDF5 save file . filepath can be stored in a local file system HDFS S3 or any Hadoop - supported file system . If by_name is False ( default ) weights are loaded based on the network s execution order topology meaning layers in the execution seq should be exactly the same the architecture
def get_weights_from_kmodel ( kmodel ) : layers_with_weights = [ layer for layer in kmodel . layers if layer . weights ] bweights = [ ] for klayer in layers_with_weights : bws = WeightsConverter . get_bigdl_weights_from_klayer ( klayer ) for w in bws : bweights . append ( w ) return bweights,Convert kmodel s weights to bigdl format . We are supposing the order is the same as the execution order . : param kmodel : keras model : return : list of ndarray
"def __build_node_id_2_klayer ( kmodel , node_id_to_config_layer ) : node_id_to_config_layer [ kmodel . name ] = kmodel def gather_result ( layers ) : if layers : for layer in layers : if layer . name not in node_id_to_config_layer : node_id_to_config_layer [ layer . name ] = layer DefinitionLoader . __build_node_id_2_klayer ( layer , node_id_to_config_layer ) if hasattr ( kmodel , ""layers"" ) : gather_result ( kmodel . layers ) if hasattr ( kmodel , ""flattened_layers"" ) : gather_result ( kmodel . flattened_layers )",The result would contain all of the layers including nested layers . : param kmodel : a keras model which can be Sequential or Model : param node_id_to_config_layer : a container to store the result
"def from_hdf5_path ( cls , hdf5_path ) : from keras . models import load_model hdf5_local_path = BCommon . get_local_file ( hdf5_path ) kmodel = load_model ( hdf5_local_path ) return kmodel , DefinitionLoader . from_kmodel ( kmodel )",: param hdf5_path : hdf5 path which can be stored in a local file system HDFS S3 or any Hadoop - supported file system . : return : BigDL Model
"def from_json_path ( cls , json_path ) : json_str = BCommon . text_from_path ( json_path ) return DefinitionLoader . from_json_str ( json_str )",: param json_path : definition path which can be stored in a local file system HDFS S3 or any Hadoop - supported file system . : return : BigDL Model
"def load_imdb ( ) : from keras . preprocessing import sequence from keras . datasets import imdb ( X_train , y_train ) , ( X_test , y_test ) = imdb . load_data ( nb_words = 20000 ) X_train = sequence . pad_sequences ( X_train , maxlen = 100 ) X_test = sequence . pad_sequences ( X_test , maxlen = 100 ) return X_train , y_train , X_test , y_test",Load IMDB dataset Transform input data into an RDD of Sample
"def build_keras_model ( ) : from keras . models import Sequential from keras . layers import Dense , Dropout , Activation from keras . layers import Embedding from keras . layers import LSTM from keras . layers import Convolution1D , MaxPooling1D keras_model = Sequential ( ) keras_model . add ( Embedding ( 20000 , 128 , input_length = 100 ) ) keras_model . add ( Dropout ( 0.25 ) ) keras_model . add ( Convolution1D ( nb_filter = 64 , filter_length = 5 , border_mode = 'valid' , activation = 'relu' , subsample_length = 1 ) ) keras_model . add ( MaxPooling1D ( pool_length = 4 ) ) keras_model . add ( LSTM ( 70 ) ) keras_model . add ( Dense ( 1 ) ) keras_model . add ( Activation ( 'sigmoid' ) ) return keras_model",Define a recurrent convolutional model in Keras 1 . 2 . 2
"def merge ( inputs , mode = ""sum"" , concat_axis = - 1 , name = None ) : return Merge ( mode = mode , concat_axis = concat_axis , name = name ) ( list ( inputs ) )",Functional merge . Only use this method if you are defining a graph model . Used to merge a list of input nodes into a single output node ( NOT layers! ) following some merge mode .
"def get_input_shape ( self ) : input = callBigDlFunc ( self . bigdl_type , ""getInputShape"" , self . value ) return self . __process_shape ( input )",Return a list of shape tuples if there are multiple inputs . Return one shape tuple otherwise .
"def get_output_shape ( self ) : output = callBigDlFunc ( self . bigdl_type , ""getOutputShape"" , self . value ) return self . __process_shape ( output )",Return a list of shape tuples if there are multiple outputs . Return one shape tuple otherwise .
"def get_mnist ( data_type = ""train"" , location = ""/tmp/mnist"" ) : X , Y = mnist . read_data_sets ( location , data_type ) return X , Y + 1",Get mnist dataset with features and label as ndarray . Data would be downloaded automatically if it doesn t present at the specific location .
"def read_data_sets ( data_dir ) : WHOLE_DATA = 'ml-1m.zip' local_file = base . maybe_download ( WHOLE_DATA , data_dir , SOURCE_URL + WHOLE_DATA ) zip_ref = zipfile . ZipFile ( local_file , 'r' ) extracted_to = os . path . join ( data_dir , ""ml-1m"" ) if not os . path . exists ( extracted_to ) : print ( ""Extracting %s to %s"" % ( local_file , data_dir ) ) zip_ref . extractall ( data_dir ) zip_ref . close ( ) rating_files = os . path . join ( extracted_to , ""ratings.dat"" ) rating_list = [ i . strip ( ) . split ( ""::"" ) for i in open ( rating_files , ""r"" ) . readlines ( ) ] movielens_data = np . array ( rating_list ) . astype ( int ) return movielens_data",Parse or download movielens 1m data if train_dir is empty .
"def get_bigdl_classpath ( ) : if os . getenv ( ""BIGDL_CLASSPATH"" ) : return os . environ [ ""BIGDL_CLASSPATH"" ] jar_dir = os . path . abspath ( __file__ + ""/../../"" ) jar_paths = glob . glob ( os . path . join ( jar_dir , ""share/lib/*.jar"" ) ) if jar_paths : assert len ( jar_paths ) == 1 , ""Expecting one jar: %s"" % len ( jar_paths ) return jar_paths [ 0 ] return """"",Get and return the jar path for bigdl if exists .
"def is_spark_below_2_2 ( ) : import pyspark if ( hasattr ( pyspark , ""version"" ) ) : full_version = pyspark . version . __version__ parts = full_version . split ( ""."" ) spark_version = parts [ 0 ] + ""."" + parts [ 1 ] if ( compare_version ( spark_version , ""2.2"" ) >= 0 ) : return False return True",Check if spark version is below 2 . 2
"def compare_version ( version1 , version2 ) : v1Arr = version1 . split ( ""."" ) v2Arr = version2 . split ( ""."" ) len1 = len ( v1Arr ) len2 = len ( v2Arr ) lenMax = max ( len1 , len2 ) for x in range ( lenMax ) : v1Token = 0 if x < len1 : v1Token = int ( v1Arr [ x ] ) v2Token = 0 if x < len2 : v2Token = int ( v2Arr [ x ] ) if v1Token < v2Token : return - 1 if v1Token > v2Token : return 1 return 0",Compare version strings . : param version1 ; : param version2 ; : return : 1 if version1 is after version2 ; - 1 if version1 is before version2 ; 0 if two versions are the same .
"def convert ( input_ops , output_ops , byte_order , bigdl_type ) : input_names = map ( lambda x : x . name . split ( "":"" ) [ 0 ] , input_ops ) output_names = map ( lambda x : x . name . split ( "":"" ) [ 0 ] , output_ops ) temp = tempfile . mkdtemp ( ) dump_model ( path = temp ) model_path = temp + '/model.pb' bin_path = temp + '/model.bin' model = Model . load_tensorflow ( model_path , input_names , output_names , byte_order , bin_path , bigdl_type ) try : shutil . rmtree ( temp ) except OSError as e : if e . errno != errno . ENOENT : raise return model",Convert tensorflow model to bigdl model : param input_ops : operation list used for input should be placeholders : param output_ops : operations list used for output : return : bigdl model
"def export_checkpoint ( checkpoint_path ) : reader = tf . train . NewCheckpointReader ( checkpoint_path ) tensor_names = filter ( lambda n : n != 'global_step' , reader . get_variable_to_shape_map ( ) . keys ( ) ) tensors = { } for tn in tensor_names : tensors [ tn ] = reader . get_tensor ( tn ) return tensors",Export variable tensors from the checkpoint files .
"def save_variable_bigdl ( tensors , target_path , bigdl_type = ""float"" ) : import numpy as np jtensors = { } for tn in tensors . keys ( ) : if not isinstance ( tensors [ tn ] , np . ndarray ) : value = np . array ( tensors [ tn ] ) else : value = tensors [ tn ] jtensors [ tn ] = JTensor . from_ndarray ( value ) callBigDlFunc ( bigdl_type , ""saveTensorDictionary"" , jtensors , target_path )",Save a variable dictionary to a Java object file so it can be read by BigDL
"def dump_model ( path , graph = None , sess = None , ckpt_file = None , bigdl_type = ""float"" ) : if not os . path . isdir ( path ) : raise ValueError ( ""Folder "" + path + "" does not exist"" ) temp = None if ckpt_file is None : if sess is None : sess = tf . Session ( ) init = tf . global_variables_initializer ( ) sess . run ( init ) temp = tempfile . mkdtemp ( ) ckpt_file = temp saver = tf . train . Saver ( ) saver . save ( sess , ckpt_file ) tensors = export_checkpoint ( ckpt_file ) save_variable_bigdl ( tensors , path + ""/model.bin"" , bigdl_type ) graph = sess . graph if graph is None else graph with gfile . GFile ( path + ""/model.pb"" , ""wb"" ) as f : f . write ( graph . as_graph_def ( ) . SerializeToString ( ) ) if temp is not None : try : shutil . rmtree ( temp ) except OSError as e : if e . errno != errno . ENOENT : raise",Dump a tensorflow model to files . The graph will be dumped to path / model . pb and the checkpoint will be dumped to path / model . bin : param path : dump folder path : param sess : if user pass in session we assume that the variable of the graph in the session has been inited : param graph : tensorflow graph . Default use the default graph of the session : param bigdl_type : model variable numeric type : return : nothing
"def merge_checkpoint ( input_graph , checkpoint , output_node_names , output_graph , sess ) : restore_op_name = ""save/restore_all"" filename_tensor_name = ""save/Const:0"" input_graph_def = graph_pb2 . GraphDef ( ) with gfile . FastGFile ( input_graph , ""r"" ) as f : text_format . Merge ( f . read ( ) . decode ( ""utf-8"" ) , input_graph_def ) for node in input_graph_def . node : node . device = """" importer . import_graph_def ( input_graph_def , name = """" ) sess . run ( [ restore_op_name ] , { filename_tensor_name : checkpoint } ) output_graph_def = graph_util . convert_variables_to_constants ( sess , input_graph_def , output_node_names , variable_names_blacklist = """" ) with gfile . GFile ( output_graph , ""wb"" ) as f : f . write ( output_graph_def . SerializeToString ( ) )",Get the variable values from the checkpoint file and merge them to the GraphDef file Args : input_graph : the GraphDef file doesn t contain variable values checkpoint : the checkpoint file output_node_names : A list of string the output names output_graph : String of the location and the name of the output graph
"def _call ( self , utterances_batch : list , utterances_ids : Optional [ list ] = None ) -> list : batch_size = len ( utterances_batch ) ids = utterances_ids or list ( range ( batch_size ) ) batch_history = [ self . history [ utt_id ] for utt_id in ids ] responses = [ ] filtered = self . skills_filter ( utterances_batch , batch_history ) for skill_i , ( filtered_utterances , skill ) in enumerate ( zip ( filtered , self . wrapped_skills ) ) : skill_i_utt_indexes = [ utt_index for utt_index , utt_filter in enumerate ( filtered_utterances ) if utt_filter ] if skill_i_utt_indexes : skill_i_utt_batch = [ utterances_batch [ i ] for i in skill_i_utt_indexes ] skill_i_utt_ids = [ ids [ i ] for i in skill_i_utt_indexes ] res = [ ( None , 0. ) ] * batch_size predicted , confidence = skill ( skill_i_utt_batch , skill_i_utt_ids ) for i , predicted , confidence in zip ( skill_i_utt_indexes , predicted , confidence ) : res [ i ] = ( predicted , confidence ) responses . append ( res ) responses = self . skills_processor ( utterances_batch , batch_history , * responses ) return responses",Processes batch of utterances and returns corresponding responses batch .
"def expand_tile ( units , axis ) : assert axis in ( 1 , 2 ) n_time_steps = K . int_shape ( units ) [ 1 ] repetitions = [ 1 , 1 , 1 , 1 ] repetitions [ axis ] = n_time_steps if axis == 1 : expanded = Reshape ( target_shape = ( ( 1 , ) + K . int_shape ( units ) [ 1 : ] ) ) ( units ) else : expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ 1 : 2 ] + ( 1 , ) + K . int_shape ( units ) [ 2 : ] ) ) ( units ) return K . tile ( expanded , repetitions )",Expand and tile tensor along given axis
"def additive_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : n_input_features = K . int_shape ( units ) [ 2 ] if n_hidden is None : n_hidden = n_input_features if n_output_features is None : n_output_features = n_input_features exp1 = Lambda ( lambda x : expand_tile ( x , axis = 1 ) ) ( units ) exp2 = Lambda ( lambda x : expand_tile ( x , axis = 2 ) ) ( units ) units_pairs = Concatenate ( axis = 3 ) ( [ exp1 , exp2 ] ) query = Dense ( n_hidden , activation = ""tanh"" ) ( units_pairs ) attention = Dense ( 1 , activation = lambda x : softmax ( x , axis = 2 ) ) ( query ) attended_units = Lambda ( lambda x : K . sum ( attention * x , axis = 2 ) ) ( exp1 ) output = Dense ( n_output_features , activation = activation ) ( attended_units ) return output",Compute additive self attention for time series of vectors ( with batch dimension ) the formula : score ( h_i h_j ) = <v tanh ( W_1 h_i + W_2 h_j ) > v is a learnable vector of n_hidden dimensionality W_1 and W_2 are learnable [ n_hidden n_input_features ] matrices
"def multiplicative_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : n_input_features = K . int_shape ( units ) [ 2 ] if n_hidden is None : n_hidden = n_input_features if n_output_features is None : n_output_features = n_input_features exp1 = Lambda ( lambda x : expand_tile ( x , axis = 1 ) ) ( units ) exp2 = Lambda ( lambda x : expand_tile ( x , axis = 2 ) ) ( units ) queries = Dense ( n_hidden ) ( exp1 ) keys = Dense ( n_hidden ) ( exp2 ) scores = Lambda ( lambda x : K . sum ( queries * x , axis = 3 , keepdims = True ) ) ( keys ) attention = Lambda ( lambda x : softmax ( x , axis = 2 ) ) ( scores ) mult = Multiply ( ) ( [ attention , exp1 ] ) attended_units = Lambda ( lambda x : K . sum ( x , axis = 2 ) ) ( mult ) output = Dense ( n_output_features , activation = activation ) ( attended_units ) return output",Compute multiplicative self attention for time series of vectors ( with batch dimension ) the formula : score ( h_i h_j ) = <W_1 h_i W_2 h_j > W_1 and W_2 are learnable matrices with dimensionality [ n_hidden n_input_features ]
"def precompute_future_symbols ( trie , n , allow_spaces = False ) : if n == 0 : return if trie . is_terminated and trie . precompute_symbols : return for index , final in enumerate ( trie . final ) : trie . data [ index ] = [ set ( ) for i in range ( n ) ] for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : node_data [ 0 ] = set ( trie . _get_letters ( index ) ) if allow_spaces and final : node_data [ 0 ] . add ( "" "" ) for d in range ( 1 , n ) : for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : children = set ( trie . _get_children ( index ) ) for child in children : node_data [ d ] |= trie . data [ child ] [ d - 1 ] if allow_spaces and final : node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] trie . terminated = True",Collecting possible continuations of length < = n for every node
"def save ( self , outfile ) : with open ( outfile , ""w"" , encoding = ""utf8"" ) as fout : attr_values = [ getattr ( self , attr ) for attr in Trie . ATTRS ] attr_values . append ( any ( x is not None for x in self . data ) ) fout . write ( ""{}\n{}\t{}\n"" . format ( "" "" . join ( ""T"" if x else ""F"" for x in attr_values ) , self . nodes_number , self . root ) ) fout . write ( "" "" . join ( str ( a ) for a in self . alphabet ) + ""\n"" ) for index , label in enumerate ( self . final ) : letters = self . _get_letters ( index , return_indexes = True ) children = self . _get_children ( index ) fout . write ( ""{}\t{}\n"" . format ( ""T"" if label else ""F"" , "" "" . join ( ""{}:{}"" . format ( * elem ) for elem in zip ( letters , children ) ) ) ) if self . precompute_symbols is not None : for elem in self . data : fout . write ( "":"" . join ( "","" . join ( map ( str , symbols ) ) for symbols in elem ) + ""\n"" ) return",    
def make_cashed ( self ) : self . _descendance_cash = [ dict ( ) for _ in self . graph ] self . descend = self . _descend_cashed,    descend
"def add ( self , s ) : if self . is_terminated : raise TypeError ( ""Impossible to add string to fitted trie"" ) if s == """" : self . _set_final ( self . root ) return curr = self . root for i , a in enumerate ( s ) : code = self . alphabet_codes [ a ] next = self . graph [ curr ] [ code ] if next == Trie . NO_NODE : curr = self . _add_descendant ( curr , s [ i : ] ) break else : curr = next self . _set_final ( curr ) return self",  s   
"def words ( self ) : branch , word , indexes = [ self . root ] , [ ] , [ 0 ] letters_with_children = [ self . _get_children_and_letters ( self . root ) ] while len ( branch ) > 0 : if self . is_final ( branch [ - 1 ] ) : yield """" . join ( word ) while indexes [ - 1 ] == len ( letters_with_children [ - 1 ] ) : indexes . pop ( ) letters_with_children . pop ( ) branch . pop ( ) if len ( indexes ) == 0 : raise StopIteration ( ) word . pop ( ) next_letter , next_child = letters_with_children [ - 1 ] [ indexes [ - 1 ] ] indexes [ - 1 ] += 1 indexes . append ( 0 ) word . append ( next_letter ) branch . append ( next_child ) letters_with_children . append ( self . _get_children_and_letters ( branch [ - 1 ] ) )",      
"def find_partitions ( self , s , max_count = 1 ) : curr_agenda = [ ( self . root , [ ] , 0 ) ] for i , a in enumerate ( s ) : next_agenda = [ ] for curr , borders , cost in curr_agenda : if cost >= max_count : continue child = self . graph [ curr ] [ self . alphabet_codes [ a ] ] if child == Trie . NO_NODE : continue next_agenda . append ( ( child , borders , cost ) ) if self . is_final ( child ) : next_agenda . append ( ( self . root , borders + [ i + 1 ] , cost + 1 ) ) curr_agenda = next_agenda answer = [ ] for curr , borders , cost in curr_agenda : if curr == self . root : borders = [ 0 ] + borders answer . append ( [ s [ left : borders [ i + 1 ] ] for i , left in enumerate ( borders [ : - 1 ] ) ] ) return answer",   s = s_1 ... s_m    s_1 ... s_m  m < = max_count
"def _add_empty_child ( self , parent , code , final = False ) : self . graph [ parent ] [ code ] = self . nodes_number self . graph . append ( self . _make_default_node ( ) ) self . data . append ( None ) self . final . append ( final ) self . nodes_number += 1 return ( self . nodes_number - 1 )",    parent     code
"def _descend_simple ( self , curr , s ) : for a in s : curr = self . graph [ curr ] [ self . alphabet_codes [ a ] ] if curr == Trie . NO_NODE : break return curr",   curr   s
"def _descend_cashed ( self , curr , s ) : if s == """" : return curr curr_cash = self . _descendance_cash [ curr ] answer = curr_cash . get ( s , None ) if answer is not None : return answer res = curr for a in s : res = self . graph [ res ] [ self . alphabet_codes [ a ] ] if res == Trie . NO_NODE : break curr_cash [ s ] = res return res",   curr   s  
"def _get_letters ( self , index , return_indexes = False ) : if self . dict_storage : answer = list ( self . graph [ index ] . keys ( ) ) else : answer = [ i for i , elem in enumerate ( self . graph [ index ] ) if elem != Trie . NO_NODE ] if not return_indexes : answer = [ ( self . alphabet [ i ] if i >= 0 else "" "" ) for i in answer ] return answer",        index
"def _get_children ( self , index ) : if self . dict_storage : return list ( self . graph [ index ] . values ( ) ) else : return [ elem for elem in self . graph [ index ] if elem != Trie . NO_NODE ]",      index
"def generate_postorder ( self , trie ) : order , stack = [ ] , [ ] stack . append ( trie . root ) colors = [ 'white' ] * len ( trie ) while len ( stack ) > 0 : index = stack [ - 1 ] color = colors [ index ] if color == 'white' : colors [ index ] = 'grey' for child in trie . _get_children ( index ) : if child != Trie . NO_NODE and colors [ child ] == 'white' : stack . append ( child ) else : if color == 'grey' : colors [ index ] = 'black' order . append ( index ) stack = stack [ : - 1 ] return order",  
"def run_population ( population , evolution , gpus ) : population_size = len ( population ) for k in range ( population_size // len ( gpus ) + 1 ) : procs = [ ] for j in range ( len ( gpus ) ) : i = k * len ( gpus ) + j if i < population_size : save_path = expand_path ( evolution . get_value_from_config ( parse_config ( population [ i ] ) , evolution . path_to_models_save_path ) ) save_path . mkdir ( parents = True , exist_ok = True ) f_name = save_path / ""config.json"" save_json ( population [ i ] , f_name ) with save_path . joinpath ( 'out.txt' ) . open ( 'w' , encoding = 'utf8' ) as outlog , save_path . joinpath ( 'err.txt' ) . open ( 'w' , encoding = 'utf8' ) as errlog : env = dict ( os . environ ) if len ( gpus ) > 1 or gpus [ 0 ] != - 1 : env [ 'CUDA_VISIBLE_DEVICES' ] = str ( gpus [ j ] ) procs . append ( Popen ( ""{} -m deeppavlov train {}"" . format ( sys . executable , str ( f_name ) ) , shell = True , stdout = outlog , stderr = errlog , env = env ) ) for j , proc in enumerate ( procs ) : i = k * len ( gpus ) + j log . info ( f'Waiting on {i}th proc' ) if proc . wait ( ) != 0 : save_path = expand_path ( evolution . get_value_from_config ( parse_config ( population [ i ] ) , evolution . path_to_models_save_path ) ) with save_path . joinpath ( 'err.txt' ) . open ( encoding = 'utf8' ) as errlog : log . warning ( f'Population {i} returned an error code {proc.returncode} and an error log:\n' + errlog . read ( ) ) return None",Change save and load paths for obtained population save config . json with model config run population via current python executor ( with which evolve . py already run ) and on given devices ( - 1 means CPU other integeres - visible for evolve . py GPUs ) Args : population : list of dictionaries - configs of current population evolution : ParamsEvolution gpus : list of given devices ( list of integers )
"def dot_attention ( inputs , memory , mask , att_size , keep_prob = 1.0 , scope = ""dot_attention"" ) : with tf . variable_scope ( scope ) : BS , IL , IH = tf . unstack ( tf . shape ( inputs ) ) BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) d_inputs = tf . nn . dropout ( inputs , keep_prob = keep_prob , noise_shape = [ BS , 1 , IH ] ) d_memory = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) with tf . variable_scope ( ""attention"" ) : inputs_att = tf . layers . dense ( d_inputs , att_size , use_bias = False , activation = tf . nn . relu ) memory_att = tf . layers . dense ( d_memory , att_size , use_bias = False , activation = tf . nn . relu ) logits = tf . matmul ( inputs_att , tf . transpose ( memory_att , [ 0 , 2 , 1 ] ) ) / ( att_size ** 0.5 ) mask = tf . tile ( tf . expand_dims ( mask , axis = 1 ) , [ 1 , IL , 1 ] ) att_weights = tf . nn . softmax ( softmax_mask ( logits , mask ) ) outputs = tf . matmul ( att_weights , memory ) res = tf . concat ( [ inputs , outputs ] , axis = 2 ) with tf . variable_scope ( ""gate"" ) : dim = res . get_shape ( ) . as_list ( ) [ - 1 ] d_res = tf . nn . dropout ( res , keep_prob = keep_prob , noise_shape = [ BS , 1 , IH + MH ] ) gate = tf . layers . dense ( d_res , dim , use_bias = False , activation = tf . nn . sigmoid ) return res * gate",Computes attention vector for each item in inputs : attention vector is a weighted sum of memory items . Dot product between input and memory vector is used as similarity measure .
"def simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = ""simple_attention"" ) : with tf . variable_scope ( scope ) : BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce_sum ( att_weights * memory , axis = 1 ) return res",Simple attention without any conditions .
"def attention ( inputs , state , att_size , mask , scope = ""attention"" ) : with tf . variable_scope ( scope ) : u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) return res , logits",Computes weighted sum of inputs conditioned on state
"def compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , smooth = False ) : matches_by_order = [ 0 ] * max_order possible_matches_by_order = [ 0 ] * max_order reference_length = 0 translation_length = 0 for ( references , translation ) in zip ( reference_corpus , translation_corpus ) : reference_length += min ( len ( r ) for r in references ) translation_length += len ( translation ) merged_ref_ngram_counts = collections . Counter ( ) for reference in references : merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) translation_ngram_counts = _get_ngrams ( translation , max_order ) overlap = translation_ngram_counts & merged_ref_ngram_counts for ngram in overlap : matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] for order in range ( 1 , max_order + 1 ) : possible_matches = len ( translation ) - order + 1 if possible_matches > 0 : possible_matches_by_order [ order - 1 ] += possible_matches precisions = [ 0 ] * max_order for i in range ( 0 , max_order ) : if smooth : precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / ( possible_matches_by_order [ i ] + 1. ) ) else : if possible_matches_by_order [ i ] > 0 : precisions [ i ] = ( float ( matches_by_order [ i ] ) / possible_matches_by_order [ i ] ) else : precisions [ i ] = 0.0 if min ( precisions ) > 0 : p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) geo_mean = math . exp ( p_log_sum ) else : geo_mean = 0 ratio = float ( translation_length ) / reference_length if ratio > 1.0 : bp = 1. else : bp = math . exp ( 1 - 1. / ratio ) bleu = geo_mean * bp return ( bleu , precisions , bp , ratio , translation_length , reference_length )",Computes BLEU score of translated segments against one or more references .
"def _get_log_file ( self ) : log_dir : Path = Path ( self . config [ 'log_path' ] ) . expanduser ( ) . resolve ( ) / self . agent_name log_dir . mkdir ( parents = True , exist_ok = True ) log_file_path = Path ( log_dir , f'{self._get_timestamp_utc_str()}_{self.agent_name}.log' ) log_file = open ( log_file_path , 'a' , buffering = 1 , encoding = 'utf8' ) return log_file",Returns opened file object for writing dialog logs .
"def _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : if isinstance ( utterance , str ) : pass elif isinstance ( utterance , RichMessage ) : utterance = utterance . json ( ) elif isinstance ( utterance , ( list , dict ) ) : utterance = jsonify_data ( utterance ) else : utterance = str ( utterance ) dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id if self . log_file . tell ( ) >= self . log_max_size * 1024 : self . log_file . close ( ) self . log_file = self . _get_log_file ( ) else : try : log_msg = { } log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) log_msg [ 'dialog_id' ] = dialog_id log_msg [ 'direction' ] = direction log_msg [ 'message' ] = utterance log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) self . log_file . write ( f'{log_str}\n' ) except IOError : log . error ( 'Failed to write dialog log.' )",Logs single dialog utterance to current dialog log file .
"def log_in ( self , utterance : Any , dialog_id : Optional [ Hashable ] = None ) -> None : if self . enabled : self . _log ( utterance , 'in' , dialog_id )",Wraps _log method for all input utterances . Args : utterance : Dialog utterance . dialog_id : Dialog ID .
"def summary_gradient_updates ( grads , opt , lr ) : vars_grads = { } for v in tf . trainable_variables ( ) : vars_grads [ v . name ] = [ v , None , None ] for g , v in grads : vars_grads [ v . name ] [ 1 ] = g vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) ret = [ ] for vname , ( v , g , a ) in vars_grads . items ( ) : if g is None : continue if isinstance ( g , tf . IndexedSlices ) : updates = lr * g . values if a is not None : updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) else : updates = lr * g if a is not None : updates /= tf . sqrt ( a ) values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( "":"" , ""_"" ) , updates_norm / values_norm ) ) return ret",get summary ops for the magnitude of gradient updates
"def _deduplicate_indexed_slices ( values , indices ) : unique_indices , new_index_positions = tf . unique ( indices ) summed_values = tf . unsorted_segment_sum ( values , new_index_positions , tf . shape ( unique_indices ) [ 0 ] ) return ( summed_values , unique_indices )",Sums values associated with any non - unique indices . Args : values : A Tensor with rank > = 1 . indices : A one - dimensional integer Tensor indexing into the first dimension of values ( as in an IndexedSlices object ) . Returns : A tuple of ( summed_values unique_indices ) where unique_indices is a de - duplicated version of indices and summed_values contains the sum of values slices associated with each unique index .
"def dump_weights ( tf_save_dir , outfile , options ) : def _get_outname ( tf_name ) : outname = re . sub ( ':0$' , '' , tf_name ) outname = outname . lstrip ( 'lm/' ) outname = re . sub ( '/rnn/' , '/RNN/' , outname ) outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) outname = re . sub ( '/cell_' , '/Cell' , outname ) outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) if '/RNN/' in outname : if 'projection' in outname : outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) else : outname = re . sub ( '/kernel' , '/W_0' , outname ) outname = re . sub ( '/bias' , '/B' , outname ) return outname ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) config = tf . ConfigProto ( allow_soft_placement = True ) with tf . Graph ( ) . as_default ( ) : with tf . Session ( config = config ) as sess : with tf . variable_scope ( 'lm' ) : LanguageModel ( options , False ) loader = tf . train . Saver ( ) loader . restore ( sess , ckpt_file ) with h5py . File ( outfile , 'w' ) as fout : for v in tf . trainable_variables ( ) : if v . name . find ( 'softmax' ) >= 0 : continue outname = _get_outname ( v . name ) shape = v . get_shape ( ) . as_list ( ) dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) values = sess . run ( [ v ] ) [ 0 ] dset [ ... ] = values",Dump the trained weights from a model to a HDF5 file .
"def read_data_by_config ( config : dict ) : dataset_config = config . get ( 'dataset' , None ) if dataset_config : config . pop ( 'dataset' ) ds_type = dataset_config [ 'type' ] if ds_type == 'classification' : reader = { 'class_name' : 'basic_classification_reader' } iterator = { 'class_name' : 'basic_classification_iterator' } config [ 'dataset_reader' ] = { * * dataset_config , * * reader } config [ 'dataset_iterator' ] = { * * dataset_config , * * iterator } else : raise Exception ( ""Unsupported dataset type: {}"" . format ( ds_type ) ) try : reader_config = dict ( config [ 'dataset_reader' ] ) except KeyError : raise ConfigError ( ""No dataset reader is provided in the JSON config."" ) reader = get_model ( reader_config . pop ( 'class_name' ) ) ( ) data_path = reader_config . pop ( 'data_path' , '' ) if isinstance ( data_path , list ) : data_path = [ expand_path ( x ) for x in data_path ] else : data_path = expand_path ( data_path ) return reader . read ( data_path , * * reader_config )",Read data by dataset_reader from specified config .
"def get_iterator_from_config ( config : dict , data : dict ) : iterator_config = config [ 'dataset_iterator' ] iterator : Union [ DataLearningIterator , DataFittingIterator ] = from_params ( iterator_config , data = data ) return iterator",Create iterator ( from config ) for specified data .
"def train_evaluate_model_from_config ( config : Union [ str , Path , dict ] , iterator : Union [ DataLearningIterator , DataFittingIterator ] = None , * , to_train : bool = True , evaluation_targets : Optional [ Iterable [ str ] ] = None , to_validate : Optional [ bool ] = None , download : bool = False , start_epoch_num : Optional [ int ] = None , recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : config = parse_config ( config ) if download : deep_download ( config ) if to_train and recursive : for subconfig in get_all_elems_from_json ( config [ 'chainer' ] , 'config_path' ) : log . info ( f'Training ""{subconfig}""' ) train_evaluate_model_from_config ( subconfig , download = False , recursive = True ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) if iterator is None : try : data = read_data_by_config ( config ) except ConfigError as e : to_train = False log . warning ( f'Skipping training. {e.message}' ) else : iterator = get_iterator_from_config ( config , data ) if 'train' not in config : log . warning ( 'Train config is missing. Populating with default values' ) train_config = config . get ( 'train' ) if start_epoch_num is not None : train_config [ 'start_epoch_num' ] = start_epoch_num if 'evaluation_targets' not in train_config and ( 'validate_best' in train_config or 'test_best' in train_config ) : log . warning ( '""validate_best"" and ""test_best"" parameters are deprecated.' ' Please, use ""evaluation_targets"" list instead' ) train_config [ 'evaluation_targets' ] = [ ] if train_config . pop ( 'validate_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'valid' ) if train_config . pop ( 'test_best' , True ) : train_config [ 'evaluation_targets' ] . append ( 'test' ) trainer_class = get_model ( train_config . pop ( 'class_name' , 'nn_trainer' ) ) trainer = trainer_class ( config [ 'chainer' ] , * * train_config ) if to_train : trainer . train ( iterator ) res = { } if iterator is not None : if to_validate is not None : if evaluation_targets is None : log . warning ( '""to_validate"" parameter is deprecated and will be removed in future versions.' ' Please, use ""evaluation_targets"" list instead' ) evaluation_targets = [ 'test' ] if to_validate : evaluation_targets . append ( 'valid' ) else : log . warn ( 'Both ""evaluation_targets"" and ""to_validate"" parameters are specified.' ' ""to_validate"" is deprecated and will be ignored' ) res = trainer . evaluate ( iterator , evaluation_targets , print_reports = True ) trainer . get_chainer ( ) . destroy ( ) res = { k : v [ 'metrics' ] for k , v in res . items ( ) } return res",Make training and evaluation of the model described in corresponding configuration file .
"def interact_alice ( agent : Agent ) : data = request . get_json ( ) text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) payload = data [ 'request' ] . get ( 'payload' ) session_id = data [ 'session' ] [ 'session_id' ] user_id = data [ 'session' ] [ 'user_id' ] message_id = data [ 'session' ] [ 'message_id' ] dialog_id = DialogID ( user_id , session_id ) response = { 'response' : { 'end_session' : True , 'text' : '' } , ""session"" : { 'session_id' : session_id , 'message_id' : message_id , 'user_id' : user_id } , 'version' : '1.0' } agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] if isinstance ( agent_response , RichMessage ) : response [ 'response' ] [ 'text' ] = '\n' . join ( [ j [ 'content' ] for j in agent_response . json ( ) if j [ 'type' ] == 'plain_text' ] ) else : response [ 'response' ] [ 'text' ] = str ( agent_response ) return jsonify ( response ) , 200",Exchange messages between basic pipelines and the Yandex . Dialogs service . If the pipeline returns multiple values only the first one is forwarded to Yandex .
"def labels2onehot ( labels : [ List [ str ] , List [ List [ str ] ] , np . ndarray ] , classes : [ list , np . ndarray ] ) -> np . ndarray : n_classes = len ( classes ) y = [ ] for sample in labels : curr = np . zeros ( n_classes ) if isinstance ( sample , list ) : for intent in sample : if intent not in classes : log . warning ( 'Unknown intent {} detected. Assigning no class' . format ( intent ) ) else : curr [ np . where ( np . array ( classes ) == intent ) [ 0 ] ] = 1 else : curr [ np . where ( np . array ( classes ) == sample ) [ 0 ] ] = 1 y . append ( curr ) y = np . asarray ( y ) return y",Convert labels to one - hot vectors for multi - class multi - label classification
"def proba2labels ( proba : [ list , np . ndarray ] , confident_threshold : float , classes : [ list , np . ndarray ] ) -> List [ List ] : y = [ ] for sample in proba : to_add = np . where ( sample > confident_threshold ) [ 0 ] if len ( to_add ) > 0 : y . append ( np . array ( classes ) [ to_add ] . tolist ( ) ) else : y . append ( np . array ( [ np . array ( classes ) [ np . argmax ( sample ) ] ] ) . tolist ( ) ) return y",Convert vectors of probabilities to labels using confident threshold ( if probability to belong with the class is bigger than confident_threshold sample belongs with the class ; if no probabilities bigger than confident threshold sample belongs with the class with the biggest probability )
"def proba2onehot ( proba : [ list , np . ndarray ] , confident_threshold : float , classes : [ list , np . ndarray ] ) -> np . ndarray : return labels2onehot ( proba2labels ( proba , confident_threshold , classes ) , classes )",Convert vectors of probabilities to one - hot representations using confident threshold
def _config_session ( ) : config = tf . ConfigProto ( ) config . gpu_options . allow_growth = True config . gpu_options . visible_device_list = '0' return tf . Session ( config = config ),Configure session for particular device
"def process_event ( self , event_name : str , data : dict ) -> None : if event_name == ""after_epoch"" : self . epochs_done = data [ ""epochs_done"" ] self . batches_seen = data [ ""batches_seen"" ] self . train_examples_seen = data [ ""train_examples_seen"" ] return",Process event after epoch Args : event_name : whether event is send after epoch or batch . Set of values : after_epoch after_batch data : event data ( dictionary )
def load ( self ) -> None : if self . load_path . exists ( ) : path = str ( self . load_path . resolve ( ) ) log . info ( '[loading model from {}]' . format ( path ) ) self . _net . load ( path ),Checks existence of the model file loads the model if the file exists
def save ( self ) -> None : path = str ( self . save_path . absolute ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) self . _net . save ( path ),Saves model to the save_path provided in config . The directory is already created by super () . __init__ which is called in __init__ of this class
"def train_on_batch ( self , * args ) -> None : * data , labels = args self . _net . train_on_batch ( data , labels )",Trains the model on a single batch .
"def get_momentum_variable ( self ) : optimizer = self . get_optimizer ( ) if hasattr ( optimizer , 'rho' ) : return optimizer . rho elif hasattr ( optimizer , 'beta_1' ) : return optimizer . beta_1 return None",Extract values of momentum variables from optimizer
"def _update_graph_variables ( self , learning_rate : float = None , momentum : float = None ) : if learning_rate is not None : K . set_value ( self . get_learning_rate_variable ( ) , learning_rate ) if momentum is not None : K . set_value ( self . get_momentum_variable ( ) , momentum )",Update graph variables setting giving learning_rate and momentum
"def process_event ( self , event_name : str , data : dict ) : if ( isinstance ( self . opt . get ( ""learning_rate"" , None ) , float ) and isinstance ( self . opt . get ( ""learning_rate_decay"" , None ) , float ) ) : pass else : if event_name == 'after_train_log' : if ( self . get_learning_rate_variable ( ) is not None ) and ( 'learning_rate' not in data ) : data [ 'learning_rate' ] = float ( K . get_value ( self . get_learning_rate_variable ( ) ) ) if ( self . get_momentum_variable ( ) is not None ) and ( 'momentum' not in data ) : data [ 'momentum' ] = float ( K . get_value ( self . get_momentum_variable ( ) ) ) else : super ( ) . process_event ( event_name , data )",Process event after epoch Args : event_name : whether event is send after epoch or batch . Set of values : after_epoch after_batch data : event data ( dictionary )
"def round_f1 ( y_true , y_predicted ) : try : predictions = [ np . round ( x ) for x in y_predicted ] except TypeError : predictions = y_predicted return f1_score ( y_true , predictions )",Calculates F1 ( binary ) measure .
"def round_f1_macro ( y_true , y_predicted ) : try : predictions = [ np . round ( x ) for x in y_predicted ] except TypeError : predictions = y_predicted return f1_score ( np . array ( y_true ) , np . array ( predictions ) , average = ""macro"" )",Calculates F1 macro measure .
"def process_word ( word : str , to_lower : bool = False , append_case : Optional [ str ] = None ) -> Tuple [ str ] : if all ( x . isupper ( ) for x in word ) and len ( word ) > 1 : uppercase = ""<ALL_UPPER>"" elif word [ 0 ] . isupper ( ) : uppercase = ""<FIRST_UPPER>"" else : uppercase = None if to_lower : word = word . lower ( ) if word . isdigit ( ) : answer = [ ""<DIGIT>"" ] elif word . startswith ( ""http://"" ) or word . startswith ( ""www."" ) : answer = [ ""<HTTP>"" ] else : answer = list ( word ) if to_lower and uppercase is not None : if append_case == ""first"" : answer = [ uppercase ] + answer elif append_case == ""last"" : answer = answer + [ uppercase ] return tuple ( answer )",Converts word to a tuple of symbols optionally converts it to lowercase and adds capitalization label .
"def stacked_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None , add_l2_losses = False ) : l2_reg = tf . nn . l2_loss if add_l2_losses else None for n_layer , n_hidden in enumerate ( n_hidden_list ) : if use_dilation : dilation_rate = 2 ** n_layer else : dilation_rate = 1 units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) , kernel_regularizer = l2_reg ) if use_batch_norm : assert training_ph is not None units = tf . layers . batch_normalization ( units , training = training_ph ) units = tf . nn . relu ( units ) return units",Number of convolutional layers stacked on top of each other
"def dense_convolutional_network ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_dilation = False , use_batch_norm = False , training_ph = None ) : units_list = [ units ] for n_layer , n_filters in enumerate ( n_hidden_list ) : total_units = tf . concat ( units_list , axis = - 1 ) if use_dilation : dilation_rate = 2 ** n_layer else : dilation_rate = 1 units = tf . layers . conv1d ( total_units , n_filters , filter_width , dilation_rate = dilation_rate , padding = 'same' , kernel_initializer = INITIALIZER ( ) ) if use_batch_norm : units = tf . layers . batch_normalization ( units , training = training_ph ) units = tf . nn . relu ( units ) units_list . append ( units ) return units",Densely connected convolutional layers . Based on the paper : [ Gao 17 ] https : // arxiv . org / abs / 1608 . 06993
"def bi_rnn ( units : tf . Tensor , n_hidden : List , cell_type = 'gru' , seq_lengths = None , trainable_initial_states = False , use_peepholes = False , name = 'Bi-' ) : with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : if cell_type == 'gru' : forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) if trainable_initial_states : initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) else : initial_state_fw = initial_state_bw = None elif cell_type == 'lstm' : forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) if trainable_initial_states : initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) else : initial_state_fw = initial_state_bw = None else : raise RuntimeError ( 'cell_type must be either ""gru"" or ""lstm""s' ) ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths , initial_state_fw = initial_state_fw , initial_state_bw = initial_state_bw ) kernels = [ var for var in forward_cell . trainable_variables + backward_cell . trainable_variables if 'kernel' in var . name ] for kernel in kernels : tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw )",Bi directional recurrent neural network . GRU or LSTM
"def stacked_bi_rnn ( units : tf . Tensor , n_hidden_list : List , cell_type = 'gru' , seq_lengths = None , use_peepholes = False , name = 'RNN_layer' ) : for n , n_hidden in enumerate ( n_hidden_list ) : with tf . variable_scope ( name + '_' + str ( n ) ) : if cell_type == 'gru' : forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) elif cell_type == 'lstm' : forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) else : raise RuntimeError ( 'cell_type must be either gru or lstm' ) ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , backward_cell , units , dtype = tf . float32 , sequence_length = seq_lengths ) units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) if cell_type == 'gru' : last_units = tf . concat ( [ fw , bw ] , axis = 1 ) else : ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) last_units = ( h , c ) return units , last_units",Stackted recurrent neural networks GRU or LSTM
"def u_shape ( units : tf . Tensor , n_hidden_list : List , filter_width = 7 , use_batch_norm = False , training_ph = None ) : units_for_skip_conn = [ ] conv_net_params = { 'filter_width' : filter_width , 'use_batch_norm' : use_batch_norm , 'training_ph' : training_ph } for n_hidden in n_hidden_list : units = stacked_cnn ( units , [ n_hidden ] , * * conv_net_params ) units_for_skip_conn . append ( units ) units = tf . layers . max_pooling1d ( units , pool_size = 2 , strides = 2 , padding = 'same' ) units = stacked_cnn ( units , [ n_hidden ] , * * conv_net_params ) for down_step , n_hidden in enumerate ( n_hidden_list [ : : - 1 ] ) : units = tf . expand_dims ( units , axis = 2 ) units = tf . layers . conv2d_transpose ( units , n_hidden , filter_width , strides = ( 2 , 1 ) , padding = 'same' ) units = tf . squeeze ( units , axis = 2 ) skip_units = units_for_skip_conn [ - ( down_step + 1 ) ] if skip_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : skip_units = tf . layers . dense ( skip_units , n_hidden ) units = skip_units + units units = stacked_cnn ( units , [ n_hidden ] , * * conv_net_params ) return units",Network architecture inspired by One Hundred layer Tiramisu . https : // arxiv . org / abs / 1611 . 09326 . U - Net like .
"def stacked_highway_cnn ( units : tf . Tensor , n_hidden_list : List , filter_width = 3 , use_batch_norm = False , use_dilation = False , training_ph = None ) : for n_layer , n_hidden in enumerate ( n_hidden_list ) : input_units = units if input_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : input_units = tf . layers . dense ( input_units , n_hidden ) if use_dilation : dilation_rate = 2 ** n_layer else : dilation_rate = 1 units = tf . layers . conv1d ( units , n_hidden , filter_width , padding = 'same' , dilation_rate = dilation_rate , kernel_initializer = INITIALIZER ( ) ) if use_batch_norm : units = tf . layers . batch_normalization ( units , training = training_ph ) sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units input_units = tf . nn . relu ( input_units ) units = input_units return units",Highway convolutional network . Skip connection with gating mechanism .
"def embedding_layer ( token_indices = None , token_embedding_matrix = None , n_tokens = None , token_embedding_dim = None , name : str = None , trainable = True ) : if token_embedding_matrix is not None : tok_mat = token_embedding_matrix if trainable : Warning ( 'Matrix of embeddings is passed to the embedding_layer, ' 'possibly there is a pre-trained embedding matrix. ' 'Embeddings paramenters are set to Trainable!' ) else : tok_mat = np . random . randn ( n_tokens , token_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( token_embedding_dim ) tok_emb_mat = tf . Variable ( tok_mat , name = name , trainable = trainable ) embedded_tokens = tf . nn . embedding_lookup ( tok_emb_mat , token_indices ) return embedded_tokens",Token embedding layer . Create matrix of for token embeddings . Can be initialized with given matrix ( for example pre - trained with word2ve algorithm
"def character_embedding_network ( char_placeholder : tf . Tensor , n_characters : int = None , emb_mat : np . array = None , char_embedding_dim : int = None , filter_widths = ( 3 , 4 , 5 , 7 ) , highway_on_top = False ) : if emb_mat is None : emb_mat = np . random . randn ( n_characters , char_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( char_embedding_dim ) else : char_embedding_dim = emb_mat . shape [ 1 ] char_emb_var = tf . Variable ( emb_mat , trainable = True ) with tf . variable_scope ( 'Char_Emb_Network' ) : c_emb = tf . nn . embedding_lookup ( char_emb_var , char_placeholder ) conv_results_list = [ ] for filter_width in filter_widths : conv_results_list . append ( tf . layers . conv2d ( c_emb , char_embedding_dim , ( 1 , filter_width ) , padding = 'same' , kernel_initializer = INITIALIZER ) ) units = tf . concat ( conv_results_list , axis = 3 ) units = tf . reduce_max ( units , axis = 2 ) if highway_on_top : sigmoid_gate = tf . layers . dense ( units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER , kernel_regularizer = tf . nn . l2_loss ) deeper_units = tf . layers . dense ( units , tf . shape ( units ) [ - 1 ] , kernel_initializer = INITIALIZER , kernel_regularizer = tf . nn . l2_loss ) units = sigmoid_gate * units + ( 1 - sigmoid_gate ) * deeper_units units = tf . nn . relu ( units ) return units",Characters to vector . Every sequence of characters ( token ) is embedded to vector space with dimensionality char_embedding_dim Convolution plus max_pooling is used to obtain vector representations of words .
"def expand_tile ( units , axis ) : assert axis in ( 1 , 2 ) n_time_steps = tf . shape ( units ) [ 1 ] repetitions = [ 1 , 1 , 1 , 1 ] repetitions [ axis ] = n_time_steps return tf . tile ( tf . expand_dims ( units , axis ) , repetitions )",Expand and tile tensor along given axis Args : units : tf tensor with dimensions [ batch_size time_steps n_input_features ] axis : axis along which expand and tile . Must be 1 or 2
"def additive_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : n_input_features = units . get_shape ( ) . as_list ( ) [ 2 ] if n_hidden is None : n_hidden = n_input_features if n_output_features is None : n_output_features = n_input_features units_pairs = tf . concat ( [ expand_tile ( units , 1 ) , expand_tile ( units , 2 ) ] , 3 ) query = tf . layers . dense ( units_pairs , n_hidden , activation = tf . tanh , kernel_initializer = INITIALIZER ( ) ) attention = tf . nn . softmax ( tf . layers . dense ( query , 1 ) , dim = 2 ) attended_units = tf . reduce_sum ( attention * expand_tile ( units , 1 ) , axis = 2 ) output = tf . layers . dense ( attended_units , n_output_features , activation , kernel_initializer = INITIALIZER ( ) ) return output",Computes additive self attention for time series of vectors ( with batch dimension ) the formula : score ( h_i h_j ) = <v tanh ( W_1 h_i + W_2 h_j ) > v is a learnable vector of n_hidden dimensionality W_1 and W_2 are learnable [ n_hidden n_input_features ] matrices
"def multiplicative_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : n_input_features = units . get_shape ( ) . as_list ( ) [ 2 ] if n_hidden is None : n_hidden = n_input_features if n_output_features is None : n_output_features = n_input_features queries = tf . layers . dense ( expand_tile ( units , 1 ) , n_hidden , kernel_initializer = INITIALIZER ( ) ) keys = tf . layers . dense ( expand_tile ( units , 2 ) , n_hidden , kernel_initializer = INITIALIZER ( ) ) scores = tf . reduce_sum ( queries * keys , axis = 3 , keep_dims = True ) attention = tf . nn . softmax ( scores , dim = 2 ) attended_units = tf . reduce_sum ( attention * expand_tile ( units , 1 ) , axis = 2 ) output = tf . layers . dense ( attended_units , n_output_features , activation , kernel_initializer = INITIALIZER ( ) ) return output",Computes multiplicative self attention for time series of vectors ( with batch dimension ) the formula : score ( h_i h_j ) = <W_1 h_i W_2 h_j > W_1 and W_2 are learnable matrices with dimensionality [ n_hidden n_input_features ] where <a b > stands for a and b dot product
"def cudnn_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , num_units = n_hidden ) if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = input_initial_h or init_h h , h_last = gru ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = tf . squeeze ( h_last , axis = 0 ) [ - 1 ] if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , h_last",Fast CuDNN GRU implementation
"def cudnn_compatible_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = input_initial_h or init_h with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) units = tf . transpose ( units , ( 1 , 0 , 2 ) ) h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = tuple ( tf . unstack ( initial_h , axis = 0 ) ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = h_last [ - 1 ] if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , h_last",CuDNN Compatible GRU implementation . It should be used to load models saved with CudnnGRUCell to run on CPU .
"def cudnn_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , num_units = n_hidden ) if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = initial_h or init_h initial_c = initial_c or init_c h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , initial_c ) ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = h_last [ - 1 ] c_last = c_last [ - 1 ] if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , ( h_last , c_last )",Fast CuDNN LSTM implementation
"def cudnn_compatible_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , initial_c = None , name = 'cudnn_lstm' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if trainable_initial_states : init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) else : init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) initial_h = initial_h or init_h initial_c = initial_c or init_c with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) units = tf . transpose ( units , ( 1 , 0 , 2 ) ) init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in zip ( tf . unstack ( initial_h , axis = 0 ) , tf . unstack ( initial_c , axis = 0 ) ) ] ) h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = init ) h = tf . transpose ( h , ( 1 , 0 , 2 ) ) h_last = state [ - 1 ] . h c_last = state [ - 1 ] . c if seq_lengths is not None : indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) h_last = tf . gather_nd ( h , indices ) return h , ( h_last , c_last )",CuDNN Compatible LSTM implementation . It should be used to load models saved with CudnnLSTMCell to run on CPU .
"def cudnn_bi_gru ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] with tf . variable_scope ( 'Forward' ) : h_fw , h_last_fw = cudnn_gru_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) with tf . variable_scope ( 'Backward' ) : reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) h_bw , h_last_bw = cudnn_gru_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths , reuse = reuse ) h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) return ( h_fw , h_bw ) , ( h_last_fw , h_last_bw )",Fast CuDNN Bi - GRU implementation
"def cudnn_bi_lstm ( units , n_hidden , seq_lengths = None , n_layers = 1 , trainable_initial_states = False , name = 'cudnn_bi_gru' , reuse = False ) : with tf . variable_scope ( name , reuse = reuse ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] with tf . variable_scope ( 'Forward' ) : h_fw , ( h_fw_last , c_fw_last ) = cudnn_lstm_wrapper ( units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) with tf . variable_scope ( 'Backward' ) : reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) h_bw , ( h_bw_last , c_bw_last ) = cudnn_lstm_wrapper ( reversed_units , n_hidden , n_layers = n_layers , trainable_initial_states = trainable_initial_states , seq_lengths = seq_lengths ) h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) return ( h_fw , h_bw ) , ( ( h_fw_last , c_fw_last ) , ( h_bw_last , c_bw_last ) )",Fast CuDNN Bi - LSTM implementation
"def cudnn_stacked_bi_gru ( units , n_hidden , seq_lengths = None , n_stacks = 2 , keep_prob = 1.0 , concat_stacked_outputs = False , trainable_initial_states = False , name = 'cudnn_stacked_bi_gru' , reuse = False ) : if seq_lengths is None : seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] outputs = [ units ] with tf . variable_scope ( name , reuse = reuse ) : for n in range ( n_stacks ) : if n == 0 : inputs = outputs [ - 1 ] else : inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , n_layers = 1 , trainable_initial_states = trainable_initial_states , name = '{}_cudnn_bi_gru' . format ( n ) , reuse = reuse ) outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) if concat_stacked_outputs : return tf . concat ( outputs [ 1 : ] , axis = 2 ) return outputs [ - 1 ]",Fast CuDNN Stacked Bi - GRU implementation
"def variational_dropout ( units , keep_prob , fixed_mask_dims = ( 1 , ) ) : units_shape = tf . shape ( units ) noise_shape = [ units_shape [ n ] for n in range ( len ( units . shape ) ) ] for dim in fixed_mask_dims : noise_shape [ dim ] = 1 return tf . nn . dropout ( units , keep_prob , noise_shape )",Dropout with the same drop mask for all fixed_mask_dims
"def build ( self ) : word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = ""int32"" ) inputs = [ word_inputs ] word_outputs = self . _build_word_cnn ( word_inputs ) if len ( self . word_vectorizers ) > 0 : additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = ""float32"" ) for input_dim , dense_dim in self . word_vectorizers ] inputs . extend ( additional_word_inputs ) additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) compile_args = { ""optimizer"" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , ""loss"" : ""categorical_crossentropy"" , ""metrics"" : [ ""accuracy"" ] } self . model_ = Model ( inputs , outputs ) self . model_ . compile ( * * compile_args ) if self . verbose > 0 : self . model_ . summary ( print_fn = log . info ) return self",Builds the network using Keras .
"def _build_word_cnn ( self , inputs ) : inputs = kl . Lambda ( kb . one_hot , arguments = { ""num_classes"" : self . symbols_number_ } , output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) conv_outputs = [ ] self . char_output_dim_ = 0 for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : curr_output = char_embeddings curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) if filters_number is None else filters_number ) for _ in range ( self . char_conv_layers - 1 ) : curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = ""same"" , activation = ""relu"" , data_format = ""channels_last"" ) ( curr_output ) if self . conv_dropout > 0.0 : curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , padding = ""same"" , activation = ""relu"" , data_format = ""channels_last"" ) ( curr_output ) conv_outputs . append ( curr_output ) self . char_output_dim_ += curr_filters_number if len ( conv_outputs ) > 1 : conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) else : conv_output = conv_outputs [ 0 ] highway_input = kl . Lambda ( kb . max , arguments = { ""axis"" : - 2 } ) ( conv_output ) if self . intermediate_dropout > 0.0 : highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) for i in range ( self . char_highway_layers - 1 ) : highway_input = Highway ( activation = ""relu"" ) ( highway_input ) if self . highway_dropout > 0.0 : highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) highway_output = Highway ( activation = ""relu"" ) ( highway_input ) return highway_output",Builds word - level network
"def _build_basic_network ( self , word_outputs ) : if self . word_dropout > 0.0 : lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) else : lstm_outputs = word_outputs for j in range ( self . word_lstm_layers - 1 ) : lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) lstm_outputs = kl . Bidirectional ( kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , dropout = self . lstm_dropout ) ) ( lstm_outputs ) pre_outputs = kl . TimeDistributed ( kl . Dense ( self . tags_number_ , activation = ""softmax"" , activity_regularizer = self . regularizer ) , name = ""p"" ) ( lstm_outputs ) return pre_outputs , lstm_outputs",Creates the basic network architecture transforming word embeddings to intermediate outputs
"def train_on_batch ( self , data : List [ Iterable ] , labels : Iterable [ list ] ) -> None : X , Y = self . _transform_batch ( data , labels ) self . model_ . train_on_batch ( X , Y )",Trains model on a single batch
"def predict_on_batch ( self , data : Union [ list , tuple ] , return_indexes : bool = False ) -> List [ List [ str ] ] : X = self . _transform_batch ( data ) objects_number , lengths = len ( X [ 0 ] ) , [ len ( elem ) for elem in data [ 0 ] ] Y = self . model_ . predict_on_batch ( X ) labels = np . argmax ( Y , axis = - 1 ) answer : List [ List [ str ] ] = [ None ] * objects_number for i , ( elem , length ) in enumerate ( zip ( labels , lengths ) ) : elem = elem [ : length ] answer [ i ] = elem if return_indexes else self . tags . idxs2toks ( elem ) return answer",Makes predictions on a single batch
"def _make_sent_vector ( self , sent : List , bucket_length : int = None ) -> np . ndarray : bucket_length = bucket_length or len ( sent ) answer = np . zeros ( shape = ( bucket_length , MAX_WORD_LENGTH + 2 ) , dtype = np . int32 ) for i , word in enumerate ( sent ) : answer [ i , 0 ] = self . tags . tok2idx ( ""BEGIN"" ) m = min ( len ( word ) , MAX_WORD_LENGTH ) for j , x in enumerate ( word [ - m : ] ) : answer [ i , j + 1 ] = self . symbols . tok2idx ( x ) answer [ i , m + 1 ] = self . tags . tok2idx ( ""END"" ) answer [ i , m + 2 : ] = self . tags . tok2idx ( ""PAD"" ) return answer",Transforms a sentence to Numpy array which will be the network input .
"def _make_tags_vector ( self , tags , bucket_length = None ) -> np . ndarray : bucket_length = bucket_length or len ( tags ) answer = np . zeros ( shape = ( bucket_length , ) , dtype = np . int32 ) for i , tag in enumerate ( tags ) : answer [ i ] = self . tags . tok2idx ( tag ) return answer",Transforms a sentence of tags to Numpy array which will be the network target .
"def bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , weights : Tuple = ( 1 , ) , smoothing_function = SMOOTH . method1 , auto_reweigh = False , penalty = True ) -> float : bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) hyp_len = len ( y_predicted ) hyp_lengths = hyp_len ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) if penalty is True or bpenalty == 0 : return bleu_measure return bleu_measure / bpenalty",Calculate BLEU score
def verify_sc_url ( url : str ) -> bool : parsed = urlsplit ( url ) scheme : str = parsed . scheme netloc : str = parsed . netloc path : str = parsed . path try : port = parsed . port except ValueError : port = None result = ( scheme . lower ( ) == 'https' and netloc . lower ( ) . split ( ':' ) [ 0 ] == 's3.amazonaws.com' and path . startswith ( '/echo.api/' ) and ( port == 443 or port is None ) ) return result,Verify signature certificate URL against Amazon Alexa requirements .
"def extract_certs ( certs_txt : str ) -> List [ crypto . X509 ] : pattern = r'-----BEGIN CERTIFICATE-----.+?-----END CERTIFICATE-----' certs_txt = re . findall ( pattern , certs_txt , flags = re . DOTALL ) certs = [ crypto . load_certificate ( crypto . FILETYPE_PEM , cert_txt ) for cert_txt in certs_txt ] return certs",Extracts pycrypto X509 objects from SSL certificates chain string .
def verify_sans ( amazon_cert : crypto . X509 ) -> bool : cert_extentions = [ amazon_cert . get_extension ( i ) for i in range ( amazon_cert . get_extension_count ( ) ) ] subject_alt_names = '' for extention in cert_extentions : if 'subjectAltName' in str ( extention . get_short_name ( ) ) : subject_alt_names = extention . __str__ ( ) break result = 'echo-api.amazon.com' in subject_alt_names return result,Verifies Subject Alternative Names ( SANs ) for Amazon certificate .
"def verify_certs_chain ( certs_chain : List [ crypto . X509 ] , amazon_cert : crypto . X509 ) -> bool : store = crypto . X509Store ( ) for cert in certs_chain : store . add_cert ( cert ) default_verify_paths = ssl . get_default_verify_paths ( ) default_verify_file = default_verify_paths . cafile default_verify_file = Path ( default_verify_file ) . resolve ( ) if default_verify_file else None default_verify_path = default_verify_paths . capath default_verify_path = Path ( default_verify_path ) . resolve ( ) if default_verify_path else None ca_files = [ ca_file for ca_file in default_verify_path . iterdir ( ) ] if default_verify_path else [ ] if default_verify_file : ca_files . append ( default_verify_file ) for ca_file in ca_files : ca_file : Path if ca_file . is_file ( ) : with ca_file . open ( 'r' , encoding = 'ascii' ) as crt_f : ca_certs_txt = crt_f . read ( ) ca_certs = extract_certs ( ca_certs_txt ) for cert in ca_certs : store . add_cert ( cert ) ssl_context = ssl . create_default_context ( ) der_certs = ssl_context . get_ca_certs ( binary_form = True ) pem_certs = '\n' . join ( [ ssl . DER_cert_to_PEM_cert ( der_cert ) for der_cert in der_certs ] ) ca_certs = extract_certs ( pem_certs ) for ca_cert in ca_certs : store . add_cert ( ca_cert ) store_context = crypto . X509StoreContext ( store , amazon_cert ) try : store_context . verify_certificate ( ) result = True except crypto . X509StoreContextError : result = False return result",Verifies if Amazon and additional certificates creates chain of trust to a root CA .
"def verify_signature ( amazon_cert : crypto . X509 , signature : str , request_body : bytes ) -> bool : signature = base64 . b64decode ( signature ) try : crypto . verify ( amazon_cert , signature , request_body , 'sha1' ) result = True except crypto . Error : result = False return result",Verifies Alexa request signature .
"def verify_cert ( signature_chain_url : str ) -> Optional [ crypto . X509 ] : try : certs_chain_get = requests . get ( signature_chain_url ) except requests . exceptions . ConnectionError as e : log . error ( f'Amazon signature chain get error: {e}' ) return None certs_chain_txt = certs_chain_get . text certs_chain = extract_certs ( certs_chain_txt ) amazon_cert : crypto . X509 = certs_chain . pop ( 0 ) sc_url_verification = verify_sc_url ( signature_chain_url ) if not sc_url_verification : log . error ( f'Amazon signature url {signature_chain_url} was not verified' ) expired_verification = not amazon_cert . has_expired ( ) if not expired_verification : log . error ( f'Amazon certificate ({signature_chain_url}) expired' ) sans_verification = verify_sans ( amazon_cert ) if not sans_verification : log . error ( f'Subject alternative names verification for ({signature_chain_url}) certificate failed' ) chain_verification = verify_certs_chain ( certs_chain , amazon_cert ) if not chain_verification : log . error ( f'Certificates chain verification for ({signature_chain_url}) certificate failed' ) result = ( sc_url_verification and expired_verification and sans_verification and chain_verification ) return amazon_cert if result else None",Conducts series of Alexa SSL certificate verifications against Amazon Alexa requirements .
def json ( self ) -> list : json_controls = [ control . json ( ) for control in self . controls ] return json_controls,Returns list of json compatible states of the RichMessage instance nested controls .
def ms_bot_framework ( self ) -> list : ms_bf_controls = [ control . ms_bot_framework ( ) for control in self . controls ] return ms_bf_controls,Returns list of MS Bot Framework compatible states of the RichMessage instance nested controls .
def telegram ( self ) -> list : telegram_controls = [ control . telegram ( ) for control in self . controls ] return telegram_controls,Returns list of Telegram compatible states of the RichMessage instance nested controls .
def alexa ( self ) -> list : alexa_controls = [ control . alexa ( ) for control in self . controls ] return alexa_controls,Returns list of Amazon Alexa compatible states of the RichMessage instance nested controls .
def main ( ) : args = parser . parse_args ( ) path = get_settings_path ( ) if args . default : if populate_settings_dir ( force = True ) : print ( f'Populated {path} with default settings files' ) else : print ( f'{path} is already a default settings directory' ) else : print ( f'Current DeepPavlov settings path: {path}' ),DeepPavlov console configuration utility .
"def _graph_wrap ( func , graph ) : @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : return func ( * args , * * kwargs ) return _wrapped",Constructs function encapsulated in the graph .
"def _keras_wrap ( func , graph , session ) : import keras . backend as K @ wraps ( func ) def _wrapped ( * args , * * kwargs ) : with graph . as_default ( ) : K . set_session ( session ) return func ( * args , * * kwargs ) return _wrapped",Constructs function encapsulated in the graph and the session .
"def roc_auc_score ( y_true : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] , y_pred : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] ) -> float : try : return sklearn . metrics . roc_auc_score ( np . squeeze ( np . array ( y_true ) ) , np . squeeze ( np . array ( y_pred ) ) , average = ""macro"" ) except ValueError : return 0.",Compute Area Under the Curve ( AUC ) from prediction scores .
"def hash_ ( token : str , hash_size : int ) -> int : return murmurhash3_32 ( token , positive = True ) % hash_size",Convert a token to a hash of given size . Args : token : a word hash_size : hash size
"def accuracy ( y_true : [ list , np . ndarray ] , y_predicted : [ list , np . ndarray ] ) -> float : examples_len = len ( y_true ) correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , y_predicted ) ] ) return correct / examples_len if examples_len else 0",Calculate accuracy in terms of absolute coincidence
"def round_accuracy ( y_true , y_predicted ) : predictions = [ round ( x ) for x in y_predicted ] examples_len = len ( y_true ) correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , predictions ) ] ) return correct / examples_len if examples_len else 0",Rounds predictions and calculates accuracy in terms of absolute coincidence .
"def _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : weight_name_map = { } for i in range ( 2 ) : for j in range ( 8 ) : root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' varname_in_file = varname [ 5 : ] if varname_in_file . startswith ( 'RNN' ) : varname_in_file = weight_name_map [ varname_in_file ] if varname_in_file == 'embedding' : with h5py . File ( embedding_weight_file , 'r' ) as fin : embed_weights = fin [ varname_in_file ] [ ... ] weights = np . zeros ( ( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , dtype = DTYPE ) weights [ 1 : , : ] = embed_weights else : with h5py . File ( weight_file , 'r' ) as fin : if varname_in_file == 'char_embed' : char_embed_weights = fin [ varname_in_file ] [ ... ] weights = np . zeros ( ( char_embed_weights . shape [ 0 ] + 1 , char_embed_weights . shape [ 1 ] ) , dtype = DTYPE ) weights [ 1 : , : ] = char_embed_weights else : weights = fin [ varname_in_file ] [ ... ] def ret ( shape , * * kwargs ) : if list ( shape ) != list ( weights . shape ) : raise ValueError ( ""Invalid shape initializing {0}, got {1}, expected {2}"" . format ( varname_in_file , shape , weights . shape ) ) return weights return ret",We ll stub out all the initializers in the pretrained LM with a function that loads the weights from the file
"def weight_layers ( name , bilm_ops , l2_coef = None , use_top_only = False , do_layer_norm = False , reuse = False ) : def _l2_regularizer ( weights ) : if l2_coef is not None : return l2_coef * tf . reduce_sum ( tf . square ( weights ) ) else : return 0.0 lm_embeddings = bilm_ops [ 'lm_embeddings' ] mask = bilm_ops [ 'mask' ] n_lm_layers = int ( lm_embeddings . get_shape ( ) [ 1 ] ) lm_dim = int ( lm_embeddings . get_shape ( ) [ 3 ] ) with tf . control_dependencies ( [ lm_embeddings , mask ] ) : mask_float = tf . cast ( mask , 'float32' ) broadcast_mask = tf . expand_dims ( mask_float , axis = - 1 ) def _do_ln ( x ) : x_masked = x * broadcast_mask N = tf . reduce_sum ( mask_float ) * lm_dim mean = tf . reduce_sum ( x_masked ) / N variance = tf . reduce_sum ( ( ( x_masked - mean ) * broadcast_mask ) ** 2 ) / N return tf . nn . batch_normalization ( x , mean , variance , None , None , 1E-12 ) if use_top_only : layers = tf . split ( lm_embeddings , n_lm_layers , axis = 1 ) sum_pieces = tf . squeeze ( layers [ - 1 ] , squeeze_dims = 1 ) reg = 0.0 else : with tf . variable_scope ( ""aggregation"" , reuse = reuse ) : W = tf . get_variable ( '{}_ELMo_W' . format ( name ) , shape = ( n_lm_layers , ) , initializer = tf . zeros_initializer , regularizer = _l2_regularizer , trainable = True , ) normed_weights = tf . split ( tf . nn . softmax ( W + 1.0 / n_lm_layers ) , n_lm_layers ) layers = tf . split ( lm_embeddings , n_lm_layers , axis = 1 ) pieces = [ ] for w , t in zip ( normed_weights , layers ) : if do_layer_norm : pieces . append ( w * _do_ln ( tf . squeeze ( t , squeeze_dims = 1 ) ) ) else : pieces . append ( w * tf . squeeze ( t , squeeze_dims = 1 ) ) sum_pieces = tf . add_n ( pieces ) reg = [ r for r in tf . get_collection ( tf . GraphKeys . REGULARIZATION_LOSSES ) if r . name . find ( '{}_ELMo_W/' . format ( name ) ) >= 0 ] if len ( reg ) != 1 : raise ValueError with tf . variable_scope ( ""aggregation"" , reuse = reuse ) : gamma = tf . get_variable ( '{}_ELMo_gamma' . format ( name ) , shape = ( 1 , ) , initializer = tf . ones_initializer , regularizer = None , trainable = True , ) weighted_lm_layers = sum_pieces * gamma weighted_lm_layers_masked = sum_pieces * broadcast_mask weighted_lm_layers_sum = tf . reduce_sum ( weighted_lm_layers_masked , 1 ) mask_sum = tf . reduce_sum ( mask_float , 1 ) mask_sum = tf . maximum ( mask_sum , [ 1 ] ) weighted_lm_layers_mean = weighted_lm_layers_sum / tf . expand_dims ( mask_sum , - 1 ) word_emb_2n = tf . squeeze ( layers [ 0 ] , [ 1 ] ) word_emb_1n = tf . slice ( word_emb_2n , [ 0 , 0 , 0 ] , [ - 1 , - 1 , lm_dim // 2 ] ) lstm_outputs1 = tf . squeeze ( layers [ 1 ] , [ 1 ] ) lstm_outputs2 = tf . squeeze ( layers [ 2 ] , [ 1 ] ) ret = { 'weighted_op' : weighted_lm_layers , 'mean_op' : weighted_lm_layers_mean , 'regularization_op' : reg , 'word_emb' : word_emb_1n , 'lstm_outputs1' : lstm_outputs1 , 'lstm_outputs2' : lstm_outputs2 , } return ret",Weight the layers of a biLM with trainable scalar weights to compute ELMo representations .
"def _build_word_char_embeddings ( self ) : projection_dim = self . options [ 'lstm' ] [ 'projection_dim' ] cnn_options = self . options [ 'char_cnn' ] filters = cnn_options [ 'filters' ] n_filters = sum ( f [ 1 ] for f in filters ) max_chars = cnn_options [ 'max_characters_per_token' ] char_embed_dim = cnn_options [ 'embedding' ] [ 'dim' ] n_chars = cnn_options [ 'n_characters' ] if n_chars != 262 : raise Exception ( ""Set n_characters=262 after training see a \
                            https://github.com/allenai/bilm-tf/blob/master/README.md"" ) if cnn_options [ 'activation' ] == 'tanh' : activation = tf . nn . tanh elif cnn_options [ 'activation' ] == 'relu' : activation = tf . nn . relu with tf . device ( ""/cpu:0"" ) : self . embedding_weights = tf . get_variable ( ""char_embed"" , [ n_chars , char_embed_dim ] , dtype = DTYPE , initializer = tf . random_uniform_initializer ( - 1.0 , 1.0 ) ) self . char_embedding = tf . nn . embedding_lookup ( self . embedding_weights , self . ids_placeholder ) def make_convolutions ( inp ) : with tf . variable_scope ( 'CNN' ) : convolutions = [ ] for i , ( width , num ) in enumerate ( filters ) : if cnn_options [ 'activation' ] == 'relu' : w_init = tf . random_uniform_initializer ( minval = - 0.05 , maxval = 0.05 ) elif cnn_options [ 'activation' ] == 'tanh' : w_init = tf . random_normal_initializer ( mean = 0.0 , stddev = np . sqrt ( 1.0 / ( width * char_embed_dim ) ) ) w = tf . get_variable ( ""W_cnn_%s"" % i , [ 1 , width , char_embed_dim , num ] , initializer = w_init , dtype = DTYPE ) b = tf . get_variable ( ""b_cnn_%s"" % i , [ num ] , dtype = DTYPE , initializer = tf . constant_initializer ( 0.0 ) ) conv = tf . nn . conv2d ( inp , w , strides = [ 1 , 1 , 1 , 1 ] , padding = ""VALID"" ) + b conv = tf . nn . max_pool ( conv , [ 1 , 1 , max_chars - width + 1 , 1 ] , [ 1 , 1 , 1 , 1 ] , 'VALID' ) conv = activation ( conv ) conv = tf . squeeze ( conv , squeeze_dims = [ 2 ] ) convolutions . append ( conv ) return tf . concat ( convolutions , 2 ) embedding = make_convolutions ( self . char_embedding ) n_highway = cnn_options . get ( 'n_highway' ) use_highway = n_highway is not None and n_highway > 0 use_proj = n_filters != projection_dim if use_highway or use_proj : batch_size_n_tokens = tf . shape ( embedding ) [ 0 : 2 ] embedding = tf . reshape ( embedding , [ - 1 , n_filters ] ) if use_proj : assert n_filters > projection_dim with tf . variable_scope ( 'CNN_proj' ) : W_proj_cnn = tf . get_variable ( ""W_proj"" , [ n_filters , projection_dim ] , initializer = tf . random_normal_initializer ( mean = 0.0 , stddev = np . sqrt ( 1.0 / n_filters ) ) , dtype = DTYPE ) b_proj_cnn = tf . get_variable ( ""b_proj"" , [ projection_dim ] , initializer = tf . constant_initializer ( 0.0 ) , dtype = DTYPE ) def high ( x , ww_carry , bb_carry , ww_tr , bb_tr ) : carry_gate = tf . nn . sigmoid ( tf . matmul ( x , ww_carry ) + bb_carry ) transform_gate = tf . nn . relu ( tf . matmul ( x , ww_tr ) + bb_tr ) return carry_gate * transform_gate + ( 1.0 - carry_gate ) * x if use_highway : highway_dim = n_filters for i in range ( n_highway ) : with tf . variable_scope ( 'CNN_high_%s' % i ) : W_carry = tf . get_variable ( 'W_carry' , [ highway_dim , highway_dim ] , initializer = tf . random_normal_initializer ( mean = 0.0 , stddev = np . sqrt ( 1.0 / highway_dim ) ) , dtype = DTYPE ) b_carry = tf . get_variable ( 'b_carry' , [ highway_dim ] , initializer = tf . constant_initializer ( - 2.0 ) , dtype = DTYPE ) W_transform = tf . get_variable ( 'W_transform' , [ highway_dim , highway_dim ] , initializer = tf . random_normal_initializer ( mean = 0.0 , stddev = np . sqrt ( 1.0 / highway_dim ) ) , dtype = DTYPE ) b_transform = tf . get_variable ( 'b_transform' , [ highway_dim ] , initializer = tf . constant_initializer ( 0.0 ) , dtype = DTYPE ) embedding = high ( embedding , W_carry , b_carry , W_transform , b_transform ) if use_proj : embedding = tf . matmul ( embedding , W_proj_cnn ) + b_proj_cnn if use_highway or use_proj : shp = tf . concat ( [ batch_size_n_tokens , [ projection_dim ] ] , axis = 0 ) embedding = tf . reshape ( embedding , shp ) self . embedding = embedding",options contains key char_cnn : {
"def read ( self , data_path : str , * args , * * kwargs ) -> Dict [ str , List [ Tuple [ Any , Any ] ] ] : raise NotImplementedError",Reads a file from a path and returns data as a list of tuples of inputs and correct outputs for every data type in train valid and test .
"def make_hello_bot_agent ( ) -> DefaultAgent : skill_hello = PatternMatchingSkill ( [ 'Hello world' ] , patterns = [ 'hi' , 'hello' , 'good day' ] ) skill_bye = PatternMatchingSkill ( [ 'Goodbye world' , 'See you around' ] , patterns = [ 'bye' , 'chao' , 'see you' ] ) skill_fallback = PatternMatchingSkill ( [ 'I don\'t understand, sorry' , 'I can say ""Hello world""' ] ) agent = DefaultAgent ( [ skill_hello , skill_bye , skill_fallback ] , skills_processor = HighestConfidenceSelector ( ) ) return agent",Builds agent based on PatternMatchingSkill and HighestConfidenceSelector .
"def to_one_hot ( x , k ) : unit = np . eye ( k , dtype = int ) return unit [ x ]",Takes an array of integers and transforms it to an array of one - hot encoded vectors
"def prettify_metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> OrderedDict : prettified_metrics = OrderedDict ( ) for key , value in metrics : value = round ( value , precision ) prettified_metrics [ key ] = value return prettified_metrics",Prettifies the dictionary of metrics .
"def populate_settings_dir ( force : bool = False ) -> bool : res = False if _default_settings_path == _settings_path : return res for src in list ( _default_settings_path . glob ( '**/*.json' ) ) : dest = _settings_path / src . relative_to ( _default_settings_path ) if not force and dest . exists ( ) : continue res = True dest . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( src , dest ) return res",Populate settings directory with default settings files
"def update_state ( self , slots : Union [ List [ Tuple [ str , Any ] ] , Dict [ str , Any ] ] ) -> 'Tracker' : pass",Updates dialogue state with new slots calculates features . Returns : Tracker : .
"def predict_with_model ( config_path : [ Path , str ] ) -> List [ Optional [ List [ str ] ] ] : config = parse_config ( config_path ) reader_config = config [ 'dataset_reader' ] reader = get_model ( reader_config [ 'class_name' ] ) ( ) data_path = expand_path ( reader_config . get ( 'data_path' , '' ) ) read_params = { k : v for k , v in reader_config . items ( ) if k not in [ 'class_name' , 'data_path' ] } data : Dict = reader . read ( data_path , * * read_params ) iterator_config = config [ 'dataset_iterator' ] iterator : MorphoTaggerDatasetIterator = from_params ( iterator_config , data = data ) model = build_model ( config , load_trained = True ) answers = [ None ] * len ( iterator . test ) batch_size = config [ 'predict' ] . get ( ""batch_size"" , - 1 ) for indexes , ( x , _ ) in iterator . gen_batches ( batch_size = batch_size , data_type = ""test"" , shuffle = False , return_indexes = True ) : y = model ( x ) for i , elem in zip ( indexes , y ) : answers [ i ] = elem outfile = config [ 'predict' ] . get ( ""outfile"" ) if outfile is not None : outfile = Path ( outfile ) if not outfile . exists ( ) : outfile . parent . mkdir ( parents = True , exist_ok = True ) with open ( outfile , ""w"" , encoding = ""utf8"" ) as fout : for elem in answers : fout . write ( elem + ""\n"" ) return answers",Returns predictions of morphotagging model given in config : config_path : .
"def run_alexa_server ( agent_generator : callable , multi_instance : bool = False , stateful : bool = False , port : Optional [ int ] = None , https : bool = False , ssl_key : str = None , ssl_cert : str = None ) -> None : server_config_path = Path ( get_settings_path ( ) , SERVER_CONFIG_FILENAME ) . resolve ( ) server_params = read_json ( server_config_path ) host = server_params [ 'common_defaults' ] [ 'host' ] port = port or server_params [ 'common_defaults' ] [ 'port' ] alexa_server_params = server_params [ 'alexa_defaults' ] alexa_server_params [ 'multi_instance' ] = multi_instance or server_params [ 'common_defaults' ] [ 'multi_instance' ] alexa_server_params [ 'stateful' ] = stateful or server_params [ 'common_defaults' ] [ 'stateful' ] alexa_server_params [ 'amazon_cert_lifetime' ] = AMAZON_CERTIFICATE_LIFETIME if https : ssh_key_path = Path ( ssl_key or server_params [ 'https_key_path' ] ) . resolve ( ) if not ssh_key_path . is_file ( ) : e = FileNotFoundError ( 'Ssh key file not found: please provide correct path in --key param or ' 'https_key_path param in server configuration file' ) log . error ( e ) raise e ssh_cert_path = Path ( ssl_cert or server_params [ 'https_cert_path' ] ) . resolve ( ) if not ssh_cert_path . is_file ( ) : e = FileNotFoundError ( 'Ssh certificate file not found: please provide correct path in --cert param or ' 'https_cert_path param in server configuration file' ) log . error ( e ) raise e ssl_context = ssl . SSLContext ( ssl . PROTOCOL_TLSv1_2 ) ssl_context . load_cert_chain ( ssh_cert_path , ssh_key_path ) else : ssl_context = None input_q = Queue ( ) output_q = Queue ( ) bot = Bot ( agent_generator , alexa_server_params , input_q , output_q ) bot . start ( ) endpoint_description = { 'description' : 'Amazon Alexa custom service endpoint' , 'parameters' : [ { 'name' : 'Signature' , 'in' : 'header' , 'required' : 'true' , 'type' : 'string' , 'example' : 'Z5H5wqd06ExFVPNfJiqhKvAFjkf+cTVodOUirucHGcEVAMO1LfvgqWUkZ/X1ITDZbI0w+SMwVkEQZlkeThbVS/54M22StNDUtfz4Ua20xNDpIPwcWIACAmZ38XxbbTEFJI5WwqrbilNcfzqiGrIPfdO5rl+/xUjHFUdcJdUY/QzBxXsceytVYfEiR9MzOCN2m4C0XnpThUavAu159KrLj8AkuzN0JF87iXv+zOEeZRgEuwmsAnJrRUwkJ4yWokEPnSVdjF0D6f6CscfyvRe9nsWShq7/zRTa41meweh+n006zvf58MbzRdXPB22RI4AN0ksWW7hSC8/QLAKQE+lvaw==' , } , { 'name' : 'Signaturecertchainurl' , 'in' : 'header' , 'required' : 'true' , 'type' : 'string' , 'example' : 'https://s3.amazonaws.com/echo.api/echo-api-cert-6-ats.pem' , } , { 'name' : 'data' , 'in' : 'body' , 'required' : 'true' , 'example' : { 'version' : '1.0' , 'session' : { 'new' : False , 'sessionId' : 'amzn1.echo-api.session.3c6ebffd-55b9-4e1a-bf3c-c921c1801b63' , 'application' : { 'applicationId' : 'amzn1.ask.skill.8b17a5de-3749-4919-aa1f-e0bbaf8a46a6' } , 'attributes' : { 'sessionId' : 'amzn1.echo-api.session.3c6ebffd-55b9-4e1a-bf3c-c921c1801b63' } , 'user' : { 'userId' : 'amzn1.ask.account.AGR4R2LOVHMNMNOGROBVNLU7CL4C57X465XJF2T2F55OUXNTLCXDQP3I55UXZIALEKKZJ6Q2MA5MEFSMZVPEL5NVZS6FZLEU444BVOLPB5WVH5CHYTQAKGD7VFLGPRFZVHHH2NIB4HKNHHGX6HM6S6QDWCKXWOIZL7ONNQSBUCVPMZQKMCYXRG5BA2POYEXFDXRXCGEVDWVSMPQ' } } , 'context' : { 'System' : { 'application' : { 'applicationId' : 'amzn1.ask.skill.8b17a5de-3749-4919-aa1f-e0bbaf8a46a6' } , 'user' : { 'userId' : 'amzn1.ask.account.AGR4R2LOVHMNMNOGROBVNLU7CL4C57X465XJF2T2F55OUXNTLCXDQP3I55UXZIALEKKZJ6Q2MA5MEFSMZVPEL5NVZS6FZLEU444BVOLPB5WVH5CHYTQAKGD7VFLGPRFZVHHH2NIB4HKNHHGX6HM6S6QDWCKXWOIZL7ONNQSBUCVPMZQKMCYXRG5BA2POYEXFDXRXCGEVDWVSMPQ' } , 'device' : { 'deviceId' : 'amzn1.ask.device.AFQAMLYOYQUUACSE7HFVYS4ZI2KUB35JPHQRUPKTDCAU3A47WESP5L57KSWT5L6RT3FVXWH4OA2DNPJRMZ2VGEIACF3PJEIDCOUWUBC4W5RPJNUB3ZVT22J4UJN5UL3T2UBP36RVHFJ5P4IPT2HUY3P2YOY33IOU4O33HUAG7R2BUNROEH4T2' , 'supportedInterfaces' : { } } , 'apiEndpoint' : 'https://api.amazonalexa.com' , 'apiAccessToken' : 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6IjEifQ.eyJhdWQiOiJodHRwczovL2FwaS5hbWF6b25hbGV4YS5jb20iLCJpc3MiOiJBbGV4YVNraWxsS2l0Iiwic3ViIjoiYW16bjEuYXNrLnNraWxsLjhiMTdhNWRlLTM3NDktNDkxOS1hYTFmLWUwYmJhZjhhNDZhNiIsImV4cCI6MTU0NTIyMzY1OCwiaWF0IjoxNTQ1MjIwMDU4LCJuYmYiOjE1NDUyMjAwNTgsInByaXZhdGVDbGFpbXMiOnsiY29uc2VudFRva2VuIjpudWxsLCJkZXZpY2VJZCI6ImFtem4xLmFzay5kZXZpY2UuQUZRQU1MWU9ZUVVVQUNTRTdIRlZZUzRaSTJLVUIzNUpQSFFSVVBLVERDQVUzQTQ3V0VTUDVMNTdLU1dUNUw2UlQzRlZYV0g0T0EyRE5QSlJNWjJWR0VJQUNGM1BKRUlEQ09VV1VCQzRXNVJQSk5VQjNaVlQyMko0VUpONVVMM1QyVUJQMzZSVkhGSjVQNElQVDJIVVkzUDJZT1kzM0lPVTRPMzNIVUFHN1IyQlVOUk9FSDRUMiIsInVzZXJJZCI6ImFtem4xLmFzay5hY2NvdW50LkFHUjRSMkxPVkhNTk1OT0dST0JWTkxVN0NMNEM1N1g0NjVYSkYyVDJGNTVPVVhOVExDWERRUDNJNTVVWFpJQUxFS0taSjZRMk1BNU1FRlNNWlZQRUw1TlZaUzZGWkxFVTQ0NEJWT0xQQjVXVkg1Q0hZVFFBS0dEN1ZGTEdQUkZaVkhISDJOSUI0SEtOSEhHWDZITTZTNlFEV0NLWFdPSVpMN09OTlFTQlVDVlBNWlFLTUNZWFJHNUJBMlBPWUVYRkRYUlhDR0VWRFdWU01QUSJ9fQ.jcomYhBhU485T4uoe2NyhWnL-kZHoPQKpcycFqa-1sy_lSIitfFGup9DKrf2NkN-I9lZ3xwq9llqx9WRN78fVJjN6GLcDhBDH0irPwt3n9_V7_5bfB6KARv5ZG-JKOmZlLBqQbnln0DAJ10D8HNiytMARNEwduMBVDNK0A5z6YxtRcLYYFD2-Ieg_V8Qx90eE2pd2U5xOuIEL0pXfSoiJ8vpxb8BKwaMO47tdE4qhg_k7v8ClwyXg3EMEhZFjixYNqdW1tCrwDGj58IWMXDyzZhIlRMh6uudMOT6scSzcNVD0v42IOTZ3S_X6rG01B7xhUDlZXMqkrCuzOyqctGaPw' } , 'Viewport' : { 'experiences' : [ { 'arcMinuteWidth' : 246 , 'arcMinuteHeight' : 144 , 'canRotate' : False , 'canResize' : False } ] , 'shape' : 'RECTANGLE' , 'pixelWidth' : 1024 , 'pixelHeight' : 600 , 'dpi' : 160 , 'currentPixelWidth' : 1024 , 'currentPixelHeight' : 600 , 'touch' : [ 'SINGLE' ] } } , 'request' : { 'type' : 'IntentRequest' , 'requestId' : 'amzn1.echo-api.request.388d0f6e-04b9-4450-a687-b9abaa73ac6a' , 'timestamp' : '2018-12-19T11:47:38Z' , 'locale' : 'en-US' , 'intent' : { 'name' : 'AskDeepPavlov' , 'confirmationStatus' : 'NONE' , 'slots' : { 'raw_input' : { 'name' : 'raw_input' , 'value' : 'my beautiful sandbox skill' , 'resolutions' : { 'resolutionsPerAuthority' : [ { 'authority' : 'amzn1.er-authority.echo-sdk.amzn1.ask.skill.8b17a5de-3749-4919-aa1f-e0bbaf8a46a6.GetInput' , 'status' : { 'code' : 'ER_SUCCESS_NO_MATCH' } } ] } , 'confirmationStatus' : 'NONE' , 'source' : 'USER' } } } } } } ] , 'responses' : { ""200"" : { ""description"" : ""A model response"" } } } @ app . route ( '/' ) def index ( ) : return redirect ( '/apidocs/' ) @ app . route ( '/interact' , methods = [ 'POST' ] ) @ swag_from ( endpoint_description ) def handle_request ( ) : request_body : bytes = request . get_data ( ) signature_chain_url : str = request . headers . get ( 'Signaturecertchainurl' ) signature : str = request . headers . get ( 'Signature' ) alexa_request : dict = request . get_json ( ) request_dict = { 'request_body' : request_body , 'signature_chain_url' : signature_chain_url , 'signature' : signature , 'alexa_request' : alexa_request } bot . input_queue . put ( request_dict ) response : dict = bot . output_queue . get ( ) response_code = 400 if 'error' in response . keys ( ) else 200 return jsonify ( response ) , response_code app . run ( host = host , port = port , threaded = True , ssl_context = ssl_context )",Initiates Flask web service with Alexa skill .
"def load ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . load_path . resolve ( ) ) if tf . train . checkpoint_exists ( path ) : log . info ( '[loading model from {}]' . format ( path ) ) var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . restore ( self . sess , path )",Load model parameters from self . load_path
"def save ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : if not hasattr ( self , 'sess' ) : raise RuntimeError ( 'Your TensorFlow model {} must' ' have sess attribute!' . format ( self . __class__ . __name__ ) ) path = str ( self . save_path . resolve ( ) ) log . info ( '[saving model to {}]' . format ( path ) ) var_list = self . _get_saveable_variables ( exclude_scopes ) saver = tf . train . Saver ( var_list ) saver . save ( self . sess , path )",Save model parameters to self . save_path
"def get_train_op ( self , loss , learning_rate , optimizer = None , clip_norm = None , learnable_scopes = None , optimizer_scope_name = None , * * kwargs ) : if optimizer_scope_name is None : opt_scope = tf . variable_scope ( 'Optimizer' ) else : opt_scope = tf . variable_scope ( optimizer_scope_name ) with opt_scope : if learnable_scopes is None : variables_to_train = tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES ) else : variables_to_train = [ ] for scope_name in learnable_scopes : variables_to_train . extend ( tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES , scope = scope_name ) ) if optimizer is None : optimizer = tf . train . AdamOptimizer extra_update_ops = tf . get_collection ( tf . GraphKeys . UPDATE_OPS ) with tf . control_dependencies ( extra_update_ops ) : def clip_if_not_none ( grad ) : if grad is not None : return tf . clip_by_norm ( grad , clip_norm ) opt = optimizer ( learning_rate , * * kwargs ) grads_and_vars = opt . compute_gradients ( loss , var_list = variables_to_train ) if clip_norm is not None : grads_and_vars = [ ( clip_if_not_none ( grad ) , var ) for grad , var in grads_and_vars ] train_op = opt . apply_gradients ( grads_and_vars ) return train_op",Get train operation for given loss
"def print_number_of_parameters ( ) : log . info ( 'Number of parameters: ' ) variables = tf . trainable_variables ( ) blocks = defaultdict ( int ) for var in variables : block_name = var . name . split ( '/' ) [ 0 ] number_of_parameters = np . prod ( var . get_shape ( ) . as_list ( ) ) blocks [ block_name ] += number_of_parameters for block_name , cnt in blocks . items ( ) : log . info ( ""{} - {}."" . format ( block_name , cnt ) ) total_num_parameters = np . sum ( list ( blocks . values ( ) ) ) log . info ( 'Total number of parameters equal {}' . format ( total_num_parameters ) )",Print number of * trainable * parameters in the network
"def _precompute_absense_costs ( dictionary , removal_costs , insertion_costs , n , allow_spaces = False ) : answer = [ dict ( ) for node in dictionary . data ] if n == 0 : return answer curr_alphabet = copy . copy ( dictionary . alphabet ) if allow_spaces : curr_alphabet += [ ' ' ] for l , ( costs_in_node , node ) in enumerate ( zip ( answer , dictionary . data ) ) : curr_node_removal_costs = np . empty ( dtype = np . float64 , shape = ( n , ) ) if len ( node [ 0 ] ) > 0 : curr_node_removal_costs [ 0 ] = min ( removal_costs [ symbol ] for symbol in node [ 0 ] ) for j , symbols in enumerate ( node [ 1 : ] , 1 ) : if len ( symbols ) == 0 : curr_node_removal_costs [ j : ] = curr_node_removal_costs [ j - 1 ] break curr_cost = min ( removal_costs [ symbol ] for symbol in symbols ) curr_node_removal_costs [ j ] = min ( curr_node_removal_costs [ j - 1 ] , curr_cost ) else : curr_node_removal_costs [ : ] = np . inf for a in curr_alphabet : curr_symbol_costs = np . empty ( dtype = np . float64 , shape = ( n , ) ) curr_symbol_costs . fill ( insertion_costs [ a ] ) for j , symbols in enumerate ( node ) : if a in symbols : curr_symbol_costs [ j : ] = 0.0 break curr_symbol_costs [ j ] = min ( curr_symbol_costs [ j ] , curr_node_removal_costs [ j ] ) costs_in_node [ a ] = curr_symbol_costs return answer",              costs
"def search ( self , word , d , allow_spaces = True , return_cost = True ) : if not all ( ( c in self . alphabet or ( c == "" "" and self . allow_spaces ) ) for c in word ) : return [ ] return self . _trie_search ( word , d , allow_spaces = allow_spaces , return_cost = return_cost )",Finds all dictionary words in d - window from word
"def _trie_search ( self , word , d , transducer = None , allow_spaces = True , return_cost = True ) : if transducer is None : transducer = self . transducer . inverse ( ) allow_spaces &= self . allow_spaces trie = self . dictionary used_agenda_keys = set ( ) agenda = SortedListWithKey ( key = ( lambda x : x [ 1 ] ) ) h = self . h_func ( word , trie . root ) key , value = ( """" , 0 , trie . root ) , ( 0.0 , 0.0 , h ) agenda . add ( ( key , value ) ) answer = dict ( ) k = 0 while len ( agenda ) > 0 : key , value = agenda . pop ( 0 ) if key in used_agenda_keys : continue used_agenda_keys . add ( key ) low , pos , index = key cost , g , h = value k += 1 max_upperside_length = min ( len ( word ) - pos , transducer . max_up_length ) for upperside_length in range ( max_upperside_length + 1 ) : new_pos = pos + upperside_length curr_up = word [ pos : new_pos ] if curr_up not in transducer . operation_costs : continue for curr_low , curr_cost in transducer . operation_costs [ curr_up ] . items ( ) : new_g = g + curr_cost if new_g > d : continue if curr_low == "" "" : if allow_spaces and trie . is_final ( index ) : new_index = trie . root else : new_index = Trie . NO_NODE else : new_index = trie . descend ( index , curr_low ) if new_index is Trie . NO_NODE : continue new_low = low + curr_low new_h = self . h_func ( word [ new_pos : ] , new_index ) new_cost = new_g + new_h if new_cost > d : continue new_key = ( new_low , new_pos , new_index ) new_value = ( new_cost , new_g , new_h ) if new_pos == len ( word ) and trie . is_final ( new_index ) : old_g = answer . get ( new_low , None ) if old_g is None or new_g < old_g : answer [ new_low ] = new_g agenda . add ( ( new_key , new_value ) ) answer = sorted ( answer . items ( ) , key = ( lambda x : x [ 1 ] ) ) if return_cost : return answer else : return [ elem [ 0 ] for elem in answer ]",                d
"def _precompute_euristics ( self ) : if self . euristics is None : return removal_costs = { a : np . inf for a in self . alphabet } insertion_costs = { a : np . inf for a in self . alphabet } if self . allow_spaces : removal_costs [ ' ' ] = np . inf insertion_costs [ ' ' ] = np . inf for up , costs in self . transducer . operation_costs . items ( ) : for low , cost in costs . items ( ) : if up == low : continue if up != '' : removal_cost = cost / len ( up ) for a in up : removal_costs [ a ] = min ( removal_costs [ a ] , removal_cost ) if low != '' : insertion_cost = cost / len ( low ) for a in low : insertion_costs [ a ] = min ( insertion_costs [ a ] , insertion_cost ) self . _absense_costs_by_node = _precompute_absense_costs ( self . dictionary , removal_costs , insertion_costs , self . euristics , self . allow_spaces ) self . _temporary_euristics = [ dict ( ) for i in range ( len ( self . dictionary ) ) ]",         h - 
"def _euristic_h_function ( self , suffix , index ) : if self . euristics > 0 : suffix = suffix [ : self . euristics ] index_temporary_euristics = self . _temporary_euristics [ index ] cost = index_temporary_euristics . get ( suffix , None ) if cost is not None : return cost absense_costs = self . _absense_costs_by_node [ index ] data = self . dictionary . data [ index ] costs = np . zeros ( dtype = np . float64 , shape = ( self . euristics , ) ) for i , a in enumerate ( suffix ) : costs [ i : ] += absense_costs [ a ] [ i : ] cost = max ( costs ) index_temporary_euristics [ suffix ] = cost return cost", h -    Hulden 2009    
"def get_operation_cost ( self , up , low ) : up_costs = self . operation_costs . get ( up , None ) if up_costs is None : return np . inf cost = up_costs . get ( low , np . inf ) return cost",    up - > low  np . inf     
"def inverse ( self ) : inversed_transducer = SegmentTransducer ( self . alphabet , operation_costs = dict ( ) ) inversed_transducer . operation_costs = self . _reversed_operation_costs inversed_transducer . _reversed_operation_costs = self . operation_costs inversed_transducer . max_low_length = self . max_up_length inversed_transducer . max_up_length = self . max_low_length inversed_transducer . max_low_lengths_by_up = self . max_up_lengths_by_low inversed_transducer . max_up_lengths_by_low = self . max_low_lengths_by_up return inversed_transducer",     
"def distance ( self , first , second , return_transduction = False ) : if return_transduction : add_pred = ( lambda x , y : ( y == np . inf or x < y ) ) else : add_pred = ( lambda x , y : ( y == np . inf or x <= y ) ) clear_pred = ( lambda x , y : ( y < np . inf and x < y ) ) update_func = lambda x , y : min ( x , y ) costs , backtraces = self . _fill_levenshtein_table ( first , second , update_func , add_pred , clear_pred ) final_cost = costs [ - 1 ] [ - 1 ] if final_cost == np . inf : transductions = [ None ] elif return_transduction : transductions = self . _backtraces_to_transductions ( first , second , backtraces , final_cost , return_cost = False ) if return_transduction : return final_cost , transductions else : return final_cost",     first  second
"def transduce ( self , first , second , threshold ) : add_pred = ( lambda x , y : x <= threshold ) clear_pred = ( lambda x , y : False ) update_func = ( lambda x , y : min ( x , y ) ) costs , backtraces = self . _fill_levenshtein_table ( first , second , update_func , add_pred , clear_pred , threshold = threshold ) result = self . _backtraces_to_transductions ( first , second , backtraces , threshold , return_cost = True ) return result",    first  second     threshold
"def lower_transductions ( self , word , max_cost , return_cost = True ) : prefixes = [ [ ] for i in range ( len ( word ) + 1 ) ] prefixes [ 0 ] . append ( ( ( ) , 0.0 ) ) for pos in range ( len ( prefixes ) ) : prefixes [ pos ] = self . _perform_insertions ( prefixes [ pos ] , max_cost ) max_upperside_length = min ( len ( word ) - pos , self . max_up_length ) for upperside_length in range ( 1 , max_upperside_length + 1 ) : up = word [ pos : pos + upperside_length ] for low , low_cost in self . operation_costs . get ( up , dict ( ) ) . items ( ) : for transduction , cost in prefixes [ pos ] : new_cost = cost + low_cost if new_cost <= max_cost : new_transduction = transduction + ( up , low ) prefixes [ pos + upperside_length ] . append ( ( new_transduction , new_cost ) ) answer = sorted ( prefixes [ - 1 ] , key = ( lambda x : x [ 0 ] ) ) if return_cost : return answer else : return [ elem [ 0 ] for elem in answer ]",      word     max_cost
"def _fill_levenshtein_table ( self , first , second , update_func , add_pred , clear_pred , threshold = None ) : m , n = len ( first ) , len ( second ) if threshold is None : threshold = 0.0 for a , b in zip ( first , second ) : threshold += self . get_operation_cost ( a , b ) if m > n : for a in first [ n : ] : threshold += self . get_operation_cost ( a , '' ) elif m < n : for b in second [ m : ] : threshold += self . get_operation_cost ( '' , b ) threshold *= 2 costs = np . zeros ( shape = ( m + 1 , n + 1 ) , dtype = np . float64 ) costs [ : ] = np . inf backtraces = [ None ] * ( m + 1 ) for i in range ( m + 1 ) : backtraces [ i ] = [ [ ] for j in range ( n + 1 ) ] costs [ 0 ] [ 0 ] = 0.0 for i in range ( m + 1 ) : for i_right in range ( i , min ( i + self . max_up_length , m ) + 1 ) : up = first [ i : i_right ] max_low_length = self . max_low_lengths_by_up . get ( up , - 1 ) if max_low_length == - 1 : continue up_costs = self . operation_costs [ up ] for j in range ( n + 1 ) : if costs [ i ] [ j ] > threshold : continue if len ( backtraces [ i ] [ j ] ) == 0 and i + j > 0 : continue for j_right in range ( ( j if i_right > i else j + 1 ) , min ( j + max_low_length , n ) + 1 ) : low = second [ j : j_right ] curr_cost = up_costs . get ( low , np . inf ) old_cost = costs [ i_right ] [ j_right ] new_cost = costs [ i ] [ j ] + curr_cost if new_cost > threshold : continue if add_pred ( new_cost , old_cost ) : if clear_pred ( new_cost , old_cost ) : backtraces [ i_right ] [ j_right ] = [ ] costs [ i_right ] [ j_right ] = update_func ( new_cost , old_cost ) backtraces [ i_right ] [ j_right ] . append ( ( i , j ) ) return costs , backtraces",    costs   costs [ i ] [ j ] ---     first [ : i ]  second [ : j ]
"def _make_reversed_operation_costs ( self ) : _reversed_operation_costs = dict ( ) for up , costs in self . operation_costs . items ( ) : for low , cost in costs . items ( ) : if low not in _reversed_operation_costs : _reversed_operation_costs [ low ] = dict ( ) _reversed_operation_costs [ low ] [ up ] = cost self . _reversed_operation_costs = _reversed_operation_costs",  _reversed_operation_costs     operation_costs
"def _make_maximal_key_lengths ( self ) : self . max_up_length = ( max ( len ( up ) for up in self . operation_costs ) if len ( self . operation_costs ) > 0 else - 1 ) self . max_low_length = ( max ( len ( low ) for low in self . _reversed_operation_costs ) if len ( self . _reversed_operation_costs ) > 0 else - 1 ) self . max_low_lengths_by_up , self . max_up_lengths_by_low = dict ( ) , dict ( ) for up , costs in self . operation_costs . items ( ) : self . max_low_lengths_by_up [ up ] = max ( len ( low ) for low in costs ) if len ( costs ) > 0 else - 1 for low , costs in self . _reversed_operation_costs . items ( ) : self . max_up_lengths_by_low [ low ] = max ( len ( up ) for up in costs ) if len ( costs ) > 0 else - 1",    low    ( up low )   up     up    ( up low )   low
"def _backtraces_to_transductions ( self , first , second , backtraces , threshold , return_cost = False ) : m , n = len ( first ) , len ( second ) agenda = [ None ] * ( m + 1 ) for i in range ( m + 1 ) : agenda [ i ] = [ [ ] for j in range ( n + 1 ) ] agenda [ m ] [ n ] = [ ( ( ) , 0.0 ) ] for i_right in range ( m , - 1 , - 1 ) : for j_right in range ( n , - 1 , - 1 ) : current_agenda = agenda [ i_right ] [ j_right ] if len ( current_agenda ) == 0 : continue for ( i , j ) in backtraces [ i_right ] [ j_right ] : up , low = first [ i : i_right ] , second [ j : j_right ] add_cost = self . operation_costs [ up ] [ low ] for elem , cost in current_agenda : new_cost = cost + add_cost if new_cost <= threshold : agenda [ i ] [ j ] . append ( ( ( ( up , low ) , ) + elem , new_cost ) ) if return_cost : return agenda [ 0 ] [ 0 ] else : return [ elem [ 0 ] for elem in agenda [ 0 ] [ 0 ] ]",     
"def _perform_insertions ( self , initial , max_cost ) : queue = list ( initial ) final = initial while len ( queue ) > 0 : transduction , cost = queue [ 0 ] queue = queue [ 1 : ] for string , string_cost in self . operation_costs [ """" ] . items ( ) : new_cost = cost + string_cost if new_cost <= max_cost : new_transduction = transduction + ( """" , string ) final . append ( ( new_transduction , new_cost ) ) queue . append ( ( new_transduction , new_cost ) ) return final",    < = max_cost      initial
"def _make_default_operation_costs ( self , allow_spaces = False ) : self . operation_costs = dict ( ) self . operation_costs [ """" ] = { c : 1.0 for c in list ( self . alphabet ) + [ ' ' ] } for a in self . alphabet : current_costs = { c : 1.0 for c in self . alphabet } current_costs [ a ] = 0.0 current_costs [ """" ] = 1.0 if allow_spaces : current_costs [ "" "" ] = 1.0 self . operation_costs [ a ] = current_costs for a , b in itertools . permutations ( self . alphabet , 2 ) : self . operation_costs [ a + b ] = { b + a : 1.0 } if allow_spaces : self . operation_costs [ "" "" ] = { c : 1.0 for c in self . alphabet } self . operation_costs [ "" "" ] [ """" ] = 1.0",sets 1 . 0 cost for every replacement insertion deletion and transposition
"def _start_timer ( self ) -> None : self . timer = Timer ( self . config [ 'conversation_lifetime' ] , self . self_destruct_callback ) self . timer . start ( )",Initiates self - destruct timer .
"def handle_request ( self , request : dict ) -> dict : request_type = request [ 'request' ] [ 'type' ] request_id = request [ 'request' ] [ 'requestId' ] log . debug ( f'Received request. Type: {request_type}, id: {request_id}' ) if request_type in self . handled_requests . keys ( ) : response : dict = self . handled_requests [ request_type ] ( request ) else : response : dict = self . handled_requests [ '_unsupported' ] ( request ) log . warning ( f'Unsupported request type: {request_type}, request id: {request_id}' ) self . _rearm_self_destruct ( ) return response",Routes Alexa requests to appropriate handlers .
"def _act ( self , utterance : str ) -> list : if self . stateful : utterance = [ [ utterance ] , [ self . key ] ] else : utterance = [ [ utterance ] ] agent_response : list = self . agent ( * utterance ) return agent_response",Infers DeepPavlov agent with raw user input extracted from Alexa request .
"def _generate_response ( self , response : dict , request : dict ) -> dict : response_template = deepcopy ( self . response_template ) response_template [ 'sessionAttributes' ] [ 'sessionId' ] = request [ 'session' ] [ 'sessionId' ] for key , value in response_template . items ( ) : if key not in response . keys ( ) : response [ key ] = value return response",Populates generated response with additional data conforming Alexa response specification .
"def _handle_intent ( self , request : dict ) -> dict : intent_name = self . config [ 'intent_name' ] slot_name = self . config [ 'slot_name' ] request_id = request [ 'request' ] [ 'requestId' ] request_intent : dict = request [ 'request' ] [ 'intent' ] if intent_name != request_intent [ 'name' ] : log . error ( f""Wrong intent name received: {request_intent['name']} in request {request_id}"" ) return { 'error' : 'wrong intent name' } if slot_name not in request_intent [ 'slots' ] . keys ( ) : log . error ( f'No slot named {slot_name} found in request {request_id}' ) return { 'error' : 'no slot found' } utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] agent_response = self . _act ( utterance ) if not agent_response : log . error ( f'Some error during response generation for request {request_id}' ) return { 'error' : 'error during response generation' } prediction : RichMessage = agent_response [ 0 ] prediction : list = prediction . alexa ( ) if not prediction : log . error ( f'Some error during response generation for request {request_id}' ) return { 'error' : 'error during response generation' } response = self . _generate_response ( prediction [ 0 ] , request ) return response",Handles IntentRequest Alexa request .
"def _handle_launch ( self , request : dict ) -> dict : response = { 'response' : { 'shouldEndSession' : False , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . config [ 'start_message' ] } , 'card' : { 'type' : 'Simple' , 'content' : self . config [ 'start_message' ] } } } response = self . _generate_response ( response , request ) return response",Handles LaunchRequest Alexa request .
"def _handle_unsupported ( self , request : dict ) -> dict : response = { 'response' : { 'shouldEndSession' : False , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . config [ 'unsupported_message' ] } , 'card' : { 'type' : 'Simple' , 'content' : self . config [ 'unsupported_message' ] } } } response = self . _generate_response ( response , request ) return response",Handles all unsupported types of Alexa requests . Returns standard message .
"def _repr_pretty_ ( self , p , cycle ) : if cycle : p . text ( 'Struct(...)' ) else : with p . group ( 7 , 'Struct(' , ')' ) : p . pretty ( self . _asdict ( ) )",method that defines Struct s pretty printing rules for iPython
def elmo_loss2ppl ( losses : List [ np . ndarray ] ) -> float : avg_loss = np . mean ( losses ) return float ( np . exp ( avg_loss ) ),Calculates perplexity by loss
"def _build_loss ( self , lstm_outputs ) : batch_size = self . options [ 'batch_size' ] unroll_steps = self . options [ 'unroll_steps' ] n_tokens_vocab = self . options [ 'n_tokens_vocab' ] def _get_next_token_placeholders ( suffix ) : name = 'next_token_id' + suffix id_placeholder = tf . placeholder ( DTYPE_INT , shape = ( batch_size , unroll_steps ) , name = name ) return id_placeholder self . next_token_id = _get_next_token_placeholders ( '' ) if self . bidirectional : self . next_token_id_reverse = _get_next_token_placeholders ( '_reverse' ) softmax_dim = self . options [ 'lstm' ] [ 'projection_dim' ] if self . share_embedding_softmax : self . softmax_W = self . embedding_weights with tf . variable_scope ( 'softmax' ) , tf . device ( '/cpu:0' ) : softmax_init = tf . random_normal_initializer ( 0.0 , 1.0 / np . sqrt ( softmax_dim ) ) if not self . share_embedding_softmax : self . softmax_W = tf . get_variable ( 'W' , [ n_tokens_vocab , softmax_dim ] , dtype = DTYPE , initializer = softmax_init ) self . softmax_b = tf . get_variable ( 'b' , [ n_tokens_vocab ] , dtype = DTYPE , initializer = tf . constant_initializer ( 0.0 ) ) self . individual_train_losses = [ ] self . individual_eval_losses = [ ] if self . bidirectional : next_ids = [ self . next_token_id , self . next_token_id_reverse ] else : next_ids = [ self . next_token_id ] for id_placeholder , lstm_output_flat in zip ( next_ids , lstm_outputs ) : next_token_id_flat = tf . reshape ( id_placeholder , [ - 1 , 1 ] ) with tf . control_dependencies ( [ lstm_output_flat ] ) : sampled_losses = tf . nn . sampled_softmax_loss ( self . softmax_W , self . softmax_b , next_token_id_flat , lstm_output_flat , self . options [ 'n_negative_samples_batch' ] , self . options [ 'n_tokens_vocab' ] , num_true = 1 ) output_scores = tf . matmul ( lstm_output_flat , tf . transpose ( self . softmax_W ) ) + self . softmax_b losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = output_scores , labels = tf . squeeze ( next_token_id_flat , squeeze_dims = [ 1 ] ) ) sampled_losses = tf . reshape ( sampled_losses , [ self . options [ 'batch_size' ] , - 1 ] ) losses = tf . reshape ( losses , [ self . options [ 'batch_size' ] , - 1 ] ) self . individual_train_losses . append ( tf . reduce_mean ( sampled_losses , axis = 1 ) ) self . individual_eval_losses . append ( tf . reduce_mean ( losses , axis = 1 ) ) if self . bidirectional : self . total_train_loss = 0.5 * ( self . individual_train_losses [ 0 ] + self . individual_train_losses [ 1 ] ) self . total_eval_loss = 0.5 * ( self . individual_eval_losses [ 0 ] + self . individual_eval_losses [ 1 ] ) else : self . total_train_loss = self . individual_train_losses [ 0 ] self . total_eval_loss = self . individual_eval_losses [ 0 ]",Create : self . total_loss : total loss op for training self . softmax_W softmax_b : the softmax variables self . next_token_id / _reverse : placeholders for gold input
"def build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , load_trained : bool = False , download : bool = False , serialized : Optional [ bytes ] = None ) -> Chainer : config = parse_config ( config ) if serialized : serialized : list = pickle . loads ( serialized ) if download : deep_download ( config ) import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) model_config = config [ 'chainer' ] model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) for component_config in model_config [ 'pipe' ] : if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : try : component_config [ 'load_path' ] = component_config [ 'save_path' ] except KeyError : log . warning ( 'No ""save_path"" parameter for the {} component, so ""load_path"" will not be renewed' . format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) if serialized and 'in' in component_config : component_serialized = serialized . pop ( 0 ) else : component_serialized = None component = from_params ( component_config , mode = mode , serialized = component_serialized ) if 'in' in component_config : c_in = component_config [ 'in' ] c_out = component_config [ 'out' ] in_y = component_config . get ( 'in_y' , None ) main = component_config . get ( 'main' , False ) model . append ( component , c_in , c_out , in_y , main ) return model",Build and return the model described in corresponding configuration file .
"def interact_model ( config : Union [ str , Path , dict ] ) -> None : model = build_model ( config ) while True : args = [ ] for in_x in model . in_x : args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : return pred = model ( * args ) if len ( model . out_params ) > 1 : pred = zip ( * pred ) print ( '>>' , * pred )",Start interaction with the model described in corresponding configuration file .
"def predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : if file_path is None or file_path == '-' : if sys . stdin . isatty ( ) : raise RuntimeError ( 'To process data from terminal please use interact mode' ) f = sys . stdin else : f = open ( file_path , encoding = 'utf8' ) model : Chainer = build_model ( config ) args_count = len ( model . in_x ) while True : batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) if not batch : break args = [ ] for i in range ( args_count ) : args . append ( batch [ i : : args_count ] ) res = model ( * args ) if len ( model . out_params ) == 1 : res = [ res ] for res in zip ( * res ) : res = json . dumps ( res , ensure_ascii = False ) print ( res , flush = True ) if f is not sys . stdin : f . close ( )",Make a prediction with the component described in corresponding configuration file .
"def read_infile ( infile : Union [ Path , str ] , from_words = False , word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , tag_column : int = TAG_COLUMN , max_sents : int = - 1 , read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] if from_words : word_column , read_only_words = 0 , True with open ( infile , ""r"" , encoding = ""utf8"" ) as fin : for line in fin : line = line . strip ( ) if line . startswith ( ""#"" ) : continue if line == """" : if len ( curr_word_sent ) > 0 : if read_only_words : curr_tag_sent = None answer . append ( ( curr_word_sent , curr_tag_sent ) ) curr_tag_sent , curr_word_sent = [ ] , [ ] if len ( answer ) == max_sents : break continue splitted = line . split ( ""\t"" ) index = splitted [ 0 ] if not from_words and not index . isdigit ( ) : continue curr_word_sent . append ( splitted [ word_column ] ) if not read_only_words : pos , tag = splitted [ pos_column ] , splitted [ tag_column ] tag = pos if tag == ""_"" else ""{},{}"" . format ( pos , tag ) curr_tag_sent . append ( tag ) if len ( curr_word_sent ) > 0 : if read_only_words : curr_tag_sent = None answer . append ( ( curr_word_sent , curr_tag_sent ) ) return answer",Reads input file in CONLL - U format
"def preprocess_data ( data : List [ Tuple [ List [ str ] , List [ str ] ] ] , to_lower : bool = True , append_case : str = ""first"" ) -> List [ Tuple [ List [ Tuple [ str ] ] , List [ str ] ] ] : new_data = [ ] for words , tags in data : new_words = [ process_word ( word , to_lower = to_lower , append_case = append_case ) for word in words ] new_tags = tags new_data . append ( ( new_words , new_tags ) ) return new_data",Processes all words in data using : func : ~deeppavlov . dataset_iterators . morphotagger_iterator . process_word .
"def fn_from_str ( name : str ) -> Callable [ ... , Any ] : try : module_name , fn_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected function description in a `module.submodules:function_name` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , fn_name )",Returns a function object with the name given in string .
"def register_metric ( metric_name : str ) -> Callable [ ... , Any ] : def decorate ( fn ) : fn_name = fn . __module__ + ':' + fn . __name__ if metric_name in _REGISTRY and _REGISTRY [ metric_name ] != fn_name : log . warning ( '""{}"" is already registered as a metric name, the old function will be ignored' . format ( metric_name ) ) _REGISTRY [ metric_name ] = fn_name return fn return decorate",Decorator for metric registration .
"def get_metric_by_name ( name : str ) -> Callable [ ... , Any ] : if name not in _REGISTRY : raise ConfigError ( f'""{name}"" is not registered as a metric' ) return fn_from_str ( _REGISTRY [ name ] )",Returns a metric callable with a corresponding name .
"def from_str ( cls , label : str ) -> int : label_norm = label . replace ( '1' , 'one' ) . upper ( ) if label_norm in cls . __members__ : return DecayType [ label_norm ] else : raise NotImplementedError",Convert given string label of decay type to special index
"def fit ( self , * args ) : data = list ( zip ( * args ) ) self . save ( ) if self . _fit_batch_size is None : raise ConfigError ( ""in order to use fit() method"" "" set `fit_batch_size` parameter"" ) bs = int ( self . _fit_batch_size ) data_len = len ( data ) num_batches = self . _fit_max_batches or ( ( data_len - 1 ) // bs + 1 ) avg_loss = 0. best_loss = float ( 'inf' ) lrs , losses = [ ] , [ ] _lr_find_schedule = DecayScheduler ( start_val = self . _fit_learning_rate [ 0 ] , end_val = self . _fit_learning_rate [ 1 ] , dec_type = ""exponential"" , num_it = num_batches ) self . _lr = _lr_find_schedule . start_val self . _mom = 0. self . _update_graph_variables ( learning_rate = self . _lr , momentum = self . _mom ) best_lr = _lr_find_schedule . start_val for i in range ( num_batches ) : batch_start = ( i * bs ) % data_len batch_end = batch_start + bs report = self . train_on_batch ( * zip ( * data [ batch_start : batch_end ] ) ) if not isinstance ( report , dict ) : report = { 'loss' : report } avg_loss = self . _fit_beta * avg_loss + ( 1 - self . _fit_beta ) * report [ 'loss' ] smoothed_loss = avg_loss / ( 1 - self . _fit_beta ** ( i + 1 ) ) lrs . append ( self . _lr ) losses . append ( smoothed_loss ) log . info ( f""Batch {i}/{num_batches}: smooth_loss = {smoothed_loss}"" f"", lr = {self._lr}, best_lr = {best_lr}"" ) if math . isnan ( smoothed_loss ) or ( smoothed_loss > 4 * best_loss ) : break if ( smoothed_loss < best_loss ) and ( i >= self . _fit_min_batches ) : best_loss = smoothed_loss best_lr = self . _lr self . _lr = _lr_find_schedule . next_val ( ) self . _update_graph_variables ( learning_rate = self . _lr ) if i >= num_batches : break end_val = self . _get_best ( lrs , losses ) start_val = end_val if self . _lr_schedule . dec_type in ( DecayType . ONECYCLE , DecayType . TRAPEZOID ) : start_val = end_val / self . _fit_learning_rate_div elif self . _lr_schedule . dec_type in ( DecayType . POLYNOMIAL , DecayType . EXPONENTIAL , DecayType . LINEAR , DecayType . COSINE ) : start_val = end_val end_val = end_val / self . _fit_learning_rate_div self . _lr_schedule = DecayScheduler ( start_val = start_val , end_val = end_val , num_it = self . _lr_schedule . nb , dec_type = self . _lr_schedule . dec_type , extra = self . _lr_schedule . extra ) log . info ( f""Found best learning rate value = {best_lr}"" f"", setting new learning rate schedule with {self._lr_schedule}."" ) self . load ( ) self . _lr = self . _lr_schedule . start_val self . _mom = self . _mom_schedule . start_val self . _update_graph_variables ( learning_rate = self . _lr , momentum = self . _mom ) return { 'smoothed_loss' : losses , 'learning_rate' : lrs }",Find the best learning rate schedule and set obtained values of learning rate and momentum for further model training . Best learning rate will be divided by fit_learning_rate_div for further training model .
"def _get_best ( values : List [ float ] , losses : List [ float ] , max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : assert len ( values ) == len ( losses ) , ""lengths of values and losses should be equal"" min_ind = np . argmin ( losses ) for i in range ( min_ind - 1 , 0 , - 1 ) : if ( losses [ i ] * max_loss_div > losses [ min_ind ] ) or ( values [ i ] * min_val_div < values [ min_ind ] ) : return values [ i + 1 ] return values [ min_ind ] / min_val_div",Find the best value according to given losses
"def process_event ( self , event_name : str , data : dict ) -> None : if event_name == ""after_validation"" : if data [ 'impatience' ] > self . _learning_rate_last_impatience : self . _learning_rate_cur_impatience += 1 else : self . _learning_rate_cur_impatience = 0 self . _learning_rate_last_impatience = data [ 'impatience' ] if ( self . _learning_rate_drop_patience is not None ) and ( self . _learning_rate_cur_impatience >= self . _learning_rate_drop_patience ) : self . _learning_rate_cur_impatience = 0 self . _learning_rate_cur_div *= self . _learning_rate_drop_div self . _lr /= self . _learning_rate_drop_div self . _update_graph_variables ( learning_rate = self . _lr ) log . info ( f""New learning rate dividor = {self._learning_rate_cur_div}"" ) if event_name == 'after_batch' : if ( self . _lr is not None ) and self . _lr_update_on_batch : self . _lr = self . _lr_schedule . next_val ( ) / self . _learning_rate_cur_div self . _update_graph_variables ( learning_rate = self . _lr ) if ( self . _mom is not None ) and self . _mom_update_on_batch : self . _mom = min ( 1. , max ( 0. , self . _mom_schedule . next_val ( ) ) ) self . _update_graph_variables ( momentum = self . _mom ) if event_name == 'after_epoch' : if ( self . _lr is not None ) and not self . _lr_update_on_batch : self . _lr = self . _lr_schedule . next_val ( ) / self . _learning_rate_cur_div self . _update_graph_variables ( learning_rate = self . _lr ) if ( self . _mom is not None ) and not self . _mom_update_on_batch : self . _mom = min ( 1. , max ( 0. , self . _mom_schedule . next_val ( ) ) ) self . _update_graph_variables ( momentum = self . _mom ) if event_name == 'after_train_log' : if ( self . _lr is not None ) and ( 'learning_rate' not in data ) : data [ 'learning_rate' ] = self . _lr if ( self . _mom is not None ) and ( 'momentum' not in data ) : data [ 'momentum' ] = self . _mom",Update learning rate and momentum variables after event ( given by event_name )
"def _encode ( self , tokens : List [ str ] , mean : bool ) -> Union [ List [ np . ndarray ] , np . ndarray ] : embedded_tokens = [ ] for t in tokens : try : emb = self . tok2emb [ t ] except KeyError : try : emb = self . _get_word_vector ( t ) except KeyError : emb = np . zeros ( self . dim , dtype = np . float32 ) self . tok2emb [ t ] = emb embedded_tokens . append ( emb ) if mean is None : mean = self . mean if mean : filtered = [ et for et in embedded_tokens if np . any ( et ) ] if filtered : return np . mean ( filtered , axis = 0 ) return np . zeros ( self . dim , dtype = np . float32 ) return embedded_tokens",Embed one text sample
"def read_requirements ( ) : reqs_path = os . path . join ( __location__ , 'requirements.txt' ) with open ( reqs_path , encoding = 'utf8' ) as f : reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] names = [ ] links = [ ] for req in reqs : if '://' in req : links . append ( req ) else : names . append ( req ) return { 'install_requires' : names , 'dependency_links' : links }",parses requirements from requirements . txt
"def detokenize ( tokens ) : text = ' ' . join ( tokens ) step0 = text . replace ( '. . .' , '...' ) step1 = step0 . replace ( ""`` "" , '""' ) . replace ( "" ''"" , '""' ) step2 = step1 . replace ( "" ( "" , "" ("" ) . replace ( "" ) "" , "") "" ) step3 = re . sub ( r' ([.,:;?!%]+)([ \'""`])' , r""\1\2"" , step2 ) step4 = re . sub ( r' ([.,:;?!%]+)$' , r""\1"" , step3 ) step5 = step4 . replace ( "" '"" , ""'"" ) . replace ( "" n't"" , ""n't"" ) . replace ( "" nt"" , ""nt"" ) . replace ( ""can not"" , ""cannot"" ) step6 = step5 . replace ( "" ` "" , "" '"" ) return step6 . strip ( )",Detokenizing a text undoes the tokenizing operation restores punctuation and spaces to the places that people expect them to be . Ideally detokenize ( tokenize ( text )) should be identical to text except for line breaks .
"def ngramize ( items : List [ str ] , ngram_range = ( 1 , 1 ) ) -> Generator [ List [ str ] , Any , None ] : ngrams = [ ] ranges = [ ( 0 , i ) for i in range ( ngram_range [ 0 ] , ngram_range [ 1 ] + 1 ) ] for r in ranges : ngrams += list ( zip ( * [ items [ j : ] for j in range ( * r ) ] ) ) formatted_ngrams = [ ' ' . join ( item ) for item in ngrams ] yield formatted_ngrams",Make ngrams from a list of tokens / lemmas : param items : list of tokens lemmas or other strings to form ngrams : param ngram_range : range for producing ngrams ex . for unigrams + bigrams should be set to ( 1 2 ) for bigrams only should be set to ( 2 2 ) : return : ngrams ( as strings ) generator
"def sk_log_loss ( y_true : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] , y_predicted : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] ) -> float : return log_loss ( y_true , y_predicted )",Calculates log loss .
"def make_module_spec ( options , weight_file ) : def module_fn ( ) : """"""Spec function for a token embedding module."""""" _bos_id = 256 _eos_id = 257 _bow_id = 258 _eow_id = 259 _pad_id = 260 _max_word_length = 50 _parallel_iterations = 10 _max_batch_size = 1024 id_dtype = tf . int32 id_nptype = np . int32 max_word_length = tf . constant ( _max_word_length , dtype = id_dtype , name = 'max_word_length' ) version = tf . constant ( 'from_dp_1' , dtype = tf . string , name = 'version' ) def _make_bos_eos ( c ) : r = np . zeros ( [ _max_word_length ] , dtype = id_nptype ) r [ : ] = _pad_id r [ 0 ] = _bow_id r [ 1 ] = c r [ 2 ] = _eow_id return tf . constant ( r , dtype = id_dtype ) bos_ids = _make_bos_eos ( _bos_id ) eos_ids = _make_bos_eos ( _eos_id ) def token2ids ( token ) : with tf . name_scope ( ""token2ids_preprocessor"" ) : char_ids = tf . decode_raw ( token , tf . uint8 , name = 'decode_raw2get_char_ids' ) char_ids = tf . cast ( char_ids , tf . int32 , name = 'cast2int_token' ) char_ids = tf . strided_slice ( char_ids , [ 0 ] , [ max_word_length - 2 ] , [ 1 ] , name = 'slice2resized_token' ) ids_num = tf . shape ( char_ids ) [ 0 ] fill_ids_num = ( _max_word_length - 2 ) - ids_num pads = tf . fill ( [ fill_ids_num ] , _pad_id ) bow_token_eow_pads = tf . concat ( [ [ _bow_id ] , char_ids , [ _eow_id ] , pads ] , 0 , name = 'concat2bow_token_eow_pads' ) return bow_token_eow_pads def sentence_tagging_and_padding ( sen_dim ) : with tf . name_scope ( ""sentence_tagging_and_padding_preprocessor"" ) : sen = sen_dim [ 0 ] dim = sen_dim [ 1 ] extra_dim = tf . shape ( sen ) [ 0 ] - dim sen = tf . slice ( sen , [ 0 , 0 ] , [ dim , max_word_length ] , name = 'slice2sen' ) bos_sen_eos = tf . concat ( [ [ bos_ids ] , sen , [ eos_ids ] ] , 0 , name = 'concat2bos_sen_eos' ) bos_sen_eos_plus_one = bos_sen_eos + 1 bos_sen_eos_pads = tf . pad ( bos_sen_eos_plus_one , [ [ 0 , extra_dim ] , [ 0 , 0 ] ] , ""CONSTANT"" , name = 'pad2bos_sen_eos_pads' ) return bos_sen_eos_pads tokens = tf . placeholder ( shape = ( None , None ) , dtype = tf . string , name = 'ph2tokens' ) sequence_len = tf . placeholder ( shape = ( None , ) , dtype = tf . int32 , name = 'ph2sequence_len' ) tok_shape = tf . shape ( tokens ) line_tokens = tf . reshape ( tokens , shape = [ - 1 ] , name = 'reshape2line_tokens' ) with tf . device ( '/cpu:0' ) : tok_ids = tf . map_fn ( token2ids , line_tokens , dtype = tf . int32 , back_prop = False , parallel_iterations = _parallel_iterations , name = 'map_fn2get_tok_ids' ) tok_ids = tf . reshape ( tok_ids , [ tok_shape [ 0 ] , tok_shape [ 1 ] , - 1 ] , name = 'reshape2tok_ids' ) with tf . device ( '/cpu:0' ) : sen_ids = tf . map_fn ( sentence_tagging_and_padding , ( tok_ids , sequence_len ) , dtype = tf . int32 , back_prop = False , parallel_iterations = _parallel_iterations , name = 'map_fn2get_sen_ids' ) bilm = BidirectionalLanguageModel ( options , str ( weight_file ) , max_batch_size = _max_batch_size ) embeddings_op = bilm ( sen_ids ) elmo_output = weight_layers ( 'elmo_output' , embeddings_op , l2_coef = 0.0 ) weighted_op = elmo_output [ 'weighted_op' ] mean_op = elmo_output [ 'mean_op' ] word_emb = elmo_output [ 'word_emb' ] lstm_outputs1 = elmo_output [ 'lstm_outputs1' ] lstm_outputs2 = elmo_output [ 'lstm_outputs2' ] hub . add_signature ( ""tokens"" , { ""tokens"" : tokens , ""sequence_len"" : sequence_len } , { ""elmo"" : weighted_op , ""default"" : mean_op , ""word_emb"" : word_emb , ""lstm_outputs1"" : lstm_outputs1 , ""lstm_outputs2"" : lstm_outputs2 , ""version"" : version } ) def_strings = tf . placeholder ( shape = ( None ) , dtype = tf . string ) def_tokens_sparse = tf . string_split ( def_strings ) def_tokens_dense = tf . sparse_to_dense ( sparse_indices = def_tokens_sparse . indices , output_shape = def_tokens_sparse . dense_shape , sparse_values = def_tokens_sparse . values , default_value = '' ) def_mask = tf . not_equal ( def_tokens_dense , '' ) def_int_mask = tf . cast ( def_mask , dtype = tf . int32 ) def_sequence_len = tf . reduce_sum ( def_int_mask , axis = - 1 ) def_tok_shape = tf . shape ( def_tokens_dense ) def_line_tokens = tf . reshape ( def_tokens_dense , shape = [ - 1 ] , name = 'reshape2line_tokens' ) with tf . device ( '/cpu:0' ) : def_tok_ids = tf . map_fn ( token2ids , def_line_tokens , dtype = tf . int32 , back_prop = False , parallel_iterations = _parallel_iterations , name = 'map_fn2get_tok_ids' ) def_tok_ids = tf . reshape ( def_tok_ids , [ def_tok_shape [ 0 ] , def_tok_shape [ 1 ] , - 1 ] , name = 'reshape2tok_ids' ) with tf . device ( '/cpu:0' ) : def_sen_ids = tf . map_fn ( sentence_tagging_and_padding , ( def_tok_ids , def_sequence_len ) , dtype = tf . int32 , back_prop = False , parallel_iterations = _parallel_iterations , name = 'map_fn2get_sen_ids' ) def_embeddings_op = bilm ( def_sen_ids ) def_elmo_output = weight_layers ( 'elmo_output' , def_embeddings_op , l2_coef = 0.0 , reuse = True ) def_weighted_op = def_elmo_output [ 'weighted_op' ] def_mean_op = def_elmo_output [ 'mean_op' ] def_word_emb = def_elmo_output [ 'word_emb' ] def_lstm_outputs1 = def_elmo_output [ 'lstm_outputs1' ] def_lstm_outputs2 = def_elmo_output [ 'lstm_outputs2' ] hub . add_signature ( ""default"" , { ""strings"" : def_strings } , { ""elmo"" : def_weighted_op , ""default"" : def_mean_op , ""word_emb"" : def_word_emb , ""lstm_outputs1"" : def_lstm_outputs1 , ""lstm_outputs2"" : def_lstm_outputs2 , ""version"" : version } ) return hub . create_module_spec ( module_fn )",Makes a module spec .
"def export2hub ( weight_file , hub_dir , options ) : spec = make_module_spec ( options , str ( weight_file ) ) try : with tf . Graph ( ) . as_default ( ) : module = hub . Module ( spec ) with tf . Session ( ) as sess : sess . run ( tf . global_variables_initializer ( ) ) if hub_dir . exists ( ) : shutil . rmtree ( hub_dir ) module . export ( str ( hub_dir ) , sess ) finally : pass",Exports a TF - Hub module
"def show_details ( item_data : Dict [ Any , Any ] ) -> str : txt = """" for key , value in item_data . items ( ) : txt += ""**"" + str ( key ) + ""**"" + ': ' + str ( value ) + ""  \n"" return txt",Format catalog item output
def make_agent ( ) -> EcommerceAgent : config_path = find_config ( 'tfidf_retrieve' ) skill = build_model ( config_path ) agent = EcommerceAgent ( skills = [ skill ] ) return agent,Make an agent
"def main ( ) : args = parser . parse_args ( ) run_ms_bot_framework_server ( agent_generator = make_agent , app_id = args . ms_id , app_secret = args . ms_secret , stateful = True )",Parse parameters and run ms bot framework
"def _call ( self , utterances_batch : List [ str ] , utterances_ids : List [ int ] = None ) -> List [ RichMessage ] : rich_message = RichMessage ( ) for utt_id , utt in enumerate ( utterances_batch ) : if utterances_ids : id_ = utterances_ids [ utt_id ] log . debug ( f'Utterance: {utt}' ) if utt == ""/start"" : welcome = ""I am a new e-commerce bot. I will help you to find products that you are looking for. Please type your request in plain text."" rich_message . add_control ( PlainText ( welcome ) ) continue if utt [ 0 ] == ""@"" : command , * parts = utt . split ( "":"" ) log . debug ( f'Actions: {parts}' ) if command == ""@details"" : batch_index = int ( parts [ 0 ] ) item_index = int ( parts [ 1 ] ) rich_message . add_control ( PlainText ( show_details ( self . history [ id_ ] [ batch_index ] [ item_index ] ) ) ) continue if command == ""@entropy"" : state = self . history [ id_ ] [ int ( parts [ 0 ] ) ] state [ parts [ 1 ] ] = parts [ 2 ] state [ ""start"" ] = 0 state [ ""stop"" ] = 5 utt = state [ 'query' ] self . states [ id_ ] = state if command == ""@next"" : state = self . history [ id_ ] [ int ( parts [ 0 ] ) ] state [ 'start' ] = state [ 'stop' ] state [ 'stop' ] = state [ 'stop' ] + 5 utt = state [ 'query' ] self . states [ id_ ] = state else : if id_ not in self . states : self . states [ id_ ] = { } self . states [ id_ ] [ ""start"" ] = 0 self . states [ id_ ] [ ""stop"" ] = 5 responses_batch , confidences_batch , state_batch = self . skills [ 0 ] ( [ utt ] , self . history [ id_ ] , [ self . states [ id_ ] ] ) self . states [ id_ ] = state_batch [ 0 ] self . states [ id_ ] [ ""query"" ] = utt items_batch , entropy_batch = responses_batch for batch_idx , items in enumerate ( items_batch ) : self . history [ id_ ] . append ( items ) self . history [ id_ ] . append ( self . states [ id_ ] ) for idx , item in enumerate ( items ) : rich_message . add_control ( _draw_item ( item , idx , self . history [ id_ ] ) ) if len ( items ) == self . states [ id_ ] [ 'stop' ] - self . states [ id_ ] [ 'start' ] : buttons_frame = _draw_tail ( entropy_batch [ batch_idx ] , self . history [ id_ ] ) rich_message . add_control ( buttons_frame ) return [ rich_message ]",Processes batch of utterances and returns corresponding responses batch .
"def TemporalDropout ( inputs , dropout = 0.0 ) : if dropout == 0.0 : return inputs inputs_func = lambda x : kb . ones_like ( inputs [ : , : , 0 : 1 ] ) inputs_mask = kl . Lambda ( inputs_func ) ( inputs ) inputs_mask = kl . Dropout ( dropout ) ( inputs_mask ) tiling_shape = [ 1 , 1 , kb . shape ( inputs ) [ 2 ] ] + [ 1 ] * ( kb . ndim ( inputs ) - 3 ) inputs_mask = kl . Lambda ( kb . tile , arguments = { ""n"" : tiling_shape } , output_shape = inputs . _keras_shape [ 1 : ] ) ( inputs_mask ) answer = kl . Multiply ( ) ( [ inputs , inputs_mask ] ) return answer",Drops with : dropout probability temporal steps of input 3D tensor
"def positions_func ( inputs , pad = 0 ) : position_inputs = kb . cumsum ( kb . ones_like ( inputs , dtype = ""float32"" ) , axis = 1 ) position_inputs *= kb . cast ( kb . not_equal ( inputs , pad ) , ""float32"" ) return kb . log ( 1.0 + position_inputs )",A layer filling i - th column of a 2D tensor with 1 + ln ( 1 + i ) when it contains a meaningful symbol and with 0 when it contains PAD
"def download ( dest_file_path : [ List [ Union [ str , Path ] ] ] , source_url : str , force_download = True ) : if isinstance ( dest_file_path , list ) : dest_file_paths = [ Path ( path ) for path in dest_file_path ] else : dest_file_paths = [ Path ( dest_file_path ) . absolute ( ) ] if not force_download : to_check = list ( dest_file_paths ) dest_file_paths = [ ] for p in to_check : if p . exists ( ) : log . info ( f'File already exists in {p}' ) else : dest_file_paths . append ( p ) if dest_file_paths : cache_dir = os . getenv ( 'DP_CACHE_DIR' ) cached_exists = False if cache_dir : first_dest_path = Path ( cache_dir ) / md5 ( source_url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] cached_exists = first_dest_path . exists ( ) else : first_dest_path = dest_file_paths . pop ( ) if not cached_exists : first_dest_path . parent . mkdir ( parents = True , exist_ok = True ) simple_download ( source_url , first_dest_path ) else : log . info ( f'Found cached {source_url} in {first_dest_path}' ) for dest_path in dest_file_paths : dest_path . parent . mkdir ( parents = True , exist_ok = True ) shutil . copy ( str ( first_dest_path ) , str ( dest_path ) )",Download a file from URL to one or several target locations
"def untar ( file_path , extract_folder = None ) : file_path = Path ( file_path ) if extract_folder is None : extract_folder = file_path . parent extract_folder = Path ( extract_folder ) tar = tarfile . open ( file_path ) tar . extractall ( extract_folder ) tar . close ( )",Simple tar archive extractor
"def ungzip ( file_path , extract_path : Path = None ) : CHUNK = 16 * 1024 file_path = Path ( file_path ) extract_path = extract_path or file_path . with_suffix ( '' ) with gzip . open ( file_path , 'rb' ) as fin , extract_path . open ( 'wb' ) as fout : while True : block = fin . read ( CHUNK ) if not block : break fout . write ( block )",Simple . gz archive extractor
"def download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : file_name = Path ( urlparse ( url ) . path ) . name download_path = Path ( download_path ) if extract_paths is None : extract_paths = [ download_path ] elif isinstance ( extract_paths , list ) : extract_paths = [ Path ( path ) for path in extract_paths ] else : extract_paths = [ Path ( extract_paths ) ] cache_dir = os . getenv ( 'DP_CACHE_DIR' ) extracted = False if cache_dir : cache_dir = Path ( cache_dir ) url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] arch_file_path = cache_dir / url_hash extracted_path = cache_dir / ( url_hash + '_extracted' ) extracted = extracted_path . exists ( ) if not extracted and not arch_file_path . exists ( ) : simple_download ( url , arch_file_path ) else : arch_file_path = download_path / file_name simple_download ( url , arch_file_path ) extracted_path = extract_paths . pop ( ) if not extracted : log . info ( 'Extracting {} archive into {}' . format ( arch_file_path , extracted_path ) ) extracted_path . mkdir ( parents = True , exist_ok = True ) if file_name . endswith ( '.tar.gz' ) : untar ( arch_file_path , extracted_path ) elif file_name . endswith ( '.gz' ) : ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) elif file_name . endswith ( '.zip' ) : with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : zip_ref . extractall ( extracted_path ) else : raise RuntimeError ( f'Trying to extract an unknown type of archive {file_name}' ) if not cache_dir : arch_file_path . unlink ( ) for extract_path in extract_paths : for src in extracted_path . iterdir ( ) : dest = extract_path / src . name if src . is_dir ( ) : copytree ( src , dest ) else : extract_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( str ( src ) , str ( dest ) )",Download and extract . tar . gz or . gz file to one or several target locations . The archive is deleted if extraction was successful .
"def update_dict_recursive ( editable_dict : dict , editing_dict : dict ) -> None : for k , v in editing_dict . items ( ) : if isinstance ( v , collections . Mapping ) : update_dict_recursive ( editable_dict . get ( k , { } ) , v ) else : editable_dict [ k ] = v",Updates dict recursively
"def path_set_md5 ( url ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) path += '.md5' return urlunsplit ( ( scheme , netloc , path , query_string , fragment ) )",Given a file URL return a md5 query of the file
"def set_query_parameter ( url , param_name , param_value ) : scheme , netloc , path , query_string , fragment = urlsplit ( url ) query_params = parse_qs ( query_string ) query_params [ param_name ] = [ param_value ] new_query_string = urlencode ( query_params , doseq = True ) return urlunsplit ( ( scheme , netloc , path , new_query_string , fragment ) )",Given a URL set or replace a query parameter and return the modified URL .
"def alexa ( self ) -> dict : response = { 'response' : { 'shouldEndSession' : False , 'outputSpeech' : { 'type' : 'PlainText' , 'text' : self . content } , 'card' : { 'type' : 'Simple' , 'content' : self . content } } } return response",Returns Amazon Alexa compatible state of the PlainText instance .
def json ( self ) -> dict : content = { } content [ 'name' ] = self . name content [ 'callback' ] = self . callback self . control_json [ 'content' ] = content return self . control_json,Returns json compatible state of the Button instance .
def ms_bot_framework ( self ) -> dict : card_action = { } card_action [ 'type' ] = 'postBack' card_action [ 'title' ] = self . name card_action [ 'value' ] = self . callback = self . callback return card_action,Returns MS Bot Framework compatible state of the Button instance .
def json ( self ) -> dict : content = { } if self . text : content [ 'text' ] = self . text content [ 'controls' ] = [ control . json ( ) for control in self . content ] self . control_json [ 'content' ] = content return self . control_json,Returns json compatible state of the ButtonsFrame instance .
"def ms_bot_framework ( self ) -> dict : rich_card = { } buttons = [ button . ms_bot_framework ( ) for button in self . content ] rich_card [ 'buttons' ] = buttons if self . text : rich_card [ 'title' ] = self . text attachments = [ { ""contentType"" : ""application/vnd.microsoft.card.thumbnail"" , ""content"" : rich_card } ] out_activity = { } out_activity [ 'type' ] = 'message' out_activity [ 'attachments' ] = attachments return out_activity",Returns MS Bot Framework compatible state of the ButtonsFrame instance .
"def squad_v2_exact_match ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : EM_total = sum ( normalize_answer ( prediction ) in map ( normalize_answer , ground_truth ) for ground_truth , prediction in zip ( y_true , y_predicted ) ) return 100 * EM_total / len ( y_true ) if len ( y_true ) > 0 else 0",Calculates Exact Match score between y_true and y_predicted EM score uses the best matching y_true answer : if y_pred equal at least to one answer in y_true then EM = 1 else EM = 0
"def squad_v1_exact_match ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : EM_total = 0 count = 0 for ground_truth , prediction in zip ( y_true , y_predicted ) : if len ( ground_truth [ 0 ] ) == 0 : continue count += 1 EMs = [ int ( normalize_answer ( gt ) == normalize_answer ( prediction ) ) for gt in ground_truth ] EM_total += max ( EMs ) return 100 * EM_total / count if count > 0 else 0",Calculates Exact Match score between y_true and y_predicted EM score uses the best matching y_true answer : if y_pred equal at least to one answer in y_true then EM = 1 else EM = 0 Skips examples without an answer . Args : y_true : list of correct answers ( correct answers are represented by list of strings ) y_predicted : list of predicted answers Returns : exact match score : float
"def squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : f1_total = 0.0 for ground_truth , prediction in zip ( y_true , y_predicted ) : prediction_tokens = normalize_answer ( prediction ) . split ( ) f1s = [ ] for gt in ground_truth : gt_tokens = normalize_answer ( gt ) . split ( ) if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : f1s . append ( float ( gt_tokens == prediction_tokens ) ) continue common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) num_same = sum ( common . values ( ) ) if num_same == 0 : f1s . append ( 0.0 ) continue precision = 1.0 * num_same / len ( prediction_tokens ) recall = 1.0 * num_same / len ( gt_tokens ) f1 = ( 2 * precision * recall ) / ( precision + recall ) f1s . append ( f1 ) f1_total += max ( f1s ) return 100 * f1_total / len ( y_true ) if len ( y_true ) > 0 else 0",Calculates F - 1 score between y_true and y_predicted F - 1 score uses the best matching y_true answer
"def recall_at_k ( y_true : List [ int ] , y_pred : List [ List [ np . ndarray ] ] , k : int ) : num_examples = float ( len ( y_pred ) ) predictions = np . array ( y_pred ) predictions = np . flip ( np . argsort ( predictions , - 1 ) , - 1 ) [ : , : k ] num_correct = 0 for el in predictions : if 0 in el : num_correct += 1 return float ( num_correct ) / num_examples",Calculates recall at k ranking metric .
"def check_gpu_existence ( ) : global _gpu_available if _gpu_available is None : sess_config = tf . ConfigProto ( ) sess_config . gpu_options . allow_growth = True try : with tf . Session ( config = sess_config ) : device_list = device_lib . list_local_devices ( ) _gpu_available = any ( device . device_type == 'GPU' for device in device_list ) except AttributeError as e : log . warning ( f'Got an AttributeError `{e}`, assuming documentation building' ) _gpu_available = False return _gpu_available",r Return True if at least one GPU is available
"def _parse_config_property ( item : _T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> _T : if isinstance ( item , str ) : return item . format ( * * variables ) elif isinstance ( item , list ) : return [ _parse_config_property ( item , variables ) for item in item ] elif isinstance ( item , dict ) : return { k : _parse_config_property ( v , variables ) for k , v in item . items ( ) } else : return item",Recursively apply config s variables values to its property
"def parse_config ( config : Union [ str , Path , dict ] ) -> dict : if isinstance ( config , ( str , Path ) ) : config = read_json ( find_config ( config ) ) variables = { 'DEEPPAVLOV_PATH' : os . getenv ( f'DP_DEEPPAVLOV_PATH' , Path ( __file__ ) . parent . parent . parent ) } for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : env_name = f'DP_{name}' if env_name in os . environ : value = os . getenv ( env_name ) variables [ name ] = value . format ( * * variables ) return _parse_config_property ( config , variables )",Read config s variables and apply their values to all its properties
"def expand_path ( path : Union [ str , Path ] ) -> Path : return Path ( path ) . expanduser ( ) . resolve ( )",Convert relative paths to absolute with resolving user directory .
"def from_params ( params : Dict , mode : str = 'infer' , serialized : Any = None , * * kwargs ) -> Component : config_params = { k : _resolve ( v ) for k , v in params . items ( ) } if 'ref' in config_params : try : component = _refs [ config_params [ 'ref' ] ] if serialized is not None : component . deserialize ( serialized ) return component except KeyError : e = ConfigError ( 'Component with id ""{id}"" was referenced but not initialized' . format ( id = config_params [ 'ref' ] ) ) log . exception ( e ) raise e elif 'config_path' in config_params : from deeppavlov . core . commands . infer import build_model refs = _refs . copy ( ) _refs . clear ( ) config = parse_config ( expand_path ( config_params [ 'config_path' ] ) ) model = build_model ( config , serialized = serialized ) _refs . clear ( ) _refs . update ( refs ) try : _refs [ config_params [ 'id' ] ] = model except KeyError : pass return model cls_name = config_params . pop ( 'class_name' , None ) if not cls_name : e = ConfigError ( 'Component config has no `class_name` nor `ref` fields' ) log . exception ( e ) raise e cls = get_model ( cls_name ) config_params = { k : _init_param ( v , mode ) for k , v in config_params . items ( ) } try : spec = inspect . getfullargspec ( cls ) if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : kwargs [ 'mode' ] = mode component = cls ( * * dict ( config_params , * * kwargs ) ) try : _refs [ config_params [ 'id' ] ] = component except KeyError : pass except Exception : log . exception ( ""Exception in {}"" . format ( cls ) ) raise if serialized is not None : component . deserialize ( serialized ) return component",Builds and returns the Component from corresponding dictionary of parameters .
def run ( self ) -> None : while True : request = self . input_queue . get ( ) response = self . _handle_request ( request ) self . output_queue . put ( response ),Thread run method implementation .
"def _del_conversation ( self , conversation_key : str ) -> None : if conversation_key in self . conversations . keys ( ) : del self . conversations [ conversation_key ] log . info ( f'Deleted conversation, key: {conversation_key}' )",Deletes Conversation instance .
"def _refresh_valid_certs ( self ) -> None : self . timer = Timer ( REFRESH_VALID_CERTS_PERIOD_SECS , self . _refresh_valid_certs ) self . timer . start ( ) expired_certificates = [ ] for valid_cert_url , valid_cert in self . valid_certificates . items ( ) : valid_cert : ValidatedCert = valid_cert cert_expiration_time : datetime = valid_cert . expiration_timestamp if datetime . utcnow ( ) > cert_expiration_time : expired_certificates . append ( valid_cert_url ) for expired_cert_url in expired_certificates : del self . valid_certificates [ expired_cert_url ] log . info ( f'Validation period of {expired_cert_url} certificate expired' )",Conducts cleanup of periodical certificates with expired validation .
"def _verify_request ( self , signature_chain_url : str , signature : str , request_body : bytes ) -> bool : if signature_chain_url not in self . valid_certificates . keys ( ) : amazon_cert : X509 = verify_cert ( signature_chain_url ) if amazon_cert : amazon_cert_lifetime : timedelta = self . config [ 'amazon_cert_lifetime' ] expiration_timestamp = datetime . utcnow ( ) + amazon_cert_lifetime validated_cert = ValidatedCert ( cert = amazon_cert , expiration_timestamp = expiration_timestamp ) self . valid_certificates [ signature_chain_url ] = validated_cert log . info ( f'Certificate {signature_chain_url} validated' ) else : log . error ( f'Certificate {signature_chain_url} validation failed' ) return False else : validated_cert : ValidatedCert = self . valid_certificates [ signature_chain_url ] amazon_cert : X509 = validated_cert . cert if verify_signature ( amazon_cert , signature , request_body ) : result = True else : log . error ( f'Failed signature verification for request: {request_body.decode(""utf-8"", ""replace"")}' ) result = False return result",Conducts series of Alexa request verifications against Amazon Alexa requirements .
"def _handle_request ( self , request : dict ) -> dict : request_body : bytes = request [ 'request_body' ] signature_chain_url : str = request [ 'signature_chain_url' ] signature : str = request [ 'signature' ] alexa_request : dict = request [ 'alexa_request' ] if not self . _verify_request ( signature_chain_url , signature , request_body ) : return { 'error' : 'failed certificate/signature check' } timestamp_str = alexa_request [ 'request' ] [ 'timestamp' ] timestamp_datetime = datetime . strptime ( timestamp_str , '%Y-%m-%dT%H:%M:%SZ' ) now = datetime . utcnow ( ) delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now if abs ( delta . seconds ) > REQUEST_TIMESTAMP_TOLERANCE_SECS : log . error ( f'Failed timestamp check for request: {request_body.decode(""utf-8"", ""replace"")}' ) return { 'error' : 'failed request timestamp check' } conversation_key = alexa_request [ 'session' ] [ 'user' ] [ 'userId' ] if conversation_key not in self . conversations . keys ( ) : if self . config [ 'multi_instance' ] : conv_agent = self . _init_agent ( ) log . info ( 'New conversation instance level agent initiated' ) else : conv_agent = self . agent self . conversations [ conversation_key ] = Conversation ( config = self . config , agent = conv_agent , conversation_key = conversation_key , self_destruct_callback = lambda : self . _del_conversation ( conversation_key ) ) log . info ( f'Created new conversation, key: {conversation_key}' ) conversation = self . conversations [ conversation_key ] response = conversation . handle_request ( alexa_request ) return response",Processes Alexa requests from skill server and returns responses to Alexa .
"def csoftmax_for_slice ( input ) : [ ten , u ] = input shape_t = ten . shape shape_u = u . shape ten -= tf . reduce_mean ( ten ) q = tf . exp ( ten ) active = tf . ones_like ( u , dtype = tf . int32 ) mass = tf . constant ( 0 , dtype = tf . float32 ) found = tf . constant ( True , dtype = tf . bool ) def loop ( q_ , mask , mass_ , found_ ) : q_list = tf . dynamic_partition ( q_ , mask , 2 ) condition_indices = tf . dynamic_partition ( tf . range ( tf . shape ( q_ ) [ 0 ] ) , mask , 2 ) p = q_list [ 1 ] * ( 1.0 - mass_ ) / tf . reduce_sum ( q_list [ 1 ] ) p_new = tf . dynamic_stitch ( condition_indices , [ q_list [ 0 ] , p ] ) less_mask = tf . cast ( tf . less ( u , p_new ) , tf . int32 ) condition_indices = tf . dynamic_partition ( tf . range ( tf . shape ( p_new ) [ 0 ] ) , less_mask , 2 ) split_p_new = tf . dynamic_partition ( p_new , less_mask , 2 ) split_u = tf . dynamic_partition ( u , less_mask , 2 ) alpha = tf . dynamic_stitch ( condition_indices , [ split_p_new [ 0 ] , split_u [ 1 ] ] ) mass_ += tf . reduce_sum ( split_u [ 1 ] ) mask = mask * ( tf . ones_like ( less_mask ) - less_mask ) found_ = tf . cond ( tf . equal ( tf . reduce_sum ( less_mask ) , 0 ) , lambda : False , lambda : True ) alpha = tf . reshape ( alpha , q_ . shape ) return alpha , mask , mass_ , found_ ( csoft , mask_ , _ , _ ) = tf . while_loop ( cond = lambda _0 , _1 , _2 , f : f , body = loop , loop_vars = ( q , active , mass , found ) ) return [ csoft , mask_ ]",It is a implementation of the constrained softmax ( csoftmax ) for slice . Based on the paper : https : // andre - martins . github . io / docs / emnlp2017_final . pdf Learning What s Easy : Fully Differentiable Neural Easy - First Taggers ( page 4 ) Args : input : A list of [ input tensor cumulative attention ] . Returns : output : A list of [ csoftmax results masks ]
"def csoftmax ( tensor , inv_cumulative_att ) : shape_ten = tensor . shape shape_cum = inv_cumulative_att . shape merge_tensor = [ tensor , inv_cumulative_att ] cs , _ = tf . map_fn ( csoftmax_for_slice , merge_tensor , dtype = [ tf . float32 , tf . float32 ] ) return cs",It is a implementation of the constrained softmax ( csoftmax ) . Based on the paper : https : // andre - martins . github . io / docs / emnlp2017_final . pdf Learning What s Easy : Fully Differentiable Neural Easy - First Taggers Args : tensor : A tensorflow tensor is score . This tensor have dimensionality [ None n_tokens ] inv_cumulative_att : A inverse cumulative attention tensor with dimensionality [ None n_tokens ] Returns : cs : Tensor at the output with dimensionality [ None n_tokens ]
"def attention_gen_step ( hidden_for_sketch , hidden_for_attn_alignment , sketch , key , cum_att ) : with tf . name_scope ( 'attention_step' ) : sketch_dims = hidden_for_sketch . get_shape ( ) . as_list ( ) batch_size = sketch_dims [ 0 ] num_tokens = sketch_dims [ 1 ] hidden_size = sketch_dims [ 2 ] attn_alignment_dims = hidden_for_attn_alignment . get_shape ( ) . as_list ( ) attn_alignment_hidden_size = attn_alignment_dims [ 2 ] repeated_sketch = tf . tile ( tf . reshape ( sketch , [ - 1 , 1 , hidden_size ] ) , ( 1 , num_tokens , 1 ) ) concat_mem = tf . concat ( [ hidden_for_sketch , repeated_sketch ] , - 1 ) concat_mem = tf . reshape ( concat_mem , [ - 1 , num_tokens , 2 * hidden_size ] ) reduce_mem = tf . layers . dense ( concat_mem , hidden_size ) projected_key = tf . layers . dense ( key , hidden_size ) t_key = tf . reshape ( projected_key , [ - 1 , hidden_size , 1 ] ) score = tf . reshape ( tf . matmul ( reduce_mem , t_key ) , [ - 1 , num_tokens ] ) inv_cum_att = tf . reshape ( tf . ones_like ( cum_att ) - cum_att , [ - 1 , num_tokens ] ) att = csoftmax ( score , inv_cum_att ) t_reduce_mem = tf . transpose ( reduce_mem , [ 0 , 2 , 1 ] ) t_hidden_for_attn_alignment = tf . transpose ( hidden_for_attn_alignment , [ 0 , 2 , 1 ] ) r_att = tf . reshape ( att , [ - 1 , num_tokens , 1 ] ) next_sketch = tf . squeeze ( tf . matmul ( t_reduce_mem , r_att ) , - 1 ) aligned_hidden_sketch = tf . squeeze ( tf . matmul ( t_hidden_for_attn_alignment , r_att ) , - 1 ) return next_sketch , att , aligned_hidden_sketch",It is a implementation one step of block of the Luong et al . attention mechanism with general score and the constrained softmax ( csoftmax ) . Based on the papers : https : // arxiv . org / abs / 1508 . 04025 Effective Approaches to Attention - based Neural Machine Translation https : // andre - martins . github . io / docs / emnlp2017_final . pdf Learning What s Easy : Fully Differentiable Neural Easy - First Taggers Args : hidden_for_sketch : A tensorflow tensor for a sketch computing . This tensor have dimensionality [ None max_num_tokens sketch_hidden_size ] hidden_for_attn_alignment : A tensorflow tensor is aligned for output during a performing . This tensor have dimensionality [ None max_num_tokens hidden_size_for_attn_alignment ] sketch : A previous step sketch tensor for a sketch computing . This tensor have dimensionality [ None sketch_hidden_size ] key : A tensorflow tensor with dimensionality [ None None key_size ] cum_att : A cumulative attention tensor with dimensionality [ None max_num_tokens ] Returns : next_sketch : Tensor of the current step sketch with dimensionality [ None sketch_hidden_size ] att : Tensor of the current step attention with dimensionality [ None max_num_tokens ] aligned_hidden_sketch : Tensor of aligned hidden state of current step with dimensionality [ None hidden_size_for_attn_alignment ]
"def attention_gen_block ( hidden_for_sketch , hidden_for_attn_alignment , key , attention_depth ) : with tf . name_scope ( 'attention_block' ) : sketch_dims = tf . shape ( hidden_for_sketch ) batch_size = sketch_dims [ 0 ] num_tokens = sketch_dims [ 1 ] hidden_size = sketch_dims [ 2 ] attn_alignment_dims = tf . shape ( hidden_for_attn_alignment ) attn_alignment_hidden_size = attn_alignment_dims [ 2 ] sketches = [ tf . zeros ( shape = [ batch_size , hidden_size ] , dtype = tf . float32 ) ] aligned_hiddens = [ ] cum_att = tf . zeros ( shape = [ batch_size , num_tokens ] ) for i in range ( attention_depth ) : sketch , cum_att_ , aligned_hidden = attention_gen_step ( hidden_for_sketch , hidden_for_attn_alignment , sketches [ - 1 ] , key , cum_att ) sketches . append ( sketch ) aligned_hiddens . append ( aligned_hidden ) cum_att += cum_att_ final_aligned_hiddens = tf . reshape ( tf . transpose ( tf . stack ( aligned_hiddens ) , [ 1 , 0 , 2 ] ) , [ 1 , attention_depth , attn_alignment_hidden_size ] ) return final_aligned_hiddens",It is a implementation of the Luong et al . attention mechanism with general score and the constrained softmax ( csoftmax ) . Based on the papers : https : // arxiv . org / abs / 1508 . 04025 Effective Approaches to Attention - based Neural Machine Translation https : // andre - martins . github . io / docs / emnlp2017_final . pdf Learning What s Easy : Fully Differentiable Neural Easy - First Taggers Args : hidden_for_sketch : A tensorflow tensor for a sketch computing . This tensor have dimensionality [ None max_num_tokens sketch_hidden_size ] hidden_for_attn_alignment : A tensorflow tensor is aligned for output during a performing . This tensor have dimensionality [ None max_num_tokens hidden_size_for_attn_alignment ] key : A tensorflow tensor with dimensionality [ None None key_size ] attention_depth : Number of usage csoftmax Returns : final_aligned_hiddens : Tensor at the output with dimensionality [ 1 attention_depth hidden_size_for_attn_alignment ]
"def cls_from_str ( name : str ) -> type : try : module_name , cls_name = name . split ( ':' ) except ValueError : raise ConfigError ( 'Expected class description in a `module.submodules:ClassName` form, but got `{}`' . format ( name ) ) return getattr ( importlib . import_module ( module_name ) , cls_name )",Returns a class object with the name given as a string .
"def register ( name : str = None ) -> type : def decorate ( model_cls : type , reg_name : str = None ) -> type : model_name = reg_name or short_name ( model_cls ) global _REGISTRY cls_name = model_cls . __module__ + ':' + model_cls . __name__ if model_name in _REGISTRY and _REGISTRY [ model_name ] != cls_name : logger . warning ( 'Registry name ""{}"" has been already registered and will be overwritten.' . format ( model_name ) ) _REGISTRY [ model_name ] = cls_name return model_cls return lambda model_cls_name : decorate ( model_cls_name , name )",Register classes that could be initialized from JSON configuration file . If name is not passed the class name is converted to snake - case .
"def get_model ( name : str ) -> type : if name not in _REGISTRY : if ':' not in name : raise ConfigError ( ""Model {} is not registered."" . format ( name ) ) return cls_from_str ( name ) return cls_from_str ( _REGISTRY [ name ] )",Returns a registered class object with the name given in the string .
"def general_attention ( key , context , hidden_size , projected_align = False ) : if hidden_size % 2 != 0 : raise ValueError ( ""hidden size must be dividable by two"" ) batch_size = tf . shape ( context ) [ 0 ] max_num_tokens , token_size = context . get_shape ( ) . as_list ( ) [ - 2 : ] r_context = tf . reshape ( context , shape = [ - 1 , max_num_tokens , token_size ] ) projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) ) r_projected_key = tf . reshape ( projected_key , shape = [ - 1 , hidden_size , 1 ] ) lstm_fw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) lstm_bw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) ( output_fw , output_bw ) , states = tf . nn . bidirectional_dynamic_rnn ( cell_fw = lstm_fw_cell , cell_bw = lstm_bw_cell , inputs = r_context , dtype = tf . float32 ) bilstm_output = tf . concat ( [ output_fw , output_bw ] , - 1 ) attn = tf . nn . softmax ( tf . matmul ( bilstm_output , r_projected_key ) , dim = 1 ) if projected_align : log . info ( ""Using projected attention alignment"" ) t_context = tf . transpose ( bilstm_output , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , hidden_size ] ) else : log . info ( ""Using without projected attention alignment"" ) t_context = tf . transpose ( r_context , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , token_size ] ) return output",It is a implementation of the Luong et al . attention mechanism with general score . Based on the paper : https : // arxiv . org / abs / 1508 . 04025 Effective Approaches to Attention - based Neural Machine Translation Args : key : A tensorflow tensor with dimensionality [ None None key_size ] context : A tensorflow tensor with dimensionality [ None None max_num_tokens token_size ] hidden_size : Number of units in hidden representation projected_align : Using bidirectional lstm for hidden representation of context . If true beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [ hidden_size ] . If false bidirectional lstm is not used . Returns : output : Tensor at the output with dimensionality [ None None hidden_size ]
"def light_general_attention ( key , context , hidden_size , projected_align = False ) : batch_size = tf . shape ( context ) [ 0 ] max_num_tokens , token_size = context . get_shape ( ) . as_list ( ) [ - 2 : ] r_context = tf . reshape ( context , shape = [ - 1 , max_num_tokens , token_size ] ) projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) ) r_projected_key = tf . reshape ( projected_key , shape = [ - 1 , hidden_size , 1 ] ) projected_context = tf . layers . dense ( r_context , hidden_size , kernel_initializer = xav ( ) ) attn = tf . nn . softmax ( tf . matmul ( projected_context , r_projected_key ) , dim = 1 ) if projected_align : log . info ( ""Using projected attention alignment"" ) t_context = tf . transpose ( projected_context , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , hidden_size ] ) else : log . info ( ""Using without projected attention alignment"" ) t_context = tf . transpose ( r_context , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , token_size ] ) return output",It is a implementation of the Luong et al . attention mechanism with general score . Based on the paper : https : // arxiv . org / abs / 1508 . 04025 Effective Approaches to Attention - based Neural Machine Translation Args : key : A tensorflow tensor with dimensionality [ None None key_size ] context : A tensorflow tensor with dimensionality [ None None max_num_tokens token_size ] hidden_size : Number of units in hidden representation projected_align : Using dense layer for hidden representation of context . If true between input and attention mechanism insert a dense layer with dimensionality [ hidden_size ] . If false a dense layer is not used . Returns : output : Tensor at the output with dimensionality [ None None hidden_size ]
"def light_bahdanau_attention ( key , context , hidden_size , projected_align = False ) : batch_size = tf . shape ( context ) [ 0 ] max_num_tokens , token_size = context . get_shape ( ) . as_list ( ) [ - 2 : ] r_context = tf . reshape ( context , shape = [ - 1 , max_num_tokens , token_size ] ) projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) ) r_projected_key = tf . tile ( tf . reshape ( projected_key , shape = [ - 1 , 1 , hidden_size ] ) , [ 1 , max_num_tokens , 1 ] ) projected_context = tf . layers . dense ( r_context , hidden_size , kernel_initializer = xav ( ) ) concat_h_state = tf . concat ( [ projected_context , r_projected_key ] , - 1 ) projected_state = tf . layers . dense ( concat_h_state , hidden_size , use_bias = False , kernel_initializer = xav ( ) ) score = tf . layers . dense ( tf . tanh ( projected_state ) , units = 1 , use_bias = False , kernel_initializer = xav ( ) ) attn = tf . nn . softmax ( score , dim = 1 ) if projected_align : log . info ( ""Using projected attention alignment"" ) t_context = tf . transpose ( projected_context , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , hidden_size ] ) else : log . info ( ""Using without projected attention alignment"" ) t_context = tf . transpose ( r_context , [ 0 , 2 , 1 ] ) output = tf . reshape ( tf . matmul ( t_context , attn ) , shape = [ batch_size , - 1 , token_size ] ) return output",It is a implementation of the Bahdanau et al . attention mechanism . Based on the paper : https : // arxiv . org / abs / 1409 . 0473 Neural Machine Translation by Jointly Learning to Align and Translate Args : key : A tensorflow tensor with dimensionality [ None None key_size ] context : A tensorflow tensor with dimensionality [ None None max_num_tokens token_size ] hidden_size : Number of units in hidden representation projected_align : Using dense layer for hidden representation of context . If true between input and attention mechanism insert a dense layer with dimensionality [ hidden_size ] . If false a dense layer is not used . Returns : output : Tensor at the output with dimensionality [ None None hidden_size ]
"def cs_bahdanau_attention ( key , context , hidden_size , depth , projected_align = False ) : if hidden_size % 2 != 0 : raise ValueError ( ""hidden size must be dividable by two"" ) batch_size = tf . shape ( context ) [ 0 ] max_num_tokens , token_size = context . get_shape ( ) . as_list ( ) [ - 2 : ] r_context = tf . reshape ( context , shape = [ - 1 , max_num_tokens , token_size ] ) projected_context = tf . layers . dense ( r_context , token_size , kernel_initializer = xav ( ) , name = 'projected_context' ) projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) , name = 'projected_key' ) r_projected_key = tf . tile ( tf . reshape ( projected_key , shape = [ - 1 , 1 , hidden_size ] ) , [ 1 , max_num_tokens , 1 ] ) lstm_fw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) lstm_bw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) ( output_fw , output_bw ) , states = tf . nn . bidirectional_dynamic_rnn ( cell_fw = lstm_fw_cell , cell_bw = lstm_bw_cell , inputs = projected_context , dtype = tf . float32 ) bilstm_output = tf . concat ( [ output_fw , output_bw ] , - 1 ) concat_h_state = tf . concat ( [ r_projected_key , output_fw , output_bw ] , - 1 ) if projected_align : log . info ( ""Using projected attention alignment"" ) h_state_for_attn_alignment = bilstm_output aligned_h_state = csoftmax_attention . attention_bah_block ( concat_h_state , h_state_for_attn_alignment , depth ) output = tf . reshape ( aligned_h_state , shape = [ batch_size , - 1 , depth * hidden_size ] ) else : log . info ( ""Using without projected attention alignment"" ) h_state_for_attn_alignment = projected_context aligned_h_state = csoftmax_attention . attention_bah_block ( concat_h_state , h_state_for_attn_alignment , depth ) output = tf . reshape ( aligned_h_state , shape = [ batch_size , - 1 , depth * token_size ] ) return output",It is a implementation of the Bahdanau et al . attention mechanism . Based on the papers : https : // arxiv . org / abs / 1409 . 0473 Neural Machine Translation by Jointly Learning to Align and Translate https : // andre - martins . github . io / docs / emnlp2017_final . pdf Learning What s Easy : Fully Differentiable Neural Easy - First Taggers Args : key : A tensorflow tensor with dimensionality [ None None key_size ] context : A tensorflow tensor with dimensionality [ None None max_num_tokens token_size ] hidden_size : Number of units in hidden representation depth : Number of csoftmax usages projected_align : Using bidirectional lstm for hidden representation of context . If true beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [ hidden_size ] . If false bidirectional lstm is not used . Returns : output : Tensor at the output with dimensionality [ None None depth * hidden_size ]
"def from_file ( file = str ) : from h2o import lazy_import , get_frame model_key = lazy_import ( file ) model_bytes_frame = get_frame ( model_key [ 0 ] ) model = H2OGenericEstimator ( model_key = model_bytes_frame ) model . train ( ) return model",Creates new Generic model by loading existing embedded model into library e . g . from H2O MOJO . The imported model must be supported by H2O . : param file : A string containing path to the file to create the model from : return : H2OGenericEstimator instance representing the generic model
"def getGLMRegularizationPath ( model ) : x = h2o . api ( ""GET /3/GetGLMRegPath"" , data = { ""model"" : model . _model_json [ ""model_id"" ] [ ""name"" ] } ) ns = x . pop ( ""coefficient_names"" ) res = { ""lambdas"" : x [ ""lambdas"" ] , ""explained_deviance_train"" : x [ ""explained_deviance_train"" ] , ""explained_deviance_valid"" : x [ ""explained_deviance_valid"" ] , ""coefficients"" : [ dict ( zip ( ns , y ) ) for y in x [ ""coefficients"" ] ] , } if ""coefficients_std"" in x : res [ ""coefficients_std"" ] = [ dict ( zip ( ns , y ) ) for y in x [ ""coefficients_std"" ] ] return res",Extract full regularization path explored during lambda search from glm model .
"def makeGLMModel ( model , coefs , threshold = .5 ) : model_json = h2o . api ( ""POST /3/MakeGLMModel"" , data = { ""model"" : model . _model_json [ ""model_id"" ] [ ""name"" ] , ""names"" : list ( coefs . keys ( ) ) , ""beta"" : list ( coefs . values ( ) ) , ""threshold"" : threshold } ) m = H2OGeneralizedLinearEstimator ( ) m . _resolve_model ( model_json [ ""model_id"" ] [ ""name"" ] , model_json ) return m",Create a custom GLM model using the given coefficients .
"def from_kvs ( keyvals ) : obj = H2OCluster ( ) obj . _retrieved_at = time . time ( ) for k , v in keyvals : if k in { ""__meta"" , ""_exclude_fields"" , ""__schema"" } : continue if k in _cloud_v3_valid_keys : obj . _props [ k ] = v else : raise AttributeError ( ""Attribute %s cannot be set on H2OCluster (= %r)"" % ( k , v ) ) return obj",Create H2OCluster object from a list of key - value pairs .
"def shutdown ( self , prompt = False ) : if not self . is_running ( ) : return assert_is_type ( prompt , bool ) if prompt : question = ""Are you sure you want to shutdown the H2O instance running at %s (Y/N)? "" % h2o . connection ( ) . base_url response = input ( question ) else : response = ""Y"" if response . lower ( ) in { ""y"" , ""yes"" } : h2o . api ( ""POST /3/Shutdown"" ) h2o . connection ( ) . close ( )",Shut down the server .
"def is_running ( self ) : try : if h2o . connection ( ) . local_server and not h2o . connection ( ) . local_server . is_running ( ) : return False h2o . api ( ""GET /"" ) return True except ( H2OConnectionError , H2OServerError ) : return False",Determine if the H2O cluster is running or not .
"def show_status ( self , detailed = False ) : if self . _retrieved_at + self . REFRESH_INTERVAL < time . time ( ) : new_info = h2o . api ( ""GET /3/Cloud"" ) self . _fill_from_h2ocluster ( new_info ) ncpus = sum ( node [ ""num_cpus"" ] for node in self . nodes ) allowed_cpus = sum ( node [ ""cpus_allowed"" ] for node in self . nodes ) free_mem = sum ( node [ ""free_mem"" ] for node in self . nodes ) unhealthy_nodes = sum ( not node [ ""healthy"" ] for node in self . nodes ) status = ""locked"" if self . locked else ""accepting new members"" if unhealthy_nodes == 0 : status += "", healthy"" else : status += "", %d nodes are not healthy"" % unhealthy_nodes api_extensions = self . list_api_extensions ( ) H2ODisplay ( [ [ ""H2O cluster uptime:"" , get_human_readable_time ( self . cloud_uptime_millis ) ] , [ ""H2O cluster timezone:"" , self . cloud_internal_timezone ] , [ ""H2O data parsing timezone:"" , self . datafile_parser_timezone ] , [ ""H2O cluster version:"" , self . version ] , [ ""H2O cluster version age:"" , ""{} {}"" . format ( self . build_age , ( ""!!!"" if self . build_too_old else """" ) ) ] , [ ""H2O cluster name:"" , self . cloud_name ] , [ ""H2O cluster total nodes:"" , self . cloud_size ] , [ ""H2O cluster free memory:"" , get_human_readable_bytes ( free_mem ) ] , [ ""H2O cluster total cores:"" , str ( ncpus ) ] , [ ""H2O cluster allowed cores:"" , str ( allowed_cpus ) ] , [ ""H2O cluster status:"" , status ] , [ ""H2O connection url:"" , h2o . connection ( ) . base_url ] , [ ""H2O connection proxy:"" , h2o . connection ( ) . proxy ] , [ ""H2O internal security:"" , self . internal_security_enabled ] , [ ""H2O API Extensions:"" , ', ' . join ( api_extensions ) ] , [ ""Python version:"" , ""%d.%d.%d %s"" % tuple ( sys . version_info [ : 4 ] ) ] , ] ) if detailed : keys = [ ""h2o"" , ""healthy"" , ""last_ping"" , ""num_cpus"" , ""sys_load"" , ""mem_value_size"" , ""free_mem"" , ""pojo_mem"" , ""swap_mem"" , ""free_disk"" , ""max_disk"" , ""pid"" , ""num_keys"" , ""tcps_active"" , ""open_fds"" , ""rpcs_active"" ] header = [ ""Nodes info:"" ] + [ ""Node %d"" % ( i + 1 ) for i in range ( len ( self . nodes ) ) ] table = [ [ k ] for k in keys ] for node in self . nodes : for i , k in enumerate ( keys ) : table [ i ] . append ( node [ k ] ) H2ODisplay ( table = table , header = header )",Print current cluster status information .
"def list_jobs ( self ) : res = h2o . api ( ""GET /3/Jobs"" ) table = [ [ ""type"" ] , [ ""dest"" ] , [ ""description"" ] , [ ""status"" ] ] for job in res [ ""jobs"" ] : job_dest = job [ ""dest"" ] table [ 0 ] . append ( self . _translate_job_type ( job_dest [ ""type"" ] ) ) table [ 1 ] . append ( job_dest [ ""name"" ] ) table [ 2 ] . append ( job [ ""description"" ] ) table [ 3 ] . append ( job [ ""status"" ] ) return table",List all jobs performed by the cluster .
"def list_timezones ( self ) : from h2o . expr import ExprNode return h2o . H2OFrame . _expr ( expr = ExprNode ( ""listTimeZones"" ) ) . _frame ( )",Return the list of all known timezones .
"def _fill_from_h2ocluster ( self , other ) : self . _props = other . _props self . _retrieved_at = other . _retrieved_at other . _props = { } other . _retrieved_at = None",Update information in this object from another H2OCluster instance .
"def metalearner_params ( self ) : if self . _parms . get ( ""metalearner_params"" ) != None : metalearner_params_dict = ast . literal_eval ( self . _parms . get ( ""metalearner_params"" ) ) for k in metalearner_params_dict : if len ( metalearner_params_dict [ k ] ) == 1 : metalearner_params_dict [ k ] = metalearner_params_dict [ k ] [ 0 ] return metalearner_params_dict else : return self . _parms . get ( ""metalearner_params"" )",Parameters for metalearner algorithm
"def check_obj_has_good_numbers ( obj , hierarchy = """" , curr_depth = 0 , max_depth = 4 , allowNaN = False ) : def serialize ( obj , hierarchy = """" , curr_depth = 0 ) : """"""Recursively walk object's hierarchy. Limit to max_depth"""""" if curr_depth > max_depth : return if isinstance ( obj , ( bool , int , long , float , basestring ) ) : try : number = float ( obj ) print ""Yay!"" , hierarchy , number except : if obj is None : print ""Not Yay! how come you're giving me None for a coefficient? %s %s"" % ( hierarchy , obj ) elif str ( obj ) == """" : print ""Not Yay! how come you're giving me an empty string for a coefficient? %s %s"" % ( hierarchy , obj ) else : raise Exception ( ""%s %s %s is not a valid float"" % ( hierarchy , obj , type ( obj ) ) ) number = 0.0 if not allowNaN and math . isnan ( number ) : raise Exception ( ""%s %s is a NaN"" % ( hierarchy , obj ) ) if not allowNaN and math . isinf ( number ) : raise Exception ( ""%s %s is a Inf"" % ( hierarchy , obj ) ) return number elif isinstance ( obj , dict ) : obj = obj . copy ( ) for key in obj : obj [ key ] = serialize ( obj [ key ] , hierarchy + "".%"" % key , curr_depth + 1 ) return obj elif isinstance ( obj , ( list , tuple ) ) : return [ serialize ( item , hierarchy + ""[%s]"" % i , curr_depth + 1 ) for ( i , item ) in enumerate ( obj ) ] elif hasattr ( obj , '__dict__' ) : return serialize ( obj . __dict__ , hierarchy , curr_depth + 1 ) else : return repr ( obj ) return ( serialize ( obj , hierarchy , curr_depth + 1 ) )",Represent instance of a class as JSON . Arguments : obj -- any object Return : String that represent JSON - encoded object .
"def stabilize ( self , test_func , error , timeoutSecs = 10 , retryDelaySecs = 0.5 ) : start = time . time ( ) numberOfRetries = 0 while h2o_args . no_timeout or ( time . time ( ) - start < timeoutSecs ) : if test_func ( self , tries = numberOfRetries , timeoutSecs = timeoutSecs ) : break time . sleep ( retryDelaySecs ) numberOfRetries += 1 if ( ( numberOfRetries % 50 ) == 0 ) : check_sandbox_for_errors ( python_test_name = h2o_args . python_test_name ) else : timeTakenSecs = time . time ( ) - start if isinstance ( error , type ( '' ) ) : raise Exception ( '%s failed after %.2f seconds having retried %d times' % ( error , timeTakenSecs , numberOfRetries ) ) else : msg = error ( self , timeTakenSecs , numberOfRetries ) raise Exception ( msg )",Repeatedly test a function waiting for it to return True .
"def _call_func_bc ( nargs , idx , ops , keys ) : named_args = { } unnamed_args = [ ] args = [ ] while nargs > 0 : if nargs >= 256 : arg , idx = _opcode_read_arg ( idx , ops , keys ) named_args [ ops [ idx ] [ 1 ] [ 0 ] ] = arg idx -= 1 nargs -= 256 else : arg , idx = _opcode_read_arg ( idx , ops , keys ) unnamed_args . insert ( 0 , arg ) nargs -= 1 op = ops [ idx ] [ 1 ] [ 0 ] args = _get_h2o_frame_method_args ( op , named_args , unnamed_args ) if is_attr ( ops [ idx ] [ 0 ] ) else [ ] op = _get_func_name ( op , args ) idx -= 1 if is_bytecode_instruction ( ops [ idx ] [ 0 ] ) : arg , idx = _opcode_read_arg ( idx , ops , keys ) args . insert ( 0 , arg ) elif is_load_fast ( ops [ idx ] [ 0 ] ) : args . insert ( 0 , _load_fast ( ops [ idx ] [ 1 ] [ 0 ] ) ) idx -= 1 return [ ExprNode ( op , * args ) , idx ]",Implements transformation of CALL_FUNCTION bc inst to Rapids expression . The implementation follows definition of behavior defined in https : // docs . python . org / 3 / library / dis . html : param nargs : number of arguments including keyword and positional arguments : param idx : index of current instruction on the stack : param ops : stack of instructions : param keys : names of instructions : return : ExprNode representing method call
"def jobs ( self , job_key = None , timeoutSecs = 10 , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'jobs' , True ) result = self . do_json_request ( '3/Jobs.json' , timeout = timeoutSecs , params = params_dict ) return result",Fetch all the jobs or a single job from the / Jobs endpoint .
"def poll_job ( self , job_key , timeoutSecs = 10 , retryDelaySecs = 0.5 , key = None , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'poll_job' , False ) start_time = time . time ( ) pollCount = 0 while True : result = self . do_json_request ( '3/Jobs.json/' + job_key , timeout = timeoutSecs , params = params_dict ) if key : frames_result = self . frames ( key = key ) print 'frames_result for key:' , key , dump_json ( result ) jobs = result [ 'jobs' ] [ 0 ] description = jobs [ 'description' ] dest = jobs [ 'dest' ] dest_name = dest [ 'name' ] msec = jobs [ 'msec' ] status = jobs [ 'status' ] progress = jobs [ 'progress' ] print description , ""dest_name:"" , dest_name , ""\tprogress:"" , ""%-10s"" % progress , ""\tstatus:"" , ""%-12s"" % status , ""\tmsec:"" , msec if status == 'DONE' or status == 'CANCELLED' or status == 'FAILED' : h2o_sandbox . check_sandbox_for_errors ( ) return result if not h2o_args . no_timeout and ( time . time ( ) - start_time > timeoutSecs ) : h2o_sandbox . check_sandbox_for_errors ( ) emsg = ""Job:"" , job_key , ""timed out in:"" , timeoutSecs a = h2o . nodes [ 0 ] . get_cloud ( ) print ""cloud.json:"" , dump_json ( a ) raise Exception ( emsg ) print emsg return None if ( pollCount % 2 ) == 0 : h2o_sandbox . check_sandbox_for_errors ( ) time . sleep ( retryDelaySecs ) pollCount += 1",Poll a single job from the / Jobs endpoint until it is status : DONE or CANCELLED or FAILED or we time out .
"def import_files ( self , path , timeoutSecs = 180 ) : a = self . do_json_request ( '3/ImportFiles.json' , timeout = timeoutSecs , params = { ""path"" : path } ) verboseprint ( ""\nimport_files result:"" , dump_json ( a ) ) h2o_sandbox . check_sandbox_for_errors ( ) return a",Import a file or files into h2o . The file parameter accepts a directory or a single file . 192 . 168 . 0 . 37 : 54323 / ImportFiles . html?file = %2Fhome%2F0xdiag%2Fdatasets
"def parse ( self , key , hex_key = None , columnTypeDict = None , timeoutSecs = 300 , retryDelaySecs = 0.2 , initialDelaySecs = None , pollTimeoutSecs = 180 , noise = None , benchmarkLogging = None , noPoll = False , intermediateResults = False , * * kwargs ) : params_dict = { 'source_frames' : None , 'destination_frame' : hex_key , 'parse_type' : None , 'separator' : None , 'single_quotes' : None , 'check_header' : None , 'number_columns' : None , 'column_names' : None , 'column_types' : None , 'na_strings' : None , 'chunk_size' : None , 'delete_on_done' : None , 'blocking' : None , } if not isinstance ( key , basestring ) : if not key : raise Exception ( ""key seems to be bad in parse. Should be list or string. %s"" % key ) source_frames = ""["" + "","" . join ( map ( ( lambda x : '""' + x + '""' ) , key ) ) + ""]"" else : source_frames = '[""' + key + '""]' params_dict [ 'source_frames' ] = source_frames h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'parse before setup merge' , print_params = False ) params_setup = { 'source_frames' : source_frames } setup_result = self . do_json_request ( jsonRequest = ""3/ParseSetup.json"" , cmd = 'post' , timeout = timeoutSecs , postData = params_setup ) h2o_sandbox . check_sandbox_for_errors ( ) verboseprint ( ""ParseSetup result:"" , dump_json ( setup_result ) ) if setup_result [ 'source_frames' ] : source_framesStr = ""["" + "","" . join ( [ ( '""%s""' % src [ 'name' ] ) for src in setup_result [ 'source_frames' ] ] ) + ""]"" else : source_framesStr = None if setup_result [ 'column_names' ] : columnNamesStr = ""["" + "","" . join ( map ( ( lambda x : '""' + x + '""' ) , setup_result [ 'column_names' ] ) ) + ""]"" else : columnNamesStr = None columnTypes = setup_result [ 'column_types' ] assert columnTypes is not None , ""%s %s"" % ( ""column_types:"" , columnTypes ) if setup_result [ 'na_strings' ] : naStrings = ""["" + "","" . join ( map ( ( lambda x : '""' + x + '""' if x != None else '""""' ) , setup_result [ 'na_strings' ] ) ) + ""]"" else : naStrings = None ct = setup_result [ 'column_types' ] columnNames = setup_result [ 'column_names' ] if columnTypeDict : for k , v in columnTypeDict . iteritems ( ) : if isinstance ( k , int ) : if k >= 0 and k < len ( ct ) : ct [ k ] = v else : raise Exception ( ""bad col index %s in columnTypeDict param %s"" % ( k , columnTypeDict ) ) elif isinstance ( k , basestring ) : if k not in columnNames : raise Exception ( ""bad col name %s in columnTypeDict param %s. columnNames: %s"" % ( k , columnTypeDict , columnNames ) ) ci = columnNames . index ( k ) ct [ ci ] = v else : raise Exception ( ""%s %s should be int or string"" % ( k , type ( k ) ) ) columnTypesStr = ""["" + "","" . join ( map ( ( lambda x : '""' + x + '""' ) , ct ) ) + ""]"" parse_params = { 'source_frames' : source_framesStr , 'destination_frame' : setup_result [ 'destination_frame' ] , 'parse_type' : setup_result [ 'parse_type' ] , 'separator' : setup_result [ 'separator' ] , 'single_quotes' : setup_result [ 'single_quotes' ] , 'check_header' : setup_result [ 'check_header' ] , 'number_columns' : setup_result [ 'number_columns' ] , 'column_names' : columnNamesStr , 'column_types' : columnTypesStr , 'na_strings' : naStrings , 'chunk_size' : setup_result [ 'chunk_size' ] , 'delete_on_done' : params_dict [ 'delete_on_done' ] , 'blocking' : params_dict [ 'blocking' ] , } tooManyColNamesToPrint = setup_result [ 'column_names' ] and len ( setup_result [ 'column_names' ] ) > 2000 if tooManyColNamesToPrint : h2p . yellow_print ( ""Not printing the parameters to Parse because the columnNames are too lengthy."" ) h2p . yellow_print ( ""See sandbox/commands.log"" ) h2o_methods . check_params_update_kwargs ( parse_params , params_dict , 'parse after merge into parse setup' , print_params = not tooManyColNamesToPrint , ignoreNone = True ) print ""parse source_frames is length:"" , len ( parse_params [ 'source_frames' ] ) parse_result = self . do_json_request ( jsonRequest = ""3/Parse.json"" , cmd = 'post' , postData = parse_params , timeout = timeoutSecs ) verboseprint ( ""Parse result:"" , dump_json ( parse_result ) ) job_key = parse_result [ 'job' ] [ 'key' ] [ 'name' ] hex_key = parse_params [ 'destination_frame' ] if noPoll : h2o_sandbox . check_sandbox_for_errors ( ) return parse_result if intermediateResults : key = hex_key else : key = None job_result = self . poll_job ( job_key , timeoutSecs = timeoutSecs , key = key ) if job_result : jobs = job_result [ 'jobs' ] [ 0 ] description = jobs [ 'description' ] dest = jobs [ 'dest' ] msec = jobs [ 'msec' ] status = jobs [ 'status' ] progress = jobs [ 'progress' ] dest_key = dest [ 'name' ] if status == 'FAILED' : print dump_json ( job_result ) raise Exception ( ""Taking exception on parse job status: %s %s %s %s %s"" % ( status , progress , msec , dest_key , description ) ) return self . frames ( dest_key ) else : raise Exception ( ""parse didn't get a job_result when it expected one"" )",Parse an imported raw file or files into a Frame .
"def frames ( self , key = None , timeoutSecs = 60 , * * kwargs ) : if not ( key is None or isinstance ( key , ( basestring , Key ) ) ) : raise Exception ( ""frames: key should be string or Key type %s %s"" % ( type ( key ) , key ) ) params_dict = { 'find_compatible_models' : 0 , 'row_offset' : 0 , 'row_count' : 5 , } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'frames' , False ) if key : if isinstance ( key , Key ) : keyStr = key . frame else : keyStr = key result = self . do_json_request ( '3/Frames.json/' + keyStr , timeout = timeoutSecs , params = params_dict ) else : result = self . do_json_request ( '3/Frames.json' , timeout = timeoutSecs , params = params_dict ) return result",Return a single Frame or all of the Frames in the h2o cluster . The frames are contained in a list called frames at the top level of the result . Currently the list is unordered . TODO : When find_compatible_models is implemented then the top level dict will also contain a models list .
"def summary ( self , key , column = ""C1"" , timeoutSecs = 10 , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'summary' , True ) result = self . do_json_request ( '3/Frames.json/%s/columns/%s/summary' % ( key , column ) , timeout = timeoutSecs , params = params_dict ) h2o_sandbox . check_sandbox_for_errors ( ) return result",Return the summary for a single column for a single Frame in the h2o cluster .
"def delete_frame ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '""key"" parameter is null' result = self . do_json_request ( '/3/Frames.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Frame key not found: ' + key ) return result",Delete a frame on the h2o cluster given its key .
"def model_builders ( self , algo = None , timeoutSecs = 10 , * * kwargs ) : params_dict = { } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'model_builders' , False ) request = '3/ModelBuilders.json' if algo : request += ""/"" + algo result = self . do_json_request ( request , timeout = timeoutSecs , params = params_dict ) h2o_sandbox . check_sandbox_for_errors ( ) return result",Return a model builder or all of the model builders known to the h2o cluster . The model builders are contained in a dictionary called model_builders at the top level of the result . The dictionary maps algorithm names to parameters lists . Each of the parameters contains all the metdata required by a client to present a model building interface to the user .
"def validate_model_parameters ( self , algo , training_frame , parameters , timeoutSecs = 60 , * * kwargs ) : assert algo is not None , '""algo"" parameter is null' assert parameters is not None , '""parameters"" parameter is null' model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) assert model_builders is not None , ""/ModelBuilders REST call failed"" assert algo in model_builders [ 'model_builders' ] builder = model_builders [ 'model_builders' ] [ algo ] if training_frame is not None : frames = self . frames ( key = training_frame ) assert frames is not None , ""/Frames/{0} REST call failed"" . format ( training_frame ) key_name = frames [ 'frames' ] [ 0 ] [ 'key' ] [ 'name' ] assert key_name == training_frame , ""/Frames/{0} returned Frame {1} rather than Frame {2}"" . format ( training_frame , key_name , training_frame ) parameters [ 'training_frame' ] = training_frame result = self . do_json_request ( '/3/ModelBuilders.json/' + algo + ""/parameters"" , cmd = 'post' , timeout = timeoutSecs , postData = parameters , ignoreH2oError = True , noExtraErrorCheck = True ) verboseprint ( ""model parameters validation: "" + repr ( result ) ) return result",Check a dictionary of model builder parameters on the h2o cluster using the given algorithm and model parameters .
"def build_model ( self , algo , training_frame , parameters , destination_frame = None , model_id = None , timeoutSecs = 60 , noPoll = False , * * kwargs ) : if 'destination_key' in kwargs : raise Exception ( 'Change destination_key in build_model() to model_id' ) assert algo is not None , '""algo"" parameter is null' assert training_frame is not None , '""training_frame"" parameter is null' assert parameters is not None , '""parameters"" parameter is null' model_builders = self . model_builders ( timeoutSecs = timeoutSecs ) assert model_builders is not None , ""/ModelBuilders REST call failed"" assert algo in model_builders [ 'model_builders' ] , ""%s %s"" % ( algo , [ k for k in model_builders [ 'model_builders' ] ] ) builder = model_builders [ 'model_builders' ] [ algo ] frames = self . frames ( key = training_frame ) assert frames is not None , ""/Frames/{0} REST call failed"" . format ( training_frame ) key_name = frames [ 'frames' ] [ 0 ] [ 'frame_id' ] [ 'name' ] assert key_name == training_frame , ""/Frames/{0} returned Frame {1} rather than Frame {2}"" . format ( training_frame , key_name , training_frame ) parameters [ 'training_frame' ] = training_frame if destination_frame is not None : print ""destination_frame should be replaced by model_id now"" parameters [ 'model_id' ] = destination_frame if model_id is not None : parameters [ 'model_id' ] = model_id print ""build_model parameters"" , parameters start = time . time ( ) result1 = self . do_json_request ( '/3/ModelBuilders.json/' + algo , cmd = 'post' , timeout = timeoutSecs , postData = parameters ) elapsed = time . time ( ) - start verboseprint ( ""build_model result"" , dump_json ( result1 ) ) if noPoll : result = result1 elif ( 'validation_error_count' in result1 ) and ( result1 [ 'validation_error_count' ] > 0 ) : h2p . yellow_print ( ""parameter error in model_builders: %s"" % result1 ) result = result1 elif 'exception_msg' in result1 : h2p . yellow_print ( ""exception msg in model_builders: %s"" % result1 [ 'exception_msg' ] ) result = result1 else : job_result = result1 [ 'job' ] job_key = job_result [ 'key' ] [ 'name' ] verboseprint ( ""build_model job_key: "" + repr ( job_key ) ) job_result = self . poll_job ( job_key , timeoutSecs = timeoutSecs ) verboseprint ( job_result ) elapsed = time . time ( ) - start print ""ModelBuilders"" , algo , ""end on"" , training_frame , 'took' , time . time ( ) - start , 'seconds' print ""%d pct. of timeout"" % ( ( elapsed / timeoutSecs ) * 100 ) if job_result : jobs = job_result [ 'jobs' ] [ 0 ] description = jobs [ 'description' ] dest = jobs [ 'dest' ] msec = jobs [ 'msec' ] status = jobs [ 'status' ] progress = jobs [ 'progress' ] if status == 'FAILED' : print dump_json ( job_result ) raise Exception ( ""Taking exception on build_model job status: %s %s %s %s"" % ( status , progress , msec , description ) ) result = job_result else : raise Exception ( ""build_model didn't get a job_result when it expected one"" ) verboseprint ( ""result:"" , result ) h2o_sandbox . check_sandbox_for_errors ( ) result [ 'python_elapsed' ] = elapsed return result",Build a model on the h2o cluster using the given algorithm training Frame and model parameters .
"def compute_model_metrics ( self , model , frame , timeoutSecs = 60 , * * kwargs ) : assert model is not None , '""model"" parameter is null' assert frame is not None , '""frame"" parameter is null' models = self . models ( key = model , timeoutSecs = timeoutSecs ) assert models is not None , ""/Models REST call failed"" assert models [ 'models' ] [ 0 ] [ 'model_id' ] [ 'name' ] == model , ""/Models/{0} returned Model {1} rather than Model {2}"" . format ( model , models [ 'models' ] [ 0 ] [ 'key' ] [ 'name' ] , model ) frames = self . frames ( key = frame ) assert frames is not None , ""/Frames/{0} REST call failed"" . format ( frame ) print ""frames:"" , dump_json ( frames ) result = self . do_json_request ( '/3/ModelMetrics.json/models/' + model + '/frames/' + frame , cmd = 'post' , timeout = timeoutSecs ) mm = result [ 'model_metrics' ] [ 0 ] verboseprint ( ""model metrics: "" + repr ( mm ) ) h2o_sandbox . check_sandbox_for_errors ( ) return mm",Score a model on the h2o cluster on the given Frame and return only the model metrics .
"def model_metrics ( self , timeoutSecs = 60 , * * kwargs ) : result = self . do_json_request ( '/3/ModelMetrics.json' , cmd = 'get' , timeout = timeoutSecs ) h2o_sandbox . check_sandbox_for_errors ( ) return result",ModelMetrics list .
"def models ( self , key = None , timeoutSecs = 10 , * * kwargs ) : params_dict = { 'find_compatible_frames' : False } h2o_methods . check_params_update_kwargs ( params_dict , kwargs , 'models' , True ) if key : result = self . do_json_request ( '3/Models.json/' + key , timeout = timeoutSecs , params = params_dict ) else : result = self . do_json_request ( '3/Models.json' , timeout = timeoutSecs , params = params_dict ) verboseprint ( ""models result:"" , dump_json ( result ) ) h2o_sandbox . check_sandbox_for_errors ( ) return result",Return all of the models in the h2o cluster or a single model given its key . The models are contained in a list called models at the top level of the result . Currently the list is unordered . TODO : When find_compatible_frames is implemented then the top level dict will also contain a frames list .
"def delete_model ( self , key , ignoreMissingKey = True , timeoutSecs = 60 , * * kwargs ) : assert key is not None , '""key"" parameter is null' result = self . do_json_request ( '/3/Models.json/' + key , cmd = 'delete' , timeout = timeoutSecs ) if not ignoreMissingKey and 'f00b4r' in result : raise ValueError ( 'Model key not found: ' + key ) verboseprint ( ""delete_model result:"" , dump_json ( result ) ) return result",Delete a model on the h2o cluster given its key .
"def _tabulate ( self , tablefmt = ""simple"" , rollups = False , rows = 10 ) : if not self . is_valid ( ) : self . fill ( rows = rows ) d = collections . OrderedDict ( ) if rollups : col = next ( iter ( viewvalues ( self . _data ) ) ) lrows = len ( col [ 'data' ] ) d [ """" ] = [ ""type"" , ""mins"" , ""mean"" , ""maxs"" , ""sigma"" , ""zeros"" , ""missing"" ] + list ( map ( str , range ( lrows ) ) ) for k , v in viewitems ( self . _data ) : x = v [ 'data' ] t = v [ ""type"" ] if t == ""enum"" : domain = v [ 'domain' ] x = [ """" if math . isnan ( idx ) else domain [ int ( idx ) ] for idx in x ] elif t == ""time"" : x = [ """" if math . isnan ( z ) else time . strftime ( ""%Y-%m-%d %H:%M:%S"" , time . gmtime ( z / 1000 ) ) for z in x ] if rollups : mins = v [ 'mins' ] [ 0 ] if v [ 'mins' ] and v [ ""type"" ] != ""enum"" else None maxs = v [ 'maxs' ] [ 0 ] if v [ 'maxs' ] and v [ ""type"" ] != ""enum"" else None if v [ 'type' ] == ""enum"" : v [ 'mean' ] = v [ 'sigma' ] = v [ 'zero_count' ] = None x = [ v [ 'type' ] , mins , v [ 'mean' ] , maxs , v [ 'sigma' ] , v [ 'zero_count' ] , v [ 'missing_count' ] ] + x d [ k ] = x return tabulate . tabulate ( d , headers = ""keys"" , tablefmt = tablefmt )",Pretty tabulated string of all the cached data and column names
"def run_instances ( count , ec2_config , region , waitForSSH = True , tags = None ) : ec2params = inheritparams ( ec2_config , EC2_API_RUN_INSTANCE ) ec2params . setdefault ( 'min_count' , count ) ec2params . setdefault ( 'max_count' , count ) reservation = None conn = ec2_connect ( region ) try : reservation = conn . run_instances ( * * ec2params ) log ( 'Reservation: {0}' . format ( reservation . id ) ) log ( 'Waiting for {0} EC2 instances {1} to come up, this can take 1-2 minutes.' . format ( len ( reservation . instances ) , reservation . instances ) ) start = time . time ( ) time . sleep ( 1 ) for instance in reservation . instances : while instance . update ( ) == 'pending' : time . sleep ( 1 ) h2o_cmd . dot ( ) if not instance . state == 'running' : raise Exception ( '\033[91m[ec2] Error waiting for running state. Instance is in state {0}.\033[0m' . format ( instance . state ) ) log ( 'Instances started in {0} seconds' . format ( time . time ( ) - start ) ) log ( 'Instances: ' ) for inst in reservation . instances : log ( ""   {0} ({1}) : public ip: {2}, private ip: {3}"" . format ( inst . public_dns_name , inst . id , inst . ip_address , inst . private_ip_address ) ) if waitForSSH : wait_for_ssh ( [ i . private_ip_address for i in reservation . instances ] ) try : if tags : conn . create_tags ( [ i . id for i in reservation . instances ] , tags ) except : warn ( 'Something wrong during tagging instances. Exceptions IGNORED!' ) print sys . exc_info ( ) pass return reservation except : print ""\033[91mUnexpected error\033[0m :"" , sys . exc_info ( ) if reservation : terminate_reservation ( reservation , region ) raise",Create a new reservation for count instances
"def terminate_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( ""Terminating instances {0}."" . format ( instances ) ) conn . terminate_instances ( instances ) log ( ""Done"" )",terminate all the instances given by its ids
"def stop_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( ""Stopping instances {0}."" . format ( instances ) ) conn . stop_instances ( instances ) log ( ""Done"" )",stop all the instances given by its ids
"def start_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( ""Starting instances {0}."" . format ( instances ) ) conn . start_instances ( instances ) log ( ""Done"" )",Start all the instances given by its ids
"def reboot_instances ( instances , region ) : if not instances : return conn = ec2_connect ( region ) log ( ""Rebooting instances {0}."" . format ( instances ) ) conn . reboot_instances ( instances ) log ( ""Done"" )",Reboot all the instances given by its ids
"def wait_for_ssh ( ips , port = 22 , skipAlive = True , requiredsuccess = 3 ) : log ( 'Waiting for SSH on following hosts: {0}' . format ( ips ) ) for ip in ips : if not skipAlive or not ssh_live ( ip , port ) : log ( 'Waiting for SSH on instance {0}...' . format ( ip ) ) count = 0 while count < requiredsuccess : if ssh_live ( ip , port ) : count += 1 else : count = 0 time . sleep ( 1 ) h2o_cmd . dot ( )",Wait for ssh service to appear on given hosts
"def _except_hook ( exc_type , exc_value , exc_tb ) : if isinstance ( exc_value , H2OJobCancelled ) : return if isinstance ( exc_value , H2OSoftError ) : _handle_soft_error ( exc_type , exc_value , exc_tb ) else : _prev_except_hook ( exc_type , exc_value , exc_tb ) return import linecache if not exc_tb : sys . __excepthook__ ( exc_type , exc_value , exc_tb ) return get_tb . tb = exc_tb err ( ""\n================================ EXCEPTION INFO ================================\n"" ) if exc_type != type ( exc_value ) : err ( ""Exception type(s): %s / %s"" % ( exc_type , type ( exc_value ) ) ) for arg in exc_value . args : if hasattr ( arg , ""stacktrace"" ) : err ( ""[SERVER STACKTRACE]"" ) for line in arg . stacktrace : err ( ""    %s"" % line . strip ( ) ) err ( ) err ( ""[LOCAL FRAMES]"" ) err ( ""Omitted: imported modules, class declarations, __future__ features, None-valued"" ) tb = exc_tb while tb : tb_line = tb . tb_lineno frame = tb . tb_frame frame_file = frame . f_code . co_filename frame_func = frame . f_code . co_name frame_locl = frame . f_locals tb = tb . tb_next if frame_func == ""decorator_invisible"" : continue if frame_func == ""__getattribute__"" : continue if not frame_locl : continue err ( ""\n  Within %s() line %s in file %s:"" % ( frame_func , tb_line , frame_file ) ) for key in sorted ( viewkeys ( frame_locl ) , reverse = True ) : if key . startswith ( ""__"" ) and key . endswith ( ""__"" ) : continue value = frame_locl [ key ] if value is None : continue if hasattr ( value , ""__class__"" ) : if value . __class__ is ModuleType : continue if value . __class__ is type : continue if value . __class__ is print_function . __class__ : continue try : strval = str ( value ) n_lines = strval . count ( ""\n"" ) if n_lines > 1 : strval = ""%s... (+ %d line%s)"" % ( strval [ : strval . index ( ""\n"" ) ] , n_lines - 1 , ""s"" if n_lines > 2 else """" ) err ( ""%25s: %s"" % ( key , strval ) ) except : err ( ""%25s: <UNABLE TO PRINT VALUE>"" % key ) err ( ) err ( ""[STACKTRACE]"" ) last_file = None tb = exc_tb prev_info = None skip_frames = 0 while tb : tb_lineno = tb . tb_lineno frame = tb . tb_frame frame_file = frame . f_code . co_filename frame_func = frame . f_code . co_name frame_glob = frame . f_globals tb = tb . tb_next if frame_func == ""decorator_invisible"" : continue if frame_func == ""__getattribute__"" : continue if ( tb_lineno , frame_file ) == prev_info : skip_frames += 1 continue else : if skip_frames : err ( ""    %20s   ... +%d nested calls"" % ( """" , skip_frames ) ) skip_frames = 0 prev_info = ( tb_lineno , frame_file ) if frame_file != last_file : last_file = frame_file err ( ""\n  File %s:"" % frame_file ) line_txt = linecache . getline ( frame_file , tb_lineno , frame_glob ) err ( ""    %20s() #%04d  %s"" % ( frame_func , tb_lineno , line_txt . strip ( ) ) ) if skip_frames : err ( ""    %20s   ... +%d nested calls"" % ( """" , skip_frames ) ) err ( ) tb = exc_tb while tb . tb_next : tb = tb . tb_next print ( ""[EXCEPTION]"" , file = sys . stderr ) print ( ""  %s: %s"" % ( exc_value . __class__ . __name__ , str ( exc_value ) ) , file = sys . stderr ) print ( ""  at line %d in %s\n"" % ( tb . tb_lineno , tb . tb_frame . f_code . co_filename ) , file = sys . stderr ) del tb del exc_tb",This is an advanced exception - handling hook function that is designed to supercede the standard Python s exception handler . It offers several enhancements : * Clearer and more readable format for the exception message and the traceback . * Decorators are filtered out from the traceback ( if they declare their implementation function to have name decorator_invisible ) . * Local variables in all execution frames are also printed out . * Print out server - side stacktrace for exceptions that carry this information ( e . g . H2OServerError ) .
"def _get_method_full_name ( func ) : if hasattr ( func , ""__qualname__"" ) : return func . __qualname__ module = inspect . getmodule ( func ) if module is None : return ""?.%s"" % getattr ( func , ""__name__"" , ""?"" ) for cls_name in dir ( module ) : cls = getattr ( module , cls_name ) if not inspect . isclass ( cls ) : continue for method_name in dir ( cls ) : cls_method = getattr ( cls , method_name ) if cls_method == func : return ""%s.%s"" % ( cls_name , method_name ) if hasattr ( func , ""__name__"" ) : return ""%s.%s"" % ( module . __name__ , func . __name__ ) return ""<unknown>""",Return fully qualified function name .
"def _find_function_from_code ( frame , code ) : def find_code ( iterable , depth = 0 ) : if depth > 3 : return for item in iterable : if item is None : continue found = None if hasattr ( item , ""__code__"" ) and item . __code__ == code : found = item elif isinstance ( item , type ) or isinstance ( item , ModuleType ) : try : found = find_code ( ( getattr ( item , n , None ) for n in dir ( item ) ) , depth + 1 ) except Exception : continue elif isinstance ( item , ( list , tuple , set ) ) : found = find_code ( item , depth + 1 ) elif isinstance ( item , dict ) : found = find_code ( item . values ( ) , depth + 1 ) if found : return found return find_code ( frame . f_locals . values ( ) ) or find_code ( frame . f_globals . values ( ) )",Given a frame and a compiled function code find the corresponding function object within the frame .
"def _get_args_str ( func , highlight = None ) : if not func : return """" s = str ( inspect . signature ( func ) ) [ 1 : - 1 ] if highlight : s = re . sub ( r""\b%s\b"" % highlight , Style . BRIGHT + Fore . WHITE + highlight + Fore . LIGHTBLACK_EX + Style . NORMAL , s ) return s",Return function s declared arguments as a string .
"def _wrap ( text , wrap_at = 120 , indent = 4 ) : out = """" curr_line_length = indent space_needed = False for word in text . split ( ) : if curr_line_length + len ( word ) > wrap_at : out += ""\n"" + "" "" * indent curr_line_length = indent space_needed = False if space_needed : out += "" "" curr_line_length += 1 out += word curr_line_length += len ( word ) space_needed = True return out",Return piece of text wrapped around if needed .
"def jenkins_h2o_port_allocate ( ) : if os . environ . has_key ( ""EXECUTOR_NUMBER"" ) : executor = int ( os . environ [ ""EXECUTOR_NUMBER"" ] ) else : executor = 1 print ""jenkins EXECUTOR_NUMBER:"" , executor if executor < 0 or executor >= EXECUTOR_NUM : raise Exception ( ""executor: %s wrong? Expecting 1-8 jenkins executors on a machine (0-7 exp.)"" % executor ) h2oPort = DEFAULT_BASE_PORT h2oPortOffset = 0 hostname = socket . gethostname ( ) if hostname not in USED_HOSTNAMES : print ""WARNING: this hostname: %s isn't in my list. You should add it?"" % hostname print ""Will use default base port"" else : hostnameIndex = USED_HOSTNAMES . index ( hostname ) h2oPortOffset = PORTS_PER_SLOT * ( executor + hostnameIndex ) h2oPort += h2oPortOffset print ""Possible h2o base_port range is %s to %s"" % ( DEFAULT_BASE_PORT , DEFAULT_BASE_PORT + ( PORTS_PER_SLOT * EXECUTOR_NUM * len ( USED_HOSTNAMES ) ) - 2 ) print ""Possible h2o ports used ranged is %s to %s"" % ( DEFAULT_BASE_PORT , DEFAULT_BASE_PORT + ( PORTS_PER_SLOT * EXECUTOR_NUM * len ( USED_HOSTNAMES ) ) - 1 ) print ""want to 'export H2O_PORT=%s'"" % h2oPort print ""want to 'export H2O_PORT_OFFSET=%s # legacy'"" % h2oPortOffset f = open ( 'H2O_BASE_PORT.sh' , 'w' ) f . write ( 'export H2O_PORT=%s\n' % h2oPort ) f . write ( 'export H2O_PORT_OFFSET=%s # legacy\n' % h2oPortOffset ) f . close ( ) print ""\nNow please:\nsource ./H2O_BASE_PORT.sh""",input : jenkins environment variable EXECUTOR_NUMBER output : creates . / BASE_PORT . sh that you should source . / PORT . sh ( can t see the env . variables directly from python? )
"def start ( self , x , y = None , training_frame = None , offset_column = None , fold_column = None , weights_column = None , validation_frame = None , * * params ) : self . _future = True self . train ( x = x , y = y , training_frame = training_frame , offset_column = offset_column , fold_column = fold_column , weights_column = weights_column , validation_frame = validation_frame , * * params )",Train the model asynchronously ( to block for results call : meth : join ) .
"def join ( self ) : self . _future = False self . _job . poll ( ) model_key = self . _job . dest_key self . _job = None model_json = h2o . api ( ""GET /%d/Models/%s"" % ( self . _rest_version , model_key ) ) [ ""models"" ] [ 0 ] self . _resolve_model ( model_key , model_json )",Wait until job s completion .
"def train ( self , x = None , y = None , training_frame = None , offset_column = None , fold_column = None , weights_column = None , validation_frame = None , max_runtime_secs = None , ignored_columns = None , model_id = None , verbose = False ) : self . _train ( x = x , y = y , training_frame = training_frame , offset_column = offset_column , fold_column = fold_column , weights_column = weights_column , validation_frame = validation_frame , max_runtime_secs = max_runtime_secs , ignored_columns = ignored_columns , model_id = model_id , verbose = verbose )",Train the H2O model .
"def fit ( self , X , y = None , * * params ) : stk = inspect . stack ( ) [ 1 : ] warn = True for s in stk : mod = inspect . getmodule ( s [ 0 ] ) if mod : warn = ""sklearn"" not in mod . __name__ if not warn : break if warn : warnings . warn ( ""\n\n\t`fit` is not recommended outside of the sklearn framework. Use `train` instead."" , UserWarning , stacklevel = 2 ) training_frame = X . cbind ( y ) if y is not None else X x = X . names y = y . names [ 0 ] if y is not None else None self . train ( x , y , training_frame , * * params ) return self",Fit an H2O model as part of a scikit - learn pipeline or grid search .
"def get_params ( self , deep = True ) : out = dict ( ) for key , value in self . parms . items ( ) : if deep and isinstance ( value , H2OEstimator ) : deep_items = list ( value . get_params ( ) . items ( ) ) out . update ( ( key + ""__"" + k , val ) for k , val in deep_items ) out [ key ] = value return out",Obtain parameters for this estimator .
"def convert_H2OXGBoostParams_2_XGBoostParams ( self ) : import xgboost as xgb nativeParams = self . _model_json [ ""output"" ] [ ""native_parameters"" ] nativeXGBoostParams = dict ( ) for ( a , keyname , keyvalue ) in nativeParams . cell_values : nativeXGBoostParams [ keyname ] = keyvalue paramsSet = self . full_parameters return nativeXGBoostParams , paramsSet [ 'ntrees' ] [ 'actual_value' ]",In order to use convert_H2OXGBoostParams_2_XGBoostParams and convert_H2OFrame_2_DMatrix you must import the following toolboxes : xgboost pandas numpy and scipy . sparse .
"def _check_and_save_parm ( self , parms , parameter_name , parameter_value ) : if parameter_name not in parms : parms [ parameter_name ] = parameter_value elif parameter_value is not None and parms [ parameter_name ] != parameter_value : parms [ parameter_name ] = parameter_value warnings . warn ( ""\n\n\t`%s` parameter has been already set and had a different value in `train` method. The last passed value \""%s\"" is used."" % ( parameter_name , parameter_value ) , UserWarning , stacklevel = 2 )",If a parameter is not stored in parms dict save it there ( even though the value is None ) . Else check if the parameter has been already set during initialization of estimator . If yes check the new value is the same or not . If the values are different set the last passed value to params dict and throw UserWarning .
"def is_rdemo ( file_name ) : packaged_demos = [ ""h2o.anomaly.R"" , ""h2o.deeplearning.R"" , ""h2o.gbm.R"" , ""h2o.glm.R"" , ""h2o.glrm.R"" , ""h2o.kmeans.R"" , ""h2o.naiveBayes.R"" , ""h2o.prcomp.R"" , ""h2o.randomForest.R"" ] if file_name in packaged_demos : return True if re . match ( ""^rdemo.*\.(r|R|ipynb)$"" , file_name ) : return True return False",Return True if file_name matches a regexp for an R demo . False otherwise . : param file_name : file to test
"def is_ipython_notebook ( file_name ) : if ( not re . match ( ""^.*checkpoint\.ipynb$"" , file_name ) ) and re . match ( ""^.*\.ipynb$"" , file_name ) : return True return False",Return True if file_name matches a regexp for an ipython notebook . False otherwise . : param file_name : file to test
"def grab_java_message ( node_list , curr_testname ) : global g_java_start_text java_messages = """" start_test = False for each_node in node_list : java_filename = each_node . output_file_name if os . path . isfile ( java_filename ) : java_file = open ( java_filename , 'r' ) for each_line in java_file : if g_java_start_text in each_line : start_str , found , end_str = each_line . partition ( g_java_start_text ) if len ( found ) > 0 : current_testname = end_str . strip ( ) if current_testname == curr_testname : start_test = True java_messages += ""\n\n**********************************************************\n"" java_messages += ""**********************************************************\n"" java_messages += ""JAVA Messages\n"" java_messages += ""**********************************************************\n"" java_messages += ""**********************************************************\n\n"" else : if start_test : break if start_test : java_messages += each_line java_file . close ( ) if start_test : break return java_messages",scan through the java output text and extract the java messages related to running test specified in curr_testname . Parameters ---------- : param node_list : list of H2O nodes List of H2o nodes associated with an H2OCloud ( cluster ) that are performing the test specified in curr_testname . : param curr_testname : str Store the unit test name ( can be R unit or Py unit ) that has been completed and failed . : return : a string object that is either empty or the java messages that associated with the test in curr_testname . The java messages can usually be found in one of the java_ * _0 . out . txt
"def signal_handler ( signum , stackframe ) : global g_runner global g_handling_signal if g_handling_signal : return g_handling_signal = True print ( """" ) print ( ""----------------------------------------------------------------------"" ) print ( """" ) print ( ""SIGNAL CAUGHT ("" + str ( signum ) + "").  TEARING DOWN CLOUDS."" ) print ( """" ) print ( ""----------------------------------------------------------------------"" ) g_runner . terminate ( )",Helper function to handle caught signals .
"def usage ( ) : print ( """" ) print ( ""Usage:  "" + g_script_name + "" [...options...]"" ) print ( """" ) print ( ""    (Output dir is: "" + str ( g_output_dir ) + "")"" ) print ( ""    (Default number of clouds is: "" + str ( g_num_clouds ) + "")"" ) print ( """" ) print ( ""    --wipeall        Remove all prior test state before starting, particularly"" ) print ( ""                     random seeds."" ) print ( ""                     (Removes master_seed file and all Rsandbox directories."" ) print ( ""                     Also wipes the output dir before starting.)"" ) print ( """" ) print ( ""    --wipe           Wipes the output dir before starting.  Keeps old random seeds."" ) print ( """" ) print ( ""    --baseport       The first port at which H2O starts searching for free ports."" ) print ( """" ) print ( ""    --numclouds      The number of clusters to start."" ) print ( ""                     Each test is randomly assigned to a cluster."" ) print ( """" ) print ( ""    --numnodes       The number of nodes in the cluster."" ) print ( ""                     When this is specified, numclouds must be 1."" ) print ( """" ) print ( ""    --test           If you only want to run one test, specify it like this."" ) print ( """" ) print ( ""    --testlist       A file containing a list of tests to run (for example the"" ) print ( ""                     'failed.txt' file from the output directory)."" ) print ( ""    --excludelist    A file containing a list of tests to NOT run."" ) print ( """" ) print ( ""    --testgroup      Test a group of tests by function:"" ) print ( ""                     pca, glm, kmeans, gbm, rf, deeplearning, algos, golden, munging, parser"" ) print ( """" ) print ( ""    --testsize       Sizes (and by extension length) of tests to run:"" ) print ( ""                     s=small (seconds), m=medium (a minute or two), l=large (longer), x=xlarge (very big)"" ) print ( ""                     (Default is to run all tests.)"" ) print ( """" ) print ( ""    --usecloud       ip:port of cluster to send tests to instead of starting clusters."" ) print ( ""                     (When this is specified, numclouds is ignored.)"" ) print ( """" ) print ( ""    --usecloud2      cloud.cfg: Use a set clusters defined in cloud.config to run tests on."" ) print ( ""                     (When this is specified, numclouds, numnodes, and usecloud are ignored.)"" ) print ( """" ) print ( ""    --client         Send REST API commands through client mode."" ) print ( """" ) print ( ""    --norun          Perform side effects like wipe, but don't actually run tests."" ) print ( """" ) print ( ""    --jvm.xmx        Configure size of launched JVM running H2O. E.g. '--jvm.xmx 3g'"" ) print ( """" ) print ( ""    --jvm.cp         Classpath argument, in addition to h2o.jar path. E.g. "" ""'--jvm.cp /Users/h2o/mysql-connector-java-5.1.38-bin.jar'"" ) print ( """" ) print ( ""    --nopass         Run the NOPASS and NOFEATURE tests only and do not ignore any failures."" ) print ( """" ) print ( ""    --nointernal     Don't run the INTERNAL tests."" ) print ( """" ) print ( ""    --c              Start the JVMs in a convenient location."" ) print ( """" ) print ( ""    --h2ojar         Supply a path to the H2O jar file."" ) print ( """" ) print ( ""    --tar            Supply a path to the R TAR."" ) print ( """" ) print ( """" ) print ( ""    --pto            The phantomjs timeout in seconds. Default is 3600 (1hr)."" ) print ( """" ) print ( ""    --noxunit        Do not produce xUnit reports."" ) print ( """" ) print ( ""    --rPkgVerChk     Check that Jenkins-approved R packages/versions are present"" ) print ( """" ) print ( ""    --onHadoop       Indication that tests will be run on h2o multinode hadoop clusters."" ) print ( ""                     `locate` and `sandbox` runit/pyunit test utilities use this indication in order to"" ) print ( ""                     behave properly. --hadoopNamenode must be specified if --onHadoop option is used."" ) print ( ""    --hadoopNamenode Specifies that the runit/pyunit tests have access to this hadoop namenode."" ) print ( ""                     runit/pyunit test utilities have ability to retrieve this value."" ) print ( """" ) print ( ""    --perf           Save Jenkins build id, date, machine ip, git hash, name, start time, finish time,"" ) print ( ""                     pass, ncpus, os, and job name of each test to perf.csv in the results directory."" ) print ( ""                     Takes three parameters: git hash, git branch, and build id, job name in that order."" ) print ( """" ) print ( ""    --jacoco         Generate code coverage data using JaCoCo. Class includes and excludes may optionally"" ) print ( ""                     follow in the format of [includes]:[excludes] where [...] denotes a list of"" ) print ( ""                     classes, each separated by a comma (,). Wildcard characters (* and ?) may be used."" ) print ( """" ) print ( ""    --geterrs        Generate xml file that contains the actual unit test errors and the actual Java error."" ) print ( """" ) print ( ""    --test.ssl       Runs all the nodes with SSL enabled."" ) print ( """" ) print ( ""    --ldap.username  Username for LDAP."" ) print ( """" ) print ( ""    --ldap.password  Password for LDAP."" ) print ( """" ) print ( ""    --ldap.config    Path to LDAP config. If set, all nodes will be started with LDAP support."" ) print ( """" ) print ( ""    --kerb.principal  Kerberos service principal."" ) print ( """" ) print ( ""    --jvm.opts       Additional JVM options."" ) print ( """" ) print ( ""    --restLog        If set, enable REST API logging. Logs will be available at <resultsDir>/rest.log."" ) print ( ""                     Please note, that enablig REST API logging will increase the execution time and that"" ) print ( ""                     the log file might be large (> 2GB)."" ) print ( ""    If neither --test nor --testlist is specified, then the list of tests is"" ) print ( ""    discovered automatically as files matching '*runit*.R'."" ) print ( """" ) print ( """" ) print ( ""Examples:"" ) print ( """" ) print ( ""    Just accept the defaults and go (note: output dir must not exist):"" ) print ( ""        "" + g_script_name ) print ( """" ) print ( ""    Remove all random seeds (i.e. make new ones) but don't run any tests:"" ) print ( ""        "" + g_script_name + "" --wipeall --norun"" ) print ( """" ) print ( ""    For a powerful laptop with 8 cores (keep default numclouds):"" ) print ( ""        "" + g_script_name + "" --wipeall"" ) print ( """" ) print ( ""    For a big server with 32 cores:"" ) print ( ""        "" + g_script_name + "" --wipeall --numclouds 16"" ) print ( """" ) print ( ""    Just run the tests that finish quickly"" ) print ( ""        "" + g_script_name + "" --wipeall --testsize s"" ) print ( """" ) print ( ""    Run one specific test, keeping old random seeds:"" ) print ( ""        "" + g_script_name + "" --wipe --test path/to/test.R"" ) print ( """" ) print ( ""    Rerunning failures from a previous run, keeping old random seeds:"" ) print ( ""        # Copy failures.txt, otherwise --wipe removes the directory with the list!"" ) print ( ""        cp "" + os . path . join ( g_output_dir , ""failures.txt"" ) + "" ."" ) print ( ""        "" + g_script_name + "" --wipe --numclouds 16 --testlist failed.txt"" ) print ( """" ) print ( ""    Run tests on a pre-existing cloud (e.g. in a debugger), keeping old random seeds:"" ) print ( ""        "" + g_script_name + "" --wipe --usecloud ip:port"" ) print ( """" ) print ( ""    Run tests with JaCoCo enabled, excluding org.example1 and org.example2"" ) print ( ""        "" + g_script_name + "" --jacoco :org.example1,org.example2"" ) sys . exit ( 1 )",Print USAGE help .
"def parse_args ( argv ) : global g_base_port global g_num_clouds global g_nodes_per_cloud global g_wipe_test_state global g_wipe_output_dir global g_test_to_run global g_test_list_file global g_exclude_list_file global g_test_group global g_run_small global g_run_medium global g_run_large global g_run_xlarge global g_use_cloud global g_use_cloud2 global g_use_client global g_config global g_use_proto global g_use_ip global g_use_port global g_no_run global g_jvm_xmx global g_jvm_cp global g_nopass global g_nointernal global g_convenient global g_path_to_h2o_jar global g_path_to_tar global g_path_to_whl global g_jacoco_include global g_jacoco_options global g_produce_unit_reports global g_phantomjs_to global g_phantomjs_packs global g_r_pkg_ver_chk global g_on_hadoop global g_hadoop_namenode global g_r_test_setup global g_py_test_setup global g_perf global g_git_hash global g_git_branch global g_machine_ip global g_date global g_build_id global g_ncpu global g_os global g_job_name global g_py3 global g_pycoverage global g_use_xml2 global g_test_ssl global g_ldap_username global g_ldap_password global g_kerb_principal global g_ldap_config global g_rest_log global g_jvm_opts i = 1 while i < len ( argv ) : s = argv [ i ] if s == ""--baseport"" : i += 1 if i >= len ( argv ) : usage ( ) g_base_port = int ( argv [ i ] ) elif s == ""--py3"" : g_py3 = True elif s == ""--coverage"" : g_pycoverage = True elif s == ""--numclouds"" : i += 1 if i >= len ( argv ) : usage ( ) g_num_clouds = int ( argv [ i ] ) elif s == ""--numnodes"" : i += 1 if i >= len ( argv ) : usage ( ) g_nodes_per_cloud = int ( argv [ i ] ) elif s == ""--wipeall"" : g_wipe_test_state = True g_wipe_output_dir = True elif s == ""--wipe"" : g_wipe_output_dir = True elif s == ""--test"" : i += 1 if i >= len ( argv ) : usage ( ) g_test_to_run = TestRunner . find_test ( argv [ i ] ) elif s == ""--testlist"" : i += 1 if i >= len ( argv ) : usage ( ) g_test_list_file = argv [ i ] elif s == ""--excludelist"" : i += 1 if i >= len ( argv ) : usage ( ) g_exclude_list_file = argv [ i ] elif s == ""--testgroup"" : i += 1 if i >= len ( argv ) : usage ( ) g_test_group = argv [ i ] elif s == ""--testsize"" : i += 1 if i >= len ( argv ) : usage ( ) v = argv [ i ] if re . match ( r'(s)?(m)?(l)?' , v ) : if 's' not in v : g_run_small = False if 'm' not in v : g_run_medium = False if 'l' not in v : g_run_large = False if 'x' not in v : g_run_xlarge = False else : bad_arg ( s ) elif s == ""--usecloud"" : i += 1 if i >= len ( argv ) : usage ( ) s = argv [ i ] proto = """" if s . lower ( ) . startswith ( ""https://"" ) : proto = ""https://"" s = s [ 8 : ] m = re . match ( r'(\S+):([1-9][0-9]*)' , s ) if m is None : unknown_arg ( s ) g_use_cloud = True g_use_proto = proto g_use_ip = m . group ( 1 ) port_string = m . group ( 2 ) g_use_port = int ( port_string ) elif s == ""--usecloud2"" : i += 1 if i >= len ( argv ) : usage ( ) s = argv [ i ] if s is None : unknown_arg ( s ) g_use_cloud2 = True g_config = s elif s == ""--client"" : g_use_client = True elif s == ""--nopass"" : g_nopass = True elif s == ""--nointernal"" : g_nointernal = True elif s == ""--c"" : g_convenient = True elif s == ""--h2ojar"" : i += 1 g_path_to_h2o_jar = os . path . abspath ( argv [ i ] ) elif s == ""--pto"" : i += 1 g_phantomjs_to = int ( argv [ i ] ) elif s == ""--ptt"" : i += 1 g_phantomjs_packs = argv [ i ] elif s == ""--tar"" : i += 1 g_path_to_tar = os . path . abspath ( argv [ i ] ) elif s == ""--whl"" : i += 1 g_path_to_whl = os . path . abspath ( argv [ i ] ) elif s == ""--jvm.xmx"" : i += 1 if i >= len ( argv ) : usage ( ) g_jvm_xmx = argv [ i ] elif s == ""--jvm.cp"" : i += 1 if i > len ( argv ) : usage ( ) g_jvm_cp = argv [ i ] elif s == ""--norun"" : g_no_run = True elif s == ""--noxunit"" : g_produce_unit_reports = False elif s == ""--jacoco"" : g_jacoco_include = True if i + 1 < len ( argv ) : s = argv [ i + 1 ] m = re . match ( r'(?P<includes>([^:,]+(,[^:,]+)*)?):(?P<excludes>([^:,]+(,[^:,]+)*)?)$' , s ) if m is not None : g_jacoco_options [ 0 ] = m . group ( ""includes"" ) g_jacoco_options [ 1 ] = m . group ( ""excludes"" ) elif s == ""-h"" or s == ""--h"" or s == ""-help"" or s == ""--help"" : usage ( ) elif s == ""--rPkgVerChk"" : g_r_pkg_ver_chk = True elif s == ""--onHadoop"" : g_on_hadoop = True elif s == ""--hadoopNamenode"" : i += 1 if i >= len ( argv ) : usage ( ) g_hadoop_namenode = argv [ i ] elif s == ""--perf"" : g_perf = True i += 1 if i >= len ( argv ) : usage ( ) g_git_hash = argv [ i ] i += 1 if i >= len ( argv ) : usage ( ) g_git_branch = argv [ i ] i += 1 if i >= len ( argv ) : usage ( ) g_build_id = argv [ i ] i += 1 if i >= len ( argv ) : usage ( ) g_job_name = argv [ i ] elif s == ""--geterrs"" : g_use_xml2 = True elif s == ""--test_ssl"" : g_test_ssl = True elif s == '--ldap.config' : i += 1 if i >= len ( argv ) : usage ( ) g_ldap_config = argv [ i ] elif s == '--ldap.username' : i += 1 if i >= len ( argv ) : usage ( ) g_ldap_username = argv [ i ] elif s == '--ldap.password' : i += 1 if i >= len ( argv ) : usage ( ) g_ldap_password = argv [ i ] elif s == '--kerb.principal' : i += 1 if i >= len ( argv ) : usage ( ) g_kerb_principal = argv [ i ] elif s == '--jvm.opts' : i += 1 if i >= len ( argv ) : usage ( ) g_jvm_opts = argv [ i ] elif s == '--restLog' : g_rest_log = True else : unknown_arg ( s ) i += 1 if int ( g_use_client ) + int ( g_use_cloud ) + int ( g_use_cloud2 ) > 1 : print ( """" ) print ( ""ERROR: --client, --usecloud and --usecloud2 are mutually exclusive."" ) print ( """" ) sys . exit ( 1 )",Parse the arguments into globals ( ain t this an ugly duckling? ) .
"def wipe_output_dir ( ) : print ( ""Wiping output directory."" ) try : if os . path . exists ( g_output_dir ) : shutil . rmtree ( str ( g_output_dir ) ) except OSError as e : print ( ""ERROR: Removing output directory %s failed: "" % g_output_dir ) print ( ""       (errno {0}): {1}"" . format ( e . errno , e . strerror ) ) print ( """" ) sys . exit ( 1 )",Clear the output directory .
"def remove_sandbox ( parent_dir , dir_name ) : if ""Rsandbox"" in dir_name : rsandbox_dir = os . path . join ( parent_dir , dir_name ) try : if sys . platform == ""win32"" : os . system ( r'C:/cygwin64/bin/rm.exe -r -f ""{0}""' . format ( rsandbox_dir ) ) else : shutil . rmtree ( rsandbox_dir ) except OSError as e : print ( """" ) print ( ""ERROR: Removing RSandbox directory failed: "" + rsandbox_dir ) print ( ""       (errno {0}): {1}"" . format ( e . errno , e . strerror ) ) print ( """" ) sys . exit ( 1 )",This function is written to remove sandbox directories if they exist under the parent_dir .
"def main ( argv ) : global g_script_name global g_num_clouds global g_nodes_per_cloud global g_output_dir global g_test_to_run global g_test_list_file global g_exclude_list_file global g_test_group global g_runner global g_nopass global g_nointernal global g_path_to_tar global g_path_to_whl global g_perf global g_git_hash global g_git_branch global g_machine_ip global g_date global g_build_id global g_ncpu global g_os global g_job_name global g_test_ssl g_script_name = os . path . basename ( argv [ 0 ] ) test_root_dir = os . path . realpath ( os . getcwd ( ) ) g_output_dir = os . path . join ( test_root_dir , str ( ""results"" ) ) g_failed_output_dir = os . path . join ( g_output_dir , str ( ""failed"" ) ) testreport_dir = os . path . join ( test_root_dir , str ( ""../build/test-results"" ) ) parse_args ( argv ) h2o_jar = g_path_to_h2o_jar if h2o_jar is None : possible_h2o_jar_parent_dir = test_root_dir while True : possible_h2o_jar_dir = os . path . join ( possible_h2o_jar_parent_dir , ""build"" ) possible_h2o_jar = os . path . join ( possible_h2o_jar_dir , ""h2o.jar"" ) if os . path . exists ( possible_h2o_jar ) : h2o_jar = possible_h2o_jar break next_possible_h2o_jar_parent_dir = os . path . dirname ( possible_h2o_jar_parent_dir ) if next_possible_h2o_jar_parent_dir == possible_h2o_jar_parent_dir : break possible_h2o_jar_parent_dir = next_possible_h2o_jar_parent_dir if g_wipe_output_dir : wipe_output_dir ( ) if g_wipe_test_state : wipe_test_state ( test_root_dir ) if g_test_to_run is not None : g_num_clouds = 1 g_runner = TestRunner ( test_root_dir , g_use_cloud , g_use_cloud2 , g_use_client , g_config , g_use_ip , g_use_port , g_num_clouds , g_nodes_per_cloud , h2o_jar , g_base_port , g_jvm_xmx , g_jvm_cp , g_output_dir , g_failed_output_dir , g_path_to_tar , g_path_to_whl , g_produce_unit_reports , testreport_dir , g_r_pkg_ver_chk , g_hadoop_namenode , g_on_hadoop , g_perf , g_test_ssl , g_ldap_config , g_jvm_opts ) if g_exclude_list_file is not None : g_runner . read_exclude_list_file ( g_exclude_list_file ) if g_test_to_run is not None : g_runner . add_test ( g_test_to_run ) elif g_test_list_file is not None : g_runner . read_test_list_file ( g_test_list_file ) else : g_runner . build_test_list ( g_test_group , g_run_small , g_run_medium , g_run_large , g_run_xlarge , g_nopass , g_nointernal ) if g_no_run : sys . exit ( 0 ) signal . signal ( signal . SIGINT , signal_handler ) signal . signal ( signal . SIGTERM , signal_handler ) if not ( h2o_jar and os . path . exists ( h2o_jar ) ) : print ( """" ) print ( ""ERROR: H2O jar not found"" ) print ( """" ) sys . exit ( 1 ) try : g_runner . start_clouds ( ) g_runner . run_tests ( g_nopass ) finally : g_runner . check_clouds ( ) g_runner . stop_clouds ( ) g_runner . report_summary ( g_nopass ) if not g_runner . get_regression_passed ( ) : sys . exit ( 1 )",Main program . : param argv Command - line arguments : return none
"def start ( self ) : if self . is_client : main_class = ""water.H2OClientApp"" else : main_class = ""water.H2OApp"" if ""JAVA_HOME"" in os . environ : java = os . environ [ ""JAVA_HOME"" ] + ""/bin/java"" else : java = ""java"" classpath_sep = "";"" if sys . platform == ""win32"" else "":"" classpath = self . h2o_jar if self . cp == """" else self . h2o_jar + classpath_sep + self . cp cmd = [ java , ""-Xmx"" + self . xmx , ""-ea"" ] if self . jvm_opts is not None : cmd += [ self . jvm_opts ] cmd += [ ""-cp"" , classpath , main_class , ""-name"" , self . cloud_name , ""-port"" , str ( self . port ) , ""-ip"" , self . ip ] if self . flatfile is not None : cmd += [ ""-flatfile"" , self . flatfile ] if self . ldap_config_path is not None : cmd . append ( '-login_conf' ) cmd . append ( self . ldap_config_path ) cmd . append ( '-ldap_login' ) if g_jacoco_include : root_dir = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , "".."" ) ) agent_dir = os . path . join ( root_dir , ""jacoco"" , ""jacocoagent.jar"" ) jresults_dir = os . path . join ( self . output_dir , ""jacoco"" ) if not os . path . exists ( jresults_dir ) : os . mkdir ( jresults_dir ) jresults_dir = os . path . join ( jresults_dir , ""{cloud}_{node}"" . format ( cloud = self . cloud_num , node = self . node_num ) ) jacoco = ""-javaagent:"" + agent_dir + ""=destfile="" + os . path . join ( jresults_dir , ""{cloud}_{node}.exec"" . format ( cloud = self . cloud_num , node = self . node_num ) ) opt0 , opt1 = g_jacoco_options if opt0 is not None : jacoco += "",includes={inc}"" . format ( inc = opt0 . replace ( ',' , ':' ) ) if opt1 is not None : jacoco += "",excludes={ex}"" . format ( ex = opt1 . replace ( ',' , ':' ) ) cmd = cmd [ : 1 ] + [ jacoco ] + cmd [ 1 : ] if self . test_ssl : cmd . append ( ""-internal_security_conf"" ) if g_convenient : cmd . append ( ""../h2o-algos/src/test/resources/ssl.properties"" ) else : cmd . append ( ""../../../h2o-algos/src/test/resources/ssl3.properties"" ) self . output_file_name = os . path . join ( self . output_dir , ""java_"" + str ( self . cloud_num ) + ""_"" + str ( self . node_num ) + "".out.txt"" ) f = open ( self . output_file_name , ""w"" ) if g_convenient : cwd = os . getcwd ( ) here = os . path . abspath ( os . path . dirname ( __file__ ) ) there = os . path . abspath ( os . path . join ( here , "".."" ) ) os . chdir ( there ) self . child = subprocess . Popen ( args = cmd , stdout = f , stderr = subprocess . STDOUT , cwd = there ) os . chdir ( cwd ) else : try : self . child = subprocess . Popen ( args = cmd , stdout = f , stderr = subprocess . STDOUT , cwd = self . output_dir ) self . pid = self . child . pid print ( ""+ CMD: "" + ' ' . join ( cmd ) ) except OSError : raise ""Failed to spawn %s in %s"" % ( cmd , self . output_dir )",Start one node of H2O . ( Stash away the self . child and self . pid internally here . )
"def scrape_port_from_stdout ( self ) : regex = re . compile ( r""Open H2O Flow in your web browser: https?://([^:]+):(\d+)"" ) retries_left = 30 while retries_left and not self . terminated : with open ( self . output_file_name , ""r"" ) as f : for line in f : mm = re . search ( regex , line ) if mm is not None : self . port = mm . group ( 2 ) print ( ""H2O cloud %d node %d listening on port %s\n    with output file %s"" % ( self . cloud_num , self . node_num , self . port , self . output_file_name ) ) return if self . terminated : break retries_left -= 1 time . sleep ( 1 ) if self . terminated : return print ( ""\nERROR: Too many retries starting cloud %d.\nCheck the output log %s.\n"" % ( self . cloud_num , self . output_file_name ) ) sys . exit ( 1 )",Look at the stdout log and figure out which port the JVM chose .
"def scrape_cloudsize_from_stdout ( self , nodes_per_cloud ) : retries = 60 while retries > 0 : if self . terminated : return f = open ( self . output_file_name , ""r"" ) s = f . readline ( ) while len ( s ) > 0 : if self . terminated : return match_groups = re . search ( r""Cloud of size (\d+) formed"" , s ) if match_groups is not None : size = match_groups . group ( 1 ) if size is not None : size = int ( size ) if size == nodes_per_cloud : f . close ( ) return s = f . readline ( ) f . close ( ) retries -= 1 if self . terminated : return time . sleep ( 1 ) print ( """" ) print ( ""ERROR: Too many retries starting cloud."" ) print ( """" ) sys . exit ( 1 )",Look at the stdout log and wait until the cluster of proper size is formed . This call is blocking . Exit if this fails .
"def stop ( self ) : if self . pid > 0 : print ( ""Killing JVM with PID {}"" . format ( self . pid ) ) try : self . child . terminate ( ) self . child . wait ( ) except OSError : pass self . pid = - 1",Normal node shutdown . Ignore failures for now .
def start ( self ) : for node in self . nodes : node . start ( ) for node in self . client_nodes : node . start ( ),Start H2O cluster . The cluster is not up until wait_for_cloud_to_be_up () is called and returns .
def stop ( self ) : for node in self . nodes : node . stop ( ) for node in self . client_nodes : node . stop ( ),Normal cluster shutdown .
def terminate ( self ) : for node in self . client_nodes : node . terminate ( ) for node in self . nodes : node . terminate ( ),Terminate a running cluster . ( Due to a signal . )
def get_ip ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_ip ( ),Return an ip to use to talk to this cluster .
def get_port ( self ) : if len ( self . client_nodes ) > 0 : node = self . client_nodes [ 0 ] else : node = self . nodes [ 0 ] return node . get_port ( ),Return a port to use to talk to this cluster .
"def get_file_out ( build_index , python_name , jenkin_name ) : global g_log_base_dir global g_jenkins_url global g_log_base_dir directoryB = g_log_base_dir + '/Build' + str ( build_index ) if not ( os . path . isdir ( directoryB ) ) : os . mkdir ( directoryB ) url_string_full = g_jenkins_url + '/' + str ( build_index ) + jenkin_name filename = os . path . join ( directoryB , python_name ) full_command = 'curl ' + url_string_full + ' > ' + filename subprocess . call ( full_command , shell = True )",This function will grab one log file from Jenkins and save it to local user directory : param g_jenkins_url : : param build_index : : param airline_java : : param airline_java_tail : : return :
"def main ( argv ) : global g_log_base_dir global g_airline_java global g_milsongs_java global g_airline_python global g_milsongs_python global g_jenkins_url global g_airline_py_tail global g_milsongs_py_tail global g_airline_java_tail global g_milsongs_java_tail if len ( argv ) < 9 : print ""python grabGLRMrunLogs logsBaseDirectory airlineJavaFileNameWithPath milsongJavaFileNameWithPath "" ""airlinePyunitWithPath airlinePyunitWithPath jenkinsJobURL startBuild# endBuild#.\n"" sys . exit ( 1 ) else : g_log_base_dir = argv [ 1 ] g_jenkins_url = argv [ 2 ] g_airline_java = argv [ 3 ] g_milsongs_java = argv [ 4 ] g_airline_python = argv [ 5 ] g_milsongs_python = argv [ 6 ] start_number = int ( argv [ 7 ] ) end_number = int ( argv [ 8 ] ) if ( start_number > end_number ) : print ""startBuild# must be <= end_number"" sys . exit ( 1 ) else : for build_index in range ( start_number , end_number + 1 ) : get_file_out ( build_index , g_airline_java , g_airline_java_tail ) get_file_out ( build_index , g_milsongs_java , g_milsongs_java_tail ) get_file_out ( build_index , g_airline_python , g_airline_py_tail ) get_file_out ( build_index , g_milsongs_python , g_milsongs_py_tail )",Main program .
"def plot ( self , timestep = ""AUTO"" , metric = ""AUTO"" , server = False , * * kwargs ) : assert_is_type ( metric , ""AUTO"" , ""logloss"" , ""auc"" , ""classification_error"" , ""rmse"" ) if self . _model_json [ ""algo"" ] in ( ""deeplearning"" , ""deepwater"" , ""xgboost"" , ""drf"" , ""gbm"" ) : if metric == ""AUTO"" : metric = ""logloss"" self . _plot ( timestep = timestep , metric = metric , server = server )",Plot training set ( and validation set if available ) scoring history for an H2OBinomialModel .
"def roc ( self , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in viewitems ( tm ) : if v is not None : m [ k ] = ( v . fprs , v . tprs ) return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m",Return the coordinates of the ROC curve for a given set of data .
"def find_idx_by_threshold ( self , threshold , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in viewitems ( tm ) : m [ k ] = None if v is None else v . find_idx_by_threshold ( threshold ) return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m",Retrieve the index in this metric s threshold list at which the given threshold is located .
def from_external ( external = H2OFrame ) : w2v_model = H2OWord2vecEstimator ( pre_trained = external ) w2v_model . train ( ) return w2v_model,Creates new H2OWord2vecEstimator based on an external model . : param external : H2OFrame with an external model : return : H2OWord2vecEstimator instance representing the external model
"def _determine_vec_size ( self ) : first_column = self . pre_trained . types [ self . pre_trained . columns [ 0 ] ] if first_column != 'string' : raise H2OValueError ( ""First column of given pre_trained model %s is required to be a String"" , self . pre_trained . frame_id ) if list ( self . pre_trained . types . values ( ) ) . count ( 'string' ) > 1 : raise H2OValueError ( ""There are multiple columns in given pre_trained model %s with a String type."" , self . pre_trained . frame_id ) self . vec_size = self . pre_trained . dim [ 1 ] - 1",Determines vec_size for a pre - trained model after basic model verification .
"def h2o_mean_absolute_error ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) return _colmean ( ( y_predicted - y_actual ) . abs ( ) )",Mean absolute error regression loss .
"def h2o_mean_squared_error ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) return _colmean ( ( y_predicted - y_actual ) ** 2 )",Mean squared error regression loss
"def h2o_median_absolute_error ( y_actual , y_predicted ) : ModelBase . _check_targets ( y_actual , y_predicted ) return ( y_predicted - y_actual ) . abs ( ) . median ( )",Median absolute error regression loss
"def h2o_explained_variance_score ( y_actual , y_predicted , weights = None ) : ModelBase . _check_targets ( y_actual , y_predicted ) _ , numerator = _mean_var ( y_actual - y_predicted , weights ) _ , denominator = _mean_var ( y_actual , weights ) if denominator == 0.0 : return 1. if numerator == 0 else 0. return 1 - numerator / denominator",Explained variance regression score function .
"def h2o_r2_score ( y_actual , y_predicted , weights = 1. ) : ModelBase . _check_targets ( y_actual , y_predicted ) numerator = ( weights * ( y_actual - y_predicted ) ** 2 ) . sum ( ) denominator = ( weights * ( y_actual - _colmean ( y_actual ) ) ** 2 ) . sum ( ) if denominator == 0.0 : return 1. if numerator == 0. else 0. return 1 - numerator / denominator",R - squared ( coefficient of determination ) regression score function
"def plot ( self , timestep = ""AUTO"" , metric = ""AUTO"" , * * kwargs ) : if self . _model_json [ ""algo"" ] in ( ""deeplearning"" , ""deepwater"" , ""xgboost"" , ""drf"" , ""gbm"" ) : if metric == ""AUTO"" : metric = ""rmse"" elif metric not in ( ""rmse"" , ""deviance"" , ""mae"" ) : raise ValueError ( ""metric for H2ORegressionModel must be one of: AUTO, rmse, deviance, or mae"" ) self . _plot ( timestep = timestep , metric = metric , * * kwargs )",Plots training set ( and validation set if available ) scoring history for an H2ORegressionModel . The timestep and metric arguments are restricted to what is available in its scoring history .
"def assert_is_type ( var , * types , * * kwargs ) : assert types , ""The list of expected types was not provided"" expected_type = types [ 0 ] if len ( types ) == 1 else U ( * types ) if _check_type ( var , expected_type ) : return assert set ( kwargs ) . issubset ( { ""message"" , ""skip_frames"" } ) , ""Unexpected keyword arguments: %r"" % kwargs message = kwargs . get ( ""message"" , None ) skip_frames = kwargs . get ( ""skip_frames"" , 1 ) args = _retrieve_assert_arguments ( ) vname = args [ 0 ] etn = _get_type_name ( expected_type , dump = "", "" . join ( args [ 1 : ] ) ) vtn = _get_type_name ( type ( var ) ) raise H2OTypeError ( var_name = vname , var_value = var , var_type_name = vtn , exp_type_name = etn , message = message , skip_frames = skip_frames )",Assert that the argument has the specified type .
"def assert_matches ( v , regex ) : m = re . match ( regex , v ) if m is None : vn = _retrieve_assert_arguments ( ) [ 0 ] message = ""Argument `{var}` (= {val!r}) did not match /{regex}/"" . format ( var = vn , regex = regex , val = v ) raise H2OValueError ( message , var_name = vn , skip_frames = 1 ) return m",Assert that string variable matches the provided regular expression .
"def assert_satisfies ( v , cond , message = None ) : if not cond : vname , vexpr = _retrieve_assert_arguments ( ) if not message : message = ""Argument `{var}` (= {val!r}) does not satisfy the condition {expr}"" . format ( var = vname , val = v , expr = vexpr ) raise H2OValueError ( message = message , var_name = vname , skip_frames = 1 )",Assert that variable satisfies the provided condition .
"def _retrieve_assert_arguments ( ) : try : raise RuntimeError ( ""Catch me!"" ) except RuntimeError : tb = sys . exc_info ( ) [ 2 ] assert tb . tb_frame . f_code . co_name == ""_retrieve_assert_arguments"" this_filename = tb . tb_frame . f_code . co_filename fr = tb . tb_frame while fr is not None and fr . f_code . co_filename == this_filename : fr = fr . f_back try : with io . open ( fr . f_code . co_filename , ""r"" , encoding = ""utf-8"" ) as f : for i in range ( fr . f_lineno - 1 ) : next ( f ) g = tokenize . generate_tokens ( f . readline ) step = 0 args_tokens = [ ] level = 0 for ttt in g : if step == 0 : if ttt [ 0 ] != tokenize . NAME : continue if not ttt [ 1 ] . startswith ( ""assert_"" ) : continue step = 1 elif step == 1 : assert ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == ""("" args_tokens . append ( [ ] ) step = 2 elif step == 2 : if level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == "","" : args_tokens . append ( [ ] ) elif level == 0 and ttt [ 0 ] == tokenize . OP and ttt [ 1 ] == "")"" : break else : if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in ""([{"" : level += 1 if ttt [ 0 ] == tokenize . OP and ttt [ 1 ] in "")]}"" : level -= 1 assert level >= 0 , ""Parse error: parentheses level became negative"" args_tokens [ - 1 ] . append ( ttt ) args = [ tokenize . untokenize ( at ) . strip ( ) . replace ( ""\n"" , "" "" ) for at in args_tokens ] return args except IOError : return ""arg"" ,",Magic variable name retrieval .
"def _check_type ( var , vtype ) : if vtype is None : return var is None if isinstance ( vtype , _primitive_type ) : return var == vtype if vtype is str : return isinstance ( var , _str_type ) if vtype is int : return isinstance ( var , _int_type ) if vtype is numeric : return isinstance ( var , _num_type ) if isinstance ( vtype , MagicType ) : return vtype . check ( var ) if isinstance ( vtype , type ) : return isinstance ( var , vtype ) if isinstance ( vtype , list ) : elem_type = U ( * vtype ) return isinstance ( var , list ) and all ( _check_type ( item , elem_type ) for item in var ) if isinstance ( vtype , set ) : elem_type = U ( * vtype ) return isinstance ( var , set ) and all ( _check_type ( item , elem_type ) for item in var ) if isinstance ( vtype , tuple ) : return ( isinstance ( var , tuple ) and len ( vtype ) == len ( var ) and all ( _check_type ( var [ i ] , vtype [ i ] ) for i in range ( len ( vtype ) ) ) ) if isinstance ( vtype , dict ) : ttkv = U ( * viewitems ( vtype ) ) return isinstance ( var , dict ) and all ( _check_type ( kv , ttkv ) for kv in viewitems ( var ) ) if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : return vtype ( var ) raise RuntimeError ( ""Ivalid type %r in _check_type()"" % vtype )",Return True if the variable is of the specified type and False otherwise .
"def _get_type_name ( vtype , dump = None ) : if vtype is None : return ""None"" if vtype is str : return ""string"" if vtype is int : return ""integer"" if vtype is numeric : return ""numeric"" if is_type ( vtype , str ) : return '""%s""' % repr ( vtype ) [ 1 : - 1 ] if is_type ( vtype , int ) : return str ( vtype ) if isinstance ( vtype , MagicType ) : return vtype . name ( dump ) if isinstance ( vtype , type ) : return vtype . __name__ if isinstance ( vtype , list ) : return ""list(%s)"" % _get_type_name ( U ( * vtype ) , dump ) if isinstance ( vtype , set ) : return ""set(%s)"" % _get_type_name ( U ( * vtype ) , dump ) if isinstance ( vtype , tuple ) : return ""(%s)"" % "", "" . join ( _get_type_name ( item , dump ) for item in vtype ) if isinstance ( vtype , dict ) : return ""dict(%s)"" % "", "" . join ( ""%s: %s"" % ( _get_type_name ( tk , dump ) , _get_type_name ( tv , dump ) ) for tk , tv in viewitems ( vtype ) ) if isinstance ( vtype , ( FunctionType , BuiltinFunctionType ) ) : if vtype . __name__ == ""<lambda>"" : return _get_lambda_source_code ( vtype , dump ) else : return vtype . __name__ raise RuntimeError ( ""Unexpected `vtype`: %r"" % vtype )",Return the name of the provided type .
"def _get_lambda_source_code ( lambda_fn , src ) : def gen_lambdas ( ) : def gen ( ) : yield src + ""\n"" g = gen ( ) step = 0 tokens = [ ] for tok in tokenize . generate_tokens ( getattr ( g , ""next"" , getattr ( g , ""__next__"" , None ) ) ) : if step == 0 : if tok [ 0 ] == tokenize . NAME and tok [ 1 ] == ""lambda"" : step = 1 tokens = [ tok ] level = 0 elif step == 1 : if tok [ 0 ] == tokenize . NAME : tokens . append ( tok ) step = 2 else : step = 0 elif step == 2 : if tok [ 0 ] == tokenize . OP and tok [ 1 ] == "":"" : tokens . append ( tok ) step = 3 else : step = 0 elif step == 3 : if level == 0 and ( tok [ 0 ] == tokenize . OP and tok [ 1 ] in "",)"" or tok [ 0 ] == tokenize . ENDMARKER ) : yield tokenize . untokenize ( tokens ) . strip ( ) step = 0 else : tokens . append ( tok ) if tok [ 0 ] == tokenize . OP : if tok [ 1 ] in ""[({"" : level += 1 if tok [ 1 ] in ""])}"" : level -= 1 assert not tokens actual_code = lambda_fn . __code__ . co_code for lambda_src in gen_lambdas ( ) : try : fn = eval ( lambda_src , globals ( ) , locals ( ) ) if fn . __code__ . co_code == actual_code : return lambda_src . split ( "":"" , 1 ) [ 1 ] . strip ( ) except Exception : pass return ""<lambda>""",Attempt to find the source code of the lambda_fn within the string src .
"def name ( self , src = None ) : res = [ _get_type_name ( tt , src ) for tt in self . _types ] if len ( res ) == 2 and ""None"" in res : res . remove ( ""None"" ) return ""?"" + res [ 0 ] else : return "" | "" . join ( res )",Return string representing the name of this type .
"def check ( self , var ) : return all ( _check_type ( var , tt ) for tt in self . _types )",Return True if the variable matches this type and False otherwise .
"def name ( self , src = None ) : return "" & "" . join ( _get_type_name ( tt , src ) for tt in self . _types )",Return string representing the name of this type .
"def check ( self , var ) : return not any ( _check_type ( var , tt ) for tt in self . _types )",Return True if the variable does not match any of the types and False otherwise .
"def name ( self , src = None ) : if len ( self . _types ) > 1 : return ""!(%s)"" % str ( ""|"" . join ( _get_type_name ( tt , src ) for tt in self . _types ) ) else : return ""!"" + _get_type_name ( self . _types [ 0 ] , src )",Return string representing the name of this type .
"def check ( self , var ) : return isinstance ( var , tuple ) and all ( _check_type ( t , self . _element_type ) for t in var )",Return True if the variable matches this type and False otherwise .
"def check ( self , var ) : if not isinstance ( var , dict ) : return False if any ( key not in self . _types for key in var ) : return False for key , ktype in viewitems ( self . _types ) : val = var . get ( key , None ) if not _check_type ( val , ktype ) : return False return True",Return True if the variable matches this type and False otherwise .
"def name ( self , src = None ) : return ""{%s}"" % "", "" . join ( ""%s: %s"" % ( key , _get_type_name ( ktype , src ) ) for key , ktype in viewitems ( self . _types ) )",Return string representing the name of this type .
"def check ( self , var ) : return ( isinstance ( var , _int_type ) and ( self . _lower_bound is None or var >= self . _lower_bound ) and ( self . _upper_bound is None or var <= self . _upper_bound ) )",Return True if the variable matches the specified type .
"def name ( self , src = None ) : if self . _upper_bound is None and self . _lower_bound is None : return ""int"" if self . _upper_bound is None : if self . _lower_bound == 1 : return ""int>0"" return ""int%d"" % s lf._ l ower_bound if self . _lower_bound is None : return ""int%d"" % s lf._ u pper_bound return ""int[%d%d]"" % ( e lf._ l ower_bound,  s lf._ u pper_bound)",Return string representing the name of this type .
"def check ( self , var ) : return ( isinstance ( var , _num_type ) and ( self . _lower_bound is None or var >= self . _lower_bound ) and ( self . _upper_bound is None or var <= self . _upper_bound ) )",Return True if the variable matches the specified type .
"def check ( self , var ) : if self . _class is None : self . _init ( ) return self . _class and self . _checker ( var , self . _class )",Return True if the variable matches this type and False otherwise .
"def check ( self , var ) : if not isinstance ( var , _str_type ) : return False return _enum_mangle ( var ) in self . _consts",Check whether the provided value is a valid enum constant .
def get_config ( ) : self = H2OConfigReader . _get_instance ( ) if not self . _config_loaded : self . _read_config ( ) return self . _config,Retrieve the config as a dictionary of key - value pairs .
"def _read_config ( self ) : self . _config_loaded = True conf = [ ] for f in self . _candidate_log_files ( ) : if os . path . isfile ( f ) : self . _logger . info ( ""Reading config file %s"" % f ) section_rx = re . compile ( r""^\[(\w+)\]$"" ) keyvalue_rx = re . compile ( r""^(\w+:)?([\w.]+)\s*=(.*)$"" ) with io . open ( f , ""rt"" , encoding = ""utf-8"" ) as config_file : section_name = None for lineno , line in enumerate ( config_file ) : line = line . strip ( ) if line == """" or line . startswith ( ""#"" ) : continue m1 = section_rx . match ( line ) if m1 : section_name = m1 . group ( 1 ) continue m2 = keyvalue_rx . match ( line ) if m2 : lng = m2 . group ( 1 ) key = m2 . group ( 2 ) val = m2 . group ( 3 ) . strip ( ) if lng and lng . lower ( ) != ""py:"" : continue if section_name : key = section_name + ""."" + key if key in H2OConfigReader . _allowed_config_keys : conf . append ( ( key , val ) ) else : self . _logger . error ( ""Key %s is not a valid config key"" % key ) continue self . _logger . error ( ""Syntax error in config file line %d: %s"" % ( lineno , line ) ) self . _config = dict ( conf ) return",Find and parse config file storing all variables in self . _config .
"def _candidate_log_files ( ) : relpath = "".h2oconfig"" prevpath = None while True : abspath = os . path . abspath ( relpath ) if abspath == prevpath : break prevpath = abspath relpath = ""../"" + relpath yield abspath yield os . path . expanduser ( ""~/.h2oconfig"" )",Return possible locations for the . h2oconfig file one at a time .
"def execute ( self , progress_fn , print_verbose_info = None ) : assert_is_type ( progress_fn , FunctionType , GeneratorType , MethodType ) if isinstance ( progress_fn , GeneratorType ) : progress_fn = ( lambda g : lambda : next ( g ) ) ( progress_fn ) self . _next_poll_time = 0 self . _t0 = time . time ( ) self . _x0 = 0 self . _v0 = 0.01 self . _ve = 0.01 progress = 0 status = None try : while True : now = time . time ( ) if self . _next_poll_time <= now : res = progress_fn ( ) assert_is_type ( res , ( numeric , numeric ) , numeric ) if not isinstance ( res , tuple ) : res = ( res , - 1 ) now = time . time ( ) self . _store_model_progress ( res , now ) self . _recalculate_model_parameters ( now ) progress = min ( self . _compute_progress_at_time ( now ) [ 0 ] , 1 ) if progress == 1 and self . _get_real_progress ( ) >= 1 : break result = self . _widget . render ( progress ) assert_is_type ( result , RenderResult ) time0 = result . next_time time1 = self . _get_time_at_progress ( result . next_progress ) next_render_time = min ( time0 , time1 ) self . _draw ( result . rendered ) wait_time = min ( next_render_time , self . _next_poll_time ) - now if wait_time > 0 : time . sleep ( wait_time ) if print_verbose_info is not None : print_verbose_info ( progress ) except KeyboardInterrupt : status = ""cancelled"" except StopIteration as e : status = str ( e ) result = self . _widget . render ( progress = progress , status = status ) self . _draw ( result . rendered , final = True ) if status == ""cancelled"" : raise StopIteration ( status )",Start the progress bar and return only when the progress reaches 100% .
"def _store_model_progress ( self , res , now ) : raw_progress , delay = res raw_progress = clamp ( raw_progress , 0 , self . _maxval ) self . _progress_data . append ( ( now , raw_progress ) ) if delay < 0 : delay = self . _guess_next_poll_interval ( ) self . _next_poll_time = now + clamp ( delay , self . MIN_PROGRESS_CHECK_INTERVAL , self . MAX_PROGRESS_CHECK_INTERVAL )",Save the current model progress into self . _progress_data and update self . _next_poll_time .
"def _recalculate_model_parameters ( self , now ) : time_until_end = self . _estimate_progress_completion_time ( now ) - now assert time_until_end >= 0 , ""Estimated progress completion cannot be in the past."" x_real = self . _get_real_progress ( ) if x_real == 1 : t0 , x0 , v0 , ve = now , 1 , 0 , 0 else : x0 , v0 = self . _compute_progress_at_time ( now ) t0 = now if x0 >= 1 : t0 , x0 , v0 = self . _t0 , self . _x0 , self . _v0 time_until_end += now - t0 z = self . BETA * time_until_end max_speed = ( 1 - x_real ** 2 ) / self . FINISH_DELAY ve = v0 + ( self . BETA * ( 1 - x0 ) - v0 * z ) / ( z - 1 + math . exp ( - z ) ) if ve < 0 : v0 = self . BETA * ( 1 - x0 ) / ( 1 - math . exp ( - z ) ) ve = 0 if ve > max_speed : ve = max_speed self . _t0 , self . _x0 , self . _v0 , self . _ve = t0 , x0 , v0 , ve",Compute t0 x0 v0 ve .
"def _estimate_progress_completion_time ( self , now ) : assert self . _next_poll_time >= now tlast , wlast = self . _progress_data [ - 1 ] if wlast == self . _maxval : current_completion_time = ( 1 - self . _x0 ) / self . _v0 + self . _t0 return clamp ( current_completion_time , now , now + self . FINISH_DELAY ) tacc , wacc = 0 , 0 factor = self . GAMMA for t , x in self . _progress_data [ - 2 : : - 1 ] : tacc += factor * ( tlast - t ) wacc += factor * ( wlast - x ) factor *= self . GAMMA if factor < 1e-2 : break if wacc == 0 : return now + 300 t_estimate = tlast + tacc * ( self . _maxval - wlast ) / wacc if t_estimate <= self . _next_poll_time : t_estimate = self . _next_poll_time + self . FINISH_DELAY return t_estimate",Estimate the moment when the underlying process is expected to reach completion .
"def _guess_next_poll_interval ( self ) : time_elapsed = self . _progress_data [ - 1 ] [ 0 ] - self . _progress_data [ 0 ] [ 0 ] real_progress = self . _get_real_progress ( ) return min ( 0.2 * time_elapsed , 0.5 + ( 1 - real_progress ) ** 0.5 )",Determine when to query the progress status next .
"def _compute_progress_at_time ( self , t ) : t0 , x0 , v0 , ve = self . _t0 , self . _x0 , self . _v0 , self . _ve z = ( v0 - ve ) * math . exp ( - self . BETA * ( t - t0 ) ) vt = ve + z xt = clamp ( x0 + ve * ( t - t0 ) + ( v0 - ve - z ) / self . BETA , 0 , 1 ) return xt , vt",Calculate the modelled progress state for the given time moment .
"def _get_time_at_progress ( self , x_target ) : t , x , v = self . _t0 , self . _x0 , self . _v0 for _ in range ( 20 ) : if v == 0 : return 1e20 t += ( x_target - x ) / v x , v = self . _compute_progress_at_time ( t ) if abs ( x - x_target ) < 1e-3 : return t return time . time ( ) + 100",Return the projected time when progress level x_target will be reached .
"def _draw ( self , txt , final = False ) : if not self . _file_mode : sys . stdout . write ( ""\r"" ) sys . stdout . write ( txt ) if final and not isinstance ( self . _widget , _HiddenWidget ) : sys . stdout . write ( ""\n"" ) else : if not self . _file_mode : sys . stdout . write ( ""\r"" ) sys . stdout . flush ( )",Print the rendered string to the stdout .
"def render ( self , progress , width = None , status = None ) : results = [ widget . render ( progress , width = self . _widget_lengths [ i ] , status = status ) for i , widget in enumerate ( self . _widgets ) ] if self . _file_mode : res = """" for i , result in enumerate ( results ) : res += result . rendered if result . length < self . _widget_lengths [ i ] and progress < 1 : break res += "" "" if i < len ( results ) - 1 else """" rendered_str = res [ len ( self . _rendered ) : ] self . _rendered = res else : rendered_str = "" "" . join ( r . rendered for r in results ) if self . _to_render : rendered_str = self . _to_render + rendered_str self . _to_render = None next_progress = min ( r . next_progress for r in results ) next_time = min ( r . next_time for r in results ) return RenderResult ( rendered_str , next_progress = next_progress , next_time = next_time )",Render the widget .
"def _compute_widget_sizes ( self ) : wl = [ 0 ] * len ( self . _widgets ) flex_count = 0 for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : flex_count += 1 else : wl [ i ] = widget . render ( 1 ) . length remaining_width = self . _width - sum ( wl ) remaining_width -= len ( self . _widgets ) - 1 if remaining_width < 10 * flex_count : if self . _file_mode : remaining_width = 10 * flex_count else : widget0 = self . _widgets [ 0 ] if isinstance ( widget0 , PBWString ) and remaining_width + widget0 . render ( 0 ) . length >= 10 * flex_count : remaining_width += widget0 . render ( 0 ) . length + 1 self . _to_render = widget0 . render ( 0 ) . rendered + ""\n"" self . _widgets = self . _widgets [ 1 : ] if remaining_width < 10 * flex_count : self . _file_mode = True remaining_width = 10 * flex_count remaining_width = max ( remaining_width , 10 * flex_count ) for i , widget in enumerate ( self . _widgets ) : if isinstance ( widget , ProgressBarFlexibleWidget ) : target_length = int ( remaining_width / flex_count ) result = widget . render ( 1 , target_length ) wl [ i ] = result . length remaining_width -= result . length flex_count -= 1 return wl",Initial rendering stage done in order to compute widths of all widgets .
"def _get_terminal_size ( ) : if not sys . stdout . isatty ( ) : return 80 try : import subprocess ret = subprocess . check_output ( [ ""stty"" , ""size"" ] ) . strip ( ) . split ( "" "" ) if len ( ret ) == 2 : return int ( ret [ 1 ] ) except : pass try : from termios import TIOCGWINSZ from fcntl import ioctl from struct import unpack res = unpack ( ""hh"" , ioctl ( sys . stdout , TIOCGWINSZ , b""1234"" ) ) return int ( res [ 1 ] ) except : pass return int ( os . environ . get ( ""COLUMNS"" , 80 ) )",Find current STDOUT s width in characters .
"def render ( self , progress , width = None , status = None ) : if width <= 3 : return RenderResult ( ) bar_width = width - 2 n_chars = int ( progress * bar_width + 0.001 ) endf , endl = self . _bar_ends if self . _file_mode : out = endf out += self . _bar_symbols [ - 1 ] * n_chars out += endl if progress == 1 else """" if status : out += "" (%s)"" % status next_progress = ( n_chars + 1 ) / bar_width rendered_len = len ( out ) else : frac_chars = int ( ( progress * bar_width - n_chars ) * len ( self . _bar_symbols ) ) out = endf out += self . _bar_symbols [ - 1 ] * n_chars out += self . _bar_symbols [ frac_chars - 1 ] if frac_chars > 0 else """" rendered_len = len ( out ) if status : out += colorama . Fore . RED + "" ("" + status + "")"" + colorama . Style . RESET_ALL rendered_len += 3 + len ( status ) out += "" "" * ( width - 1 - rendered_len ) out += endl next_progress = ( n_chars + ( frac_chars + 1 ) / len ( self . _bar_symbols ) ) / bar_width rendered_len += max ( 0 , width - 1 - rendered_len ) + 1 return RenderResult ( rendered = out , length = rendered_len , next_progress = next_progress )",Render the widget .
"def set_encoding ( self , encoding ) : self . _bar_ends = ""[]"" self . _bar_symbols = ""#"" if not encoding : return s1 = ""\u258F\u258E\u258D\u258C\u258B\u258A\u2589\u2588"" s2 = ""\u258C\u2588"" s3 = ""\u2588"" if self . _file_mode : s1 = s2 = None assert len ( s3 ) == 1 for s in ( s1 , s2 , s3 ) : if s is None : continue try : s . encode ( encoding ) self . _bar_ends = ""||"" self . _bar_symbols = s return except UnicodeEncodeError : pass except LookupError : print ( ""Warning: unknown encoding %s"" % encoding )",Inform the widget about the encoding of the underlying character stream .
"def render ( self , progress , width = None , status = None ) : current_pct = int ( progress * 100 + 0.1 ) return RenderResult ( rendered = ""%3d%%"" % current_pct , next_progress = ( current_pct + 1 ) / 100 )",Render the widget .
"def genDataFrame ( sizeMat , lowBound , uppderBound , numRep , numZeros , numNans , numInfs ) : if ( numNans > 0 ) : floatA = [ float ( 'NaN' ) ] * numNans intA = [ float ( 'NaN' ) ] * numNans if ( numInfs > 0 ) : floatA . extend ( [ float ( 'inf' ) ] * numInfs ) intA . extend ( [ float ( 'inf' ) ] * numInfs ) floatA . extend ( [ - 1.0 * float ( 'inf' ) ] * numInfs ) intA . extend ( [ - 1 * float ( 'inf' ) ] * numInfs ) for index in range ( numZeros ) : floatA . append ( 0.0 * randint ( - 1 , 1 ) ) intA . append ( 0 * randint ( - 1 , 1 ) ) for rad in sizeMat : tempInt = pow ( 2 , rad ) tempIntN = pow ( 2 , rad + 1 ) intA . append ( tempInt ) intA . append ( - 1 * tempInt ) randInt = randint ( tempInt , tempIntN ) intA . append ( randInt ) intA . append ( - 1 * randInt ) intA . append ( randint ( tempInt , tempIntN ) ) intA . append ( - 1 * ( randint ( tempInt , tempIntN ) ) ) floatA . append ( tempInt * 1.0 ) floatA . append ( - 1.0 * tempInt ) tempD = uniform ( tempInt , tempIntN ) floatA . append ( tempD ) floatA . append ( - 1.0 * tempD ) floatA . append ( uniform ( tempInt , tempIntN ) ) floatA . append ( - 1.0 * uniform ( tempInt , tempIntN ) )",This function will generate an H2OFrame of two columns . One column will be float and the other will be long . : param sizeMat : integer denoting size of bounds : param lowBound : lower bound : param uppderBound : : param trueRandom : : param numRep : number of times to repeat arrays in order to generate duplicated rows : param numZeros : : param numNans : : param numInfs : : return :
"def fit ( self , frame = None ) : self . _teColumns = list ( map ( lambda i : frame . names [ i ] , self . _teColumns ) ) if all ( isinstance ( n , int ) for n in self . _teColumns ) else self . _teColumns self . _responseColumnName = frame . names [ self . _responseColumnName ] if isinstance ( self . _responseColumnName , int ) else self . _responseColumnName self . _foldColumnName = frame . names [ self . _foldColumnName ] if isinstance ( self . _foldColumnName , int ) else self . _foldColumnName self . _encodingMap = ExprNode ( ""target.encoder.fit"" , frame , self . _teColumns , self . _responseColumnName , self . _foldColumnName ) . _eager_map_frame ( ) return self . _encodingMap",Returns encoding map as an object that maps column_name - > frame_with_encoding_map_for_this_column_name
"def transform ( self , frame = None , holdout_type = None , noise = - 1 , seed = - 1 ) : assert_is_type ( holdout_type , ""kfold"" , ""loo"" , ""none"" ) assert self . _encodingMap . map_keys [ 'string' ] == self . _teColumns encodingMapKeys = self . _encodingMap . map_keys [ 'string' ] encodingMapFramesKeys = list ( map ( lambda x : x [ 'key' ] [ 'name' ] , self . _encodingMap . frames ) ) return H2OFrame . _expr ( expr = ExprNode ( ""target.encoder.transform"" , encodingMapKeys , encodingMapFramesKeys , frame , self . _teColumns , holdout_type , self . _responseColumnName , self . _foldColumnName , self . _blending , self . _inflectionPoint , self . _smoothing , noise , seed ) )",Apply transformation to te_columns based on the encoding maps generated during TargetEncoder . fit () call . You must not pass encodings manually from . fit () method because they are being stored internally after . fit () had been called .
"def generatePandaEnumCols ( pandaFtrain , cname , nrows , domainL ) : import numpy as np import pandas as pd cmissingNames = [ cname + "".missing(NA)"" ] tempnp = np . zeros ( ( nrows , 1 ) , dtype = np . int ) colVals = pandaFtrain [ cname ] for ind in range ( nrows ) : try : if not ( colVals [ ind ] in domainL ) : tempnp [ ind ] = 1 except ValueError : pass zeroFrame = pd . DataFrame ( tempnp ) zeroFrame . columns = cmissingNames temp = pd . get_dummies ( pandaFtrain [ cname ] , prefix = cname , drop_first = False ) tempNames = list ( temp ) colLength = len ( tempNames ) newNames = [ 'a' ] * colLength for ind in range ( 0 , colLength ) : newNames [ ind ] = cname + ""_"" + domainL [ ind ] ftemp = temp [ newNames ] ctemp = pd . concat ( [ ftemp , zeroFrame ] , axis = 1 ) return ctemp",For an H2O Enum column we perform one - hot - encoding here and add one more column missing ( NA ) to it .
"def get_frame ( frame_id , rows = 10 , rows_offset = 0 , cols = - 1 , full_cols = - 1 , cols_offset = 0 , light = False ) : fr = H2OFrame ( ) fr . _ex . _cache . _id = frame_id try : fr . _ex . _cache . fill ( rows = rows , rows_offset = rows_offset , cols = cols , full_cols = full_cols , cols_offset = cols_offset , light = light ) except EnvironmentError : return None return fr",Retrieve an existing H2OFrame from the H2O cluster using the frame s id .
def refresh ( self ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ),Reload frame information from the backend H2O server .
def names ( self ) : if not self . _ex . _cache . names_valid ( ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ) return list ( self . _ex . _cache . names ),The list of column names ( List [ str ] ) .
def nrows ( self ) : if not self . _ex . _cache . nrows_valid ( ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ) return self . _ex . _cache . nrows,Number of rows in the dataframe ( int ) .
def ncols ( self ) : if not self . _ex . _cache . ncols_valid ( ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ) return self . _ex . _cache . ncols,Number of columns in the dataframe ( int ) .
def types ( self ) : if not self . _ex . _cache . types_valid ( ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ) return dict ( self . _ex . _cache . types ),The dictionary of column name / type pairs .
"def type ( self , col ) : assert_is_type ( col , int , str ) if not self . _ex . _cache . types_valid ( ) or not self . _ex . _cache . names_valid ( ) : self . _ex . _cache . flush ( ) self . _frame ( fill_cache = True ) types = self . _ex . _cache . types if is_type ( col , str ) : if col in types : return types [ col ] else : names = self . _ex . _cache . names if - len ( names ) <= col < len ( names ) : return types [ names [ col ] ] raise H2OValueError ( ""Column '%r' does not exist in the frame"" % col )",The type for the given column .
"def columns_by_type ( self , coltype = ""numeric"" ) : assert_is_type ( coltype , ""numeric"" , ""categorical"" , ""string"" , ""time"" , ""uuid"" , ""bad"" ) assert_is_type ( self , H2OFrame ) return ExprNode ( ""columnsByType"" , self , coltype ) . _eager_scalar ( )",Extract columns of the specified type from the frame .
"def show ( self , use_pandas = False , rows = 10 , cols = 200 ) : if self . _ex is None : print ( ""This H2OFrame has been removed."" ) return if not self . _has_content ( ) : print ( ""This H2OFrame is empty and not initialized."" ) return if self . nrows == 0 : print ( ""This H2OFrame is empty."" ) return if not self . _ex . _cache . is_valid ( ) : self . _frame ( ) . _ex . _cache . fill ( ) if H2ODisplay . _in_ipy ( ) : import IPython . display if use_pandas and can_use_pandas ( ) : IPython . display . display ( self . head ( rows = rows , cols = cols ) . as_data_frame ( fill_cache = True ) ) else : IPython . display . display_html ( self . _ex . _cache . _tabulate ( ""html"" , False ) , raw = True ) else : if use_pandas and can_use_pandas ( ) : print ( self . head ( rows = rows , cols = cols ) . as_data_frame ( True ) ) else : s = self . __unicode__ ( ) stk = traceback . extract_stack ( ) if ""IPython"" in stk [ - 3 ] [ 0 ] : s = ""\n%s"" % s try : print ( s ) except UnicodeEncodeError : print ( s . encode ( ""ascii"" , ""replace"" ) )",Used by the H2OFrame . __repr__ method to print or display a snippet of the data frame .
"def summary ( self , return_data = False ) : if not self . _has_content ( ) : print ( ""This H2OFrame is empty and not initialized."" ) return self . _ex . _cache . _data if not self . _ex . _cache . is_valid ( ) : self . _frame ( ) . _ex . _cache . fill ( ) if not return_data : if self . nrows == 0 : print ( ""This H2OFrame is empty."" ) elif H2ODisplay . _in_ipy ( ) : import IPython . display IPython . display . display_html ( self . _ex . _cache . _tabulate ( ""html"" , True ) , raw = True ) else : print ( self . _ex . _cache . _tabulate ( ""simple"" , True ) ) else : return self . _ex . _cache . _data",Display summary information about the frame .
"def describe ( self , chunk_summary = False ) : if self . _has_content ( ) : res = h2o . api ( ""GET /3/Frames/%s"" % self . frame_id , data = { ""row_count"" : 10 } ) [ ""frames"" ] [ 0 ] self . _ex . _cache . _fill_data ( res ) print ( ""Rows:{}"" . format ( self . nrow ) ) print ( ""Cols:{}"" . format ( self . ncol ) ) if chunk_summary : res [ ""chunk_summary"" ] . show ( ) res [ ""distribution_summary"" ] . show ( ) print ( ""\n"" ) self . summary ( )",Generate an in - depth description of this H2OFrame .
"def head ( self , rows = 10 , cols = 200 ) : assert_is_type ( rows , int ) assert_is_type ( cols , int ) nrows = min ( self . nrows , rows ) ncols = min ( self . ncols , cols ) newdt = self [ : nrows , : ncols ] return newdt . _frame ( rows = nrows , cols = cols , fill_cache = True )",Return the first rows and cols of the frame as a new H2OFrame .
"def mult ( self , matrix ) : if self . ncols != matrix . nrows : raise H2OValueError ( ""Matrix is not compatible for multiplication with the current frame"" ) return H2OFrame . _expr ( expr = ExprNode ( ""x"" , self , matrix ) )",Multiply this frame viewed as a matrix by another matrix .
"def moment ( year = None , month = None , day = None , hour = None , minute = None , second = None , msec = None , date = None , time = None ) : assert_is_type ( date , None , datetime . date , numpy_datetime , pandas_timestamp ) assert_is_type ( time , None , datetime . time ) assert_is_type ( year , None , int , H2OFrame ) assert_is_type ( month , None , int , H2OFrame ) assert_is_type ( day , None , int , H2OFrame ) assert_is_type ( hour , None , int , H2OFrame ) assert_is_type ( minute , None , int , H2OFrame ) assert_is_type ( second , None , int , H2OFrame ) assert_is_type ( msec , None , int , H2OFrame ) if time is not None : if hour is not None or minute is not None or second is not None or msec is not None : raise H2OValueError ( ""Arguments hour, minute, second, msec cannot be used together with `time`."" ) hour = time . hour minute = time . minute second = time . second msec = time . microsecond // 1000 if date is not None : if is_type ( date , pandas_timestamp ) : date = date . to_pydatetime ( ) if is_type ( date , numpy_datetime ) : date = date . astype ( ""M8[ms]"" ) . astype ( ""O"" ) if year is not None or month is not None or day is not None : raise H2OValueError ( ""Arguments year, month and day cannot be used together with `date`."" ) year = date . year month = date . month day = date . day if isinstance ( date , datetime . datetime ) : if time is not None : raise H2OValueError ( ""Argument `time` cannot be used together with `date` of datetime type."" ) if hour is not None or minute is not None or second is not None or msec is not None : raise H2OValueError ( ""Arguments hour, minute, second, msec cannot be used together with `date` "" ""of datetime type."" ) hour = date . hour minute = date . minute second = date . second msec = date . microsecond // 1000 if year is None or month is None or day is None : raise H2OValueError ( ""Either arguments (`year`, `month` and `day`) or the `date` are required."" ) if hour is None : hour = 0 if minute is None : minute = 0 if second is None : second = 0 if msec is None : msec = 0 local_vars = locals ( ) res_nrows = None for n in [ ""year"" , ""month"" , ""day"" , ""hour"" , ""minute"" , ""second"" , ""msec"" ] : x = local_vars [ n ] if isinstance ( x , H2OFrame ) : if x . ncols != 1 : raise H2OValueError ( ""Argument `%s` is a frame with more than 1 column"" % n ) if x . type ( 0 ) not in { ""int"" , ""real"" } : raise H2OValueError ( ""Column `%s` is not numeric (type = %s)"" % ( n , x . type ( 0 ) ) ) if res_nrows is None : res_nrows = x . nrows if x . nrows == 0 or x . nrows != res_nrows : raise H2OValueError ( ""Incompatible column `%s` having %d rows"" % ( n , x . nrows ) ) if res_nrows is None : res_nrows = 1 res = H2OFrame . _expr ( ExprNode ( ""moment"" , year , month , day , hour , minute , second , msec ) ) res . _ex . _cache . _names = [ ""name"" ] res . _ex . _cache . _types = { ""name"" : ""time"" } res . _ex . _cache . _nrows = res_nrows res . _ex . _cache . _ncols = 1 return res",Create a time column from individual components .
"def levels ( self ) : lol = H2OFrame . _expr ( expr = ExprNode ( ""levels"" , self ) ) . as_data_frame ( False ) lol . pop ( 0 ) lol = list ( zip ( * lol ) ) return [ [ ll for ll in l if ll != '' ] for l in lol ]",Get the factor levels .
def nlevels ( self ) : levels = self . levels ( ) return [ len ( l ) for l in levels ] if levels else 0,Get the number of factor levels for each categorical column .
"def set_level ( self , level ) : return H2OFrame . _expr ( expr = ExprNode ( ""setLevel"" , self , level ) , cache = self . _ex . _cache )",A method to set all column values to one of the levels .
"def set_levels ( self , levels ) : assert_is_type ( levels , [ str ] ) return H2OFrame . _expr ( expr = ExprNode ( ""setDomain"" , self , False , levels ) , cache = self . _ex . _cache )",Replace the levels of a categorical column .
"def rename ( self , columns = None ) : assert_is_type ( columns , None , dict ) new_names = self . names ncols = self . ncols for col , name in columns . items ( ) : col_index = None if is_type ( col , int ) and ( - ncols <= col < ncols ) : col_index = ( col + ncols ) % ncols elif is_type ( col , str ) and col in self . names : col_index = self . names . index ( col ) if col_index is not None : new_names [ col_index ] = name return self . set_names ( new_names )",Change names of columns in the frame .
"def set_names ( self , names ) : assert_is_type ( names , [ str ] ) assert_satisfies ( names , len ( names ) == self . ncol ) self . _ex = ExprNode ( ""colnames="" , self , range ( self . ncol ) , names ) return self",Change names of all columns in the frame .
"def set_name ( self , col = None , name = None ) : assert_is_type ( col , None , int , str ) assert_is_type ( name , str ) ncols = self . ncols col_index = None if is_type ( col , int ) : if not ( - ncols <= col < ncols ) : raise H2OValueError ( ""Index %d is out of bounds for a frame with %d columns"" % ( col , ncols ) ) col_index = ( col + ncols ) % ncols elif is_type ( col , str ) : if col not in self . names : raise H2OValueError ( ""Column %s doesn't exist in the frame."" % col ) col_index = self . names . index ( col ) else : assert col is None if ncols != 1 : raise H2OValueError ( ""The frame has %d columns; please specify which one to rename"" % ncols ) col_index = 0 if name != self . names [ col_index ] and name in self . types : raise H2OValueError ( ""Column '%s' already exists in the frame"" % name ) oldname = self . names [ col_index ] old_cache = self . _ex . _cache self . _ex = ExprNode ( ""colnames="" , self , col_index , name ) self . _ex . _cache . fill_from ( old_cache ) if self . names is None : self . _frame ( ) . _ex . _cache . fill ( ) else : self . _ex . _cache . _names = self . names [ : col_index ] + [ name ] + self . names [ col_index + 1 : ] self . _ex . _cache . _types [ name ] = self . _ex . _cache . _types . pop ( oldname ) return",Set a new name for a column .
"def cumsum ( self , axis = 0 ) : return H2OFrame . _expr ( expr = ExprNode ( ""cumsum"" , self , axis ) , cache = self . _ex . _cache )",Compute cumulative sum over rows / columns of the frame .
"def isin ( self , item ) : if is_type ( item , list , tuple , set ) : if self . ncols == 1 and ( self . type ( 0 ) == 'str' or self . type ( 0 ) == 'enum' ) : return self . match ( item ) else : return functools . reduce ( H2OFrame . __or__ , ( self == i for i in item ) ) else : return self == item",Test whether elements of an H2OFrame are contained in the item .
"def modulo_kfold_column ( self , n_folds = 3 ) : return H2OFrame . _expr ( expr = ExprNode ( ""modulo_kfold_column"" , self , n_folds ) ) . _frame ( )",Build a fold assignments column for cross - validation .
"def stratified_kfold_column ( self , n_folds = 3 , seed = - 1 ) : return H2OFrame . _expr ( expr = ExprNode ( ""stratified_kfold_column"" , self , n_folds , seed ) ) . _frame ( )",Build a fold assignment column with the constraint that each fold has the same class distribution as the fold column .
"def structure ( self ) : df = self . as_data_frame ( use_pandas = False ) cn = df . pop ( 0 ) nr = self . nrow nc = self . ncol width = max ( [ len ( c ) for c in cn ] ) isfactor = self . isfactor ( ) numlevels = self . nlevels ( ) lvls = self . levels ( ) print ( ""H2OFrame: '{}' \nDimensions: {} obs. of {} variables"" . format ( self . frame_id , nr , nc ) ) for i in range ( nc ) : print ( ""$ {} {}: "" . format ( cn [ i ] , ' ' * ( width - max ( 0 , len ( cn [ i ] ) ) ) ) , end = ' ' ) if isfactor [ i ] : nl = numlevels [ i ] print ( ""Factor w/ {} level(s) {} "" . format ( nl , '""' + '"",""' . join ( lvls [ i ] ) + '""' ) , end = '\n' ) else : print ( ""num {}"" . format ( "" "" . join ( it [ 0 ] if it else ""nan"" for it in h2o . as_list ( self [ : 10 , i ] , False ) [ 1 : ] ) ) )",Compactly display the internal structure of an H2OFrame .
"def as_data_frame ( self , use_pandas = True , header = True ) : if can_use_pandas ( ) and use_pandas : import pandas return pandas . read_csv ( StringIO ( self . get_frame_data ( ) ) , low_memory = False , skip_blank_lines = False ) from h2o . utils . csv . readers import reader frame = [ row for row in reader ( StringIO ( self . get_frame_data ( ) ) ) ] if not header : frame . pop ( 0 ) return frame",Obtain the dataset as a python - local object .
"def drop ( self , index , axis = 1 ) : if axis == 1 : if not isinstance ( index , list ) : if is_type ( index , str ) : if index not in self . names : raise H2OValueError ( ""Column(s) selected to drop are not in original frame: %r"" % index ) index = self . names . index ( index ) elif is_type ( index , int ) : if index > self . ncol : raise H2OValueError ( ""Column index selected to drop is not part of the frame: %r"" % index ) if index < 0 : raise H2OValueError ( ""Column index selected to drop is not positive: %r"" % index ) fr = H2OFrame . _expr ( expr = ExprNode ( ""cols"" , self , - ( index + 1 ) ) , cache = self . _ex . _cache ) fr . _ex . _cache . ncols -= 1 fr . _ex . _cache . names = self . names [ : index ] + self . names [ index + 1 : ] fr . _ex . _cache . types = { name : self . types [ name ] for name in fr . _ex . _cache . names } return fr elif isinstance ( index , list ) : if is_type ( index , [ int ] ) : if max ( index ) > self . ncol : raise H2OValueError ( ""Column index selected to drop is not part of the frame: %r"" % index ) if min ( index ) < 0 : raise H2OValueError ( ""Column index selected to drop is not positive: %r"" % index ) for i in range ( len ( index ) ) : index [ i ] = - ( index [ i ] + 1 ) elif is_type ( index , [ str ] ) : if not set ( index ) . issubset ( self . names ) : raise H2OValueError ( ""Column(s) selected to drop are not in original frame: %r"" % index ) for i in range ( len ( index ) ) : index [ i ] = - ( self . names . index ( index [ i ] ) + 1 ) fr = H2OFrame . _expr ( expr = ExprNode ( ""cols"" , self , index ) , cache = self . _ex . _cache ) fr . _ex . _cache . ncols -= len ( index ) fr . _ex . _cache . names = [ i for i in self . names if self . names . index ( i ) not in list ( map ( lambda x : abs ( x ) - 1 , index ) ) ] fr . _ex . _cache . types = { name : fr . types [ name ] for name in fr . _ex . _cache . names } else : raise ValueError ( ""Invalid column index types. Must either be a list of all int indexes, "" ""a string list of all column names, a single int index, or"" ""a single string for dropping columns."" ) return fr elif axis == 0 : if is_type ( index , [ int ] ) : if max ( index ) > self . nrow : raise H2OValueError ( ""Row index selected to drop is not part of the frame: %r"" % index ) if min ( index ) < 0 : raise H2OValueError ( ""Row index selected to drop is not positive: %r"" % index ) index = [ - ( x + 1 ) for x in index ] fr = H2OFrame . _expr ( expr = ExprNode ( ""rows"" , self , index ) , cache = self . _ex . _cache ) fr . _ex . _cache . nrows -= len ( index ) else : raise ValueError ( ""Invalid row indexes. Must be a list of int row indexes to drop from the H2OFrame."" ) return fr",Drop a single column or row or a set of columns or rows from a H2OFrame .
"def pop ( self , i ) : if is_type ( i , str ) : i = self . names . index ( i ) col = H2OFrame . _expr ( expr = ExprNode ( ""cols"" , self , i ) ) old_cache = self . _ex . _cache self . _ex = ExprNode ( ""cols"" , self , - ( i + 1 ) ) self . _ex . _cache . ncols -= 1 self . _ex . _cache . names = old_cache . names [ : i ] + old_cache . names [ i + 1 : ] self . _ex . _cache . types = { name : old_cache . types [ name ] for name in self . _ex . _cache . names } self . _ex . _cache . _data = None col . _ex . _cache . ncols = 1 col . _ex . _cache . names = [ old_cache . names [ i ] ] return col",Pop a column from the H2OFrame at index i .
"def quantile ( self , prob = None , combine_method = ""interpolate"" , weights_column = None ) : if len ( self ) == 0 : return self if prob is None : prob = [ 0.01 , 0.1 , 0.25 , 0.333 , 0.5 , 0.667 , 0.75 , 0.9 , 0.99 ] if weights_column is None : weights_column = ""_"" else : assert_is_type ( weights_column , str , I ( H2OFrame , lambda wc : wc . ncol == 1 and wc . nrow == self . nrow ) ) if isinstance ( weights_column , H2OFrame ) : merged = self . cbind ( weights_column ) weights_column = merged . names [ - 1 ] return H2OFrame . _expr ( expr = ExprNode ( ""quantile"" , merged , prob , combine_method , weights_column ) ) return H2OFrame . _expr ( expr = ExprNode ( ""quantile"" , self , prob , combine_method , weights_column ) )",Compute quantiles .
"def concat ( self , frames , axis = 1 ) : if len ( frames ) == 0 : raise ValueError ( ""Input list of frames is empty! Nothing to concat."" ) if axis == 1 : df = self . cbind ( frames ) else : df = self . rbind ( frames ) return df",Append multiple H2OFrames to this frame column - wise or row - wise .
"def cbind ( self , data ) : assert_is_type ( data , H2OFrame , numeric , [ H2OFrame , numeric ] ) frames = [ data ] if not isinstance ( data , list ) else data new_cols = list ( self . columns ) new_types = dict ( self . types ) for frame in frames : if isinstance ( frame , H2OFrame ) : if frame . nrow != self . nrow : raise H2OValueError ( ""Cannot bind a dataframe with %d rows to a data frame with %d rows: "" ""the number of rows should match"" % ( frame . nrow , self . nrow ) ) new_cols += frame . columns new_types . update ( frame . types ) else : new_cols += [ None ] unique_cols = set ( new_cols ) fr = H2OFrame . _expr ( expr = ExprNode ( ""cbind"" , self , * frames ) , cache = self . _ex . _cache ) fr . _ex . _cache . ncols = len ( new_cols ) if len ( new_cols ) == len ( unique_cols ) and None not in unique_cols : fr . _ex . _cache . names = new_cols fr . _ex . _cache . types = new_types else : fr . _ex . _cache . names = None fr . _ex . _cache . types = None return fr",Append data to this frame column - wise .
"def rbind ( self , data ) : assert_is_type ( data , H2OFrame , [ H2OFrame ] ) frames = [ data ] if not isinstance ( data , list ) else data for frame in frames : if frame . ncol != self . ncol : raise H2OValueError ( ""Cannot row-bind a dataframe with %d columns to a data frame with %d columns: "" ""the columns must match"" % ( frame . ncol , self . ncol ) ) if frame . columns != self . columns or frame . types != self . types : raise H2OValueError ( ""Column names and types must match for rbind() to work"" ) fr = H2OFrame . _expr ( expr = ExprNode ( ""rbind"" , self , * frames ) , cache = self . _ex . _cache ) fr . _ex . _cache . nrows = self . nrow + sum ( frame . nrow for frame in frames ) return fr",Append data to this frame row - wise .
"def split_frame ( self , ratios = None , destination_frames = None , seed = None ) : assert_is_type ( ratios , [ numeric ] , None ) assert_is_type ( destination_frames , [ str ] , None ) assert_is_type ( seed , int , None ) if ratios is None : ratios = [ 0.75 ] if not ratios : raise ValueError ( ""Ratios array may not be empty"" ) if destination_frames is not None : if len ( ratios ) + 1 != len ( destination_frames ) : raise ValueError ( ""The number of provided destination_frames must be one more "" ""than the number of provided ratios"" ) num_slices = len ( ratios ) + 1 boundaries = [ ] last_boundary = 0 i = 0 while i < num_slices - 1 : ratio = ratios [ i ] if ratio < 0 : raise ValueError ( ""Ratio must be greater than 0"" ) boundary = last_boundary + ratio if boundary >= 1.0 : raise ValueError ( ""Ratios must add up to less than 1.0"" ) boundaries . append ( boundary ) last_boundary = boundary i += 1 splits = [ ] tmp_runif = self . runif ( seed ) tmp_runif . frame_id = ""%s_splitter"" % _py_tmp_key ( h2o . connection ( ) . session_id ) i = 0 while i < num_slices : if i == 0 : upper_boundary = boundaries [ i ] tmp_slice = self [ ( tmp_runif <= upper_boundary ) , : ] elif i == num_slices - 1 : lower_boundary = boundaries [ i - 1 ] tmp_slice = self [ ( tmp_runif > lower_boundary ) , : ] else : lower_boundary = boundaries [ i - 1 ] upper_boundary = boundaries [ i ] tmp_slice = self [ ( ( tmp_runif > lower_boundary ) & ( tmp_runif <= upper_boundary ) ) , : ] if destination_frames is None : splits . append ( tmp_slice ) else : destination_frame_id = destination_frames [ i ] tmp_slice . frame_id = destination_frame_id splits . append ( tmp_slice ) i += 1 del tmp_runif return splits",Split a frame into distinct subsets of size determined by the given ratios .
"def group_by ( self , by ) : assert_is_type ( by , str , int , [ str , int ] ) return GroupBy ( self , by )",Return a new GroupBy object using this frame and the desired grouping columns .
"def sort ( self , by , ascending = [ ] ) : assert_is_type ( by , str , int , [ str , int ] ) if type ( by ) != list : by = [ by ] if type ( ascending ) != list : ascending = [ ascending ] ascendingI = [ 1 ] * len ( by ) for c in by : if self . type ( c ) not in [ ""enum"" , ""time"" , ""int"" , ""real"" , ""string"" ] : raise H2OValueError ( ""Sort by column: "" + str ( c ) + "" not of enum, time, int, real, or string type"" ) if len ( ascending ) > 0 : assert len ( ascending ) == len ( by ) , ""Sorting direction must be specified for each sorted column."" for index in range ( len ( by ) ) : ascendingI [ index ] = 1 if ascending [ index ] else - 1 return H2OFrame . _expr ( expr = ExprNode ( ""sort"" , self , by , ascendingI ) )",Return a new Frame that is sorted by column ( s ) in ascending order . A fully distributed and parallel sort . However the original frame can contain String columns but sorting cannot be done on String columns . Default sorting direction is ascending .
"def fillna ( self , method = ""forward"" , axis = 0 , maxlen = 1 ) : assert_is_type ( axis , 0 , 1 ) assert_is_type ( method , str ) assert_is_type ( maxlen , int ) return H2OFrame . _expr ( expr = ExprNode ( ""h2o.fillna"" , self , method , axis , maxlen ) )",Return a new Frame that fills NA along a given axis and along a given direction with a maximum fill length
"def impute ( self , column = - 1 , method = ""mean"" , combine_method = ""interpolate"" , by = None , group_by_frame = None , values = None ) : if is_type ( column , str ) : column = self . names . index ( column ) if is_type ( by , str ) : by = self . names . index ( by ) if values is None : values = ""_"" else : assert len ( values ) == len ( self . columns ) , ""Length of values does not match length of columns"" values2 = [ ] for i in range ( 0 , len ( values ) ) : if self . type ( i ) == ""enum"" : try : values2 . append ( self . levels ( ) [ i ] . index ( values [ i ] ) ) except : raise H2OValueError ( ""Impute value of: "" + values [ i ] + "" not found in existing levels of"" "" column: "" + self . col_names [ i ] ) else : values2 . append ( values [ i ] ) values = values2 if group_by_frame is None : group_by_frame = ""_"" self . _ex . _eager_frame ( ) if by is not None or group_by_frame is not ""_"" : res = H2OFrame . _expr ( expr = ExprNode ( ""h2o.impute"" , self , column , method , combine_method , by , group_by_frame , values ) ) . _frame ( ) else : res = ExprNode ( ""h2o.impute"" , self , column , method , combine_method , by , group_by_frame , values ) . _eager_scalar ( ) self . _ex . _cache . flush ( ) self . _ex . _cache . fill ( 10 ) return res",Impute missing values into the frame modifying it in - place .
"def merge ( self , other , all_x = False , all_y = False , by_x = None , by_y = None , method = ""auto"" ) : if by_x is None and by_y is None : common_names = list ( set ( self . names ) & set ( other . names ) ) if not common_names : raise H2OValueError ( ""No columns in common to merge on!"" ) if by_x is None : by_x = [ self . names . index ( c ) for c in common_names ] else : by_x = _getValidCols ( by_x , self ) if by_y is None : by_y = [ other . names . index ( c ) for c in common_names ] else : by_y = _getValidCols ( by_y , other ) return H2OFrame . _expr ( expr = ExprNode ( ""merge"" , self , other , all_x , all_y , by_x , by_y , method ) )",Merge two datasets based on common column names . We do not support all_x = True and all_y = True . Only one can be True or none is True . The default merge method is auto and it will default to the radix method . The radix method will return the correct merge result regardless of duplicated rows in the right frame . In addition the radix method can perform merge even if you have string columns in your frames . If there are duplicated rows in your rite frame they will not be included if you use the hash method . The hash method cannot perform merge if you have string columns in your left frame . Hence we consider the radix method superior to the hash method and is the default method to use .
"def relevel ( self , y ) : return H2OFrame . _expr ( expr = ExprNode ( ""relevel"" , self , quote ( y ) ) )",Reorder levels of an H2O factor for one single column of a H2O frame
"def insert_missing_values ( self , fraction = 0.1 , seed = None ) : kwargs = { } kwargs [ 'dataset' ] = self . frame_id kwargs [ 'fraction' ] = fraction if seed is not None : kwargs [ 'seed' ] = seed job = { } job [ 'job' ] = h2o . api ( ""POST /3/MissingInserter"" , data = kwargs ) H2OJob ( job , job_type = ( ""Insert Missing Values"" ) ) . poll ( ) self . _ex . _cache . flush ( ) return self",Insert missing values into the current frame modifying it in - place .
"def sum ( self , skipna = True , axis = 0 , * * kwargs ) : assert_is_type ( skipna , bool ) assert_is_type ( axis , 0 , 1 ) if ""na_rm"" in kwargs : warnings . warn ( ""Parameter na_rm is deprecated; use skipna instead"" , category = DeprecationWarning ) na_rm = kwargs . pop ( ""na_rm"" ) assert_is_type ( na_rm , bool ) skipna = na_rm return_frame = get_config_value ( ""general.allow_breaking_changes"" , False ) if ""return_frame"" in kwargs : return_frame = kwargs . pop ( ""return_frame"" ) assert_is_type ( return_frame , bool ) if kwargs : raise H2OValueError ( ""Unknown parameters %r"" % list ( kwargs ) ) if return_frame : return H2OFrame . _expr ( ExprNode ( ""sumaxis"" , self , skipna , axis ) ) else : return ExprNode ( ""sumNA"" if skipna else ""sum"" , self ) . _eager_scalar ( )",Compute the frame s sum by - column ( or by - row ) .
"def var ( self , y = None , na_rm = False , use = None ) : symmetric = False if y is None : y = self symmetric = True if use is None : use = ""complete.obs"" if na_rm else ""everything"" if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : return ExprNode ( ""var"" , self , y , use , symmetric ) . _eager_scalar ( ) return H2OFrame . _expr ( expr = ExprNode ( ""var"" , self , y , use , symmetric ) ) . _frame ( )",Compute the variance - covariance matrix of one or two H2OFrames .
"def cor ( self , y = None , na_rm = False , use = None ) : assert_is_type ( y , H2OFrame , None ) assert_is_type ( na_rm , bool ) assert_is_type ( use , None , ""everything"" , ""all.obs"" , ""complete.obs"" ) if y is None : y = self if use is None : use = ""complete.obs"" if na_rm else ""everything"" if self . nrow == 1 or ( self . ncol == 1 and y . ncol == 1 ) : return ExprNode ( ""cor"" , self , y , use ) . _eager_scalar ( ) return H2OFrame . _expr ( expr = ExprNode ( ""cor"" , self , y , use ) ) . _frame ( )",Compute the correlation matrix of one or two H2OFrames .
"def distance ( self , y , measure = None ) : assert_is_type ( y , H2OFrame ) if measure is None : measure = ""l2"" return H2OFrame . _expr ( expr = ExprNode ( ""distance"" , self , y , measure ) ) . _frame ( )",Compute a pairwise distance measure between all rows of two numeric H2OFrames .
"def strdistance ( self , y , measure = None , compare_empty = True ) : assert_is_type ( y , H2OFrame ) assert_is_type ( measure , Enum ( 'lv' , 'lcs' , 'qgram' , 'jaccard' , 'jw' , 'soundex' ) ) assert_is_type ( compare_empty , bool ) return H2OFrame . _expr ( expr = ExprNode ( ""strDistance"" , self , y , measure , compare_empty ) ) . _frame ( )",Compute element - wise string distances between two H2OFrames . Both frames need to have the same shape and only contain string / factor columns .
"def asfactor ( self ) : for colname in self . names : t = self . types [ colname ] if t not in { ""bool"" , ""int"" , ""string"" , ""enum"" } : raise H2OValueError ( ""Only 'int' or 'string' are allowed for "" ""asfactor(), got %s:%s "" % ( colname , t ) ) fr = H2OFrame . _expr ( expr = ExprNode ( ""as.factor"" , self ) , cache = self . _ex . _cache ) if fr . _ex . _cache . types_valid ( ) : fr . _ex . _cache . types = { name : ""enum"" for name in self . types } else : raise H2OTypeError ( ""Types are not available in result"" ) return fr",Convert columns in the current frame to categoricals .
"def categories ( self ) : if self . ncols != 1 : raise H2OValueError ( ""This operation only applies to a single factor column"" ) if self . types [ self . names [ 0 ] ] != ""enum"" : raise H2OValueError ( ""Input is not a factor. This operation only applies to a single factor column"" ) return self . levels ( ) [ 0 ]",Return the list of levels for an enum ( categorical ) column .
"def strsplit ( self , pattern ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""strsplit"" , self , pattern ) ) fr . _ex . _cache . nrows = self . nrow return fr",Split the strings in the target column on the given regular expression pattern .
"def tokenize ( self , split ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""tokenize"" , self , split ) ) return fr",Tokenize String
"def countmatches ( self , pattern ) : assert_is_type ( pattern , str , [ str ] ) fr = H2OFrame . _expr ( expr = ExprNode ( ""countmatches"" , self , pattern ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncols = self . ncol return fr",For each string in the frame count the occurrences of the provided pattern . If countmathces is applied to a frame all columns of the frame must be type string otherwise the returned frame will contain errors .
"def substring ( self , start_index , end_index = None ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""substring"" , self , start_index , end_index ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr",For each string return a new string that is a substring of the original string .
"def lstrip ( self , set = "" "" ) : if set is None : set = "" "" fr = H2OFrame . _expr ( expr = ExprNode ( ""lstrip"" , self , set ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr",Return a copy of the column with leading characters removed .
"def entropy ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""entropy"" , self ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr",For each string compute its Shannon entropy if the string is empty the entropy is 0 .
"def num_valid_substrings ( self , path_to_words ) : assert_is_type ( path_to_words , str ) fr = H2OFrame . _expr ( expr = ExprNode ( ""num_valid_substrings"" , self , path_to_words ) ) fr . _ex . _cache . nrows = self . nrow fr . _ex . _cache . ncol = self . ncol return fr",For each string find the count of all possible substrings with 2 characters or more that are contained in the line - separated text file whose path is given .
"def table ( self , data2 = None , dense = True ) : return H2OFrame . _expr ( expr = ExprNode ( ""table"" , self , data2 , dense ) ) if data2 is not None else H2OFrame . _expr ( expr = ExprNode ( ""table"" , self , dense ) )",Compute the counts of values appearing in a column or co - occurence counts between two columns .
"def hist ( self , breaks = ""sturges"" , plot = True , * * kwargs ) : server = kwargs . pop ( ""server"" ) if ""server"" in kwargs else False assert_is_type ( breaks , int , [ numeric ] , Enum ( ""sturges"" , ""rice"" , ""sqrt"" , ""doane"" , ""fd"" , ""scott"" ) ) assert_is_type ( plot , bool ) assert_is_type ( server , bool ) if kwargs : raise H2OValueError ( ""Unknown parameters to hist(): %r"" % kwargs ) hist = H2OFrame . _expr ( expr = ExprNode ( ""hist"" , self , breaks ) ) . _frame ( ) if plot : try : import matplotlib if server : matplotlib . use ( ""Agg"" , warn = False ) import matplotlib . pyplot as plt except ImportError : print ( ""ERROR: matplotlib is required to make the histogram plot. "" ""Set `plot` to False, if a plot is not desired."" ) return hist [ ""widths"" ] = hist [ ""breaks"" ] . difflag1 ( ) lefts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ ""breaks"" ] , use_pandas = False ) [ 2 : ] ] widths = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ ""widths"" ] , use_pandas = False ) [ 2 : ] ] counts = [ float ( c [ 0 ] ) for c in h2o . as_list ( hist [ ""counts"" ] , use_pandas = False ) [ 2 : ] ] plt . xlabel ( self . names [ 0 ] ) plt . ylabel ( ""Frequency"" ) plt . title ( ""Histogram of %s"" % self . names [ 0 ] ) plt . bar ( left = lefts , width = widths , height = counts , bottom = 0 ) if not server : plt . show ( ) else : hist [ ""density"" ] = hist [ ""counts"" ] / ( hist [ ""breaks"" ] . difflag1 ( ) * hist [ ""counts"" ] . sum ( ) ) return hist",Compute a histogram over a numeric column .
"def isax ( self , num_words , max_cardinality , optimize_card = False , * * kwargs ) : if num_words <= 0 : raise H2OValueError ( ""num_words must be greater than 0"" ) if max_cardinality <= 0 : raise H2OValueError ( ""max_cardinality must be greater than 0"" ) return H2OFrame . _expr ( expr = ExprNode ( ""isax"" , self , num_words , max_cardinality , optimize_card ) )",Compute the iSAX index for DataFrame which is assumed to be numeric time series data .
"def convert_H2OFrame_2_DMatrix ( self , predictors , yresp , h2oXGBoostModel ) : import xgboost as xgb import pandas as pd import numpy as np from scipy . sparse import csr_matrix assert isinstance ( predictors , list ) or isinstance ( predictors , tuple ) assert h2oXGBoostModel . _model_json [ 'algo' ] == 'xgboost' , ""convert_H2OFrame_2_DMatrix is used for H2OXGBoost model only."" tempFrame = self [ predictors ] . cbind ( self [ yresp ] ) colnames = tempFrame . names if type ( predictors [ 0 ] ) == type ( 1 ) : temp = [ ] for colInd in predictors : temp . append ( colnames [ colInd ] ) predictors = temp if ( type ( yresp ) == type ( 1 ) ) : tempy = colnames [ yresp ] yresp = tempy enumCols = [ ] enumColsIndices = [ ] typeDict = self . types for predName in predictors : if str ( typeDict [ predName ] ) == 'enum' : enumCols . append ( predName ) enumColsIndices . append ( colnames . index ( predName ) ) pandaFtrain = tempFrame . as_data_frame ( use_pandas = True , header = True ) nrows = tempFrame . nrow if len ( enumCols ) > 0 : allDomain = tempFrame . levels ( ) domainLen = [ ] for enumIndex in enumColsIndices : if len ( allDomain [ enumIndex ] ) > 0 : domainLen . append ( len ( allDomain [ enumIndex ] ) * - 1 ) incLevel = np . argsort ( domainLen ) c2 = tempFrame [ enumCols [ incLevel [ 0 ] ] ] tempFrame = tempFrame . drop ( enumCols [ incLevel [ 0 ] ] ) for index in range ( 1 , len ( incLevel ) ) : c2 = c2 . cbind ( tempFrame [ enumCols [ incLevel [ index ] ] ] ) tempFrame = tempFrame . drop ( enumCols [ incLevel [ index ] ] ) enumCols = c2 . names tempFrame = c2 . cbind ( tempFrame ) pandaFtrain = tempFrame . as_data_frame ( use_pandas = True , header = True ) pandaTrainPart = generatePandaEnumCols ( pandaFtrain , enumCols [ 0 ] , nrows , tempFrame [ enumCols [ 0 ] ] . categories ( ) ) pandaFtrain . drop ( [ enumCols [ 0 ] ] , axis = 1 , inplace = True ) for colInd in range ( 1 , len ( enumCols ) ) : cname = enumCols [ colInd ] ctemp = generatePandaEnumCols ( pandaFtrain , cname , nrows , tempFrame [ enumCols [ colInd ] ] . categories ( ) ) pandaTrainPart = pd . concat ( [ pandaTrainPart , ctemp ] , axis = 1 ) pandaFtrain . drop ( [ cname ] , axis = 1 , inplace = True ) pandaFtrain = pd . concat ( [ pandaTrainPart , pandaFtrain ] , axis = 1 ) c0 = tempFrame [ yresp ] . asnumeric ( ) . as_data_frame ( use_pandas = True , header = True ) pandaFtrain . drop ( [ yresp ] , axis = 1 , inplace = True ) pandaF = pd . concat ( [ c0 , pandaFtrain ] , axis = 1 ) pandaF . rename ( columns = { c0 . columns [ 0 ] : yresp } , inplace = True ) newX = list ( pandaFtrain . columns . values ) data = pandaF . as_matrix ( newX ) label = pandaF . as_matrix ( [ yresp ] ) return xgb . DMatrix ( data = csr_matrix ( data ) , label = label ) if h2oXGBoostModel . _model_json [ 'output' ] [ 'sparse' ] else xgb . DMatrix ( data = data , label = label )",This method requires that you import the following toolboxes : xgboost pandas numpy and scipy . sparse .
"def pivot ( self , index , column , value ) : assert_is_type ( index , str ) assert_is_type ( column , str ) assert_is_type ( value , str ) col_names = self . names if index not in col_names : raise H2OValueError ( ""Index not in H2OFrame"" ) if column not in col_names : raise H2OValueError ( ""Column not in H2OFrame"" ) if value not in col_names : raise H2OValueError ( ""Value column not in H2OFrame"" ) if self . type ( column ) not in [ ""enum"" , ""time"" , ""int"" ] : raise H2OValueError ( ""'column' argument is not type enum, time or int"" ) if self . type ( index ) not in [ ""enum"" , ""time"" , ""int"" ] : raise H2OValueError ( ""'index' argument is not type enum, time or int"" ) return H2OFrame . _expr ( expr = ExprNode ( ""pivot"" , self , index , column , value ) )",Pivot the frame designated by the three columns : index column and value . Index and column should be of type enum int or time . For cases of multiple indexes for a column label the aggregation method is to pick the first occurrence in the data frame
"def rank_within_group_by ( self , group_by_cols , sort_cols , ascending = [ ] , new_col_name = ""New_Rank_column"" , sort_cols_sorted = False ) : assert_is_type ( group_by_cols , str , int , [ str , int ] ) if type ( group_by_cols ) != list : group_by_cols = [ group_by_cols ] if type ( sort_cols ) != list : sort_cols = [ sort_cols ] if type ( ascending ) != list : ascending = [ ascending ] ascendingI = [ 1 ] * len ( sort_cols ) for c in sort_cols : if self . type ( c ) not in [ ""enum"" , ""time"" , ""int"" , ""real"" ] : raise H2OValueError ( ""Sort by column: "" + str ( c ) + "" not of enum, time, int or real type"" ) for c in group_by_cols : if self . type ( c ) not in [ ""enum"" , ""time"" , ""int"" , ""real"" ] : raise H2OValueError ( ""Group by column: "" + str ( c ) + "" not of enum, time, int or real type"" ) if len ( ascending ) > 0 : assert len ( ascending ) == len ( sort_cols ) , ""Sorting direction must be specified for each sorted column."" for index in range ( len ( sort_cols ) ) : ascendingI [ index ] = 1 if ascending [ index ] else - 1 finalSortedOrder = 0 if ( sort_cols_sorted ) : finalSortedOrder = 1 return H2OFrame . _expr ( expr = ExprNode ( ""rank_within_groupby"" , self , group_by_cols , sort_cols , ascendingI , new_col_name , finalSortedOrder ) )",This function will add a new column rank where the ranking is produced as follows : 1 . sorts the H2OFrame by columns sorted in by columns specified in group_by_cols and sort_cols in the directions specified by the ascending for the sort_cols . The sort directions for the group_by_cols are ascending only . 2 . A new rank column is added to the frame which will contain a rank assignment performed next . The user can choose to assign a name to this new column . The default name is New_Rank_column . 3 . For each groupby groups a rank is assigned to the row starting from 1 2 ... to the end of that group . 4 . If sort_cols_sorted is TRUE a final sort on the frame will be performed frame according to the sort_cols and the sort directions in ascending . If sort_cols_sorted is FALSE ( by default ) the frame from step 3 will be returned as is with no extra sort . This may provide a small speedup if desired .
"def topNBottomN ( self , column = 0 , nPercent = 10 , grabTopN = - 1 ) : assert ( nPercent >= 0 ) and ( nPercent <= 100.0 ) , ""nPercent must be between 0.0 and 100.0"" assert round ( nPercent * 0.01 * self . nrows ) > 0 , ""Increase nPercent.  Current value will result in top 0 row."" if isinstance ( column , int ) : if ( column < 0 ) or ( column >= self . ncols ) : raise H2OValueError ( ""Invalid column index H2OFrame"" ) else : colIndex = column else : col_names = self . names if column not in col_names : raise H2OValueError ( ""Column name not found H2OFrame"" ) else : colIndex = col_names . index ( column ) if not ( self [ colIndex ] . isnumeric ( ) ) : raise H2OValueError ( ""Wrong column type!  Selected column must be numeric."" ) return H2OFrame . _expr ( expr = ExprNode ( ""topn"" , self , colIndex , nPercent , grabTopN ) )",Given a column name or one column index a percent N this function will return the top or bottom N% of the values of the column of a frame . The column must be a numerical column . : param column : a string for column name or an integer index : param nPercent : a top or bottom percentage of the column values to return : param grabTopN : - 1 to grab bottom N percent and 1 to grab top N percent : returns : a H2OFrame containing two columns . The first column contains the original row indices where the top / bottom values are extracted from . The second column contains the values .
"def sub ( self , pattern , replacement , ignore_case = False ) : return H2OFrame . _expr ( expr = ExprNode ( ""replacefirst"" , self , pattern , replacement , ignore_case ) )",Substitute the first occurrence of pattern in a string with replacement .
"def interaction ( self , factors , pairwise , max_factors , min_occurrence , destination_frame = None ) : return h2o . interaction ( data = self , factors = factors , pairwise = pairwise , max_factors = max_factors , min_occurrence = min_occurrence , destination_frame = destination_frame )",Categorical Interaction Feature Creation in H2O .
"def toupper ( self ) : return H2OFrame . _expr ( expr = ExprNode ( ""toupper"" , self ) , cache = self . _ex . _cache )",Translate characters from lower to upper case for a particular column .
"def grep ( self , pattern , ignore_case = False , invert = False , output_logical = False ) : return H2OFrame . _expr ( expr = ExprNode ( ""grep"" , self , pattern , ignore_case , invert , output_logical ) )",Searches for matches to argument pattern within each element of a string column .
"def scale ( self , center = True , scale = True ) : return H2OFrame . _expr ( expr = ExprNode ( ""scale"" , self , center , scale ) , cache = self . _ex . _cache )",Center and / or scale the columns of the current frame .
"def signif ( self , digits = 6 ) : return H2OFrame . _expr ( expr = ExprNode ( ""signif"" , self , digits ) , cache = self . _ex . _cache )",Round doubles / floats to the given number of significant digits .
"def na_omit ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""na.omit"" , self ) , cache = self . _ex . _cache ) fr . _ex . _cache . nrows = - 1 return fr",Remove rows with NAs from the H2OFrame .
"def difflag1 ( self ) : if self . ncols > 1 : raise H2OValueError ( ""Only single-column frames supported"" ) if self . types [ self . columns [ 0 ] ] not in { ""real"" , ""int"" , ""bool"" } : raise H2OValueError ( ""Numeric column expected"" ) fr = H2OFrame . _expr ( expr = ExprNode ( ""difflag1"" , self ) , cache = self . _ex . _cache ) return fr",Conduct a diff - 1 transform on a numeric frame column .
"def isna ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""is.na"" , self ) ) fr . _ex . _cache . nrows = self . _ex . _cache . nrows fr . _ex . _cache . ncols = self . _ex . _cache . ncols if self . _ex . _cache . names : fr . _ex . _cache . names = [ ""isNA(%s)"" % n for n in self . _ex . _cache . names ] fr . _ex . _cache . types = { ""isNA(%s)"" % n : ""int"" for n in self . _ex . _cache . names } return fr",For each element in an H2OFrame determine if it is NA or not .
"def minute ( self ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""minute"" , self ) , cache = self . _ex . _cache ) if fr . _ex . _cache . types_valid ( ) : fr . _ex . _cache . types = { k : ""int"" for k in self . _ex . _cache . types . keys ( ) } return fr",Extract the minute part from a date column .
"def runif ( self , seed = None ) : fr = H2OFrame . _expr ( expr = ExprNode ( ""h2o.runif"" , self , - 1 if seed is None else seed ) ) fr . _ex . _cache . ncols = 1 fr . _ex . _cache . nrows = self . nrow return fr",Generate a column of random numbers drawn from a uniform distribution [ 0 1 ) and having the same data layout as the source frame .
"def stratified_split ( self , test_frac = 0.2 , seed = - 1 ) : return H2OFrame . _expr ( expr = ExprNode ( 'h2o.random_stratified_split' , self , test_frac , seed ) )",Construct a column that can be used to perform a random stratified split .
"def match ( self , table , nomatch = 0 ) : return H2OFrame . _expr ( expr = ExprNode ( ""match"" , self , table , nomatch , None ) )",Make a vector of the positions of ( first ) matches of its first argument in its second .
"def cut ( self , breaks , labels = None , include_lowest = False , right = True , dig_lab = 3 ) : assert_is_type ( breaks , [ numeric ] ) if self . ncols != 1 : raise H2OValueError ( ""Single-column frame is expected"" ) if self . types [ self . names [ 0 ] ] not in { ""int"" , ""real"" } : raise H2OValueError ( ""A numeric column is expected"" ) fr = H2OFrame . _expr ( expr = ExprNode ( ""cut"" , self , breaks , labels , include_lowest , right , dig_lab ) , cache = self . _ex . _cache ) fr . _ex . _cache . types = { k : ""enum"" for k in self . names } return fr",Cut a numeric vector into categorical buckets .
"def idxmax ( self , skipna = True , axis = 0 ) : return H2OFrame . _expr ( expr = ExprNode ( ""which.max"" , self , skipna , axis ) )",Get the index of the max value in a column or row
"def ifelse ( self , yes , no ) : return H2OFrame . _expr ( expr = ExprNode ( ""ifelse"" , self , yes , no ) )",Equivalent to [ y if t else n for t y n in zip ( self yes no ) ] .
"def apply ( self , fun = None , axis = 0 ) : from . astfun import lambda_to_expr assert_is_type ( axis , 0 , 1 ) assert_is_type ( fun , FunctionType ) assert_satisfies ( fun , fun . __name__ == ""<lambda>"" ) res = lambda_to_expr ( fun ) return H2OFrame . _expr ( expr = ExprNode ( ""apply"" , self , 1 + ( axis == 0 ) , * res ) )",Apply a lambda expression to an H2OFrame .
"def mktime ( year = 1970 , month = 0 , day = 0 , hour = 0 , minute = 0 , second = 0 , msec = 0 ) : return H2OFrame . _expr ( ExprNode ( ""mktime"" , year , month , day , hour , minute , second , msec ) )",Deprecated use : func : moment instead .
"def from_python ( python_obj , destination_frame = None , header = 0 , separator = "","" , column_names = None , column_types = None , na_strings = None ) : return H2OFrame ( python_obj , destination_frame , header , separator , column_names , column_types , na_strings )",[ DEPRECATED ] Use constructor H2OFrame () instead .
"def parse_text ( text ) : assert isinstance ( text , _str_type ) , ""`text` parameter should be a string, got %r"" % type ( text ) gen = iter ( text . splitlines ( True ) ) readline = gen . next if hasattr ( gen , ""next"" ) else gen . __next__ return Code ( _tokenize ( readline ) )",Parse code from a string of text .
"def parse_file ( filename ) : assert isinstance ( filename , _str_type ) , ""`filename` parameter should be a string, got %r"" % type ( filename ) with open ( filename , ""rt"" , encoding = ""utf-8"" ) as f : return Code ( _tokenize ( f . readline ) )",Parse the provided file and return Code object .
"def _tokenize ( readline ) : assert callable ( readline ) , ""`readline` should be a function"" tokens = [ Token ( tok ) for tok in tokenize . generate_tokens ( readline ) ] indents_stack = [ 0 ] for tok in tokens : if tok . op == INDENT : tok . pre_indent = indents_stack [ - 1 ] indents_stack . append ( tok . end_col ) tok . post_indent = tok . end_col elif tok . op == DEDENT : tok . pre_indent = indents_stack . pop ( ) tok . post_indent = indents_stack [ - 1 ] elif tok . op == COMMENT : tok . pre_indent = tok . post_indent = indents_stack [ - 1 ] i = len ( tokens ) - 1 while i >= 2 : pptok , ptok , tok = tokens [ i - 2 : i + 1 ] if tok . op == INDENT : if ptok . op == NL and pptok . op == COMMENT : indent , nl , comment = tok , ptok , pptok assert nl . start_col == comment . end_col underindent = indent . post_indent - comment . start_col if underindent > 0 : _warn ( ""Comment '%s' is under-indented. Fixing..."" % comment . str ) comment . move ( 0 , underindent ) nl . move ( 0 , underindent ) indent . move ( - 1 , 0 ) tokens [ i - 2 : i + 1 ] = indent , comment , nl comment . pre_indent = comment . post_indent = indent . post_indent assert indent . end_row == comment . start_row and indent . end_col <= comment . start_col elif ptok . op == NL and ptok . start_col == 0 : indent , nl = tok , ptok indent . move ( - 1 , 0 ) tokens [ i - 1 : i + 1 ] = indent , nl elif tok . op == DEDENT and ptok . op == NL : if pptok . op == COMMENT : dedent , nl , comment = tok , ptok , pptok if comment . start_col <= dedent . post_indent : rel_indent = comment . start_col - dedent . start_col if rel_indent < 0 : _warn ( ""Comment '%s' has wrong indentation"" % comment . str ) ptok . move ( 0 , - rel_indent ) comment . move ( 0 , - rel_indent ) dedent . move ( - 1 ) tokens [ i - 2 : i + 1 ] = dedent , comment , nl comment . pre_indent = comment . post_indent = dedent . post_indent i += 1 continue elif ptok . start_col == 0 : dedent , nl = tok , ptok dedent . move ( - 1 , - dedent . start_col ) tokens [ i - 1 : i + 1 ] = dedent , nl i += 1 continue else : assert False , ""Unexpected sequence of tokens: %r %r %r"" % ( pptok , ptok , tok ) elif tok . op == COMMENT : if tok . start_col < tok . pre_indent : _warn ( ""Comment '%s' is under-indented relative to the surrounding block"" % tok . str ) i -= 1 return tokens",Parse any object accessible through a readline interface into a list of : class : Token s .
"def move ( self , drow , dcol = 0 ) : self . _start_row += drow self . _start_col += dcol self . _end_row += drow self . _end_col += dcol",Move the token by drow rows and dcol columns .
def unparse ( self ) : ut = Untokenizer ( start_row = self . _tokens [ 0 ] . start_row ) self . _unparse ( ut ) return ut . result ( ),Convert the parsed representation back into the source code .
"def _parse1 ( self ) : fragments = [ ] tokens = self . _tokens def advance_after_newline ( i0 ) : """"""Return the index of the first token after the end of the current (logical) line."""""" for i in range ( i0 , len ( tokens ) ) : if tokens [ i ] . op == NEWLINE : break return i + 1 i = 0 while i < len ( tokens ) : i0 = i tok = tokens [ i ] fragment_type = ""???"" if tok . op == ENDMARKER : fragment_type = ""end"" i += 1 assert i == len ( tokens ) , ""ENDMARKER token encountered before the end of the stream"" elif tok . op == NL : fragment_type = ""whitespace"" while tokens [ i ] . op == NL : i += 1 elif tok . op == COMMENT : fragment_type = ""comment"" is_banner = False while i < len ( tokens ) and tokens [ i ] . op == COMMENT and tokens [ i ] . start_col == tok . start_col : assert tokens [ i + 1 ] . op == NL , ""Unexpected token after a comment: %r"" % tokens [ i + 1 ] s = tokens [ i ] . str if re . match ( r""^#\s?[#*=-]{10,}$"" , s ) or re . match ( r""^#\s?[#*=-]{4,}.*?[#*=-]{4,}$"" , s ) : is_banner = True i += 2 if is_banner : fragment_type = ""banner-comment"" elif ( tok . op == STRING and tokens [ i + 1 ] . op == NEWLINE and all ( frag [ 0 ] == ""whitespace"" or frag [ 0 ] == ""comment"" for frag in fragments ) ) : i += 2 fragment_type = ""docstring"" elif tok . op == OP and tok . str == ""@"" and tokens [ i + 1 ] . op == NAME : while tokens [ i ] . op == OP and tokens [ i ] . str == ""@"" and tokens [ i + 1 ] . op == NAME : i = advance_after_newline ( i ) fragment_type = ""decorator"" elif tok . op == NAME and tok . str in { ""from"" , ""import"" } : while tokens [ i ] . op == NAME and tokens [ i ] . str in { ""from"" , ""import"" } : i = advance_after_newline ( i ) fragment_type = ""import"" elif tok . op in { INDENT , DEDENT , NEWLINE } : assert False , ""Unexpected token %d: %r"" % ( i , tok ) else : i = advance_after_newline ( i ) if i < len ( tokens ) and tokens [ i ] . op == INDENT : level = 1 while level > 0 : i += 1 level += tokens [ i ] . indent ( ) assert tokens [ i ] . op == DEDENT i += 1 while i < len ( tokens ) and tokens [ i ] . op == COMMENT and tokens [ i ] . start_col > tok . start_col : assert tokens [ i + 1 ] . op == NL i += 2 if tok . op == NAME and tok . str in { ""def"" , ""class"" } : fragment_type = tok . str else : fragment_type = ""code"" assert i > i0 , ""Stuck at i = %d"" % i fragments . append ( ( fragment_type , i0 , i ) ) return fragments",First stage of parsing the code ( stored as a raw stream of tokens ) .
"def _parse2 ( self , fragments ) : out = [ ] tokens = self . _tokens i = 0 saved_start = None while i < len ( fragments ) : ftype , start , end = fragments [ i ] assert start == ( 0 if i == 0 else fragments [ i - 1 ] [ 2 ] ) , ""Discontinuity in `fragments` at i = %d"" % i if ftype == ""whitespace"" or ftype == ""end"" : assert saved_start is None obj = Whitespace ( tokens [ start : end ] ) elif ftype == ""docstring"" : assert saved_start is None obj = Docstring ( tokens [ start : end ] ) elif ftype == ""comment"" : assert saved_start is None next_frag = fragments [ i + 1 ] [ 0 ] if i + 1 < len ( fragments ) else ""end"" if next_frag in { ""docstring"" , ""end"" , ""whitespace"" , ""comment"" , ""banner-comment"" } : obj = Comment ( tokens [ start : end ] ) elif next_frag in { ""decorator"" , ""import"" , ""def"" , ""class"" , ""code"" } : saved_start = start i += 1 continue else : raise RuntimeError ( ""Unknown token type %s"" % next_frag ) elif ftype == ""banner-comment"" : assert saved_start is None obj = Comment ( tokens [ start : end ] ) obj . type = ""banner"" elif ftype == ""decorator"" : if saved_start is None : saved_start = start i += 1 continue elif ftype == ""import"" : real_start = start if saved_start is None else saved_start saved_start = None obj = ImportBlock ( tokens [ real_start : end ] ) elif ftype in { ""class"" , ""def"" } : real_start = start if saved_start is None else saved_start saved_start = None obj = Callable ( tokens [ real_start : end ] ) obj . type = ftype elif ftype == ""code"" : real_start = start if saved_start is None else saved_start saved_start = None obj = Expression ( tokens [ real_start : end ] ) else : assert False , ""Unknown fragment type %s"" % ftype out . append ( obj ) i += 1 return out",Second stage of parsing : convert fragments into the list of code objects .
"def size ( self , train = False , valid = False , xval = False ) : tm = ModelBase . _get_metrics ( self , train , valid , xval ) m = { } for k , v in tm . items ( ) : m [ k ] = None if v is None else [ v [ 2 ] for v in v . _metric_json [ ""centroid_stats"" ] . cell_values ] return list ( m . values ( ) ) [ 0 ] if len ( m ) == 1 else m",Get the sizes of each cluster .
"def centers ( self ) : o = self . _model_json [ ""output"" ] cvals = o [ ""centers"" ] . cell_values centers = [ list ( cval [ 1 : ] ) for cval in cvals ] return centers",The centers for the KMeans model .
"def centers_std ( self ) : o = self . _model_json [ ""output"" ] cvals = o [ ""centers_std"" ] . cell_values centers_std = [ list ( cval [ 1 : ] ) for cval in cvals ] centers_std = [ list ( x ) for x in zip ( * centers_std ) ] return centers_std",The standardized centers for the kmeans model .
"def connect ( server = None , url = None , ip = None , port = None , https = None , verify_ssl_certificates = None , auth = None , proxy = None , cookies = None , verbose = True , config = None ) : global h2oconn if config : if ""connect_params"" in config : h2oconn = _connect_with_conf ( config [ ""connect_params"" ] ) else : h2oconn = _connect_with_conf ( config ) else : h2oconn = H2OConnection . open ( server = server , url = url , ip = ip , port = port , https = https , auth = auth , verify_ssl_certificates = verify_ssl_certificates , proxy = proxy , cookies = cookies , verbose = verbose ) if verbose : h2oconn . cluster . show_status ( ) return h2oconn",Connect to an existing H2O server remote or local .
"def api ( endpoint , data = None , json = None , filename = None , save_to = None ) : _check_connection ( ) return h2oconn . request ( endpoint , data = data , json = json , filename = filename , save_to = save_to )",Perform a REST API request to a previously connected server .
"def version_check ( ) : from . __init__ import __version__ as ver_pkg ci = h2oconn . cluster if not ci : raise H2OConnectionError ( ""Connection not initialized. Did you run h2o.connect()?"" ) ver_h2o = ci . version if ver_pkg == ""SUBST_PROJECT_VERSION"" : ver_pkg = ""UNKNOWN"" if str ( ver_h2o ) != str ( ver_pkg ) : branch_name_h2o = ci . branch_name build_number_h2o = ci . build_number if build_number_h2o is None or build_number_h2o == ""unknown"" : raise H2OConnectionError ( ""Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. "" ""Upgrade H2O and h2o-Python to latest stable version - "" ""http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html"" """" . format ( ver_h2o , ver_pkg ) ) elif build_number_h2o == ""99999"" : raise H2OConnectionError ( ""Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. "" ""This is a developer build, please contact your developer."" """" . format ( ver_h2o , ver_pkg ) ) else : raise H2OConnectionError ( ""Version mismatch. H2O is version {0}, but the h2o-python package is version {1}. "" ""Install the matching h2o-Python version from - "" ""http://h2o-release.s3.amazonaws.com/h2o/{2}/{3}/index.html."" """" . format ( ver_h2o , ver_pkg , branch_name_h2o , build_number_h2o ) ) if ci . build_too_old : print ( ""Warning: Your H2O cluster version is too old ({})! Please download and install the latest "" ""version from http://h2o.ai/download/"" . format ( ci . build_age ) )",Used to verify that h2o - python module and the H2O server are compatible with each other .
"def init ( url = None , ip = None , port = None , name = None , https = None , insecure = None , username = None , password = None , cookies = None , proxy = None , start_h2o = True , nthreads = - 1 , ice_root = None , log_dir = None , log_level = None , enable_assertions = True , max_mem_size = None , min_mem_size = None , strict_version_check = None , ignore_config = False , extra_classpath = None , jvm_custom_args = None , bind_to_localhost = True , * * kwargs ) : global h2oconn assert_is_type ( url , str , None ) assert_is_type ( ip , str , None ) assert_is_type ( port , int , str , None ) assert_is_type ( name , str , None ) assert_is_type ( https , bool , None ) assert_is_type ( insecure , bool , None ) assert_is_type ( username , str , None ) assert_is_type ( password , str , None ) assert_is_type ( cookies , str , [ str ] , None ) assert_is_type ( proxy , { str : str } , None ) assert_is_type ( start_h2o , bool , None ) assert_is_type ( nthreads , int ) assert_is_type ( ice_root , str , None ) assert_is_type ( log_dir , str , None ) assert_is_type ( log_level , str , None ) assert_satisfies ( log_level , log_level in [ None , ""TRACE"" , ""DEBUG"" , ""INFO"" , ""WARN"" , ""ERRR"" , ""FATA"" ] ) assert_is_type ( enable_assertions , bool ) assert_is_type ( max_mem_size , int , str , None ) assert_is_type ( min_mem_size , int , str , None ) assert_is_type ( strict_version_check , bool , None ) assert_is_type ( extra_classpath , [ str ] , None ) assert_is_type ( jvm_custom_args , [ str ] , None ) assert_is_type ( bind_to_localhost , bool ) assert_is_type ( kwargs , { ""proxies"" : { str : str } , ""max_mem_size_GB"" : int , ""min_mem_size_GB"" : int , ""force_connect"" : bool , ""as_port"" : bool } ) def get_mem_size ( mmint , mmgb ) : if not mmint : if mmgb is None : return None return mmgb << 30 if is_type ( mmint , int ) : if mmint < 1000 : return mmint << 30 return mmint if is_type ( mmint , str ) : last = mmint [ - 1 ] . upper ( ) num = mmint [ : - 1 ] if not ( num . isdigit ( ) and last in ""MGT"" ) : raise H2OValueError ( ""Wrong format for a *_memory_size argument: %s (should be a number followed by "" ""a suffix 'M', 'G' or 'T')"" % mmint ) if last == ""T"" : return int ( num ) << 40 if last == ""G"" : return int ( num ) << 30 if last == ""M"" : return int ( num ) << 20 scheme = ""https"" if https else ""http"" proxy = proxy [ scheme ] if proxy is not None and scheme in proxy else kwargs [ ""proxies"" ] [ scheme ] if ""proxies"" in kwargs and scheme in kwargs [ ""proxies"" ] else None mmax = get_mem_size ( max_mem_size , kwargs . get ( ""max_mem_size_GB"" ) ) mmin = get_mem_size ( min_mem_size , kwargs . get ( ""min_mem_size_GB"" ) ) auth = ( username , password ) if username and password else None check_version = True verify_ssl_certificates = True if not ignore_config : config = H2OConfigReader . get_config ( ) if url is None and ip is None and port is None and https is None and ""init.url"" in config : url = config [ ""init.url"" ] if proxy is None and ""init.proxy"" in config : proxy = config [ ""init.proxy"" ] if cookies is None and ""init.cookies"" in config : cookies = config [ ""init.cookies"" ] . split ( "";"" ) if auth is None and ""init.username"" in config and ""init.password"" in config : auth = ( config [ ""init.username"" ] , config [ ""init.password"" ] ) if strict_version_check is None : if ""init.check_version"" in config : check_version = config [ ""init.check_version"" ] . lower ( ) != ""false"" elif os . environ . get ( ""H2O_DISABLE_STRICT_VERSION_CHECK"" ) : check_version = False else : check_version = strict_version_check if insecure is None : if ""init.verify_ssl_certificates"" in config : verify_ssl_certificates = config [ ""init.verify_ssl_certificates"" ] . lower ( ) != ""false"" else : verify_ssl_certificates = not insecure if not start_h2o : print ( ""Warning: if you don't want to start local H2O server, then use of `h2o.connect()` is preferred."" ) try : h2oconn = H2OConnection . open ( url = url , ip = ip , port = port , name = name , https = https , verify_ssl_certificates = verify_ssl_certificates , auth = auth , proxy = proxy , cookies = cookies , verbose = True , _msgs = ( ""Checking whether there is an H2O instance running at {url} "" , ""connected."" , ""not found."" ) ) except H2OConnectionError : if port and not str ( port ) . endswith ( ""+"" ) and not kwargs . get ( ""as_port"" , False ) : port = str ( port ) + ""+"" if not start_h2o : raise if ip and not ( ip == ""localhost"" or ip == ""127.0.0.1"" ) : raise H2OConnectionError ( 'Can only start H2O launcher if IP address is localhost.' ) hs = H2OLocalServer . start ( nthreads = nthreads , enable_assertions = enable_assertions , max_mem_size = mmax , min_mem_size = mmin , ice_root = ice_root , log_dir = log_dir , log_level = log_level , port = port , name = name , extra_classpath = extra_classpath , jvm_custom_args = jvm_custom_args , bind_to_localhost = bind_to_localhost ) h2oconn = H2OConnection . open ( server = hs , https = https , verify_ssl_certificates = not insecure , auth = auth , proxy = proxy , cookies = cookies , verbose = True ) if check_version : version_check ( ) h2oconn . cluster . timezone = ""UTC"" h2oconn . cluster . show_status ( )",Attempt to connect to a local server or if not successful start a new server and connect to it .
"def lazy_import ( path , pattern = None ) : assert_is_type ( path , str , [ str ] ) assert_is_type ( pattern , str , None ) paths = [ path ] if is_type ( path , str ) else path return _import_multi ( paths , pattern )",Import a single file or collection of files .
"def upload_file ( path , destination_frame = None , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , skipped_columns = None ) : coltype = U ( None , ""unknown"" , ""uuid"" , ""string"" , ""float"" , ""real"" , ""double"" , ""int"" , ""numeric"" , ""categorical"" , ""factor"" , ""enum"" , ""time"" ) natype = U ( str , [ str ] ) assert_is_type ( path , str ) assert_is_type ( destination_frame , str , None ) assert_is_type ( header , - 1 , 0 , 1 ) assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == 1 ) ) assert_is_type ( col_names , [ str ] , None ) assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) assert ( skipped_columns == None ) or isinstance ( skipped_columns , list ) , ""The skipped_columns should be an list of column names!"" check_frame_id ( destination_frame ) if path . startswith ( ""~"" ) : path = os . path . expanduser ( path ) return H2OFrame ( ) . _upload_parse ( path , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns )",Upload a dataset from the provided local path to the H2O cluster .
"def import_file ( path = None , destination_frame = None , parse = True , header = 0 , sep = None , col_names = None , col_types = None , na_strings = None , pattern = None , skipped_columns = None , custom_non_data_line_markers = None ) : coltype = U ( None , ""unknown"" , ""uuid"" , ""string"" , ""float"" , ""real"" , ""double"" , ""int"" , ""numeric"" , ""categorical"" , ""factor"" , ""enum"" , ""time"" ) natype = U ( str , [ str ] ) assert_is_type ( path , str , [ str ] ) assert_is_type ( pattern , str , None ) assert_is_type ( destination_frame , str , None ) assert_is_type ( parse , bool ) assert_is_type ( header , - 1 , 0 , 1 ) assert_is_type ( sep , None , I ( str , lambda s : len ( s ) == 1 ) ) assert_is_type ( col_names , [ str ] , None ) assert_is_type ( col_types , [ coltype ] , { str : coltype } , None ) assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) assert isinstance ( skipped_columns , ( type ( None ) , list ) ) , ""The skipped_columns should be an list of column names!"" check_frame_id ( destination_frame ) patharr = path if isinstance ( path , list ) else [ path ] if any ( os . path . split ( p ) [ 0 ] == ""~"" for p in patharr ) : raise H2OValueError ( ""Paths relative to a current user (~) are not valid in the server environment. "" ""Please use absolute paths if possible."" ) if not parse : return lazy_import ( path , pattern ) else : return H2OFrame ( ) . _import_parse ( path , pattern , destination_frame , header , sep , col_names , col_types , na_strings , skipped_columns , custom_non_data_line_markers )",Import a dataset that is already on the cluster .
"def import_hive_table ( database = None , table = None , partitions = None , allow_multi_format = False ) : assert_is_type ( database , str , None ) assert_is_type ( table , str ) assert_is_type ( partitions , [ [ str ] ] , None ) p = { ""database"" : database , ""table"" : table , ""partitions"" : partitions , ""allow_multi_format"" : allow_multi_format } j = H2OJob ( api ( ""POST /3/ImportHiveTable"" , data = p ) , ""Import Hive Table"" ) . poll ( ) return get_frame ( j . dest_key )",Import Hive table to H2OFrame in memory .
"def import_sql_table ( connection_url , table , username , password , columns = None , optimize = True , fetch_mode = None ) : assert_is_type ( connection_url , str ) assert_is_type ( table , str ) assert_is_type ( username , str ) assert_is_type ( password , str ) assert_is_type ( columns , [ str ] , None ) assert_is_type ( optimize , bool ) assert_is_type ( fetch_mode , str , None ) p = { ""connection_url"" : connection_url , ""table"" : table , ""username"" : username , ""password"" : password , ""fetch_mode"" : fetch_mode } if columns : p [ ""columns"" ] = "", "" . join ( columns ) j = H2OJob ( api ( ""POST /99/ImportSQLTable"" , data = p ) , ""Import SQL Table"" ) . poll ( ) return get_frame ( j . dest_key )",Import SQL table to H2OFrame in memory .
"def import_sql_select ( connection_url , select_query , username , password , optimize = True , use_temp_table = None , temp_table_name = None , fetch_mode = None ) : assert_is_type ( connection_url , str ) assert_is_type ( select_query , str ) assert_is_type ( username , str ) assert_is_type ( password , str ) assert_is_type ( optimize , bool ) assert_is_type ( use_temp_table , bool , None ) assert_is_type ( temp_table_name , str , None ) assert_is_type ( fetch_mode , str , None ) p = { ""connection_url"" : connection_url , ""select_query"" : select_query , ""username"" : username , ""password"" : password , ""use_temp_table"" : use_temp_table , ""temp_table_name"" : temp_table_name , ""fetch_mode"" : fetch_mode } j = H2OJob ( api ( ""POST /99/ImportSQLTable"" , data = p ) , ""Import SQL Table"" ) . poll ( ) return get_frame ( j . dest_key )",Import the SQL table that is the result of the specified SQL query to H2OFrame in memory .
"def parse_setup ( raw_frames , destination_frame = None , header = 0 , separator = None , column_names = None , column_types = None , na_strings = None , skipped_columns = None , custom_non_data_line_markers = None ) : coltype = U ( None , ""unknown"" , ""uuid"" , ""string"" , ""float"" , ""real"" , ""double"" , ""int"" , ""numeric"" , ""categorical"" , ""factor"" , ""enum"" , ""time"" ) natype = U ( str , [ str ] ) assert_is_type ( raw_frames , str , [ str ] ) assert_is_type ( destination_frame , None , str ) assert_is_type ( header , - 1 , 0 , 1 ) assert_is_type ( separator , None , I ( str , lambda s : len ( s ) == 1 ) ) assert_is_type ( column_names , [ str ] , None ) assert_is_type ( column_types , [ coltype ] , { str : coltype } , None ) assert_is_type ( na_strings , [ natype ] , { str : natype } , None ) check_frame_id ( destination_frame ) if is_type ( raw_frames , str ) : raw_frames = [ raw_frames ] kwargs = { ""check_header"" : header , ""source_frames"" : [ quoted ( frame_id ) for frame_id in raw_frames ] } if separator : kwargs [ ""separator"" ] = ord ( separator ) if custom_non_data_line_markers is not None : kwargs [ ""custom_non_data_line_markers"" ] = custom_non_data_line_markers j = api ( ""POST /3/ParseSetup"" , data = kwargs ) if ""warnings"" in j and j [ ""warnings"" ] : for w in j [ ""warnings"" ] : warnings . warn ( w ) if destination_frame : j [ ""destination_frame"" ] = destination_frame parse_column_len = len ( j [ ""column_types"" ] ) if skipped_columns is None else ( len ( j [ ""column_types"" ] ) - len ( skipped_columns ) ) tempColumnNames = j [ ""column_names"" ] if j [ ""column_names"" ] is not None else gen_header ( j [ ""number_columns"" ] ) useType = [ True ] * len ( tempColumnNames ) if skipped_columns is not None : useType = [ True ] * len ( tempColumnNames ) for ind in range ( len ( tempColumnNames ) ) : if ind in skipped_columns : useType [ ind ] = False if column_names is not None : if not isinstance ( column_names , list ) : raise ValueError ( ""col_names should be a list"" ) if ( skipped_columns is not None ) and len ( skipped_columns ) > 0 : if ( len ( column_names ) ) != parse_column_len : raise ValueError ( ""length of col_names should be equal to the number of columns parsed: %d vs %d"" % ( len ( column_names ) , parse_column_len ) ) else : if len ( column_names ) != len ( j [ ""column_types"" ] ) : raise ValueError ( ""length of col_names should be equal to the number of columns: %d vs %d"" % ( len ( column_names ) , len ( j [ ""column_types"" ] ) ) ) j [ ""column_names"" ] = column_names counter = 0 for ind in range ( len ( tempColumnNames ) ) : if useType [ ind ] : tempColumnNames [ ind ] = column_names [ counter ] counter = counter + 1 if ( column_types is not None ) : if isinstance ( column_types , dict ) : if j [ ""column_names"" ] is None : j [ ""column_names"" ] = gen_header ( j [ ""number_columns"" ] ) if not set ( column_types . keys ( ) ) . issubset ( set ( j [ ""column_names"" ] ) ) : raise ValueError ( ""names specified in col_types is not a subset of the column names"" ) idx = 0 column_types_list = [ ] for name in tempColumnNames : if name in column_types : column_types_list . append ( column_types [ name ] ) else : column_types_list . append ( j [ ""column_types"" ] [ idx ] ) idx += 1 column_types = column_types_list elif isinstance ( column_types , list ) : if len ( column_types ) != parse_column_len : raise ValueError ( ""length of col_types should be equal to the number of parsed columns"" ) column_types_list = j [ ""column_types"" ] counter = 0 for ind in range ( len ( j [ ""column_types"" ] ) ) : if useType [ ind ] and ( column_types [ counter ] != None ) : column_types_list [ ind ] = column_types [ counter ] counter = counter + 1 column_types = column_types_list else : raise ValueError ( ""col_types should be a list of types or a dictionary of column names to types"" ) j [ ""column_types"" ] = column_types if na_strings is not None : if isinstance ( na_strings , dict ) : if not j [ ""column_names"" ] : raise ValueError ( ""column names should be specified"" ) if not set ( na_strings . keys ( ) ) . issubset ( set ( j [ ""column_names"" ] ) ) : raise ValueError ( ""names specified in na_strings is not a subset of the column names"" ) j [ ""na_strings"" ] = [ [ ] for _ in range ( len ( j [ ""column_names"" ] ) ) ] for name , na in na_strings . items ( ) : idx = j [ ""column_names"" ] . index ( name ) if is_type ( na , str ) : na = [ na ] for n in na : j [ ""na_strings"" ] [ idx ] . append ( quoted ( n ) ) elif is_type ( na_strings , [ [ str ] ] ) : if len ( na_strings ) != len ( j [ ""column_types"" ] ) : raise ValueError ( ""length of na_strings should be equal to the number of columns"" ) j [ ""na_strings"" ] = [ [ quoted ( na ) for na in col ] if col is not None else [ ] for col in na_strings ] elif isinstance ( na_strings , list ) : j [ ""na_strings"" ] = [ [ quoted ( na ) for na in na_strings ] ] * len ( j [ ""column_types"" ] ) else : raise ValueError ( ""na_strings should be a list, a list of lists (one list per column), or a dictionary of column "" ""names to strings which are to be interpreted as missing values"" ) if skipped_columns is not None : if isinstance ( skipped_columns , list ) : j [ ""skipped_columns"" ] = [ ] for colidx in skipped_columns : if ( colidx < 0 ) : raise ValueError ( ""skipped column index cannot be negative"" ) j [ ""skipped_columns"" ] . append ( colidx ) if j [ ""column_names"" ] : j [ ""column_names"" ] = list ( map ( quoted , j [ ""column_names"" ] ) ) j [ ""column_types"" ] = list ( map ( quoted , j [ ""column_types"" ] ) ) return j",Retrieve H2O s best guess as to what the structure of the data file is .
"def parse_raw ( setup , id = None , first_line_is_header = 0 ) : assert_is_type ( setup , dict ) assert_is_type ( id , str , None ) assert_is_type ( first_line_is_header , - 1 , 0 , 1 ) check_frame_id ( id ) if id : setup [ ""destination_frame"" ] = id if first_line_is_header != ( - 1 , 0 , 1 ) : if first_line_is_header not in ( - 1 , 0 , 1 ) : raise ValueError ( ""first_line_is_header should be -1, 0, or 1"" ) setup [ ""check_header"" ] = first_line_is_header fr = H2OFrame ( ) fr . _parse_raw ( setup ) return fr",Parse dataset using the parse setup structure .
"def assign ( data , xid ) : assert_is_type ( data , H2OFrame ) assert_is_type ( xid , str ) assert_satisfies ( xid , xid != data . frame_id ) check_frame_id ( xid ) data . _ex = ExprNode ( ""assign"" , xid , data ) . _eval_driver ( False ) data . _ex . _cache . _id = xid data . _ex . _children = None return data",( internal ) Assign new id to the frame .
"def deep_copy ( data , xid ) : assert_is_type ( data , H2OFrame ) assert_is_type ( xid , str ) assert_satisfies ( xid , xid != data . frame_id ) check_frame_id ( xid ) duplicate = data . apply ( lambda x : x ) duplicate . _ex = ExprNode ( ""assign"" , xid , duplicate ) . _eval_driver ( False ) duplicate . _ex . _cache . _id = xid duplicate . _ex . _children = None return duplicate",Create a deep clone of the frame data .
"def get_model ( model_id ) : assert_is_type ( model_id , str ) model_json = api ( ""GET /3/Models/%s"" % model_id ) [ ""models"" ] [ 0 ] algo = model_json [ ""algo"" ] if algo == ""svd"" : m = H2OSVD ( ) elif algo == ""pca"" : m = H2OPrincipalComponentAnalysisEstimator ( ) elif algo == ""drf"" : m = H2ORandomForestEstimator ( ) elif algo == ""naivebayes"" : m = H2ONaiveBayesEstimator ( ) elif algo == ""kmeans"" : m = H2OKMeansEstimator ( ) elif algo == ""glrm"" : m = H2OGeneralizedLowRankEstimator ( ) elif algo == ""glm"" : m = H2OGeneralizedLinearEstimator ( ) elif algo == ""gbm"" : m = H2OGradientBoostingEstimator ( ) elif algo == ""deepwater"" : m = H2ODeepWaterEstimator ( ) elif algo == ""xgboost"" : m = H2OXGBoostEstimator ( ) elif algo == ""word2vec"" : m = H2OWord2vecEstimator ( ) elif algo == ""generic"" : m = H2OGenericEstimator ( ) elif algo == ""deeplearning"" : if model_json [ ""output"" ] [ ""model_category"" ] == ""AutoEncoder"" : m = H2OAutoEncoderEstimator ( ) else : m = H2ODeepLearningEstimator ( ) elif algo == ""stackedensemble"" : m = H2OStackedEnsembleEstimator ( ) elif algo == ""isolationforest"" : m = H2OIsolationForestEstimator ( ) else : raise ValueError ( ""Unknown algo type: "" + algo ) m . _resolve_model ( model_id , model_json ) return m",Load a model from the server .
"def get_grid ( grid_id ) : assert_is_type ( grid_id , str ) grid_json = api ( ""GET /99/Grids/%s"" % grid_id ) models = [ get_model ( key [ ""name"" ] ) for key in grid_json [ ""model_ids"" ] ] first_model_json = api ( ""GET /3/Models/%s"" % grid_json [ ""model_ids"" ] [ 0 ] [ ""name"" ] ) [ ""models"" ] [ 0 ] gs = H2OGridSearch ( None , { } , grid_id ) gs . _resolve_grid ( grid_id , grid_json , first_model_json ) gs . models = models hyper_params = { param : set ( ) for param in gs . hyper_names } for param in gs . hyper_names : for model in models : if isinstance ( model . full_parameters [ param ] [ ""actual_value"" ] , list ) : hyper_params [ param ] . add ( model . full_parameters [ param ] [ ""actual_value"" ] [ 0 ] ) else : hyper_params [ param ] . add ( model . full_parameters [ param ] [ ""actual_value"" ] ) hyper_params = { str ( param ) : list ( vals ) for param , vals in hyper_params . items ( ) } gs . hyper_params = hyper_params gs . model = model . __class__ ( ) return gs",Return the specified grid .
"def get_frame ( frame_id , * * kwargs ) : assert_is_type ( frame_id , str ) return H2OFrame . get_frame ( frame_id , * * kwargs )",Obtain a handle to the frame in H2O with the frame_id key .
"def remove ( x ) : item_type = U ( str , H2OFrame , H2OEstimator ) assert_is_type ( x , item_type , [ item_type ] ) if not isinstance ( x , list ) : x = [ x ] for xi in x : if isinstance ( xi , H2OFrame ) : xi_id = xi . _ex . _cache . _id if xi_id is None : return rapids ( ""(rm {})"" . format ( xi_id ) ) xi . _ex = None elif isinstance ( xi , H2OEstimator ) : api ( ""DELETE /3/DKV/%s"" % xi . model_id ) xi . _id = None else : try : rapids ( ""(rm {})"" . format ( xi ) ) except : api ( ""DELETE /3/DKV/%s"" % xi )",Remove object ( s ) from H2O .
"def download_pojo ( model , path = """" , get_jar = True , jar_name = """" ) : assert_is_type ( model , ModelBase ) assert_is_type ( path , str ) assert_is_type ( get_jar , bool ) if not model . have_pojo : raise H2OValueError ( ""Export to POJO not supported"" ) if path == """" : java_code = api ( ""GET /3/Models.java/%s"" % model . model_id ) print ( java_code ) return None else : filename = api ( ""GET /3/Models.java/%s"" % model . model_id , save_to = path ) if get_jar : if jar_name == """" : api ( ""GET /3/h2o-genmodel.jar"" , save_to = os . path . join ( path , ""h2o-genmodel.jar"" ) ) else : api ( ""GET /3/h2o-genmodel.jar"" , save_to = os . path . join ( path , jar_name ) ) return filename",Download the POJO for this model to the directory specified by path ; if path is then dump to screen .
"def download_csv ( data , filename ) : assert_is_type ( data , H2OFrame ) assert_is_type ( filename , str ) url = h2oconn . make_url ( ""DownloadDataset"" , 3 ) + ""?frame_id={}&hex_string=false"" . format ( data . frame_id ) with open ( filename , ""wb"" ) as f : f . write ( urlopen ( ) ( url ) . read ( ) )",Download an H2O data set to a CSV file on the local disk .
"def download_all_logs ( dirname = ""."" , filename = None ) : assert_is_type ( dirname , str ) assert_is_type ( filename , str , None ) url = ""%s/3/Logs/download"" % h2oconn . base_url opener = urlopen ( ) response = opener ( url ) if not os . path . exists ( dirname ) : os . mkdir ( dirname ) if filename is None : if PY3 : headers = [ h [ 1 ] for h in response . headers . _headers ] else : headers = response . headers . headers for h in headers : if ""filename="" in h : filename = h . split ( ""filename="" ) [ 1 ] . strip ( ) break path = os . path . join ( dirname , filename ) response = opener ( url ) . read ( ) print ( ""Writing H2O logs to "" + path ) with open ( path , ""wb"" ) as f : f . write ( response ) return path",Download H2O log files to disk .
"def save_model ( model , path = """" , force = False ) : assert_is_type ( model , ModelBase ) assert_is_type ( path , str ) assert_is_type ( force , bool ) path = os . path . join ( os . getcwd ( ) if path == """" else path , model . model_id ) return api ( ""GET /99/Models.bin/%s"" % model . model_id , data = { ""dir"" : path , ""force"" : force } ) [ ""dir"" ]",Save an H2O Model object to disk . ( Note that ensemble binary models can now be saved using this method . )
"def load_model ( path ) : assert_is_type ( path , str ) res = api ( ""POST /99/Models.bin/%s"" % """" , data = { ""dir"" : path } ) return get_model ( res [ ""models"" ] [ 0 ] [ ""model_id"" ] [ ""name"" ] )",Load a saved H2O model from disk . ( Note that ensemble binary models can now be loaded using this method . )
"def export_file ( frame , path , force = False , parts = 1 ) : assert_is_type ( frame , H2OFrame ) assert_is_type ( path , str ) assert_is_type ( force , bool ) assert_is_type ( parts , int ) H2OJob ( api ( ""POST /3/Frames/%s/export"" % ( frame . frame_id ) , data = { ""path"" : path , ""num_parts"" : parts , ""force"" : force } ) , ""Export File"" ) . poll ( )",Export a given H2OFrame to a path on the machine this python session is currently connected to .
"def create_frame ( frame_id = None , rows = 10000 , cols = 10 , randomize = True , real_fraction = None , categorical_fraction = None , integer_fraction = None , binary_fraction = None , time_fraction = None , string_fraction = None , value = 0 , real_range = 100 , factors = 100 , integer_range = 100 , binary_ones_fraction = 0.02 , missing_fraction = 0.01 , has_response = False , response_factors = 2 , positive_response = False , seed = None , seed_for_column_types = None ) : t_fraction = U ( None , BoundNumeric ( 0 , 1 ) ) assert_is_type ( frame_id , str , None ) assert_is_type ( rows , BoundInt ( 1 ) ) assert_is_type ( cols , BoundInt ( 1 ) ) assert_is_type ( randomize , bool ) assert_is_type ( value , numeric ) assert_is_type ( real_range , BoundNumeric ( 0 ) ) assert_is_type ( real_fraction , t_fraction ) assert_is_type ( categorical_fraction , t_fraction ) assert_is_type ( integer_fraction , t_fraction ) assert_is_type ( binary_fraction , t_fraction ) assert_is_type ( time_fraction , t_fraction ) assert_is_type ( string_fraction , t_fraction ) assert_is_type ( missing_fraction , t_fraction ) assert_is_type ( binary_ones_fraction , t_fraction ) assert_is_type ( factors , BoundInt ( 1 ) ) assert_is_type ( integer_range , BoundInt ( 1 ) ) assert_is_type ( has_response , bool ) assert_is_type ( response_factors , None , BoundInt ( 1 ) ) assert_is_type ( positive_response , bool ) assert_is_type ( seed , int , None ) assert_is_type ( seed_for_column_types , int , None ) check_frame_id ( frame_id ) if randomize and value : raise H2OValueError ( ""Cannot set data to a `value` if `randomize` is true"" ) if ( categorical_fraction or integer_fraction ) and not randomize : raise H2OValueError ( ""`randomize` should be True when either categorical or integer columns are used."" ) frcs = [ real_fraction , categorical_fraction , integer_fraction , binary_fraction , time_fraction , string_fraction ] wgts = [ 0.5 , 0.2 , 0.2 , 0.1 , 0.0 , 0.0 ] sum_explicit_fractions = sum ( 0 if f is None else f for f in frcs ) count_explicit_fractions = sum ( 0 if f is None else 1 for f in frcs ) remainder = 1 - sum_explicit_fractions if sum_explicit_fractions >= 1 + 1e-10 : raise H2OValueError ( ""Fractions of binary, integer, categorical, time and string columns should add up "" ""to a number less than 1."" ) elif sum_explicit_fractions >= 1 - 1e-10 : pass else : if count_explicit_fractions == 6 : raise H2OValueError ( ""Fraction of binary, integer, categorical, time and string columns add up to a "" ""number less than 1."" ) sum_implicit_weights = sum ( wgts [ i ] if frcs [ i ] is None else 0 for i in range ( 6 ) ) for i , f in enumerate ( frcs ) : if frcs [ i ] is not None : continue if sum_implicit_weights == 0 : frcs [ i ] = remainder else : frcs [ i ] = remainder * wgts [ i ] / sum_implicit_weights remainder -= frcs [ i ] sum_implicit_weights -= wgts [ i ] for i , f in enumerate ( frcs ) : if f is None : frcs [ i ] = 0 real_fraction , categorical_fraction , integer_fraction , binary_fraction , time_fraction , string_fraction = frcs parms = { ""dest"" : frame_id if frame_id else py_tmp_key ( append = h2oconn . session_id ) , ""rows"" : rows , ""cols"" : cols , ""randomize"" : randomize , ""categorical_fraction"" : categorical_fraction , ""integer_fraction"" : integer_fraction , ""binary_fraction"" : binary_fraction , ""time_fraction"" : time_fraction , ""string_fraction"" : string_fraction , ""value"" : value , ""real_range"" : real_range , ""factors"" : factors , ""integer_range"" : integer_range , ""binary_ones_fraction"" : binary_ones_fraction , ""missing_fraction"" : missing_fraction , ""has_response"" : has_response , ""response_factors"" : response_factors , ""positive_response"" : positive_response , ""seed"" : - 1 if seed is None else seed , ""seed_for_column_types"" : - 1 if seed_for_column_types is None else seed_for_column_types , } H2OJob ( api ( ""POST /3/CreateFrame"" , data = parms ) , ""Create Frame"" ) . poll ( ) return get_frame ( parms [ ""dest"" ] )",Create a new frame with random data .
"def interaction ( data , factors , pairwise , max_factors , min_occurrence , destination_frame = None ) : assert_is_type ( data , H2OFrame ) assert_is_type ( factors , [ str , int ] ) assert_is_type ( pairwise , bool ) assert_is_type ( max_factors , int ) assert_is_type ( min_occurrence , int ) assert_is_type ( destination_frame , str , None ) factors = [ data . names [ n ] if is_type ( n , int ) else n for n in factors ] parms = { ""dest"" : py_tmp_key ( append = h2oconn . session_id ) if destination_frame is None else destination_frame , ""source_frame"" : data . frame_id , ""factor_columns"" : [ quoted ( f ) for f in factors ] , ""pairwise"" : pairwise , ""max_factors"" : max_factors , ""min_occurrence"" : min_occurrence , } H2OJob ( api ( ""POST /3/Interaction"" , data = parms ) , ""Interactions"" ) . poll ( ) return get_frame ( parms [ ""dest"" ] )",Categorical Interaction Feature Creation in H2O .
"def as_list ( data , use_pandas = True , header = True ) : assert_is_type ( data , H2OFrame ) assert_is_type ( use_pandas , bool ) assert_is_type ( header , bool ) return H2OFrame . as_data_frame ( data , use_pandas = use_pandas , header = header )",Convert an H2O data object into a python - specific object .
"def demo ( funcname , interactive = True , echo = True , test = False ) : import h2o . demos as h2odemo assert_is_type ( funcname , str ) assert_is_type ( interactive , bool ) assert_is_type ( echo , bool ) assert_is_type ( test , bool ) demo_function = getattr ( h2odemo , funcname , None ) if demo_function and type ( demo_function ) is type ( demo ) : demo_function ( interactive , echo , test ) else : print ( ""Demo for %s is not available."" % funcname )",H2O built - in demo facility .
"def load_dataset ( relative_path ) : assert_is_type ( relative_path , str ) h2o_dir = os . path . split ( __file__ ) [ 0 ] for possible_file in [ os . path . join ( h2o_dir , relative_path ) , os . path . join ( h2o_dir , ""h2o_data"" , relative_path ) , os . path . join ( h2o_dir , ""h2o_data"" , relative_path + "".csv"" ) ] : if os . path . exists ( possible_file ) : return upload_file ( possible_file ) raise H2OValueError ( ""Data file %s cannot be found"" % relative_path )",Imports a data file within the h2o_data folder .
"def make_metrics ( predicted , actual , domain = None , distribution = None ) : assert_is_type ( predicted , H2OFrame ) assert_is_type ( actual , H2OFrame ) assert actual . ncol == 1 , ""`actual` frame should have exactly 1 column"" assert_is_type ( distribution , str , None ) assert_satisfies ( actual . ncol , actual . ncol == 1 ) if domain is None and any ( actual . isfactor ( ) ) : domain = actual . levels ( ) [ 0 ] res = api ( ""POST /3/ModelMetrics/predictions_frame/%s/actuals_frame/%s"" % ( predicted . frame_id , actual . frame_id ) , data = { ""domain"" : domain , ""distribution"" : distribution } ) return res [ ""model_metrics"" ]",Create Model Metrics from predicted and actual values in H2O .
"def _put_key ( file_path , dest_key = None , overwrite = True ) : ret = api ( ""POST /3/PutKey?destination_key={}&overwrite={}"" . format ( dest_key if dest_key else '' , overwrite ) , filename = file_path ) return ret [ ""destination_key"" ]",Upload given file into DKV and save it under give key as raw object .
"def upload_custom_metric ( func , func_file = ""metrics.py"" , func_name = None , class_name = None , source_provider = None ) : import tempfile import inspect if not source_provider : source_provider = _default_source_provider _CFUNC_CODE_TEMPLATE = """"""# Generated code
import water.udf.CMetricFunc as MetricFunc

# User given metric function as a class implementing
# 3 methods defined by interface CMetricFunc
{}

# Generated user metric which satisfies the interface
# of Java MetricFunc
class {}Wrapper({}, MetricFunc, object):
    pass

"""""" assert_satisfies ( func , inspect . isclass ( func ) or isinstance ( func , str ) , ""The argument func needs to be string or class !"" ) assert_satisfies ( func_file , func_file is not None , ""The argument func_file is missing!"" ) assert_satisfies ( func_file , func_file . endswith ( '.py' ) , ""The argument func_file needs to end with '.py'"" ) code = None derived_func_name = None module_name = func_file [ : - 3 ] if isinstance ( func , str ) : assert_satisfies ( class_name , class_name is not None , ""The argument class_name is missing! "" + ""It needs to reference the class in given string!"" ) code = _CFUNC_CODE_TEMPLATE . format ( func , class_name , class_name ) derived_func_name = ""metrics_{}"" . format ( class_name ) class_name = ""{}.{}Wrapper"" . format ( module_name , class_name ) else : assert_satisfies ( func , inspect . isclass ( func ) , ""The parameter `func` should be str or class"" ) for method in [ 'map' , 'reduce' , 'metric' ] : assert_satisfies ( func , method in func . __dict__ , ""The class `func` needs to define method `{}`"" . format ( method ) ) assert_satisfies ( class_name , class_name is None , ""If class is specified then class_name parameter needs to be None"" ) class_name = ""{}.{}Wrapper"" . format ( module_name , func . __name__ ) derived_func_name = ""metrics_{}"" . format ( func . __name__ ) code = _CFUNC_CODE_TEMPLATE . format ( source_provider ( func ) , func . __name__ , func . __name__ ) if not func_name : func_name = derived_func_name tmpdir = tempfile . mkdtemp ( prefix = ""h2o-func"" ) func_arch_file = _create_zip_file ( ""{}/func.jar"" . format ( tmpdir ) , ( func_file , code ) ) dest_key = _put_key ( func_arch_file , dest_key = func_name ) return ""python:{}={}"" . format ( dest_key , class_name )",Upload given metrics function into H2O cluster .
"def main ( argv ) : global g_log_base_dir global g_airline_java global g_milsongs_java global g_airline_python global g_milsongs_python if len ( argv ) < 2 : print ""python grabGLRMrunLogs logsBaseDirectory\n"" sys . exit ( 1 ) else : g_log_base_dir = argv [ 1 ] if ( os . path . isdir ( g_log_base_dir ) ) : airline_java_dict = init_java_dict ( ) milsongs_java_dict = init_java_dict ( ) airline_py_dict = init_python_dict ( ) milsongs_py_dict = init_python_dict ( ) allBuilds = os . listdir ( g_log_base_dir ) for dirName in allBuilds : airline_java_dict = grab_java_results ( dirName , g_airline_java , airline_java_dict ) milsongs_java_dict = grab_java_results ( dirName , g_milsongs_java , milsongs_java_dict ) airline_py_dict = grab_py_results ( dirName , g_airline_python , airline_py_dict ) milsongs_py_dict = grab_py_results ( dirName , g_milsongs_python , milsongs_py_dict ) airline_py_dict = transform_time_python ( airline_py_dict ) milsongs_py_dict = transform_time_python ( milsongs_py_dict ) print ( ""Airline Java log results: \n {0}"" . format ( airline_java_dict ) ) print ( ""Airline Python log results: \n {0}"" . format ( airline_py_dict ) ) print ( ""Milsongs Java log results: \n {0}"" . format ( milsongs_java_dict ) ) print ( ""Milsongs Python log results: \n {0}"" . format ( milsongs_py_dict ) ) with open ( os . path . join ( g_log_base_dir , ""airline_java_dict"" ) , 'wb' ) as test_file : json . dump ( airline_java_dict , test_file ) with open ( os . path . join ( g_log_base_dir , ""airline_py_dict"" ) , 'wb' ) as test_file : json . dump ( airline_py_dict , test_file ) with open ( os . path . join ( g_log_base_dir , ""milsongs_java_dict"" ) , 'wb' ) as test_file : json . dump ( milsongs_java_dict , test_file ) with open ( os . path . join ( g_log_base_dir , ""milsongs_py_dict"" ) , 'wb' ) as test_file : json . dump ( milsongs_py_dict , test_file ) generate_octave_java_ascii ( airline_java_dict , ""airline_java_octave"" ) generate_octave_java_ascii ( milsongs_java_dict , ""milsongs_java_octave"" ) generate_octave_py_ascii ( airline_py_dict , ""airline_py_octave"" ) generate_octave_py_ascii ( milsongs_py_dict , ""milsongs_py_octave"" )",Main program .
"def main ( argv ) : global g_script_name global g_tmp_dir g_script_name = os . path . basename ( argv [ 0 ] ) parse_args ( argv ) g_tmp_dir = tempfile . mkdtemp ( suffix = "".tmp_minicran"" ) print ""Created tmp directory: "" + g_tmp_dir atexit . register ( remove_tmp_dir ) try : b = MinicranBuilder ( g_print_only , g_output_dir , g_tmp_dir , g_platform , g_rversion , g_branch , g_buildnum ) b . build ( ) except KeyboardInterrupt : print ( """" ) pass",Main program .
"def check_frame_id ( frame_id ) : if frame_id is None : return if frame_id . strip ( ) == """" : raise H2OValueError ( ""Frame id cannot be an empty string: %r"" % frame_id ) for i , ch in enumerate ( frame_id ) : if ch == ""$"" and i == 0 : continue if ch not in _id_allowed_characters : raise H2OValueError ( ""Character '%s' is illegal in frame id: %s"" % ( ch , frame_id ) ) if re . match ( r""-?[0-9]"" , frame_id ) : raise H2OValueError ( ""Frame id cannot start with a number: %s"" % frame_id )",Check that the provided frame id is valid in Rapids language .
"def _locate ( path ) : tmp_dir = os . path . realpath ( os . getcwd ( ) ) possible_result = os . path . join ( tmp_dir , path ) while True : if os . path . exists ( possible_result ) : return possible_result next_tmp_dir = os . path . dirname ( tmp_dir ) if next_tmp_dir == tmp_dir : raise ValueError ( ""File not found: "" + path ) tmp_dir = next_tmp_dir possible_result = os . path . join ( tmp_dir , path )",Search for a relative path and turn it into an absolute path . This is handy when hunting for data files to be passed into h2o and used by import file . Note : This function is for unit testing purposes only .
"def get_human_readable_bytes ( size ) : if size == 0 : return ""0"" if size is None : return """" assert_is_type ( size , int ) assert size >= 0 , ""`size` cannot be negative, got %d"" % size suffixes = ""PTGMk"" maxl = len ( suffixes ) for i in range ( maxl + 1 ) : shift = ( maxl - i ) * 10 if size >> shift == 0 : continue ndigits = 0 for nd in [ 3 , 2 , 1 ] : if size >> ( shift + 12 - nd * 3 ) == 0 : ndigits = nd break if ndigits == 0 or size == ( size >> shift ) << shift : rounded_val = str ( size >> shift ) else : rounded_val = ""%.*f"" % ( ndigits , size / ( 1 << shift ) ) return ""%s %sb"" % ( rounded_val , suffixes [ i ] if i < maxl else """" )",Convert given number of bytes into a human readable representation i . e . add prefix such as kb Mb Gb etc . The size argument must be a non - negative integer .
"def get_human_readable_time ( time_ms ) : millis = time_ms % 1000 secs = ( time_ms // 1000 ) % 60 mins = ( time_ms // 60000 ) % 60 hours = ( time_ms // 3600000 ) % 24 days = ( time_ms // 86400000 ) res = """" if days > 1 : res += ""%d days"" % days elif days == 1 : res += ""1 day"" if hours > 1 or ( hours == 0 and res ) : res += "" %d hours"" % hours elif hours == 1 : res += "" 1 hour"" if mins > 1 or ( mins == 0 and res ) : res += "" %d mins"" % mins elif mins == 1 : res += "" 1 min"" if days == 0 and hours == 0 : res += "" %02d secs"" % secs if not res : res = "" %d ms"" % millis return res . strip ( )",Convert given duration in milliseconds into a human - readable representation i . e . hours minutes seconds etc . More specifically the returned string may look like following : 1 day 3 hours 12 mins 3 days 0 hours 0 mins 8 hours 12 mins 34 mins 02 secs 13 secs 541 ms In particular the following rules are applied : * milliseconds are printed only if the duration is less than a second ; * seconds are printed only if the duration is less than an hour ; * for durations greater than 1 hour we print days hours and minutes keeping zeros in the middle ( i . e . we return 4 days 0 hours 12 mins instead of 4 days 12 mins ) .
"def print2 ( msg , flush = False , end = ""\n"" ) : print ( msg , end = end ) if flush : sys . stdout . flush ( )",This function exists here ONLY because Sphinx . ext . autodoc gets into a bad state when seeing the print () function . When in that state autodoc doesn t display any errors or warnings but instead completely ignores the bysource member - order option .
"def normalize_slice ( s , total ) : newstart = 0 if s . start is None else max ( 0 , s . start + total ) if s . start < 0 else min ( s . start , total ) newstop = total if s . stop is None else max ( 0 , s . stop + total ) if s . stop < 0 else min ( s . stop , total ) newstep = 1 if s . step is None else s . step return slice ( newstart , newstop , newstep )",Return a canonical version of slice s .
def slice_is_normalized ( s ) : return ( s . start is not None and s . stop is not None and s . step is not None and s . start <= s . stop ),Return True if slice s in normalized form .
"def mojo_predict_pandas ( dataframe , mojo_zip_path , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : tmp_dir = tempfile . mkdtemp ( ) try : if not can_use_pandas ( ) : raise RuntimeException ( 'Cannot import pandas' ) import pandas assert_is_type ( dataframe , pandas . DataFrame ) input_csv_path = os . path . join ( tmp_dir , 'input.csv' ) prediction_csv_path = os . path . join ( tmp_dir , 'prediction.csv' ) dataframe . to_csv ( input_csv_path ) mojo_predict_csv ( input_csv_path = input_csv_path , mojo_zip_path = mojo_zip_path , output_csv_path = prediction_csv_path , genmodel_jar_path = genmodel_jar_path , classpath = classpath , java_options = java_options , verbose = verbose ) return pandas . read_csv ( prediction_csv_path ) finally : shutil . rmtree ( tmp_dir )",MOJO scoring function to take a Pandas frame and use MOJO model as zip file to score .
"def mojo_predict_csv ( input_csv_path , mojo_zip_path , output_csv_path = None , genmodel_jar_path = None , classpath = None , java_options = None , verbose = False ) : default_java_options = '-Xmx4g -XX:ReservedCodeCacheSize=256m' prediction_output_file = 'prediction.csv' java = H2OLocalServer . _find_java ( ) H2OLocalServer . _check_java ( java = java , verbose = verbose ) if verbose : print ( ""input_csv:\t%s"" % input_csv_path ) if not os . path . isfile ( input_csv_path ) : raise RuntimeError ( ""Input csv cannot be found at %s"" % input_csv_path ) mojo_zip_path = os . path . abspath ( mojo_zip_path ) if verbose : print ( ""mojo_zip:\t%s"" % mojo_zip_path ) if not os . path . isfile ( mojo_zip_path ) : raise RuntimeError ( ""MOJO zip cannot be found at %s"" % mojo_zip_path ) parent_dir = os . path . dirname ( mojo_zip_path ) if output_csv_path is None : output_csv_path = os . path . join ( parent_dir , prediction_output_file ) if genmodel_jar_path is None : genmodel_jar_path = os . path . join ( parent_dir , gen_model_file_name ) if verbose : print ( ""genmodel_jar:\t%s"" % genmodel_jar_path ) if not os . path . isfile ( genmodel_jar_path ) : raise RuntimeError ( ""Genmodel jar cannot be found at %s"" % genmodel_jar_path ) if verbose and output_csv_path is not None : print ( ""output_csv:\t%s"" % output_csv_path ) if classpath is None : classpath = genmodel_jar_path if verbose : print ( ""classpath:\t%s"" % classpath ) if java_options is None : java_options = default_java_options if verbose : print ( ""java_options:\t%s"" % java_options ) cmd = [ java ] for option in java_options . split ( ' ' ) : cmd += [ option ] cmd += [ ""-cp"" , classpath , h2o_predictor_class , ""--mojo"" , mojo_zip_path , ""--input"" , input_csv_path , '--output' , output_csv_path , '--decimal' ] if verbose : cmd_str = "" "" . join ( cmd ) print ( ""java cmd:\t%s"" % cmd_str ) subprocess . check_call ( cmd , shell = False ) with open ( output_csv_path ) as csv_file : result = list ( csv . DictReader ( csv_file ) ) return result",MOJO scoring function to take a CSV file and use MOJO model as zip file to score .
"def deprecated ( message ) : from traceback import extract_stack assert message , ""`message` argument in @deprecated is required."" def deprecated_decorator ( fun ) : def decorator_invisible ( * args , * * kwargs ) : stack = extract_stack ( ) assert len ( stack ) >= 2 and stack [ - 1 ] [ 2 ] == ""decorator_invisible"" , ""Got confusing stack... %r"" % stack print ( ""[WARNING] in %s line %d:"" % ( stack [ - 2 ] [ 0 ] , stack [ - 2 ] [ 1 ] ) ) print ( ""    >>> %s"" % ( stack [ - 2 ] [ 3 ] or ""????"" ) ) print ( ""        ^^^^ %s"" % message ) return fun ( * args , * * kwargs ) decorator_invisible . __doc__ = message decorator_invisible . __name__ = fun . __name__ decorator_invisible . __module__ = fun . __module__ decorator_invisible . __deprecated__ = True return decorator_invisible return deprecated_decorator",The decorator to mark deprecated functions .
def join ( self ) : self . _future = False self . _job . poll ( ) self . _job = None,Wait until grid finishes computing .
"def train ( self , x = None , y = None , training_frame = None , offset_column = None , fold_column = None , weights_column = None , validation_frame = None , * * params ) : algo_params = locals ( ) parms = self . _parms . copy ( ) parms . update ( { k : v for k , v in algo_params . items ( ) if k not in [ ""self"" , ""params"" , ""algo_params"" , ""parms"" ] } ) parms [ ""search_criteria"" ] = None if self . search_criteria is None else str ( self . search_criteria ) parms [ ""hyper_parameters"" ] = None if self . hyper_params is None else str ( self . hyper_params ) parms . update ( { k : v for k , v in list ( self . model . _parms . items ( ) ) if v is not None } ) parms . update ( params ) if '__class__' in parms : del parms [ '__class__' ] y = algo_params [ ""y"" ] tframe = algo_params [ ""training_frame"" ] if tframe is None : raise ValueError ( ""Missing training_frame"" ) if y is not None : if is_type ( y , list , tuple ) : if len ( y ) == 1 : parms [ ""y"" ] = y [ 0 ] else : raise ValueError ( 'y must be a single column reference' ) if x is None : if ( isinstance ( y , int ) ) : xset = set ( range ( training_frame . ncols ) ) - { y } else : xset = set ( training_frame . names ) - { y } else : xset = set ( ) if is_type ( x , int , str ) : x = [ x ] for xi in x : if is_type ( xi , int ) : if not ( - training_frame . ncols <= xi < training_frame . ncols ) : raise H2OValueError ( ""Column %d does not exist in the training frame"" % xi ) xset . add ( training_frame . names [ xi ] ) else : if xi not in training_frame . names : raise H2OValueError ( ""Column %s not in the training frame"" % xi ) xset . add ( xi ) x = list ( xset ) parms [ ""x"" ] = x self . build_model ( parms )",Train the model synchronously ( i . e . do not return until the model finishes training ) .
"def build_model ( self , algo_params ) : if algo_params [ ""training_frame"" ] is None : raise ValueError ( ""Missing training_frame"" ) x = algo_params . pop ( ""x"" ) y = algo_params . pop ( ""y"" , None ) training_frame = algo_params . pop ( ""training_frame"" ) validation_frame = algo_params . pop ( ""validation_frame"" , None ) is_auto_encoder = ( algo_params is not None ) and ( ""autoencoder"" in algo_params and algo_params [ ""autoencoder"" ] ) algo = self . model . _compute_algo ( ) is_unsupervised = is_auto_encoder or algo == ""pca"" or algo == ""svd"" or algo == ""kmeans"" or algo == ""glrm"" if is_auto_encoder and y is not None : raise ValueError ( ""y should not be specified for autoencoder."" ) if not is_unsupervised and y is None : raise ValueError ( ""Missing response"" ) if not is_unsupervised : y = y if y in training_frame . names else training_frame . names [ y ] self . model . _estimator_type = ""classifier"" if training_frame . types [ y ] == ""enum"" else ""regressor"" self . _model_build ( x , y , training_frame , validation_frame , algo_params )",( internal )
"def predict ( self , test_data ) : return { model . model_id : model . predict ( test_data ) for model in self . models }",Predict on a dataset .
"def get_xval_models ( self , key = None ) : return { model . model_id : model . get_xval_models ( key ) for model in self . models }",Return a Model object .
"def deepfeatures ( self , test_data , layer ) : return { model . model_id : model . deepfeatures ( test_data , layer ) for model in self . models }",Obtain a hidden layer s details on a dataset .
"def weights ( self , matrix_id = 0 ) : return { model . model_id : model . weights ( matrix_id ) for model in self . models }",Return the frame for the respective weight matrix .
"def biases ( self , vector_id = 0 ) : return { model . model_id : model . biases ( vector_id ) for model in self . models }",Return the frame for the respective bias vector .
"def model_performance ( self , test_data = None , train = False , valid = False , xval = False ) : return { model . model_id : model . model_performance ( test_data , train , valid , xval ) for model in self . models }",Generate model metrics for this model on test_data .
"def summary ( self , header = True ) : table = [ ] for model in self . models : model_summary = model . _model_json [ ""output"" ] [ ""model_summary"" ] r_values = list ( model_summary . cell_values [ 0 ] ) r_values [ 0 ] = model . model_id table . append ( r_values ) print ( ) if header : print ( 'Grid Summary:' ) print ( ) H2ODisplay ( table , [ 'Model Id' ] + model_summary . col_header [ 1 : ] , numalign = ""left"" , stralign = ""left"" )",Print a detailed summary of the explored models .
"def show ( self ) : hyper_combos = itertools . product ( * list ( self . hyper_params . values ( ) ) ) if not self . models : c_values = [ [ idx + 1 , list ( val ) ] for idx , val in enumerate ( hyper_combos ) ] print ( H2OTwoDimTable ( col_header = [ 'Model' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ] , table_header = 'Grid Search of Model ' + self . model . __class__ . __name__ , cell_values = c_values ) ) else : print ( self . sorted_metric_table ( ) )",Print models sorted by metric .
"def varimp ( self , use_pandas = False ) : return { model . model_id : model . varimp ( use_pandas ) for model in self . models }",Pretty print the variable importances or return them in a list / pandas DataFrame .
"def pprint_coef ( self ) : for i , model in enumerate ( self . models ) : print ( 'Model' , i ) model . pprint_coef ( ) print ( )",Pretty print the coefficents table ( includes normalized coefficients ) .
"def get_hyperparams ( self , id , display = True ) : idx = id if is_type ( id , int ) else self . model_ids . index ( id ) model = self [ idx ] if model . _is_xvalidated : model = h2o . get_model ( model . _xval_keys [ 0 ] ) res = [ model . params [ h ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ h ] [ 'actual' ] , list ) else model . params [ h ] [ 'actual' ] for h in self . hyper_params ] if display : print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) return res",Get the hyperparameters of a model explored by grid search .
"def get_hyperparams_dict ( self , id , display = True ) : idx = id if is_type ( id , int ) else self . model_ids . index ( id ) model = self [ idx ] model_params = dict ( ) if model . _is_xvalidated : model = h2o . get_model ( model . _xval_keys [ 0 ] ) for param_name in self . hyper_names : model_params [ param_name ] = model . params [ param_name ] [ 'actual' ] [ 0 ] if isinstance ( model . params [ param_name ] [ 'actual' ] , list ) else model . params [ param_name ] [ 'actual' ] if display : print ( 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' ) return model_params",Derived and returned the model parameters used to train the particular grid search model .
"def get_grid ( self , sort_by = None , decreasing = None ) : if sort_by is None and decreasing is None : return self grid_json = h2o . api ( ""GET /99/Grids/%s"" % self . _id , data = { ""sort_by"" : sort_by , ""decreasing"" : decreasing } ) grid = H2OGridSearch ( self . model , self . hyper_params , self . _id ) grid . models = [ h2o . get_model ( key [ 'name' ] ) for key in grid_json [ 'model_ids' ] ] first_model_json = h2o . api ( ""GET /99/Models/%s"" % grid_json [ 'model_ids' ] [ 0 ] [ 'name' ] ) [ 'models' ] [ 0 ] model_class = H2OGridSearch . _metrics_class ( first_model_json ) m = model_class ( ) m . _id = self . _id m . _grid_json = grid_json m . _parms = grid . _parms H2OEstimator . mixin ( grid , model_class ) grid . __dict__ . update ( m . __dict__ . copy ( ) ) return grid",Retrieve an H2OGridSearch instance .
"def sort_by ( self , metric , increasing = True ) : if metric [ - 1 ] != ')' : metric += '()' c_values = [ list ( x ) for x in zip ( * sorted ( eval ( 'self.' + metric + '.items()' ) , key = lambda k_v : k_v [ 1 ] ) ) ] c_values . insert ( 1 , [ self . get_hyperparams ( model_id , display = False ) for model_id in c_values [ 0 ] ] ) if not increasing : for col in c_values : col . reverse ( ) if metric [ - 2 ] == '(' : metric = metric [ : - 2 ] return H2OTwoDimTable ( col_header = [ 'Model Id' , 'Hyperparameters: [' + ', ' . join ( list ( self . hyper_params . keys ( ) ) ) + ']' , metric ] , table_header = 'Grid Search Results for ' + self . model . __class__ . __name__ , cell_values = [ list ( x ) for x in zip ( * c_values ) ] )",Deprecated since 2016 - 12 - 12 use grid . get_grid () instead .
"def anomaly ( self , test_data , per_feature = False ) : return { model . model_id : model . anomaly ( test_data , per_feature ) for model in self . models }",Obtain the reconstruction error for the input test_data .
"def F1 ( self , thresholds = None , train = False , valid = False , xval = False ) : return { model . model_id : model . F1 ( thresholds , train , valid , xval ) for model in self . models }",Get the F1 values for a set of thresholds for the models explored .
"def confusion_matrix ( self , metrics = None , thresholds = None , train = False , valid = False , xval = False ) : return { model . model_id : model . confusion_matrix ( metrics , thresholds , train , valid , xval ) for model in self . models }",Get the confusion matrix for the specified metrics / thresholds .
"def find_idx_by_threshold ( self , threshold , train = False , valid = False , xval = False ) : return { model . model_id : model . find_idx_by_threshold ( threshold , train , valid , xval ) for model in self . models }",Retrieve the index in this metric s threshold list at which the given threshold is located .
"def confusion_matrix ( self , data ) : return { model . model_id : model . confusion_matrix ( data ) for model in self . models }",Returns a confusion matrix based of H2O s default prediction threshold for a dataset .
"def varimp ( self , use_pandas = False ) : model = self . _model_json [ ""output"" ] if ""importance"" in list ( model . keys ( ) ) and model [ ""importance"" ] : vals = model [ ""importance"" ] . cell_values header = model [ ""importance"" ] . col_header if use_pandas and can_use_pandas ( ) : import pandas return pandas . DataFrame ( vals , columns = header ) else : return vals else : print ( ""Warning: This model doesn't have importances of components."" )",Return the Importance of components associcated with a pca model .
"def archetypes ( self ) : o = self . _model_json [ ""output"" ] yvals = o [ ""archetypes"" ] . cell_values archetypes = [ ] for yidx , yval in enumerate ( yvals ) : archetypes . append ( list ( yvals [ yidx ] ) [ 1 : ] ) return archetypes",The archetypes ( Y ) of the GLRM model .
"def proj_archetypes ( self , test_data , reverse_transform = False ) : if test_data is None or test_data . nrow == 0 : raise ValueError ( ""Must specify test data"" ) j = h2o . api ( ""POST /3/Predictions/models/%s/frames/%s"" % ( self . model_id , test_data . frame_id ) , data = { ""project_archetypes"" : True , ""reverse_transform"" : reverse_transform } ) return h2o . get_frame ( j [ ""model_metrics"" ] [ 0 ] [ ""predictions"" ] [ ""frame_id"" ] [ ""name"" ] )",Convert archetypes of the model into original feature space .
"def screeplot ( self , type = ""barplot"" , * * kwargs ) : is_server = kwargs . pop ( ""server"" ) if kwargs : raise ValueError ( ""Unknown arguments %s to screeplot()"" % "", "" . join ( kwargs . keys ( ) ) ) try : import matplotlib if is_server : matplotlib . use ( 'Agg' , warn = False ) import matplotlib . pyplot as plt except ImportError : print ( ""matplotlib is required for this function!"" ) return variances = [ s ** 2 for s in self . _model_json [ 'output' ] [ 'importance' ] . cell_values [ 0 ] [ 1 : ] ] plt . xlabel ( 'Components' ) plt . ylabel ( 'Variances' ) plt . title ( 'Scree Plot' ) plt . xticks ( list ( range ( 1 , len ( variances ) + 1 ) ) ) if type == ""barplot"" : plt . bar ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances ) elif type == ""lines"" : plt . plot ( list ( range ( 1 , len ( variances ) + 1 ) ) , variances , 'b--' ) if not is_server : plt . show ( )",Produce the scree plot .
"def translate_name ( name ) : parts = name . split ( ""_"" ) i = 0 while parts [ i ] == """" : parts [ i ] = ""_"" i += 1 parts [ i ] = parts [ i ] . lower ( ) for j in range ( i + 1 , len ( parts ) ) : parts [ j ] = parts [ j ] . capitalize ( ) i = len ( parts ) - 1 while parts [ i ] == """" : parts [ i ] = ""_"" i -= 1 return """" . join ( parts )",Convert names with underscores into camelcase .
"def dedent ( ind , text ) : text2 = textwrap . dedent ( text ) if ind == 0 : return text2 indent_str = "" "" * ind return ""\n"" . join ( indent_str + line for line in text2 . split ( ""\n"" ) )",Dedent text to the specific indentation level .
"def generate_schema ( class_name , schema ) : superclass = schema [ ""superclass"" ] if superclass == ""Schema"" : superclass = ""Object"" has_map = False is_model_builder = False has_inherited = False for field in schema [ ""fields"" ] : if field [ ""name"" ] == ""__meta"" : continue if field [ ""is_inherited"" ] : has_inherited = True continue if field [ ""type"" ] . startswith ( ""Map"" ) : has_map = True if field [ ""name"" ] == ""can_build"" : is_model_builder = True fields = [ ] for field in schema [ ""fields"" ] : if field [ ""name"" ] == ""__meta"" : continue java_type = translate_type ( field [ ""type"" ] , field [ ""schema_name"" ] ) java_value = get_java_value ( field ) if False and is_model_builder and field [ ""name"" ] == ""parameters"" : fields . append ( ( ""parameters"" , ""null"" , ""ModelParameterSchemaV3[]"" , field [ ""help"" ] , field [ ""is_inherited"" ] ) ) else : fields . append ( ( field [ ""name"" ] , java_value , java_type , field [ ""help"" ] , field [ ""is_inherited"" ] ) ) class_decl = class_name if ""generics"" in schema : class_decl += ""<"" + "", "" . join ( ""%s extends %s"" % ( t , long_type ) for t , long_type in schema [ ""generics"" ] ) + "">"" super_decl = superclass if ""super_generics"" in schema : super_decl += ""<"" + "", "" . join ( schema [ ""super_generics"" ] ) + "">"" yield ""/*"" yield "" * This file is auto-generated by h2o-3/h2o-bindings/bin/gen_java.py"" yield "" * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)"" yield "" */"" yield ""package water.bindings.pojos;"" yield """" yield ""import com.google.gson.Gson;"" yield ""import com.google.gson.annotations.*;"" yield ""import java.util.Map;"" if has_map else None yield """" yield """" yield ""public class %s extends %s {"" % ( class_decl , super_decl ) if super_decl != ""Object"" else None yield ""public class %s {"" % ( class_decl ) if super_decl == ""Object"" else None yield """" for name , value , ftype , fhelp , inherited in fields : if inherited : continue ccname = translate_name ( name ) yield ""    /**"" yield bi . wrap ( fhelp , indent = ""     * "" ) yield ""     */"" yield ""    @SerializedName(\""%s\"")"" % name if name != ccname else None yield ""    public %s %s;"" % ( ftype , ccname ) yield """" if has_inherited : yield """" yield ""    /*"" + ( ""-"" * 114 ) yield ""    //"" + ( "" "" * 50 ) + ""INHERITED"" yield ""    //"" + ( ""-"" * 114 ) yield """" for name , value , ftype , fhelp , inherited in fields : if not inherited : continue yield bi . wrap ( fhelp , ""    // "" ) yield ""    public %s %s;"" % ( ftype , translate_name ( name ) ) yield """" yield ""    */"" yield """" yield ""    /**"" yield ""     * Public constructor"" yield ""     */"" yield ""    public %s() {"" % class_name for name , value , _ , _ , _ in fields : if name == ""parameters"" : continue if value == ""null"" : continue yield ""        %s = %s;"" % ( translate_name ( name ) , value ) yield ""    }"" yield """" yield ""    /**"" yield ""     * Return the contents of this object as a JSON String."" yield ""     */"" yield ""    @Override"" yield ""    public String toString() {"" yield ""        return new Gson().toJson(this);"" yield ""    }"" yield """" yield ""}""",Generate schema Java class .
"def generate_proxy ( classname , endpoints ) : var_pattern = re . compile ( r""\{(\w+)\}"" ) helper_class = [ ] found_key_array_parameter = False yield ""/*"" yield "" * This file is auto-generated by h2o-3/h2o-bindings/bin/gen_java.py"" yield "" * Copyright 2016 H2O.ai;  Apache License Version 2.0 (see LICENSE for details)"" yield "" */"" yield ""package water.bindings.proxies.retrofit;"" yield """" yield ""import water.bindings.pojos.*;"" yield ""import retrofit2.*;"" yield ""import retrofit2.http.*;"" yield ""import java.util.Map;"" if classname == ""Grid"" or classname == ""ModelBuilders"" else None yield """" yield ""public interface "" + classname + "" {"" yield """" for e in endpoints : method = e [ ""handler_method"" ] if method == ""exec"" : method = e [ ""api_name"" ] param_strs = [ ] required_param_strs = [ ] for field in e [ ""input_params"" ] : fname = field [ ""name"" ] ftype = ""Path"" if field [ ""is_path_param"" ] else ""Field"" ptype = translate_type ( field [ ""type"" ] , field [ ""schema_name"" ] ) if ptype . endswith ( ""KeyV3"" ) or ptype == ""ColSpecifierV3"" : ptype = ""String"" if ptype . endswith ( ""KeyV3[]"" ) : ptype = ""String[]"" param_str = ""@{ftype}(\""{fname}\"") {ptype} {fname}"" . format ( * * locals ( ) ) param_strs . append ( param_str ) if field [ ""required"" ] : required_param_strs . append ( param_str ) if len ( param_strs ) == len ( required_param_strs ) : required_param_strs = None yield u""  /** "" yield bi . wrap ( e [ ""summary"" ] , indent = ""   * "" ) for field in e [ ""input_params"" ] : s = ""   *   @param %s "" % field [ ""name"" ] yield s + bi . wrap ( field [ ""help"" ] , indent = ""   *"" + "" "" * ( len ( s ) - 4 ) , indent_first = False ) yield u""   */"" for params in [ param_strs , required_param_strs ] : if params is None : continue yield u""  @FormUrlEncoded"" if e [ ""http_method"" ] == ""POST"" else None yield u""  @{method}(\""{path}\"")"" . format ( method = e [ ""http_method"" ] , path = e [ ""url_pattern"" ] ) if len ( params ) <= 1 : args = params [ 0 ] if params else """" yield ""  Call<{schema}> {method}({args});"" . format ( schema = e [ ""output_schema"" ] , method = method , args = args ) else : yield ""  Call<{schema}> {method}("" . format ( schema = e [ ""output_schema"" ] , method = method ) for arg in params : yield ""    "" + arg + ( """" if arg == params [ - 1 ] else "","" ) yield ""  );"" yield """" if ""algo"" in e : helper_class . append ( ""    /**"" ) helper_class . append ( bi . wrap ( e [ ""summary"" ] , indent = ""     * "" ) ) helper_class . append ( ""     */"" ) helper_class . append ( ""    public static Call<{oschema}> {method}({outer_class} z, {ischema} p) {{"" . format ( ischema = e [ ""input_schema"" ] , oschema = e [ ""output_schema"" ] , method = method , outer_class = classname ) ) helper_class . append ( ""      return z.{method}("" . format ( method = method ) ) for field in e [ ""input_params"" ] : ptype = translate_type ( field [ ""type"" ] , field [ ""schema_name"" ] ) pname = translate_name ( field [ ""name"" ] ) if ptype . endswith ( ""KeyV3"" ) : s = ""(p.{parm} == null? null : p.{parm}.name)"" . format ( parm = pname ) elif ptype . endswith ( ""KeyV3[]"" ) : found_key_array_parameter = True s = ""(p.{parm} == null? null : keyArrayToStringArray(p.{parm}))"" . format ( parm = pname ) elif ptype . startswith ( ""ColSpecifier"" ) : s = ""(p.{parm} == null? null : p.{parm}.columnName)"" . format ( parm = pname ) else : s = ""p."" + pname if field != e [ ""input_params"" ] [ - 1 ] : s += "","" helper_class . append ( ""        "" + s ) helper_class . append ( ""      );"" ) helper_class . append ( ""    }"" ) helper_class . append ( """" ) if helper_class : yield """" yield ""  @SuppressWarnings(\""unused\"")"" yield ""  class Helper {"" for line in helper_class : yield line if found_key_array_parameter : yield ""    /**"" yield ""     * Return an array of Strings for an array of keys."" yield ""     */"" yield ""    public static String[] keyArrayToStringArray(KeyV3[] keys) {"" yield ""      if (keys == null) return null;"" yield ""      String[] ids = new String[keys.length];"" yield ""      int i = 0;"" yield ""      for (KeyV3 key : keys) ids[i++] = key.name;"" yield ""      return ids;"" yield ""    }"" yield ""  }"" yield """" yield ""}""",Generate a Retrofit Proxy class .
"def main ( argv ) : global g_script_name global g_parse_log_path g_script_name = os . path . basename ( argv [ 0 ] ) parse_args ( argv ) if ( g_parse_log_path is None ) : print ( """" ) print ( ""ERROR: -f not specified"" ) usage ( ) d = Dataset ( g_parse_log_path ) d . parse ( ) d . emit_header ( ) for i in range ( 0 , g_num_rows ) : d . emit_one_row ( )",Main program .
"def parse ( self ) : f = open ( self . parse_log_path , ""r"" ) self . parse2 ( f ) f . close ( )",Parse file specified by constructor .
"def parse2 ( self , f ) : line_num = 0 s = f . readline ( ) while ( len ( s ) > 0 ) : line_num += 1 match_groups = re . search ( r""Parse result for (.*) .(\d+) rows.:"" , s ) if ( match_groups is not None ) : dataset_name = match_groups . group ( 1 ) if ( self . dataset_name is not None ) : print ( ""ERROR: Too many datasets found on file {} line {}"" . format ( self . parse_log_path , line_num ) ) sys . exit ( 1 ) self . dataset_name = dataset_name num_rows = int ( match_groups . group ( 2 ) ) self . num_rows = num_rows s = f . readline ( ) continue match_groups = re . search ( r""INFO WATER:"" + r""\s*C(\d+):"" + r""\s*numeric"" + r""\s*min\((\S*)\)"" + r""\s*max\((\S*).\)"" + r""\s*(na\((\S+)\))?"" + r""\s*(constant)?"" , s ) if ( match_groups is not None ) : col_num = int ( match_groups . group ( 1 ) ) min_val = float ( match_groups . group ( 2 ) ) max_val = float ( match_groups . group ( 3 ) ) na_count = match_groups . group ( 5 ) if ( na_count is None ) : na_count = 0 else : na_count = int ( na_count ) constant_str = match_groups . group ( 6 ) is_constant = constant_str is not None if ( is_constant ) : if ( min_val != max_val ) : print ( ""ERROR: is_constant mismatch on file {} line {}"" . format ( self . parse_log_path , line_num ) ) sys . exit ( 1 ) na_fraction = float ( na_count ) / float ( self . num_rows ) is_min_integer = float ( int ( min_val ) ) == float ( min_val ) is_max_integer = float ( int ( min_val ) ) == float ( min_val ) is_integer = is_min_integer and is_max_integer c = RealColumn ( col_num , ""C"" + str ( col_num ) , min_val , max_val , na_fraction , is_integer ) self . add_col ( c ) s = f . readline ( ) continue match_groups = re . search ( r""INFO WATER:"" + r""\s*C(\d+):"" + r""\s*categorical"" + r""\s*min\((\S*)\)"" + r""\s*max\((\S*).\)"" + r""\s*(na\((\S+)\))?"" + r""\s*(constant)?"" + r""\s*cardinality\((\d+)\)"" , s ) if ( match_groups is not None ) : col_num = int ( match_groups . group ( 1 ) ) min_val = float ( match_groups . group ( 2 ) ) max_val = float ( match_groups . group ( 3 ) ) na_count = match_groups . group ( 5 ) if ( na_count is None ) : na_count = 0 else : na_count = int ( na_count ) constant_str = match_groups . group ( 6 ) is_constant = constant_str is not None if ( is_constant ) : if ( min_val != max_val ) : print ( ""ERROR: is_constant mismatch on file {} line {}"" . format ( self . parse_log_path , line_num ) ) sys . exit ( 1 ) num_levels = int ( match_groups . group ( 7 ) ) if ( is_constant ) : if ( num_levels != 1 ) : print ( ""ERROR: num_levels mismatch on file {} line {}"" . format ( self . parse_log_path , line_num ) ) sys . exit ( 1 ) na_fraction = float ( na_count ) / float ( self . num_rows ) c = CategoricalColumn ( col_num , ""C"" + str ( col_num ) , num_levels , na_fraction ) self . add_col ( c ) s = f . readline ( ) continue print ( ""ERROR: Unrecognized regexp pattern on file {} line {}"" . format ( self . parse_log_path , line_num ) ) sys . exit ( 1 )",Parse file specified by constructor .
"def anomaly ( self , test_data , per_feature = False ) : if test_data is None or test_data . nrow == 0 : raise ValueError ( ""Must specify test data"" ) j = h2o . api ( ""POST /3/Predictions/models/%s/frames/%s"" % ( self . model_id , test_data . frame_id ) , data = { ""reconstruction_error"" : True , ""reconstruction_error_per_feature"" : per_feature } ) return h2o . get_frame ( j [ ""model_metrics"" ] [ 0 ] [ ""predictions"" ] [ ""frame_id"" ] [ ""name"" ] )",Obtain the reconstruction error for the input test_data .
